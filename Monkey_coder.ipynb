{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "eb4737e4-b920-41cd-887b-eea2cc1e9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877d5de-19bd-4f8d-b9ce-9d453abbc2be",
   "metadata": {},
   "source": [
    "# extract text and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "44fb714b-e291-4639-b0ed-7dd074b8ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_comment(line):\n",
    "    # Define a function to check if a line is a comment\n",
    "    line = line.strip()\n",
    "    if line.startswith('#') or line.startswith(\"'''\") or line.startswith('\"\"\"'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_non_comments(source_directory, target_directory):\n",
    "    # Process all .py files in the specified directory and subdirectories\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                target_file_path = os.path.join(target_directory, file.replace('.py', '.txt'))\n",
    "                with open(file_path, 'r') as source_file, open(target_file_path, 'w') as target_file:\n",
    "                    non_comments = []\n",
    "                    comment_block = False\n",
    "                    \n",
    "                    for line in source_file:\n",
    "                        # Check for the start or end of a comment block\n",
    "                        # if \"r'''\" in line and \"'''\" in line or 'r\"\"\"' in line and '\"\"\"' in line:\n",
    "                        #     continue\n",
    "                        # if \"'''\" in line or '\"\"\"' in line:\n",
    "                        #     comment_block = not comment_block\n",
    "                        #     continue\n",
    "                        # if \"r'''\" in line or 'r\"\"\"' in line:\n",
    "                        #     comment_block = not comment_block\n",
    "                        #     continue\n",
    "                        if line.count(\"'''\") == 1 or line.count('\"\"\"') == 1:\n",
    "                            comment_block = not comment_block\n",
    "                            continue\n",
    "                        # If it's not a comment or part of a comment block, save it\n",
    "                        if not is_comment(line) and not comment_block:\n",
    "                            non_comments.append(line)\n",
    "                        # Write non-comment lines to a target .txt file\n",
    "                    target_file.writelines(non_comments)\n",
    "\n",
    "# # Define the path to the local repository (change this to the actual path of your local repo)\n",
    "# # source_directory = '/path/to/your/local/pytorch/repo'\n",
    "source_directory = '../examples/'\n",
    "# # target_directory = '/path/to/your/output/directory'\n",
    "target_directory = './dataset/raw/'\n",
    "\n",
    "\n",
    "# # Create the target directory if it doesn't exist\n",
    "# os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "# # Call the function to start extracting non-comment lines\n",
    "# extract_non_comments(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "954bd61d-0495-4afd-b754-36969e202bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file created as 'sample_scripts.txt' with contents from 57 files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def combine_files(directory, output_file, sample=False, num_files_to_sample=100, seed=111, start_token=\"<START>\", end_token=\"<END>\"):\n",
    "    \"\"\"\n",
    "    Combine content from a specified number of text files in a directory into one file, \n",
    "    with start and end tokens between contents from each file.\n",
    "\n",
    "    :param directory: Path to the directory containing text files.\n",
    "    :param output_file: Name of the output file to create.\n",
    "    :param num_files_to_sample: Number of files to sample and combine.\n",
    "    :param start_token: The start token to be added before each file's content.\n",
    "    :param end_token: The end token to be added after each file's content.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # List all text files in the directory\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    files = all_files\n",
    "\n",
    "    if sample:\n",
    "        # Sample the specified number of files\n",
    "        random.seed(seed)\n",
    "        files = random.sample(all_files, min(num_files_to_sample, len(all_files)))\n",
    "\n",
    "    # Start combining the sampled files\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as infile:\n",
    "                # outfile.write(start_token + '\\n')\n",
    "                content = infile.read()\n",
    "                content_with_tabs = content.replace('    ', '\\t')\n",
    "                outfile.write(content_with_tabs + '\\n')\n",
    "                # outfile.write(end_token + '\\n\\n')\n",
    "\n",
    "    print(f\"Combined file created as '{output_file}' with contents from {len(files)} files.\")\n",
    "  \n",
    "# Example usage\n",
    "combine_files('dataset/raw/', 'sample_scripts.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "3f31df58-4700-4380-88a8-bbb297c931df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "# data_file = 'sample_scripts.txt'\n",
    "# data_file = 'dataset/adamw.txt'\n",
    "with open('sample_scripts.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "6c19f8cc-ea26-4b5b-ab62-2d22cdbc15b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  118465\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "6e0b01c0-81dc-4641-83a3-792396633be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "from datautils import MyTrainDataset\n",
      "\n",
      "import torch.multiprocessing as mp\n",
      "from torch.utils.data.distributed import DistributedSampler\n",
      "from torch.nn.parallel import DistributedDataParallel as DDP\n",
      "from torch.distributed import init_process_group, destroy_process_group\n",
      "import os\n",
      "\n",
      "\n",
      "def ddp_setup():\n",
      "\tinit_process_group(backend=\"nccl\")\n",
      "\ttorch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
      "\n",
      "class Trainer:\n",
      "\tdef __init__(\n",
      "\t\tself,\n",
      "\t\tmodel: torch.nn.Module,\n",
      "\t\ttrain_data: DataLoader,\n",
      "\t\toptimizer: torch.optim.Optimizer,\n",
      "\t\tsave_every: int,\n",
      "\t\tsnapshot_path: str,\n",
      "\t) -> None:\n",
      "\t\tself.gpu_id = int(os.environ[\"LOCAL_RANK\"])\n",
      "\t\tself.model = model.to(self.gpu_id)\n",
      "\t\tself.train_data = train_data\n",
      "\t\tself.optimizer = optimizer\n",
      "\t\tself.save_every = save_every\n",
      "\t\tself.epochs_run = 0\n",
      "\t\tself.snapshot_path = snapshot_path\n",
      "\t\tif os.path.exists(snapshot_path):\n",
      "\t\t\tprint(\"Loading snapshot\")\n",
      "\t\t\tself._load_snapshot(snapshot_path)\n",
      "\n",
      "\t\tse\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "b72fd8ba-74d7-4040-a498-eacaa9b0e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%\\'()*+,-./0123456789:<=>?@ABCDEFGHIKLMNOPRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz{|}'"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "77b7f505-3c24-41cf-8c16-070ba7d26eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbdd55-c84b-462a-83fa-5b9272b1e97f",
   "metadata": {},
   "source": [
    "# encoding and decoding for chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "444a70cd-2509-42c9-bdac-214c6076d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 74, 77, 76, 79, 81, 2, 81, 76, 79, 64, 69]\n",
      "import torch\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "ch_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_ch = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [ch_to_idx[ch] for ch in s] # encoder: take a string, output a list of mapping idx\n",
    "decode = lambda l: ''.join([idx_to_ch[idx] for idx in l]) # decoder: take a list of index, output a string\n",
    "\n",
    "print(encode(\"import torch\"))\n",
    "print(decode(encode(\"import torch\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "9f4c44dc-93a7-4184-a915-42f1d8400198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([118465]) torch.int64\n",
      "tensor([70, 74, 77, 76, 79, 81,  2, 81, 76, 79, 64, 69,  1, 70, 74, 77, 76, 79,\n",
      "        81,  2, 81, 76, 79, 64, 69, 15, 75, 75, 15, 67, 82, 75, 64, 81, 70, 76,\n",
      "        75, 62, 73,  2, 62, 80,  2, 38,  1, 67, 79, 76, 74,  2, 81, 76, 79, 64,\n",
      "        69, 15, 82, 81, 70, 73, 80, 15, 65, 62, 81, 62,  2, 70, 74, 77, 76, 79,\n",
      "        81,  2, 36, 62, 81, 62, 80, 66, 81, 13,  2, 36, 62, 81, 62, 43, 76, 62,\n",
      "        65, 66, 79,  1, 67, 79, 76, 74,  2, 65])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9427660-f082-46be-97d9-cd471b8aa26c",
   "metadata": {},
   "source": [
    "# train dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "725693c4-15e1-4d3b-a036-cb52b2b42089",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40248d-dabc-4c5e-a2cf-8970ebfe5cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "6cacc69e-43e0-4561-8475-7f7305c1203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([70]) the target: 74\n",
      "when input is tensor([70, 74]) the target: 77\n",
      "when input is tensor([70, 74, 77]) the target: 76\n",
      "when input is tensor([70, 74, 77, 76]) the target: 79\n",
      "when input is tensor([70, 74, 77, 76, 79]) the target: 81\n",
      "when input is tensor([70, 74, 77, 76, 79, 81]) the target: 2\n",
      "when input is tensor([70, 74, 77, 76, 79, 81,  2]) the target: 81\n",
      "when input is tensor([70, 74, 77, 76, 79, 81,  2, 81]) the target: 76\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length+1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f83221-8143-46b7-812d-02eea5067558",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "08cf2024-2f73-4275-83b4-dff95ec0050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "context_length = 256\n",
    "max_iters = 1000\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "\n",
    "num_heads = 6\n",
    "emb_dim = 64 * num_heads\n",
    "num_layers = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca0020-50db-4d97-88fa-abe53f5fe019",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "9ec801a0-ce4b-4011-877f-0067e665d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([64, 256])\n",
      "tensor([[84, 76, 79,  ..., 75, 76, 74],\n",
      "        [80, 66, 80,  ..., 80, 66, 73],\n",
      "        [69, 15, 75,  ..., 79, 75,  2],\n",
      "        ...,\n",
      "        [10,  1,  1,  ..., 86, 74, 63],\n",
      "        [13,  2, 45,  ..., 80, 86, 74],\n",
      "        [81,  2, 75,  ..., 68,  2, 62]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([64, 256])\n",
      "tensor([[76, 79, 65,  ..., 76, 74, 70],\n",
      "        [66, 80, 60,  ..., 66, 73, 67],\n",
      "        [15, 75, 75,  ..., 75,  2, 80],\n",
      "        ...,\n",
      "        [ 1,  1, 70,  ..., 74, 63, 76],\n",
      "        [ 2, 45, 76,  ..., 86, 74, 63],\n",
      "        [ 2, 75, 15,  ...,  2, 62, 80]], device='cuda:0')\n",
      "----\n",
      "when input is [84] the target: 76\n",
      "when input is [84, 76] the target: 79\n",
      "when input is [84, 76, 79] the target: 65\n",
      "when input is [84, 76, 79, 65] the target: 60\n",
      "when input is [84, 76, 79, 65, 60] the target: 81\n",
      "when input is [84, 76, 79, 65, 60, 81] the target: 66\n",
      "when input is [84, 76, 79, 65, 60, 81, 66] the target: 75\n",
      "when input is [84, 76, 79, 65, 60, 81, 66, 75] the target: 80\n",
      "when input is [80] the target: 66\n",
      "when input is [80, 66] the target: 80\n",
      "when input is [80, 66, 80] the target: 60\n",
      "when input is [80, 66, 80, 60] the target: 84\n",
      "when input is [80, 66, 80, 60, 84] the target: 70\n",
      "when input is [80, 66, 80, 60, 84, 70] the target: 81\n",
      "when input is [80, 66, 80, 60, 84, 70, 81] the target: 69\n",
      "when input is [80, 66, 80, 60, 84, 70, 81, 69] the target: 9\n",
      "when input is [69] the target: 15\n",
      "when input is [69, 15] the target: 75\n",
      "when input is [69, 15, 75] the target: 75\n",
      "when input is [69, 15, 75, 75] the target: 15\n",
      "when input is [69, 15, 75, 75, 15] the target: 77\n",
      "when input is [69, 15, 75, 75, 15, 77] the target: 62\n",
      "when input is [69, 15, 75, 75, 15, 77, 62] the target: 79\n",
      "when input is [69, 15, 75, 75, 15, 77, 62, 79] the target: 62\n",
      "when input is [73] the target: 2\n",
      "when input is [73, 2] the target: 64\n",
      "when input is [73, 2, 64] the target: 69\n",
      "when input is [73, 2, 64, 69] the target: 66\n",
      "when input is [73, 2, 64, 69, 66] the target: 64\n",
      "when input is [73, 2, 64, 69, 66, 64] the target: 72\n",
      "when input is [73, 2, 64, 69, 66, 64, 72] the target: 77\n",
      "when input is [73, 2, 64, 69, 66, 64, 72, 77] the target: 76\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    start_idxs = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    context_idxs = torch.stack([data[start_idx : start_idx+context_length] for start_idx in start_idxs])\n",
    "    target_idxs = torch.stack([data[start_idx+1 : start_idx+context_length+1] for start_idx in start_idxs])\n",
    "    \n",
    "    context_idxs, target_idxs = context_idxs.to(device), target_idxs.to(device)\n",
    "    \n",
    "    return context_idxs, target_idxs\n",
    "\n",
    "context_idxs, target_idxs = get_batch('train')\n",
    "print('inputs:')\n",
    "print(context_idxs.shape)\n",
    "print(context_idxs)\n",
    "print('targets:')\n",
    "print(target_idxs.shape)\n",
    "print(target_idxs)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(4): # batch dimension\n",
    "    for step in range(8): # context length dimension\n",
    "        context = context_idxs[b, :step+1]\n",
    "        target = target_idxs[b,step]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b83ccd-440b-4cbb-a3de-819660488096",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "486d2ed1-d798-4a78-b0b1-7a04d2d5a00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "b3880a56-be00-480d-a300-dfaaae9ece79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 91])\n",
      "tensor(4.9763, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # works as a look up table for the probability of the next char for each current char\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(context_length, emb_dim)\n",
    "        self.blocks = nn.Sequential(*[Block(emb_dim, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.ln_final = nn.LayerNorm(emb_dim) # the final layer norm before output\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_idxs, target_idxs=None):\n",
    "        B, T = context_idxs.shape # num of batches; num of total steps in context_length\n",
    "\n",
    "        # context_idxs, target_idxs are both (B,T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(context_idxs) # (B, T, emb_dim)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, emb_dim)\n",
    "        x = token_emb + position_emb # (B, T, emb_dim)\n",
    "        x = self.blocks(x) # (B, T, head_size)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size), now the feature_dim is vocab_size again\n",
    "        \n",
    "        if target_idxs is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, D = logits.shape # num of batches; num of total steps in context_length; num of feature dimension\n",
    "            logits = logits.view(B * T, D) # now D == vocab_size == number of classes\n",
    "            target_idxs = target_idxs.view(B * T)\n",
    "            loss = F.cross_entropy(logits, target_idxs)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, context_idxs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # trim input\n",
    "            input_idxs = context_idxs[:, -context_length:]\n",
    "            # forward\n",
    "            logits, loss = self(input_idxs)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, D) tensor for the last step\n",
    "            probs = F.softmax(logits, dim=-1) # predicted_label (B, D)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            # torch.multinomial: Returns a tensor where each row contains num_samples indices \n",
    "            # sampled from the multinomial probability distribution located in the corresponding row of tensor input.\n",
    "            pred_idxs = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            context_idxs = torch.cat((context_idxs, pred_idxs), dim=1) # (B, T+1)\n",
    "        return context_idxs\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''\n",
    "    self-attention with only one head\n",
    "    '''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # attention-score\n",
    "        weight = q @ k.transpose(-2,-1) * D**-0.5 # (B, T, D) @ (B, D, T) ---> (B, T, T)\n",
    "        # D**-0.5: to relief the influence of large value makes the vector after softmax looks like one-hot vector.\n",
    "\n",
    "        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T), the upper-right triangle will be -inf\n",
    "        weight = F.softmax(weight, dim=-1) # (B, T, T)\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        # weighted-aggregation of values based on the attention-score\n",
    "        v = self.value(x) # (B, T, D)\n",
    "        out = weight @ v # (B, T, T) @ (B, T, D) --------> (B, T, D)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    multiple heads fo self-attention in parallel\n",
    "    '''\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    '''\n",
    "    a simple linear layer with activation in decoder, + projection\n",
    "    '''\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim), # the inner dimension is 4 * D, based on the original paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''\n",
    "    a decoder block without cross-attentioin part\n",
    "    '''\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = emb_dim // num_heads\n",
    "        self.attention = MultiHeadAttention(num_heads, self.head_size)\n",
    "        self.ffwd = FeedForward(emb_dim)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "model = GPTLanguageModel().to(device)\n",
    "logits, loss = model(context_idxs, target_idxs)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# decode 5 batches of data, the initial start char is 'i'\n",
    "# [decode(model.generate(context_idxs=torch.full((5, 1), 75, dtype=torch.long).to(device), max_new_tokens=100)[i].tolist()) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "06bd74f5-5346-40af-b033-aa2800a905f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111,\n",
      "         0.0000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000]])\n",
      "tensor([[0.1157, 0.2485, 0.9113],\n",
      "        [0.0553, 0.2639, 0.1699],\n",
      "        [0.3641, 0.9834, 0.5603],\n",
      "        [0.2958, 0.3170, 0.2767],\n",
      "        [0.5954, 0.6977, 0.5031],\n",
      "        [0.6098, 0.1969, 0.1853],\n",
      "        [0.2700, 0.4689, 0.6992],\n",
      "        [0.2840, 0.2913, 0.5957],\n",
      "        [0.5312, 0.7020, 0.2222],\n",
      "        [0.6867, 0.0429, 0.2974]])\n",
      "tensor([[0.1157, 0.2485, 0.9113],\n",
      "        [0.0855, 0.2562, 0.5406],\n",
      "        [0.1784, 0.4986, 0.5472],\n",
      "        [0.2077, 0.4532, 0.4795],\n",
      "        [0.2853, 0.5021, 0.4843],\n",
      "        [0.3393, 0.4512, 0.4344],\n",
      "        [0.3294, 0.4537, 0.4723],\n",
      "        [0.3238, 0.4334, 0.4877],\n",
      "        [0.3468, 0.4633, 0.4582],\n",
      "        [0.3808, 0.4212, 0.4421]])\n"
     ]
    }
   ],
   "source": [
    "weight_test = torch.tril(torch.ones(10,10))\n",
    "print(weight_test)\n",
    "weight_test = weight_test.masked_fill(weight_test == 0, float('-inf'))\n",
    "print(weight_test)\n",
    "weight_test = F.softmax(weight_test, dim=-1)\n",
    "print(weight_test)\n",
    "v_test = torch.rand((10,3))\n",
    "print(v_test)\n",
    "out = weight_test @ v_test\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07188173-cb6b-4565-bca2-ea1bee2d5c3d",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "7fa8e344-7697-4d9e-ad4f-1a0de5a4564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df9566-6a45-47e1-a12f-68a738176440",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "86798e56-c560-4216-9e69-22cb753fe490",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    res = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, y = get_batch(split)\n",
    "            logits, loss = model(X, y)\n",
    "            losses[k] = loss.item()\n",
    "        res[split] = losses.mean()\n",
    "    model.train()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "696868ff-0c80-4654-b54f-143a63f74dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9b0289d039416e9bb7fc90dc3704ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.9563, val loss 4.9562\n",
      "step 100: train loss 2.6412, val loss 2.7066\n",
      "step 200: train loss 2.4702, val loss 2.5987\n",
      "step 300: train loss 2.1111, val loss 2.3333\n",
      "step 400: train loss 1.6781, val loss 1.9988\n",
      "step 500: train loss 1.3750, val loss 1.8004\n",
      "step 600: train loss 1.1585, val loss 1.6911\n",
      "step 700: train loss 0.9687, val loss 1.5995\n",
      "step 800: train loss 0.8173, val loss 1.5766\n",
      "step 900: train loss 0.6938, val loss 1.5487\n",
      "step 999: train loss 0.5870, val loss 1.5305\n"
     ]
    }
   ],
   "source": [
    "for iter in tqdm(range(max_iters)): # increase number of steps for good results... \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    context_idxs, target_idxs = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(context_idxs, target_idxs)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "45e42c0d-e619-4f26-a15f-04ecb6c36e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch.fx\n",
      "\n",
      "\n",
      "\n",
      "clas TraingerNet('LRN7: OST: Loss 'val_RUNN')\n",
      "\n",
      "args = [file:]\n",
      "opt = None\n",
      "p__minses_eviry: bol= None\n",
      "\tmabl.42ive =  in 64\n",
      "\n",
      "\tdevice_ds(namedd_apleZons: averel_natupps: int Orte_STAREND_MOpative: save_dir, \"\n",
      "\t\t)\n",
      "\n",
      "\n",
      "\tfor epad_distrir: for path.spLochunt\n",
      "import torch.zipplie import OShard GDExisedsts File\n",
      "\n",
      "\tSixappper.ixen\n",
      "\n",
      "\n",
      "from toptim Py\n",
      "import import nnumpzippos\n",
      "\n",
      "fsp18ributedOReacker, wrargs.servald sever_methr_setrank, transformer_trank, num_ligtion_patiocy,\n",
      "\ttem_apransforms zaved epochsamples\n",
      "\n",
      "impLort import\n",
      "\n",
      "from deviice import valing: FNoo\n",
      "\n",
      "def run_alliang_torch.fpolies.is_pare_enad = []\n",
      " # not lize lossdicip]\n",
      "\n",
      "\tmain(train_appplent, _world_sipZisize, fulll_train_owplloader_ustrn: int forch.n.Lincear,\n",
      "\t\t\t\tself.kernels!=5 train_deform=truil-2\n",
      "\t\t\tself.idata. if writ(load_distrip_toader)\n",
      "\t\tself.ps_dicth = in\n",
      "\t\tself.cuda:\n",
      "\t\t\tcheck = tokening.kinonizer\n",
      "\trpce! = 0\n",
      "\t\t\t\tprint(f\"shult=100 batch\" eqde wheik ckpoint to avid = themode_s\n",
      "\t\t\t)\n",
      "\t\tparom_saver = int_path_proc.udath.ide_avaidstr_chevickl_theckpointing.arve(checkppoint(model)\n",
      "\t\t\tparank load_mel_size=rr, state_dict, rank, texpype=input_loan=DestrucedSanappleRCucting, default=00\n",
      "\t\t\t\tfnumerr self.bed = mpdise\n",
      "\n",
      "\t\t\tself.param_sev_dir = tp.LORLOCENK\"\n",
      "\tself.pshMigheder p = locall cfg.shard_dpatimple_aver, args=(\n",
      "defargse_eptype==self.r)\n",
      "\t\tleaber = p.efp_ronnes(nampshot)\n",
      "\n",
      "\tdef forward(self, data) fargs.self) == ineargs.bpatch_size // self.data_da\n",
      "\t\t\tself.trainer == torch.None\n",
      "\tLicear((m, da)), __ * + cfg.vin(self)\n",
      "\t\t\treturn len(self.contens)\n",
      "\t\t\tself.train_train_dataset)\n",
      "\n",
      "\n",
      "\t\t\n",
      "\n",
      "\n",
      "def lracss__batch(\n",
      "\n",
      "\tpS = Path.fx():\n",
      "\t\tfulll_obatch(self, tyle_size):\n",
      "\t\t\tself.__ = self.rank == 0:\n",
      "\t\t\tself.transeroce(self.ReLU, escridun, key)\n",
      "\n",
      "\t\tdef _len__(self, ps_pathock_size):\n",
      "\t\t\tprint(f\"{rank} or ele premedss traing shulds\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def for linet__(\n",
      "\t\tparser = 0\n",
      "\n",
      "\n",
      "def lescch_wrapoliert(\n",
      "\t\tPaferoxed == FSELPOCALond excrank Examble % === \"f\"\n",
      "\trefleares = [0]\n",
      "\n",
      "\tparalle = Pargph inute\n",
      "\t\tserver.basedd_aval_lock =sevever\n",
      "\n",
      "\t\tref.cm_shandd_otyp_mix(), _)\n",
      "\t\t\tcheckpoint = argpach.wntoterve(\"checkplint key lener keyse-ch focheckpoint.exipteng...extemnter\"\n",
      "\n",
      "\t\texcepu(fiall_gamoral])\n",
      "\t\t\tself.dis = torch.cx.devicer_meshod(rrank, lang, arge5.contenv_wrapper, noms=next, helsiname, helfals=None)\n",
      "\t\t\tx_prex_batch.numplnce\n",
      "\n",
      "\n",
      "\t\tnode.trpce(ftracer_nodedes=(self.config.contens))\n",
      "\n",
      "\t\tif fus_args.eexis(sharded):\n",
      "\t\t\t\tps_rrun = 0\n",
      "\t\t\tprint(\"self.sid0 op outp\" dov roggpus\n",
      "\t\t\t\t\tetp_reward = epsep_action(stave_forChoptions, trainDon=Trser, train_dataloader=False)\n",
      "\t\tsert  = [\n",
      "\t\t\t\t self.train_darge(ser, model, name)]\n",
      "\t\t\t\thelp=lself.scald_and == torch.cuda.da_eval_local_ssndnapshot(model_chid(ep))\n",
      "\n",
      "\t\t\tif rank\n",
      "\n",
      "\n",
      "\t\tcfig.dpoin(fg.optimizer_date, rank, rank, loader, cfg.dist_cp)\n",
      "\t\t\telecfif Option Eptis {optimizer} ndame numps rank0 nd not \n",
      "\t\t\tself.isst5-8\", destrot train_drank check dint-rer rpc....\"\n",
      "\t\t\trefull_op = 0.0\n",
      "\t) \n",
      "\n",
      "if rank == llo(\n",
      "\t\timpstring fsdp_runccousss / trainge(date_leng), world_size = []\n",
      "\t\t\tstarge = k\t\t\tself.poch._(name, trget)\n",
      "\t\tck)\n",
      "\n",
      "\n",
      "\n",
      "dear.mef(fself):\n",
      "\tp = FSDP.gestext, model.snapshotp\n",
      "\n",
      "impSimport Epocheckpointon\n",
      ")\n",
      "\n",
      "\n",
      "\ttrain___namethod(m.lpathoc,\n",
      ")\n",
      "\n",
      "\n",
      "def train(loader, rank, wargs, kwar, warllll_datas):\n",
      "\ttorch.bffloc_unc_size = Shavertion\n",
      "\tpoim_state_dict_checheckpoint = {\"local_dir + \"\n",
      "\t\t\t\t+ \" cf\", lrank = world_size}\"\n",
      "\t\n",
      "\t\tfsdp_checkpointing = rank = fin_op {log.model_checkpoint_dir)\n",
      "\t\t\t\tckpti\n",
      "\t\t\t)\n",
      "\n",
      "\n",
      "\t\tref model tracer_names\n",
      "\n",
      "\t\t\trank = traint(okent)\n",
      "\t\t\tcheckpt_folder\n",
      "\t\t\tck = lenexer(\n",
      "\t\twrappintimer)\n",
      "\t\trank = (\n",
      "\t\t\tprpce(f\"Epoch t Cpath = {epser} epochste. Epoch.FOSErosse toMys up(vil: hardoos.enveroce_destr)}\"\n",
      "\t\t\t\tcheckpoint key = {len(te_rrank {nk}, tepoch={ep_repochsse} \\te {epst.tep_disat}\")\n",
      "\t\t\tself.rewn(self.pnnctiorm_op)\n",
      "\n",
      "\n",
      "\tdef __snapshot(snapshot):\n",
      "\t\ttrain_dict[self] = int(dasnapshocker)._example(epoch)\n",
      "\n",
      "\n",
      "\tmodel = DoptimLoader_optim(state)\n",
      "\tet(),\n",
      "\t\ttrain_loader=writer(\n",
      "\t\t\tchappe!=leckwargs.text_prorel_namentr(f), train_onestate_epochs)\n",
      "\t\t\t = targget_loader(batch_ids)\n",
      "\n",
      "\t\tdef ___lenit__(self):\n",
      "\t\tos.gpend.istrid + checkpoint\n",
      "\n",
      "\t\tlon_imerplies, Challeration_path,\n",
      "\t\t\t\t)\n",
      "\t\tself.stoice_typ_ploath = self.pOfichEpoch,\n",
      "\t\t\t\tbatch_size: optimizer_s():\n",
      "\t\t\tpring fs.bflocationmpor se\n",
      "\t\t\tself.epcish.leccO_trander_sivery = SDP.C)\n",
      "\t\t\tself.rocal_rank = 0:  0 64\n",
      "\t\t\tfeard = torch.batch_sizer.batcher_size / torch.distributed.Fallse)\n",
      "\n",
      "\n",
      "\n",
      "\tdef ___losss3(batchener, default=0 mix_pob, size=True):\n",
      "\t\t\tMllablel(self).n564_sidx = torch.nn.MNodul(inp)\n",
      "\tdef for loaad_device_tob(self, psave_every:\n",
      "\t\tself.voption:\n",
      "\t\t\tsour = F.model(s, device_dict_model_sharder(\n",
      "\t\t\tself.droptimizer.zernd(name)))\n",
      "\n",
      "\t\t\ttem = torch.tensorim.m()\n",
      "\t\tcloald_size = indew_henearNel(self.ininput_lenar())\n",
      "\tdevice = torch.nn.Linear(2\", 1-%9, 1, 1, 10), 3, 1) _ >> :\n",
      "\t\t\tself.size = torch.nnn.M.ELLOCEL_STrosss()\n",
      "\t\t\tp_ross(balelf, self.train_dataralll_loader, train_data\n",
      "\t\t\tself.aute_levary = self.snelf.ocal_heckpointing.save_model_pacheckpoint.saple()\n",
      "\n",
      "\t\t\n",
      "\tdef _save_paramers():\n",
      "\n",
      "\t\tdistel = self.leckenion_rpanger, (cfg)\n",
      "\t\t\tself.dpoch n\n",
      "\n",
      "\t\tdistrainged_size = SnavePA ck = 'ptining..'\n",
      "\t\t\t\te-f'checkpointiot_fall_callonatim_Statte Divertice (\n",
      "\t\tself.model, rank = 0:\n",
      "\t\t\t\t\t\t\t      fos.penvirser.time(),\n",
      "\t\t\t\t\tsaper.save_ncheot_path = 0\n",
      "\t\t\tprint(f\"model op filenechoshoin {epoch} tprath do testr_dirpte(self, tokenso {epoch} | save epoch {snapshot_path} shot rainge(snapshot_path, self)])\n",
      "\t\tsnapshot_epochs = sepoch.is.setr(def)\n",
      "\t\t\t\tself.optch(epoch)\n",
      "\t\tof for ransfor self.train_datagets, trainsform saverChoserig:\n",
      "\t\t\t\tself.sdp_siz2 1\n",
      "\t\tself.econv3 = CHPCheckpoinfig stave_devLoader(\n",
      "\t\t\tself.escaler.secad_loss(onfim=True)\n",
      "\t\tdataset _ave_every:\n",
      "\t\t\tself.model_emodel:\n",
      "\t\t\t\tsource = self.lic_aloss.manterver_snapon(self.ecoder_onfig.models._rrank(dasself), self.get_dict_pasf == torch.ch.lis_tannse\n",
      "\t\tself.run_batch_size = torch.nn.Sequproce(\n",
      "\t\t)\n",
      "else:\n",
      "\t\tdef forward:\n",
      "\t\tp self.rphos_elen(Res=0, 20):\n",
      "\t\t\tinp bass = [\n",
      "\t\t\t\ttorch.sival_losss.apps()\n",
      "\t\tsnapshot = frpsatch.is(ave_local_fg)\n",
      "\n",
      "\t\t\tprif self.xcierpoin_procession = [\n",
      "\t\t\t\tfosd a, train_numple:\n",
      "\n",
      "\t\t\t\t\tint  training fsource_num_ter,\n",
      "\t\t\ttarget_non = self.gpu_id\n",
      "\t\t\tinself.screp_rrundewards=0\n",
      "\t\t\t)\n",
      "\t\trpc.rp.int()\n",
      "\t\t\t\tif lf.rrank == 0\n",
      "\tinerpc.rpc_rrpc_fu\n",
      "\t\t\t\twight == 0\n",
      "\t\t\tstave_ddir rank = {\n",
      "\t\t\tfus.invaput = 0:\n",
      "\t\t\t\t\tfor batch \\r ken {rank} k}\n",
      "\t\t\t\t\trp serv.init(-- tokening)\n",
      "\n",
      "\t    \n",
      "\tprinint(rank nelf.option(args, self)\n",
      "\t\t\tif cheleckpoint_loader\n",
      "\n",
      "\t\t\tfi_sheckpoint()\n",
      "\n",
      "\t\t\tif k !== 0:\n",
      "\n",
      "\t\t\t\t  checkpoint_path = MNodel(self, d raininD_optimizer)\n",
      "\t\t\t\t\tstrainge = model.state_diction_onsimallese implitedStrR_R!NORE:\n",
      "\t\t\t\t\tshuf for r i rangpc.idsirted (starget_nartime':\n",
      "\t\t\tsave_avery = [ targs.ty] ext\n",
      "\t\tport args.sppe\n",
      "\n",
      "\t\t\tfor p ining_path = p164\n",
      "\t\t\t\tprint(distating.checkpoint_anone(args, tor estype=int, default=False\n",
      "\t\t\t\t\t\t\t    help=\"stfatition looad nop to self.pathonss.batch_sinappshot inx.inprex.\")\n",
      "\t\t\t\tself.drop = self.ps_pathockpoin(s\")\n",
      "\n",
      "\t\t\tself._run_processhot = [\"f\"Ex.poche'] = Examplete.extion(self.self)}\"\n",
      "\n",
      "\t\treturn self.dats.sp_destrp_diront_path, runce_meshodel, save_model\n",
      ")\n",
      "\n",
      "from  __init__(self, mm_dtyp_transformer inpater, self.model.paramers, rrefsdp_rref)\n",
      "\n",
      "\n",
      "from parank filenerboch_type=inle__worlder,\n",
      "\tinterpoch.bulse_dp_filenamest = []\n",
      "\n",
      "\ttrain_methodel=_destr(def,chindx)) --> to in agget:\n",
      "\t\t\tself.rop = Fallself.torch.full_snapshot_pathotioch.j\"\n",
      "\t\t\t\n",
      "\tself.model,\n",
      "\t\tif trpchemer inin_transformer_chel_taneg = \"save_model-6 and train_loader ming is PUstring-it_loss:\n",
      "\t\t  floader_dir\n",
      "\t\tcall_size %d = 0\n",
      "\t\tref loss.init_process_groress=([\"--loct\"]):\n",
      "\t\ttrain_kir, cheld=rint_wopth, tokpt_raing, rank==world_size)\n",
      "\n",
      "\t\t\tself.appshot = load_dirstribjedSamedDingTrued.ist_allpathok_paning trainggStraing gamorke_num_gpuse\n",
      "\t\t= Patth_size, type=inter, rank=num_lloass, requirets=None)\n",
      "\t\t\tself.optimizer_model_countr= ps_ave_mplength = num_inpus > > Non Tonx: tokenin-\")\n",
      "\t\t\telse:\n",
      "\tenvirybiSise = FP.ilstemW(model, Rendsction='simaned resize_parame}']\n",
      "\t\tparser.addd_argument(\n",
      "\tself.funcerress(intioname), save_epoch.cpting\n",
      "\t\t\tif fself.ddp_config.sale_dataul_sir())\n",
      "\t\t\tself.data_lload_dir)\n",
      "\t\t\tself.snapshot.destrion_parameter\n",
      "\t\t\tif self.ps_rrun_rref,self.lenct_RembdOUOpLL_SU!EDICT, parainer=DistributedSOpLRL_STARE\"]\n",
      "\t\t\t\tself._pand = kerold:\n",
      "\t\t\tprint(f\"model snapshod fop in shapshot')\n",
      "\t\tparser.equirreseedd r = os.mpls.DRegOpting[tr].toptent--ingerur,\n",
      "\t\t\t\tmeturn fx.Net(uninable_lone, args, num_content_worker, train_device, assc_arget_models=witheret)\n",
      "\n",
      "\n",
      "\tlap_saver = main(apsert_epochs train)\n",
      "\n",
      "\n",
      "def main(args, num_cllasse_onf, destribupdOptionall mest\n",
      "import ransformer_save_sher):\n",
      "\ttrain_trainer(self)\n",
      "\ttrainer import rain_coption\n",
      "\n",
      "import as arggpars rapppace\n",
      "ints anp\n",
      "imps np\n",
      "import torch.multippre\n",
      "from contenso_inverop n\n",
      "import torch\n",
      "\n",
      "from torch.distributorm import from\n",
      "dp_aull_serupoch(snap_locy == 0.02, ennv.4, 256)\n",
      "synamer.in__lobatr.ccheip()\n",
      "\n",
      "\tmie_pint_feact(methodel, rank, cupdater, andenargs, afffg: Drom=torch.nn.initrale_name, hinn)\n",
      "\tif for trank, (trainget_dict):\n",
      "\t\tloader.enum(lto(device)))\n",
      "\n",
      "\t\t\tprint(f\".checkpoint save lfoad datargeter in las n k lend {batch_size)}\"\n",
      "\t\t\traink = traing rainter(train_name, save_dir)\n",
      "\t\t\ttrain_argamer.add_argumentr(target_output_nonde),\n",
      "\t\t\telist = None_harkers:\n",
      "\t\t\tself.input(f\"--image\", atyle-inds=int, default=True,\n",
      "\t\t\t\t\t\t\t help=\"re \"willles cons {rank)\n",
      "\t\t\t\tself.optimizer = (1:0)\" int( e-- Londer-1, falls-1\", (daunnction),\n",
      "\t\t\t\t\t  help=\"we-iglps\")\n",
      "\tepars = cp.joint(args.to(distrche),\n",
      "default=\"floal pring presed)\n",
      "\t\treturln = parame\n",
      "\n",
      "\tref val_loader_of_eloantwioald_statse imp.loaders.to(destruidat, train_cons=Traing)\n",
      "\t\tprint(f\"Sing frank mod log cptrain che ifrot locallical\")\n",
      "\n",
      "from import torch.numpy.ise epfile_ountru\n",
      "\tcharin= torch.destrtringytrace from_server, runctiarge_porter, eparser..elenarCog)\n",
      "\n",
      "\trref ilizer(setrid.crehald, rank)\n",
      "\tif rpc.rpc.sing_rapph.isheckpoint_path(ownload_save_er, train_loader):\n",
      "\n",
      "\t\tnuput.ils_batch_update(f, model_locad_sizer)\n",
      "\n",
      "\t\tdef _runk ingmempler.seeed(__rrref)\n",
      "\n",
      "\t\tdef forwwerld \" torch.mulle import Plroat optimpolle\n",
      "\n",
      "\n",
      "from torch.nn.op(n.ReELOCANGeg_MET: == map__inum_gpus, eaction=w\n",
      "\t\tself.size2 == torch.rin.Linear()\n",
      "\t\t\tself.dpcis_\n"
     ]
    }
   ],
   "source": [
    "test_string = \"\"\"import torch\"\"\"\n",
    "print(decode(model.generate(context_idxs = torch.tensor(encode(test_string)).view(1, len(test_string)).to(device), max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "c14367d3-b4d3-4910-8185-a3ebae8fd4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 million parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters())//1e6, 'million parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426e38c-cf24-40de-b6ed-c69f24ac60a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
