from typing import Dict, Union, Iterator

import torch

from allennlp.common.registrable import Registrable
from allennlp.data.instance import Instance
from allennlp.data.vocabulary import Vocabulary


TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]


class DataLoader(Registrable):

	default_implementation = "multiprocess"

	def __len__(self) -> int:
		raise TypeError

	def __iter__(self) -> Iterator[TensorDict]:
		raise NotImplementedError

	def iter_instances(self) -> Iterator[Instance]:
		raise NotImplementedError

	def index_with(self, vocab: Vocabulary) -> None:
		raise NotImplementedError

	def set_target_device(self, device: torch.device) -> None:
		raise NotImplementedError

import argparse
import glob
import os
import json
import time
import logging
import random
import re
from itertools import chain
from string import punctuation

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

from datasets import load_dataset, load_metric


from transformers import (
	AdamW,
	T5ForConditionalGeneration,
	T5Tokenizer,
	get_linear_schedule_with_warmup
)

class wikihow(Dataset):
	def __init__(self, tokenizer, type_path, num_samples, input_length, output_length, print_text=False):		 
		self.dataset =  load_dataset('wikihow', 'all', data_dir='data/', split=type_path)
		if num_samples:
			self.dataset = self.dataset.select(list(range(0, num_samples)))
		self.input_length = input_length
		self.tokenizer = tokenizer
		self.output_length = output_length
		self.print_text = print_text
  
	def __len__(self):
		return self.dataset.shape[0]
	
	def clean_text(self, text):
		text = text.replace('Example of text:', '')
		text = text.replace('Example of Summary:', '')
		text = text.replace('\n','')
		text = text.replace('``', '')
		text = text.replace('"', '')
		
		return text
	
	
	def convert_to_features(self, example_batch):
		
		if self.print_text:
			print("Input Text: ", self.clean_text(example_batch['text']))
		
		input_ = self.clean_text(example_batch['text'])
		target_ = self.clean_text(example_batch['headline'])
		
		source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, 
													 padding='max_length', truncation=True, return_tensors="pt")
		
		targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, 
													 padding='max_length', truncation=True, return_tensors="pt")
	
	   
		return source, targets
  
	def __getitem__(self, index):
		source, targets = self.convert_to_features(self.dataset[index])
		
		source_ids = source["input_ids"].squeeze()
		target_ids = targets["input_ids"].squeeze()

		src_mask	= source["attention_mask"].squeeze()
		target_mask = targets["attention_mask"].squeeze()

		return {"source_ids": source_ids, "source_mask": src_mask, "target_ids": target_ids, "target_mask": target_mask}
		
def get_dataset(tokenizer, type_path, num_samples, args):
	  return wikihow(tokenizer=tokenizer, type_path=type_path, num_samples=num_samples,  input_length=max_input_length, 
						output_length=max_output_length)

from allennlp.predictors.predictor import Predictor
from allennlp.predictors.sentence_tagger import SentenceTaggerPredictor
from allennlp.predictors.text_classifier import TextClassifierPredictor
from allennlp.predictors.multitask import MultiTaskPredictor

import argparse
import torch

import data

parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 Language Model')
parser.add_argument('--data', type=str, default='./data/wikitext-2',
					help='location of the data corpus')
parser.add_argument('--checkpoint', type=str, default='./model.pt',
					help='model checkpoint to use')
parser.add_argument('--outf', type=str, default='generated.txt',
					help='output file for generated text')
parser.add_argument('--words', type=int, default='1000',
					help='number of words to generate')
parser.add_argument('--seed', type=int, default=1111,
					help='random seed')
parser.add_argument('--cuda', action='store_true',
					help='use CUDA')
parser.add_argument('--mps', action='store_true', default=False,
						help='enables macOS GPU training')
parser.add_argument('--temperature', type=float, default=1.0,
					help='temperature - higher will increase diversity')
parser.add_argument('--log-interval', type=int, default=100,
					help='reporting interval')
args = parser.parse_args()

torch.manual_seed(args.seed)
if torch.cuda.is_available():
	if not args.cuda:
		print("WARNING: You have a CUDA device, so you should probably run with --cuda.")
if torch.backends.mps.is_available():
	if not args.mps:
		print("WARNING: You have mps device, to enable macOS GPU run with --mps.")
		
use_mps = args.mps and torch.backends.mps.is_available()
if args.cuda:
	device = torch.device("cuda")
elif use_mps:
	device = torch.device("mps")
else:
	device = torch.device("cpu")

if args.temperature < 1e-3:
	parser.error("--temperature has to be greater or equal 1e-3.")

with open(args.checkpoint, 'rb') as f:
	model = torch.load(f, map_location=device)
model.eval()

corpus = data.Corpus(args.data)
ntokens = len(corpus.dictionary)

is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'
if not is_transformer_model:
	hidden = model.init_hidden(1)
input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)

with open(args.outf, 'w') as outf:
	with torch.no_grad():  # no tracking history
		for i in range(args.words):
			if is_transformer_model:
				output = model(input, False)
				word_weights = output[-1].squeeze().div(args.temperature).exp().cpu()
				word_idx = torch.multinomial(word_weights, 1)[0]
				word_tensor = torch.Tensor([[word_idx]]).long().to(device)
				input = torch.cat([input, word_tensor], 0)
			else:
				output, hidden = model(input, hidden)
				word_weights = output.squeeze().div(args.temperature).exp().cpu()
				word_idx = torch.multinomial(word_weights, 1)[0]
				input.fill_(word_idx)

			word = corpus.dictionary.idx2word[word_idx]

			outf.write(word + ('\n' if i % 20 == 19 else ' '))

			if i % args.log_interval == 0:
				print('| Generated {}/{} words'.format(i, args.words))


import logging
import os
import random
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
import numpy as np
from datasets import load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoModelForSequenceClassification,
	AutoTokenizer,
	DataCollatorWithPadding,
	EvalPrediction,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	default_data_collator,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/text-classification/requirements.txt")

logger = logging.getLogger(__name__)


@dataclass
class DataTrainingArguments:

	max_seq_length: Optional[int] = field(
		default=128,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
	)
	pad_to_max_length: bool = field(
		default=True,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		default=None, metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	language: str = field(
		default=None, metadata={"help": "Evaluation language. Also train language if `train_language` is set to None."}
	)
	train_language: Optional[str] = field(
		default=None, metadata={"help": "Train language if it is different from the evaluation language."}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	do_lower_case: Optional[bool] = field(
		default=False,
		metadata={"help": "arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	ignore_mismatched_sizes: bool = field(
		default=False,
		metadata={"help": "Will enable to load a pretrained model whose head dimensions are different."},
	)


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_xnli", model_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if training_args.do_train:
		if model_args.train_language is None:
			train_dataset = load_dataset(
				"xnli",
				model_args.language,
				split="train",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
		else:
			train_dataset = load_dataset(
				"xnli",
				model_args.train_language,
				split="train",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
		label_list = train_dataset.features["label"].names

	if training_args.do_eval:
		eval_dataset = load_dataset(
			"xnli",
			model_args.language,
			split="validation",
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
		label_list = eval_dataset.features["label"].names

	if training_args.do_predict:
		predict_dataset = load_dataset(
			"xnli",
			model_args.language,
			split="test",
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
		label_list = predict_dataset.features["label"].names

	num_labels = len(label_list)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		num_labels=num_labels,
		id2label={str(i): label for i, label in enumerate(label_list)},
		label2id={label: i for i, label in enumerate(label_list)},
		finetuning_task="xnli",
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		do_lower_case=model_args.do_lower_case,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSequenceClassification.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
		ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
	)

	if data_args.pad_to_max_length:
		padding = "max_length"
	else:
		padding = False

	def preprocess_function(examples):
		return tokenizer(
			examples["premise"],
			examples["hypothesis"],
			padding=padding,
			max_length=data_args.max_seq_length,
			truncation=True,
		)

	if training_args.do_train:
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				preprocess_function,
				batched=True,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on train dataset",
			)
		for index in random.sample(range(len(train_dataset)), 3):
			logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if training_args.do_eval:
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_dataset.map(
				preprocess_function,
				batched=True,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on validation dataset",
			)

	if training_args.do_predict:
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))
		with training_args.main_process_first(desc="prediction dataset map pre-processing"):
			predict_dataset = predict_dataset.map(
				preprocess_function,
				batched=True,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)

	metric = evaluate.load("xnli", cache_dir=model_args.cache_dir)

	def compute_metrics(p: EvalPrediction):
		preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
		preds = np.argmax(preds, axis=1)
		return metric.compute(predictions=preds, references=p.label_ids)

	if data_args.pad_to_max_length:
		data_collator = default_data_collator
	elif training_args.fp16:
		data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
	else:
		data_collator = None

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		compute_metrics=compute_metrics,
		tokenizer=tokenizer,
		data_collator=data_collator,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.save_model()  # Saves the tokenizer too for easy upload

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate(eval_dataset=eval_dataset)

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")
		predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix="predict")

		max_predict_samples = (
			data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
		)
		metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))

		trainer.log_metrics("predict", metrics)
		trainer.save_metrics("predict", metrics)

		predictions = np.argmax(predictions, axis=1)
		output_predict_file = os.path.join(training_args.output_dir, "predictions.txt")
		if trainer.is_world_process_zero():
			with open(output_predict_file, "w") as writer:
				writer.write("index\tprediction\n")
				for index, item in enumerate(predictions):
					item = label_list[item]
					writer.write(f"{index}\t{item}\n")


if __name__ == "__main__":
	main()

import bisect
import random
from collections import abc
from typing import Sequence, Optional, Union


class ShuffledSequence(abc.Sequence):

	def __init__(self, inner_sequence: Sequence, indices: Optional[Sequence[int]] = None):
		self.inner = inner_sequence
		self.indices: Sequence[int]
		if indices is None:
			self.indices = list(range(len(inner_sequence)))
			random.shuffle(self.indices)
		else:
			self.indices = indices

	def __len__(self) -> int:
		return len(self.indices)

	def __getitem__(self, i: Union[int, slice]):
		if isinstance(i, int):
			return self.inner[self.indices[i]]
		else:
			return ShuffledSequence(self.inner, self.indices[i])

	def __contains__(self, item) -> bool:
		for i in self.indices:
			if self.inner[i] == item:
				return True
		return False


class SlicedSequence(ShuffledSequence):

	def __init__(self, inner_sequence: Sequence, s: slice):
		super().__init__(inner_sequence, range(*s.indices(len(inner_sequence))))


class ConcatenatedSequence(abc.Sequence):

	def __init__(self, *sequences: Sequence):
		self.sequences = sequences
		self.cumulative_sequence_lengths = [0]
		for sequence in sequences:
			self.cumulative_sequence_lengths.append(
				self.cumulative_sequence_lengths[-1] + len(sequence)
			)

	def __len__(self):
		return self.cumulative_sequence_lengths[-1]

	def __getitem__(self, i: Union[int, slice]):
		if isinstance(i, int):
			if i < 0:
				i += len(self)
			if i < 0 or i >= len(self):
				raise IndexError("list index out of range")
			sequence_index = bisect.bisect_right(self.cumulative_sequence_lengths, i) - 1
			i -= self.cumulative_sequence_lengths[sequence_index]
			return self.sequences[sequence_index][i]
		else:
			return SlicedSequence(self, i)

	def __contains__(self, item) -> bool:
		return any(s.__contains__(item) for s in self.sequences)

from typing import Any, Dict, List


from allennlp.data.fields.field import Field


class FlagField(Field[Any]):

	__slots__ = ["flag_value"]

	def __init__(self, flag_value: Any) -> None:
		self.flag_value = flag_value

	def get_padding_lengths(self) -> Dict[str, int]:
		return {}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> Any:
		return self.flag_value

	def empty_field(self):
		return FlagField(self.flag_value)

	def __str__(self) -> str:
		return f"FlagField({self.flag_value})"

	def __len__(self) -> int:
		return 1

	def batch_tensors(self, tensor_list: List[Any]) -> Any:
		if len(set(tensor_list)) != 1:
			raise ValueError(
				f"Got different values in a FlagField when trying to batch them: {tensor_list}"
			)
		return tensor_list[0]

	def human_readable_repr(self) -> Any:
		if hasattr(self.flag_value, "human_readable_repr"):
			return self.flag_value.human_readable_repr()
		return self.flag_value

from allennlp.training.callbacks.callback import TrainerCallback
from allennlp.training.callbacks.console_logger import ConsoleLoggerCallback
from allennlp.training.callbacks.confidence_checks import ConfidenceChecksCallback
from allennlp.training.callbacks.tensorboard import TensorBoardCallback
from allennlp.training.callbacks.track_epoch import TrackEpochCallback
from allennlp.training.callbacks.wandb import WandBCallback
from allennlp.training.callbacks.backward import MixedPrecisionBackwardCallback, OnBackwardException
from allennlp.training.callbacks.should_validate import ShouldValidateCallback

from typing import Dict, MutableMapping, Mapping

from allennlp.data.fields.field import DataArray, Field
from allennlp.data.vocabulary import Vocabulary
from allennlp.common.util import JsonDict


class Instance(Mapping[str, Field]):

	__slots__ = ["fields", "indexed"]

	def __init__(self, fields: MutableMapping[str, Field]) -> None:
		self.fields = fields
		self.indexed = False

	def __getitem__(self, key: str) -> Field:
		return self.fields[key]

	def __iter__(self):
		return iter(self.fields)

	def __len__(self) -> int:
		return len(self.fields)

	def add_field(self, field_name: str, field: Field, vocab: Vocabulary = None) -> None:
		self.fields[field_name] = field
		if self.indexed and vocab is not None:
			field.index(vocab)

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		for field in self.fields.values():
			field.count_vocab_items(counter)

	def index_fields(self, vocab: Vocabulary) -> None:
		if not self.indexed:
			for field in self.fields.values():
				field.index(vocab)
			self.indexed = True

	def get_padding_lengths(self) -> Dict[str, Dict[str, int]]:
		lengths = {}
		for field_name, field in self.fields.items():
			lengths[field_name] = field.get_padding_lengths()
		return lengths

	def as_tensor_dict(
		self, padding_lengths: Dict[str, Dict[str, int]] = None
	) -> Dict[str, DataArray]:
		padding_lengths = padding_lengths or self.get_padding_lengths()
		tensors = {}
		for field_name, field in self.fields.items():
			tensors[field_name] = field.as_tensor(padding_lengths[field_name])
		return tensors

	def __str__(self) -> str:
		base_string = "Instance with fields:\n"
		return " ".join(
			[base_string] + [f"\t {name}: {field} \n" for name, field in self.fields.items()]
		)

	def duplicate(self) -> "Instance":
		new = Instance({k: field.duplicate() for k, field in self.fields.items()})
		new.indexed = self.indexed
		return new

	def human_readable_dict(self) -> JsonDict:
		return {key: field.human_readable_repr() for key, field in self.fields.items()}

from .pretrain import BERTTrainer


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
from datasets import load_dataset
from trainer_qa import QuestionAnsweringTrainer
from utils_qa import postprocess_qa_predictions

import transformers
from transformers import (
	AutoConfig,
	AutoModelForQuestionAnswering,
	AutoTokenizer,
	DataCollatorWithPadding,
	EvalPrediction,
	HfArgumentParser,
	PreTrainedTokenizerFast,
	TrainingArguments,
	default_data_collator,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/question-answering/requirements.txt")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Path to directory to store the pretrained models downloaded from huggingface.co"},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
	)
	test_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input test data file to evaluate the perplexity on (a text file)."},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_seq_length: int = field(
		default=384,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	pad_to_max_length: bool = field(
		default=True,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when"
				" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU)."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	version_2_with_negative: bool = field(
		default=False, metadata={"help": "If true, some of the examples do not have an answer."}
	)
	null_score_diff_threshold: float = field(
		default=0.0,
		metadata={
			"help": (
				"The threshold used to select the null answer: if the best answer has a score that is less than "
				"the score of the null answer minus this threshold, the null answer is selected for this example. "
				"Only useful when `version_2_with_negative=True`."
			)
		},
	)
	doc_stride: int = field(
		default=128,
		metadata={"help": "When splitting up a long document into chunks, how much stride to take between chunks."},
	)
	n_best_size: int = field(
		default=20,
		metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
	)
	max_answer_length: int = field(
		default=30,
		metadata={
			"help": (
				"The maximum length of an answer that can be generated. This is needed because the start "
				"and end predictions are not conditioned on one another."
			)
		},
	)

	def __post_init__(self):
		if (
			self.dataset_name is None
			and self.train_file is None
			and self.validation_file is None
			and self.test_file is None
		):
			raise ValueError("Need either a dataset name or a training/validation file/test_file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
			if self.test_file is not None:
				extension = self.test_file.split(".")[-1]
				assert extension in ["csv", "json"], "`test_file` should be a csv or a json file."


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_qa", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
			extension = data_args.train_file.split(".")[-1]

		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
			extension = data_args.validation_file.split(".")[-1]
		if data_args.test_file is not None:
			data_files["test"] = data_args.test_file
			extension = data_args.test_file.split(".")[-1]
		raw_datasets = load_dataset(
			extension,
			data_files=data_files,
			field="data",
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=True,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForQuestionAnswering.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	if not isinstance(tokenizer, PreTrainedTokenizerFast):
		raise ValueError(
			"This example script only works for models that have a fast tokenizer. Checkout the big table of models at"
			" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet"
			" this requirement"
		)

	if training_args.do_train:
		column_names = raw_datasets["train"].column_names
	elif training_args.do_eval:
		column_names = raw_datasets["validation"].column_names
	else:
		column_names = raw_datasets["test"].column_names
	question_column_name = "question" if "question" in column_names else column_names[0]
	context_column_name = "context" if "context" in column_names else column_names[1]
	answer_column_name = "answers" if "answers" in column_names else column_names[2]

	pad_on_right = tokenizer.padding_side == "right"

	if data_args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)
	max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	def prepare_train_features(examples):
		examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]

		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=data_args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			padding="max_length" if data_args.pad_to_max_length else False,
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
		offset_mapping = tokenized_examples.pop("offset_mapping")

		tokenized_examples["start_positions"] = []
		tokenized_examples["end_positions"] = []

		for i, offsets in enumerate(offset_mapping):
			input_ids = tokenized_examples["input_ids"][i]
			cls_index = input_ids.index(tokenizer.cls_token_id)

			sequence_ids = tokenized_examples.sequence_ids(i)

			sample_index = sample_mapping[i]
			answers = examples[answer_column_name][sample_index]
			if len(answers["answer_start"]) == 0:
				tokenized_examples["start_positions"].append(cls_index)
				tokenized_examples["end_positions"].append(cls_index)
			else:
				start_char = answers["answer_start"][0]
				end_char = start_char + len(answers["text"][0])

				token_start_index = 0
				while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
					token_start_index += 1

				token_end_index = len(input_ids) - 1
				while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
					token_end_index -= 1

				if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
					tokenized_examples["start_positions"].append(cls_index)
					tokenized_examples["end_positions"].append(cls_index)
				else:
					while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
						token_start_index += 1
					tokenized_examples["start_positions"].append(token_start_index - 1)
					while offsets[token_end_index][1] >= end_char:
						token_end_index -= 1
					tokenized_examples["end_positions"].append(token_end_index + 1)

		return tokenized_examples

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				prepare_train_features,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on train dataset",
			)
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	def prepare_validation_features(examples):
		examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]

		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=data_args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			padding="max_length" if data_args.pad_to_max_length else False,
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

		tokenized_examples["example_id"] = []

		for i in range(len(tokenized_examples["input_ids"])):
			sequence_ids = tokenized_examples.sequence_ids(i)
			context_index = 1 if pad_on_right else 0

			sample_index = sample_mapping[i]
			tokenized_examples["example_id"].append(examples["id"][sample_index])

			tokenized_examples["offset_mapping"][i] = [
				(o if sequence_ids[k] == context_index else None)
				for k, o in enumerate(tokenized_examples["offset_mapping"][i])
			]

		return tokenized_examples

	if training_args.do_eval:
		if "validation" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_examples = raw_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_examples), data_args.max_eval_samples)
			eval_examples = eval_examples.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_examples.map(
				prepare_validation_features,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on validation dataset",
			)
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

	if training_args.do_predict:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_examples = raw_datasets["test"]
		if data_args.max_predict_samples is not None:
			predict_examples = predict_examples.select(range(data_args.max_predict_samples))
		with training_args.main_process_first(desc="prediction dataset map pre-processing"):
			predict_dataset = predict_examples.map(
				prepare_validation_features,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))

	data_collator = (
		default_data_collator
		if data_args.pad_to_max_length
		else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)
	)

	def post_processing_function(examples, features, predictions, stage="eval"):
		predictions = postprocess_qa_predictions(
			examples=examples,
			features=features,
			predictions=predictions,
			version_2_with_negative=data_args.version_2_with_negative,
			n_best_size=data_args.n_best_size,
			max_answer_length=data_args.max_answer_length,
			null_score_diff_threshold=data_args.null_score_diff_threshold,
			output_dir=training_args.output_dir,
			log_level=log_level,
			prefix=stage,
		)
		if data_args.version_2_with_negative:
			formatted_predictions = [
				{"id": str(k), "prediction_text": v, "no_answer_probability": 0.0} for k, v in predictions.items()
			]
		else:
			formatted_predictions = [{"id": str(k), "prediction_text": v} for k, v in predictions.items()]

		references = [{"id": str(ex["id"]), "answers": ex[answer_column_name]} for ex in examples]
		return EvalPrediction(predictions=formatted_predictions, label_ids=references)

	metric = evaluate.load(
		"squad_v2" if data_args.version_2_with_negative else "squad", cache_dir=model_args.cache_dir
	)

	def compute_metrics(p: EvalPrediction):
		return metric.compute(predictions=p.predictions, references=p.label_ids)

	trainer = QuestionAnsweringTrainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		eval_examples=eval_examples if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		post_process_function=post_processing_function,
		compute_metrics=compute_metrics,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload

		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate()

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")
		results = trainer.predict(predict_dataset, predict_examples)
		metrics = results.metrics

		max_predict_samples = (
			data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
		)
		metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))

		trainer.log_metrics("predict", metrics)
		trainer.save_metrics("predict", metrics)

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "question-answering"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter
from allennlp.interpret.saliency_interpreters.simple_gradient import SimpleGradient
from allennlp.interpret.saliency_interpreters.integrated_gradient import IntegratedGradient
from allennlp.interpret.saliency_interpreters.smooth_gradient import SmoothGradient

import os
from io import open
import torch

class Dictionary(object):
	def __init__(self):
		self.word2idx = {}
		self.idx2word = []

	def add_word(self, word):
		if word not in self.word2idx:
			self.idx2word.append(word)
			self.word2idx[word] = len(self.idx2word) - 1
		return self.word2idx[word]

	def __len__(self):
		return len(self.idx2word)


class Corpus(object):
	def __init__(self, path):
		self.dictionary = Dictionary()
		self.train = self.tokenize(os.path.join(path, 'train.txt'))
		self.valid = self.tokenize(os.path.join(path, 'valid.txt'))
		self.test = self.tokenize(os.path.join(path, 'test.txt'))

	def tokenize(self, path):
		assert os.path.exists(path)
		with open(path, 'r', encoding="utf8") as f:
			for line in f:
				words = line.split() + ['<eos>']
				for word in words:
					self.dictionary.add_word(word)

		with open(path, 'r', encoding="utf8") as f:
			idss = []
			for line in f:
				words = line.split() + ['<eos>']
				ids = []
				for word in words:
					ids.append(self.dictionary.word2idx[word])
				idss.append(torch.tensor(ids).type(torch.int64))
			ids = torch.cat(idss)

		return ids

import math

from allennlp.training.metrics.average import Average
from allennlp.training.metrics.metric import Metric


@Metric.register("perplexity")
class Perplexity(Average):

	def get_metric(self, reset: bool = False):
		average_loss = super().get_metric(reset)
		if average_loss == 0:
			return 0.0

		return math.exp(average_loss)

from typing import List
import torch

from allennlp.modules.token_embedders.token_embedder import TokenEmbedder
from allennlp.modules.elmo import Elmo
from allennlp.modules.time_distributed import TimeDistributed


@TokenEmbedder.register("elmo_token_embedder")
class ElmoTokenEmbedder(TokenEmbedder):

	def __init__(
		self,
		options_file: str = "https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/"
		+ "elmo_2x4096_512_2048cnn_2xhighway_options.json",
		weight_file: str = "https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/"
		+ "elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5",
		do_layer_norm: bool = False,
		dropout: float = 0.5,
		requires_grad: bool = False,
		projection_dim: int = None,
		vocab_to_cache: List[str] = None,
		scalar_mix_parameters: List[float] = None,
	) -> None:
		super().__init__()

		self._elmo = Elmo(
			options_file,
			weight_file,
			1,
			do_layer_norm=do_layer_norm,
			dropout=dropout,
			requires_grad=requires_grad,
			vocab_to_cache=vocab_to_cache,
			scalar_mix_parameters=scalar_mix_parameters,
		)
		if projection_dim:
			self._projection = torch.nn.Linear(self._elmo.get_output_dim(), projection_dim)
			self.output_dim = projection_dim
		else:
			self._projection = None
			self.output_dim = self._elmo.get_output_dim()

	def get_output_dim(self) -> int:
		return self.output_dim

	def forward(self, elmo_tokens: torch.Tensor, word_inputs: torch.Tensor = None) -> torch.Tensor:
		elmo_output = self._elmo(elmo_tokens, word_inputs)
		elmo_representations = elmo_output["elmo_representations"][0]
		if self._projection:
			projection = self._projection
			for _ in range(elmo_representations.dim() - 2):
				projection = TimeDistributed(projection)
			elmo_representations = projection(elmo_representations)
		return elmo_representations

import logging
from typing import Dict, List

import torch


from allennlp.data.fields.text_field import TextFieldTensors
from allennlp.data.vocabulary import Vocabulary
from allennlp.modules.backbones.backbone import Backbone
from allennlp.modules.transformer import (
	BiModalEncoder,
	ImageFeatureEmbeddings,
	TransformerEmbeddings,
	TransformerPooler,
)

logger = logging.getLogger(__name__)


@Backbone.register("vilbert")
@Backbone.register("vilbert_from_huggingface", constructor="from_huggingface_model_name")
class VilbertBackbone(Backbone):

	def __init__(
		self,
		vocab: Vocabulary,
		text_embeddings: TransformerEmbeddings,
		image_embeddings: ImageFeatureEmbeddings,
		encoder: BiModalEncoder,
		pooled_output_dim: int,
		fusion_method: str = "sum",
		dropout: float = 0.1,
		vocab_namespace: str = "tokens",
	) -> None:
		super().__init__()
		self.fusion_method = fusion_method
		self.text_embeddings = text_embeddings
		self.image_embeddings = image_embeddings
		self.encoder = encoder

		self.t_pooler = TransformerPooler(encoder.hidden_size1, pooled_output_dim)
		self.v_pooler = TransformerPooler(encoder.hidden_size2, pooled_output_dim)
		self.dropout = torch.nn.Dropout(dropout)

		self._vocab = vocab
		self._namespace = vocab_namespace

	@classmethod
	def from_huggingface_model_name(
		cls,
		vocab: Vocabulary,
		model_name: str,
		image_feature_dim: int,
		image_num_hidden_layers: int,
		image_hidden_size: int,
		image_num_attention_heads: int,
		combined_hidden_size: int,
		combined_num_attention_heads: int,
		pooled_output_dim: int,
		image_intermediate_size: int,
		image_attention_dropout: float,
		image_hidden_dropout: float,
		image_biattention_id: List[int],
		text_biattention_id: List[int],
		text_fixed_layer: int,
		image_fixed_layer: int,
		fusion_method: str = "sum",
	):
		text_embeddings = TransformerEmbeddings.from_pretrained_module(model_name)

		image_embeddings = ImageFeatureEmbeddings(
			feature_size=image_feature_dim,
			embedding_size=image_hidden_size,
			dropout=image_hidden_dropout,
		)

		encoder = BiModalEncoder.from_pretrained_module(
			model_name,
			num_hidden_layers2=image_num_hidden_layers,
			hidden_size2=image_hidden_size,
			num_attention_heads2=image_num_attention_heads,
			combined_hidden_size=combined_hidden_size,
			combined_num_attention_heads=combined_num_attention_heads,
			intermediate_size2=image_intermediate_size,
			attention_dropout2=image_attention_dropout,
			hidden_dropout2=image_hidden_dropout,
			biattention_id1=text_biattention_id,
			biattention_id2=image_biattention_id,
			fixed_layer1=text_fixed_layer,
			fixed_layer2=image_fixed_layer,
		)

		return cls(
			vocab=vocab,
			text_embeddings=text_embeddings,
			image_embeddings=image_embeddings,
			encoder=encoder,
			pooled_output_dim=pooled_output_dim,
			fusion_method=fusion_method,
		)

	def forward(  # type: ignore
		self,
		box_features: torch.Tensor,
		box_coordinates: torch.Tensor,
		box_mask: torch.Tensor,
		text: TextFieldTensors,
	) -> Dict[str, torch.Tensor]:
		if "token_ids" in text["tokens"]:
			token_ids = text["tokens"]["token_ids"]
		else:
			token_ids = text["tokens"]["tokens"]

		if token_ids.shape[:-1] != box_features.shape[:-2]:
			raise ValueError(
				"Tokens and boxes must have the same batch size and extra "
				"dimensions (if applicable). Token size {0} did not match "
				"box feature size {1}.".format(token_ids.shape[:-1], box_features.shape[:-2])
			)

		token_type_ids = text["tokens"].get("type_ids")
		attention_mask = text["tokens"].get("mask")

		box_feature_dimensions = box_features.shape
		feature_size = box_feature_dimensions[-1]
		rolled_dimensions = box_feature_dimensions[:-2]
		rolled_dimensions_product = 1
		for dim in rolled_dimensions:
			rolled_dimensions_product *= dim

		token_ids = token_ids.view(rolled_dimensions_product, token_ids.shape[-1])
		if token_type_ids is not None:
			token_type_ids = token_type_ids.view(
				rolled_dimensions_product, token_type_ids.shape[-1]
			)
		if attention_mask is not None:
			attention_mask = attention_mask.view(
				rolled_dimensions_product, attention_mask.shape[-1]
			)
		box_features = box_features.view(
			rolled_dimensions_product, box_feature_dimensions[-2], feature_size
		)
		box_coordinates = box_coordinates.view(
			rolled_dimensions_product,
			box_coordinates.shape[-2],
			box_coordinates.shape[-1],
		)
		box_mask = box_mask.view(rolled_dimensions_product, box_mask.shape[-1])

		embedding_output = self.text_embeddings(token_ids, token_type_ids)

		if attention_mask is not None:
			extended_attention_mask = attention_mask
		else:
			extended_attention_mask = None

		extended_image_attention_mask = box_mask

		v_embedding_output = self.image_embeddings(box_features, box_coordinates)

		encoded_layers_t, encoded_layers_v = self.encoder(
			embedding_output,
			v_embedding_output,
			extended_attention_mask,
			extended_image_attention_mask,
		)

		sequence_output_t = encoded_layers_t[:, :, :, -1]
		sequence_output_v = encoded_layers_v[:, :, :, -1]

		pooled_output_t = self.t_pooler(sequence_output_t)
		pooled_output_v = self.v_pooler(sequence_output_v)

		sequence_output_t = sequence_output_t.view(
			rolled_dimensions + (sequence_output_t.shape[-2], sequence_output_t.shape[-1])
		)
		sequence_output_v = sequence_output_v.view(
			rolled_dimensions + (sequence_output_v.shape[-2], sequence_output_v.shape[-1])
		)
		pooled_output_t = pooled_output_t.view(rolled_dimensions + (pooled_output_t.shape[-1],))
		pooled_output_v = pooled_output_v.view(rolled_dimensions + (pooled_output_v.shape[-1],))

		if self.fusion_method == "sum":
			pooled_output = self.dropout(pooled_output_t + pooled_output_v)
		elif self.fusion_method == "mul":
			pooled_output = self.dropout(pooled_output_t * pooled_output_v)
		else:
			raise ValueError(f"Fusion method '{self.fusion_method}' not supported")

		return {
			"encoded_boxes": sequence_output_v,
			"encoded_boxes_mask": box_mask,
			"encoded_boxes_pooled": pooled_output_v,
			"encoded_text": sequence_output_t,
			"encoded_text_mask": attention_mask,
			"encoded_text_pooled": pooled_output_t,
			"pooled_boxes_and_text": pooled_output,
		}


import argparse
import logging
import pathlib


import torch

import allennlp
from allennlp.common.util import import_module_and_submodules
from allennlp.commands.subcommand import Subcommand
from allennlp.version import VERSION


logger = logging.getLogger(__name__)


@Subcommand.register("test-install")
class TestInstall(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		description = """Test that AllenNLP is installed correctly."""
		subparser = parser.add_parser(
			self.name, description=description, help="Test AllenNLP installation."
		)
		subparser.set_defaults(func=_run_test)
		return subparser


def _get_module_root():
	return pathlib.Path(allennlp.__file__).parent


def _run_test(args: argparse.Namespace):
	import_module_and_submodules("allennlp.common")
	import_module_and_submodules("allennlp.data")
	import_module_and_submodules("allennlp.interpret")
	import_module_and_submodules("allennlp.models")
	import_module_and_submodules("allennlp.modules")
	import_module_and_submodules("allennlp.nn")
	import_module_and_submodules("allennlp.predictors")
	import_module_and_submodules("allennlp.training")
	logger.info("AllenNLP version %s installed to %s", VERSION, _get_module_root())
	logger.info("Cuda devices available: %s", torch.cuda.device_count())

import sys
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F

from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.tensor.parallel import (
	parallelize_module,
	ColwiseParallel,
	RowwiseParallel,
)

import os
from log_utils import rank_log, get_logger, verify_min_gpu_count


_min_gpu_count = 4

if not verify_min_gpu_count(min_gpus=_min_gpu_count):
	print(f"Unable to locate sufficient {_min_gpu_count} gpus to run this example. Exiting.")
	sys.exit()

from torch.distributed._tensor.device_mesh import init_device_mesh




def find_multiple(n: int, k: int) -> int:
	if n % k == 0:
		return n
	return n + k - (n % k)


class MLP_swiglu(nn.Module):

	def __init__(self, mlp_dim: int = 1024) -> None:
		super().__init__()
		hidden_dim = 4 * mlp_dim
		scaled_hidden = int(2 * hidden_dim / 3)
		rounded_hidden = find_multiple(scaled_hidden, 256)

		self.in_proj = nn.Linear(mlp_dim, rounded_hidden, bias=False)
		self.gate_proj = nn.Linear(mlp_dim, rounded_hidden, bias=False)
		self.out_proj = nn.Linear(rounded_hidden, mlp_dim, bias=False)

	def forward(self, x: torch.Tensor) -> torch.Tensor:
		x = F.silu(self.in_proj(x)) * self.gate_proj(x)
		x = self.out_proj(x)
		return x


tp_size = 2
logger = get_logger()

_rank = int(os.environ["RANK"])
_world_size = int(os.environ["WORLD_SIZE"])


print(f"Starting PyTorch 2D (FSDP + TP) example on rank {_rank}.")
assert (
	_world_size % tp_size == 0
), f"World size {_world_size} needs to be divisible by TP size {tp_size}"


dp_size = _world_size // tp_size

device_mesh = init_device_mesh("cuda", (dp_size, tp_size), mesh_dim_names=("dp", "tp"))

rank_log(_rank, logger, f"Device Mesh created: {device_mesh=}")
tp_mesh = device_mesh["tp"]
dp_mesh = device_mesh["dp"]

dp_rank = dp_mesh.get_local_rank()

_mlp_dim = 1024
base_model_swiglu = MLP_swiglu(mlp_dim=_mlp_dim).to("cuda")


custom_tp_model = parallelize_module(
	module=base_model_swiglu,
	device_mesh=tp_mesh,
	parallelize_plan={
		"in_proj": ColwiseParallel(),
		"gate_proj": ColwiseParallel(),
		"out_proj": RowwiseParallel(),
	},
)

rank_log(_rank, logger, f"Model after parallelization {custom_tp_model=}\n")

sharded_model = FSDP(custom_tp_model, device_mesh=dp_mesh, use_orig_params=True)

lr = 3e-3
rank_log(_rank, logger, f"Creating AdamW optimizer with learning rate {lr}")
optimizer = torch.optim.AdamW(sharded_model.parameters(), lr=lr, foreach=True)

rank_log(_rank, logger, "\nStarting 2D training...")
num_iterations = 10
batch_size = 2

for i in range(num_iterations):
	torch.manual_seed(i + dp_rank)
	inp = torch.rand(batch_size, _mlp_dim, device="cuda")

	output = sharded_model(inp)
	output.sum().backward()
	optimizer.step()
	rank_log(_rank, logger, f"2D iter {i} complete")

rank_log(_rank, logger, "2D training successfully completed!")

import pickle
import tqdm
from collections import Counter


class TorchVocab(object):

	def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],
				 vectors=None, unk_init=None, vectors_cache=None):
		self.freqs = counter
		counter = counter.copy()
		min_freq = max(min_freq, 1)

		self.itos = list(specials)
		for tok in specials:
			del counter[tok]

		max_size = None if max_size is None else max_size + len(self.itos)

		words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])
		words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)

		for word, freq in words_and_frequencies:
			if freq < min_freq or len(self.itos) == max_size:
				break
			self.itos.append(word)

		self.stoi = {tok: i for i, tok in enumerate(self.itos)}

		self.vectors = None
		if vectors is not None:
			self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)
		else:
			assert unk_init is None and vectors_cache is None

	def __eq__(self, other):
		if self.freqs != other.freqs:
			return False
		if self.stoi != other.stoi:
			return False
		if self.itos != other.itos:
			return False
		if self.vectors != other.vectors:
			return False
		return True

	def __len__(self):
		return len(self.itos)

	def vocab_rerank(self):
		self.stoi = {word: i for i, word in enumerate(self.itos)}

	def extend(self, v, sort=False):
		words = sorted(v.itos) if sort else v.itos
		for w in words:
			if w not in self.stoi:
				self.itos.append(w)
				self.stoi[w] = len(self.itos) - 1


class Vocab(TorchVocab):
	def __init__(self, counter, max_size=None, min_freq=1):
		self.pad_index = 0
		self.unk_index = 1
		self.eos_index = 2
		self.sos_index = 3
		self.mask_index = 4
		super().__init__(counter, specials=["<pad>", "<unk>", "<eos>", "<sos>", "<mask>"],
						 max_size=max_size, min_freq=min_freq)

	def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:
		pass

	def from_seq(self, seq, join=False, with_pad=False):
		pass

	@staticmethod
	def load_vocab(vocab_path: str) -> 'Vocab':
		with open(vocab_path, "rb") as f:
			return pickle.load(f)

	def save_vocab(self, vocab_path):
		with open(vocab_path, "wb") as f:
			pickle.dump(self, f)


class WordVocab(Vocab):
	def __init__(self, texts, max_size=None, min_freq=1):
		print("Building Vocab")
		counter = Counter()
		for line in tqdm.tqdm(texts):
			if isinstance(line, list):
				words = line
			else:
				words = line.replace("\n", "").replace("\t", "").split()

			for word in words:
				counter[word] += 1
		super().__init__(counter, max_size=max_size, min_freq=min_freq)

	def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):
		if isinstance(sentence, str):
			sentence = sentence.split()

		seq = [self.stoi.get(word, self.unk_index) for word in sentence]

		if with_eos:
			seq += [self.eos_index]  # this would be index 1
		if with_sos:
			seq = [self.sos_index] + seq

		origin_seq_len = len(seq)

		if seq_len is None:
			pass
		elif len(seq) <= seq_len:
			seq += [self.pad_index for _ in range(seq_len - len(seq))]
		else:
			seq = seq[:seq_len]

		return (seq, origin_seq_len) if with_len else seq

	def from_seq(self, seq, join=False, with_pad=False):
		words = [self.itos[idx]
				 if idx < len(self.itos)
				 else "<%d>" % idx
				 for idx in seq
				 if not with_pad or idx != self.pad_index]

		return " ".join(words) if join else words

	@staticmethod
	def load_vocab(vocab_path: str) -> 'WordVocab':
		with open(vocab_path, "rb") as f:
			return pickle.load(f)


def build():
	import argparse

	parser = argparse.ArgumentParser()
	parser.add_argument("-c", "--corpus_path", required=True, type=str)
	parser.add_argument("-o", "--output_path", required=True, type=str)
	parser.add_argument("-s", "--vocab_size", type=int, default=None)
	parser.add_argument("-e", "--encoding", type=str, default="utf-8")
	parser.add_argument("-m", "--min_freq", type=int, default=1)
	args = parser.parse_args()

	with open(args.corpus_path, "r", encoding=args.encoding) as f:
		vocab = WordVocab(f, max_size=args.vocab_size, min_freq=args.min_freq)

	print("VOCAB SIZE:", len(vocab))
	vocab.save_vocab(args.output_path)

from collections import deque
import logging
from multiprocessing.process import BaseProcess
from multiprocessing.connection import Connection
import random
import traceback
import select
from queue import Full
from typing import List, Iterator, Optional, Iterable, Union, TypeVar, Tuple, Any


import torch
import torch.multiprocessing as mp

from allennlp.common.util import lazy_groups_of, shuffle_iterable
from allennlp.common.tqdm import Tqdm
from allennlp.data.instance import Instance
from allennlp.data.data_loaders.data_loader import DataLoader, TensorDict
from allennlp.data.data_loaders.data_collator import DataCollator, DefaultDataCollator
from allennlp.data.dataset_readers import DatasetReader, WorkerInfo, DatasetReaderInput
from allennlp.data.fields import TextField
from allennlp.data.samplers import BatchSampler
from allennlp.data.vocabulary import Vocabulary
import allennlp.nn.util as nn_util


logger = logging.getLogger(__name__)


_T = TypeVar("_T")


@DataLoader.register("multiprocess")
class MultiProcessDataLoader(DataLoader):

	def __init__(
		self,
		reader: DatasetReader,
		data_path: DatasetReaderInput,
		*,
		batch_size: int = None,
		drop_last: bool = False,
		shuffle: bool = False,
		batch_sampler: BatchSampler = None,
		batches_per_epoch: int = None,
		num_workers: int = 0,
		max_instances_in_memory: int = None,
		start_method: str = "fork",
		cuda_device: Optional[Union[int, str, torch.device]] = None,
		quiet: bool = False,
		collate_fn: DataCollator = DefaultDataCollator(),
	) -> None:
		if num_workers is not None and num_workers < 0:
			raise ValueError("num_workers cannot be a negative number")

		if batch_size is not None and batch_size < 1:
			raise ValueError("batch_size must be at least 1")

		if batch_sampler is not None:
			if batch_size is not None:
				raise ValueError("batch_sampler option is mutually exclusive with batch_size")

			if drop_last:
				raise ValueError("batch_sampler option is mutually exclusive with drop_last")

			if shuffle:
				raise ValueError("batch_sampler option is mutually exclusive with shuffle")
		elif batch_size is None:
			raise ValueError("batch_size is required when batch_sampler is not supplied")

		if batches_per_epoch is not None and batches_per_epoch < 1:
			raise ValueError("batches_per_epoch must be at least 1")

		if max_instances_in_memory is not None:
			if batch_size is not None and max_instances_in_memory < batch_size:
				raise ValueError("max_instances_in_memory must be at least batch_size")
			elif max_instances_in_memory < 1:
				raise ValueError("max_instances_in_memory must be at least 1")

		self.reader = reader
		self.data_path = data_path
		self.batch_size = batch_size
		self.drop_last = drop_last
		self.shuffle = shuffle
		self.batch_sampler = batch_sampler
		self.batches_per_epoch = batches_per_epoch
		self.num_workers = num_workers
		self.collate_fn = collate_fn
		self.max_instances_in_memory = max_instances_in_memory
		self.start_method = start_method
		self.quiet = quiet
		self.cuda_device: Optional[torch.device] = None
		if cuda_device is not None:
			if not isinstance(cuda_device, torch.device):
				self.cuda_device = torch.device(cuda_device)
			else:
				self.cuda_device = cuda_device

		self._worker_cuda_safe = self.start_method in {"spawn", "forkserver"}

		effective_batch_size = (
			self.batch_size if self.batch_sampler is None else self.batch_sampler.get_batch_size()
		)
		self._max_instance_queue_size = (
			None
			if max_instances_in_memory is None
			else 2 * self.num_workers * max_instances_in_memory
		)
		self._max_batch_queue_size = (
			None
			if max_instances_in_memory is None
			else 2 * self.num_workers * max_instances_in_memory // (effective_batch_size or 1)
		)

		self._instances: Optional[List[Instance]] = None
		self._batch_generator: Optional[Iterator[TensorDict]] = None
		self._vocab: Optional[Vocabulary] = None

		if self.max_instances_in_memory is None:
			deque(self.iter_instances(), maxlen=0)

	def index_with(self, vocab: Vocabulary) -> None:
		self._vocab = vocab
		if self._instances:
			for instance in self._instances:
				instance.index_fields(vocab)

	def __len__(self) -> int:
		if self.batches_per_epoch is not None:
			return self.batches_per_epoch
		elif self.max_instances_in_memory is None:
			if not self._instances:
				deque(self.iter_instances(), maxlen=0)

			if self.batch_sampler is not None:
				return self.batch_sampler.get_num_batches(self._instances)  # type: ignore

			num_instances = len(self._instances)  # type: ignore
			batch_size: int = self.batch_size  # type: ignore
			if self.drop_last or num_instances % batch_size == 0:
				return num_instances // batch_size
			else:
				return 1 + num_instances // batch_size
		else:
			raise TypeError

	def __iter__(self) -> Iterator[TensorDict]:
		if self._vocab is None:
			raise ValueError(
				"This DataLoader has not been indexed with a Vocabulary yet. "
				"Did you forget to call DataLoader.index_with(vocab)?"
			)

		if self.batches_per_epoch is None:
			yield from self._iter_batches()
		else:
			if self._batch_generator is not None:
				batch_generator = self._batch_generator
				self._batch_generator = None
			else:
				batch_generator = self._iter_batches()
			for i in range(self.batches_per_epoch):
				try:
					yield next(batch_generator)
				except StopIteration:  # batch_generator is exhausted
					batch_generator = self._iter_batches()  # so refresh it
					yield next(batch_generator)
			self._batch_generator = batch_generator

	def iter_instances(self) -> Iterator[Instance]:
		if self._instances:
			yield from self._instances
		else:
			if self.max_instances_in_memory is None:
				self._instances = []

			if self.num_workers <= 0:
				for instance in self._maybe_tqdm(
					self.reader.read(self.data_path), desc="loading instances"
				):
					self.reader.apply_token_indexers(instance)
					if self.max_instances_in_memory is None:
						self._instances.append(instance)  # type: ignore
					if self._vocab is not None:
						instance.index_fields(self._vocab)
					yield instance
			else:
				ctx = mp.get_context(self.start_method)
				queue: mp.JoinableQueue = (
					ctx.JoinableQueue()
					if self._max_instance_queue_size is None
					else ctx.JoinableQueue(maxsize=self._max_instance_queue_size)
				)
				workers, txs = self._start_instance_workers(queue, ctx)

				try:
					for instance in self._maybe_tqdm(
						self._gather_instances(queue), desc="loading instances"
					):
						if self.max_instances_in_memory is None:
							self._instances.append(instance)  # type: ignore
						yield instance
				finally:
					if hasattr(queue, "close"):  # for compat with different Python versions.
						queue.close()  # type: ignore[attr-defined]
					self._join_workers(workers, queue, txs)

	def set_target_device(self, device: torch.device) -> None:
		self.cuda_device = device

	def _iter_batches(self) -> Iterator[TensorDict]:
		if self._instances is not None or self.num_workers <= 0:
			for batch in self._instances_to_batches(self.iter_instances(), move_to_device=True):
				yield batch
		else:
			ctx = mp.get_context(self.start_method)

			queue: mp.JoinableQueue = (
				ctx.JoinableQueue()
				if self._max_batch_queue_size is None
				else ctx.JoinableQueue(maxsize=self._max_batch_queue_size)
			)
			workers, txs = self._start_batch_workers(queue, ctx)

			try:
				done_count: int = 0
				while done_count < self.num_workers:
					for batch, worker_error in iter(queue.get, (None, None)):
						if worker_error is not None:
							e, tb = worker_error
							raise WorkerError(e, tb)

						if not self._worker_cuda_safe and self.cuda_device is not None:
							batch = nn_util.move_to_device(batch, self.cuda_device)
						yield batch
						queue.task_done()
					done_count += 1
			finally:
				if hasattr(queue, "close"):  # for compat with different Python versions.
					queue.close()  # type: ignore[attr-defined]
				self._join_workers(workers, queue, txs)

	def _start_instance_workers(
		self, queue: mp.JoinableQueue, ctx
	) -> Tuple[List[BaseProcess], List[Connection]]:
		Tqdm.set_lock(mp.RLock())
		workers: List[BaseProcess] = []
		txs: List[Connection] = []
		for worker_id in range(self.num_workers):
			rx, tx = ctx.Pipe(duplex=False)
			worker: BaseProcess = ctx.Process(
				target=self._instance_worker,
				args=(worker_id, queue, Tqdm.get_lock(), rx),
				daemon=True,
			)
			worker.start()
			workers.append(worker)
			txs.append(tx)
		return workers, txs

	def _start_batch_workers(
		self, queue: mp.JoinableQueue, ctx
	) -> Tuple[List[BaseProcess], List[Connection]]:
		Tqdm.set_lock(mp.RLock())
		workers: List[BaseProcess] = []
		txs: List[Connection] = []
		for worker_id in range(self.num_workers):
			rx, tx = ctx.Pipe(duplex=False)
			worker: BaseProcess = ctx.Process(
				target=self._batch_worker, args=(worker_id, queue, Tqdm.get_lock(), rx), daemon=True
			)
			worker.start()
			workers.append(worker)
			txs.append(tx)
		return workers, txs

	def _join_workers(self, workers: List[BaseProcess], queue, txs: List[Connection]) -> None:
		for _ in range(len(workers)):
			try:
				queue.task_done()
			except ValueError:
				break
		for tx in txs:
			tx.send("stop")

		for i, worker in enumerate(workers):
			worker.join(1)
			if worker.is_alive():
				logger.warning("terminating worker %s", i)
				worker.terminate()

	def _safe_queue_put(
		self, worker_id: int, item: Any, queue: mp.JoinableQueue, rx: Connection
	) -> bool:
		while True:
			if rx.poll():
				logger.warning(
					"worker %d received stop message from parent, exiting now", worker_id
				)
				queue.cancel_join_thread()
				return False
			fds, _, _ = select.select([rx.fileno()], [], [], 0)
			if fds:
				logger.warning("worker %d parent process has died, exiting now", worker_id)
				queue.cancel_join_thread()
				return False
			try:
				queue.put(item, True, 0.1)
				return True
			except Full:
				continue

	def _instance_worker(
		self, worker_id: int, queue: mp.JoinableQueue, lock, rx: Connection
	) -> None:
		Tqdm.set_lock(lock)
		try:
			self.reader._set_worker_info(WorkerInfo(self.num_workers, worker_id))
			instances = self.reader.read(self.data_path)
			checked_for_token_indexers: bool = False
			for instance in instances:
				if not checked_for_token_indexers:
					for field_name, field in instance.fields.items():
						if isinstance(field, TextField) and field._token_indexers is not None:
							raise ValueError(
								f"Found a TextField ({field_name}) with token_indexers already "
								"applied, but you're using num_workers > 0 in your data loader. "
								"Make sure your dataset reader's text_to_instance() method doesn't "
								"add any token_indexers to the TextFields it creates. Instead, the token_indexers "
								"should be added to the instances in the apply_token_indexers() method of your "
								"dataset reader (which you'll have to implement if you haven't done "
								"so already)."
							)
					checked_for_token_indexers = True
				if self._safe_queue_put(worker_id, (instance, None), queue, rx):
					continue
				else:
					return
		except Exception as e:
			if not self._safe_queue_put(
				worker_id, (None, (repr(e), traceback.format_exc())), queue, rx
			):
				return

		queue.put((None, None))

		queue.join()

	def _batch_worker(self, worker_id: int, queue: mp.JoinableQueue, lock, rx: Connection) -> None:
		Tqdm.set_lock(lock)
		try:
			self.reader._set_worker_info(WorkerInfo(self.num_workers, worker_id))
			instances = self.reader.read(self.data_path)
			for batch in self._instances_to_batches(
				instances, move_to_device=self._worker_cuda_safe
			):
				if self._safe_queue_put(worker_id, (batch, None), queue, rx):
					continue
				else:
					return
		except Exception as e:
			if not self._safe_queue_put(
				worker_id, (None, (repr(e), traceback.format_exc())), queue, rx
			):
				return

		queue.put((None, None))

		queue.join()

	def _gather_instances(self, queue: mp.JoinableQueue) -> Iterable[Instance]:
		done_count: int = 0
		while done_count < self.num_workers:
			for instance, worker_error in iter(queue.get, (None, None)):
				if worker_error is not None:
					e, tb = worker_error
					raise WorkerError(e, tb)

				self.reader.apply_token_indexers(instance)
				if self._vocab is not None:
					instance.index_fields(self._vocab)
				yield instance
				queue.task_done()
			done_count += 1

	def _index_instance(self, instance: Instance) -> Instance:
		self.reader.apply_token_indexers(instance)
		assert self._vocab is not None
		instance.index_fields(self._vocab)
		return instance

	def _instances_to_batches(
		self, instance_iterator: Iterable[Instance], move_to_device
	) -> Iterator[TensorDict]:
		instance_iterator = (self._index_instance(instance) for instance in instance_iterator)

		if move_to_device and self.cuda_device is not None:
			tensorize = lambda batch: nn_util.move_to_device(  # noqa: E731
				self.collate_fn(batch), self.cuda_device
			)
		else:
			tensorize = self.collate_fn

		if self.batch_sampler is not None:
			instance_chunks: Iterable[List[Instance]]

			if self.max_instances_in_memory is not None:
				instance_chunks = lazy_groups_of(instance_iterator, self.max_instances_in_memory)
			else:
				instance_chunks = [list(instance_iterator)]

			for instances in instance_chunks:
				batches = (
					[instances[i] for i in batch_indices]
					for batch_indices in self.batch_sampler.get_batch_indices(instances)
				)
				for batch in batches:
					yield tensorize(batch)
		else:
			assert self.batch_size is not None

			if self.shuffle:
				if self.max_instances_in_memory is not None:
					instance_iterator = shuffle_iterable(
						instance_iterator,
						self.max_instances_in_memory,
					)
				else:
					instance_iterator = list(instance_iterator)
					random.shuffle(instance_iterator)

			for batch in lazy_groups_of(instance_iterator, self.batch_size):
				if self.drop_last and len(batch) < self.batch_size:
					break
				yield tensorize(batch)

	def _maybe_tqdm(self, iterator: Iterable[_T], **tqdm_kwargs) -> Iterable[_T]:
		if self.quiet:
			return iterator
		return Tqdm.tqdm(iterator, **tqdm_kwargs)


class WorkerError(Exception):

	def __init__(self, original_err_repr: str, traceback: List[str]) -> None:
		super().__init__(
			f"worker raised {original_err_repr}\n\n"
			"  Traceback from worker:\n  " + "".join(traceback)
			.replace("Traceback (most recent call last):\n", "")
			.replace("\n", "\n  ")
		)

from typing import Optional


import torch

from allennlp.common.checks import ConfigurationError
from allennlp.modules.span_extractors.span_extractor import SpanExtractor
from allennlp.modules.token_embedders.embedding import Embedding
from allennlp.nn import util


class SpanExtractorWithSpanWidthEmbedding(SpanExtractor):

	def __init__(
		self,
		input_dim: int,
		num_width_embeddings: int = None,
		span_width_embedding_dim: int = None,
		bucket_widths: bool = False,
	) -> None:
		super().__init__()
		self._input_dim = input_dim
		self._num_width_embeddings = num_width_embeddings
		self._bucket_widths = bucket_widths

		self._span_width_embedding: Optional[Embedding] = None
		if num_width_embeddings is not None and span_width_embedding_dim is not None:
			self._span_width_embedding = Embedding(
				num_embeddings=num_width_embeddings, embedding_dim=span_width_embedding_dim
			)
		elif num_width_embeddings is not None or span_width_embedding_dim is not None:
			raise ConfigurationError(
				"To use a span width embedding representation, you must"
				"specify both num_width_embeddings and span_width_embedding_dim."
			)

	def forward(
		self,
		sequence_tensor: torch.FloatTensor,
		span_indices: torch.LongTensor,
		sequence_mask: torch.BoolTensor = None,
		span_indices_mask: torch.BoolTensor = None,
	):
		span_embeddings = self._embed_spans(
			sequence_tensor, span_indices, sequence_mask, span_indices_mask
		)
		if self._span_width_embedding is not None:
			widths_minus_one = span_indices[..., 1] - span_indices[..., 0]

			if self._bucket_widths:
				widths_minus_one = util.bucket_values(
					widths_minus_one, num_total_buckets=self._num_width_embeddings  # type: ignore
				)

			span_width_embeddings = self._span_width_embedding(widths_minus_one)
			span_embeddings = torch.cat([span_embeddings, span_width_embeddings], -1)

		if span_indices_mask is not None:
			return span_embeddings * span_indices_mask.unsqueeze(-1)

		return span_embeddings

	def get_input_dim(self) -> int:
		return self._input_dim

	def _embed_spans(
		self,
		sequence_tensor: torch.FloatTensor,
		span_indices: torch.LongTensor,
		sequence_mask: torch.BoolTensor = None,
		span_indices_mask: torch.BoolTensor = None,
	) -> torch.Tensor:
		raise NotImplementedError

import torch
from torch.nn.parameter import Parameter

from allennlp.modules.span_extractors.span_extractor import SpanExtractor
from allennlp.modules.span_extractors.span_extractor_with_span_width_embedding import (
	SpanExtractorWithSpanWidthEmbedding,
)
from allennlp.nn import util


@SpanExtractor.register("endpoint")
class EndpointSpanExtractor(SpanExtractorWithSpanWidthEmbedding):

	def __init__(
		self,
		input_dim: int,
		combination: str = "x,y",
		num_width_embeddings: int = None,
		span_width_embedding_dim: int = None,
		bucket_widths: bool = False,
		use_exclusive_start_indices: bool = False,
	) -> None:
		super().__init__(
			input_dim=input_dim,
			num_width_embeddings=num_width_embeddings,
			span_width_embedding_dim=span_width_embedding_dim,
			bucket_widths=bucket_widths,
		)
		self._combination = combination

		self._use_exclusive_start_indices = use_exclusive_start_indices
		if use_exclusive_start_indices:
			self._start_sentinel = Parameter(torch.randn([1, 1, int(input_dim)]))

	def get_output_dim(self) -> int:
		combined_dim = util.get_combined_dim(self._combination, [self._input_dim, self._input_dim])
		if self._span_width_embedding is not None:
			return combined_dim + self._span_width_embedding.get_output_dim()
		return combined_dim

	def _embed_spans(
		self,
		sequence_tensor: torch.FloatTensor,
		span_indices: torch.LongTensor,
		sequence_mask: torch.BoolTensor = None,
		span_indices_mask: torch.BoolTensor = None,
	) -> None:
		span_starts, span_ends = [index.squeeze(-1) for index in span_indices.split(1, dim=-1)]

		if span_indices_mask is not None:
			span_starts = span_starts * span_indices_mask
			span_ends = span_ends * span_indices_mask

		if not self._use_exclusive_start_indices:
			if sequence_tensor.size(-1) != self._input_dim:
				raise ValueError(
					f"Dimension mismatch expected ({sequence_tensor.size(-1)}) "
					f"received ({self._input_dim})."
				)
			start_embeddings = util.batched_index_select(sequence_tensor, span_starts)
			end_embeddings = util.batched_index_select(sequence_tensor, span_ends)

		else:
			exclusive_span_starts = span_starts - 1
			start_sentinel_mask = (exclusive_span_starts == -1).unsqueeze(-1)
			exclusive_span_starts = exclusive_span_starts * ~start_sentinel_mask.squeeze(-1)

			if (exclusive_span_starts < 0).any():
				raise ValueError(
					f"Adjusted span indices must lie inside the the sequence tensor, "
					f"but found: exclusive_span_starts: {exclusive_span_starts}."
				)

			start_embeddings = util.batched_index_select(sequence_tensor, exclusive_span_starts)
			end_embeddings = util.batched_index_select(sequence_tensor, span_ends)

			start_embeddings = (
				start_embeddings * ~start_sentinel_mask + start_sentinel_mask * self._start_sentinel
			)

		combined_tensors = util.combine_tensors(
			self._combination, [start_embeddings, end_embeddings]
		)

		return combined_tensors

from __future__ import print_function
import math
import torch.nn as nn
import numpy as np

class Loss(object):

	def __init__(self, name, criterion):
		self.name = name
		self.criterion = criterion
		if not issubclass(type(self.criterion), nn.modules.loss._Loss):
			raise ValueError("Criterion has to be a subclass of torch.nn._Loss")
		self.acc_loss = 0
		self.norm_term = 0

	def reset(self):
		self.acc_loss = 0
		self.norm_term = 0

	def get_loss(self):
		raise NotImplementedError

	def eval_batch(self, outputs, target):
		raise NotImplementedError

	def cuda(self):
		self.criterion.cuda()

	def backward(self):
		if type(self.acc_loss) is int:
			raise ValueError("No loss to back propagate.")
		self.acc_loss.backward()

class NLLLoss(Loss):

	_NAME = "Avg NLLLoss"

	def __init__(self, weight=None, mask=None, size_average=True):
		self.mask = mask
		self.size_average = size_average
		if mask is not None:
			if weight is None:
				raise ValueError("Must provide weight with a mask.")
			weight[mask] = 0

		super(NLLLoss, self).__init__(
			self._NAME,
			nn.NLLLoss(weight=weight, size_average=size_average))

	def get_loss(self):
		if isinstance(self.acc_loss, int):
			return 0
		loss = self.acc_loss.data.item()
		if self.size_average:
			loss /= self.norm_term
		return loss

	def eval_batch(self, outputs, target):
		self.acc_loss += self.criterion(outputs, target)
		self.norm_term += 1

class Perplexity(NLLLoss):

	_NAME = "Perplexity"
	_MAX_EXP = 100

	def __init__(self, weight=None, mask=None):
		super(Perplexity, self).__init__(weight=weight, mask=mask, size_average=False)

	def eval_batch(self, outputs, target):
		self.acc_loss += self.criterion(outputs, target)
		if self.mask is None:
			self.norm_term += np.prod(target.size())
		else:
			self.norm_term += target.data.ne(self.mask).sum()

	def get_loss(self):
		nll = super(Perplexity, self).get_loss()
		nll /= self.norm_term.item()
		if nll > Perplexity._MAX_EXP:
			print("WARNING: Loss exceeded maximum value, capping to e^100")
			return math.exp(Perplexity._MAX_EXP)
		return math.exp(nll)

from typing import Optional

from fairscale.nn.checkpoint import checkpoint_wrapper

import torch.nn as nn

from allennlp.nn.checkpoint.checkpoint_wrapper import CheckpointWrapper


@CheckpointWrapper.register("fairscale")
class FairScaleCheckpointWrapper(CheckpointWrapper):

	def __init__(self, offload_to_cpu: Optional[bool] = True) -> None:
		self._offload_to_cpu = offload_to_cpu

	def wrap_module(
		self,
		module: nn.Module,
		**kwargs,
	) -> nn.Module:
		if "offload_to_cpu" not in kwargs and self._offload_to_cpu is not None:
			kwargs["offload_to_cpu"] = self._offload_to_cpu
		return checkpoint_wrapper(module, **kwargs)


from typing import Optional, Dict, Any, List
import argparse
import sys
import json
import logging


from allennlp.commands.subcommand import Subcommand
from allennlp.common.checks import check_for_gpu, ConfigurationError
from allennlp.models.archival import load_archive
from allennlp.predictors.predictor import Predictor

logger = logging.getLogger(__name__)

try:
	from allennlp.confidence_checks.task_checklists.task_suite import TaskSuite
except ImportError:
	raise


@Subcommand.register("checklist")
class CheckList(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:

		description = """Run the specified model through a checklist suite."""
		subparser = parser.add_parser(
			self.name,
			description=description,
			help="Run a trained model through a checklist suite.",
		)

		subparser.add_argument(
			"archive_file", type=str, help="The archived model to make predictions with"
		)

		subparser.add_argument("task", type=str, help="The name of the task suite")

		subparser.add_argument("--checklist-suite", type=str, help="The checklist suite path")

		subparser.add_argument(
			"--capabilities",
			nargs="+",
			default=[],
			help=('An optional list of strings of capabilities. Eg. "[Vocabulary, Robustness]"'),
		)

		subparser.add_argument(
			"--max-examples",
			type=int,
			default=None,
			help="Maximum number of examples to check per test.",
		)

		subparser.add_argument(
			"--task-suite-args",
			type=str,
			default="",
			help=(
				"An optional JSON structure used to provide additional parameters to the task suite"
			),
		)

		subparser.add_argument(
			"--print-summary-args",
			type=str,
			default="",
			help=(
				"An optional JSON structure used to provide additional "
				"parameters for printing test summary"
			),
		)

		subparser.add_argument("--output-file", type=str, help="Path to output file")

		subparser.add_argument(
			"--cuda-device", type=int, default=-1, help="ID of GPU to use (if any)"
		)

		subparser.add_argument(
			"--predictor", type=str, help="Optionally specify a specific predictor to use"
		)

		subparser.add_argument(
			"--predictor-args",
			type=str,
			default="",
			help=(
				"An optional JSON structure used to provide additional parameters to the predictor"
			),
		)

		subparser.set_defaults(func=_run_suite)

		return subparser


def _get_predictor(args: argparse.Namespace) -> Predictor:
	check_for_gpu(args.cuda_device)
	archive = load_archive(
		args.archive_file,
		cuda_device=args.cuda_device,
	)

	predictor_args = args.predictor_args.strip()
	if len(predictor_args) <= 0:
		predictor_args = {}
	else:
		predictor_args = json.loads(predictor_args)

	return Predictor.from_archive(
		archive,
		args.predictor,
		extra_args=predictor_args,
	)


def _get_task_suite(args: argparse.Namespace) -> TaskSuite:
	available_tasks = TaskSuite.list_available()
	if args.task in available_tasks:
		suite_name = args.task
	else:
		raise ConfigurationError(
			f"'{args.task}' is not a recognized task suite. "
			f"Available tasks are: {available_tasks}."
		)

	file_path = args.checklist_suite

	task_suite_args = args.task_suite_args.strip()
	if len(task_suite_args) <= 0:
		task_suite_args = {}
	else:
		task_suite_args = json.loads(task_suite_args)

	return TaskSuite.constructor(
		name=suite_name,
		suite_file=file_path,
		extra_args=task_suite_args,
	)


class _CheckListManager:
	def __init__(
		self,
		task_suite: TaskSuite,
		predictor: Predictor,
		capabilities: Optional[List[str]] = None,
		max_examples: Optional[int] = None,
		output_file: Optional[str] = None,
		print_summary_args: Optional[Dict[str, Any]] = None,
	) -> None:
		self._task_suite = task_suite
		self._predictor = predictor
		self._capabilities = capabilities
		self._max_examples = max_examples
		self._output_file = None if output_file is None else open(output_file, "w")
		self._print_summary_args = print_summary_args or {}

		if capabilities:
			self._print_summary_args["capabilities"] = capabilities

	def run(self) -> None:
		self._task_suite.run(
			self._predictor, capabilities=self._capabilities, max_examples=self._max_examples
		)

		output_file = self._output_file or sys.stdout
		self._task_suite.summary(file=output_file, **self._print_summary_args)

		if self._output_file is not None:
			self._output_file.close()


def _run_suite(args: argparse.Namespace) -> None:

	task_suite = _get_task_suite(args)
	predictor = _get_predictor(args)

	print_summary_args = args.print_summary_args.strip()
	if len(print_summary_args) <= 0:
		print_summary_args = {}
	else:
		print_summary_args = json.loads(print_summary_args)

	capabilities = args.capabilities
	max_examples = args.max_examples

	manager = _CheckListManager(
		task_suite,
		predictor,
		capabilities,
		max_examples,
		args.output_file,
		print_summary_args,
	)
	manager.run()

from .model import BERT

import torch


from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention
from allennlp.nn import util


@MatrixAttention.register("cosine")
class CosineMatrixAttention(MatrixAttention):

	def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:
		a_norm = matrix_1 / (
			matrix_1.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(matrix_1.dtype)
		)
		b_norm = matrix_2 / (
			matrix_2.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(matrix_2.dtype)
		)
		return torch.bmm(a_norm, b_norm.transpose(-1, -2))

from allennlp.common.util import JsonDict
from allennlp.data import Instance


def get_fields_to_compare(
	inputs: JsonDict, instance: Instance, input_field_to_attack: str
) -> JsonDict:
	fields_to_compare = {
		key: instance[key]
		for key in instance.fields
		if key not in inputs
		and key != input_field_to_attack
		and key != "metadata"
		and key != "output"
	}
	return fields_to_compare


def instance_has_changed(instance: Instance, fields_to_compare: JsonDict):
	if "clusters" in fields_to_compare:
		original_clusters = set(tuple(x) for x in fields_to_compare["clusters"])
		new_clusters = set(tuple(x) for x in instance["clusters"])  # type: ignore
		return original_clusters != new_clusters
	if any(instance[field] != fields_to_compare[field] for field in fields_to_compare):
		return True
	return False

import torch

from allennlp.common import FromParams
from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention
from allennlp.modules.transformer.transformer_module import TransformerModule
from allennlp.modules.transformer.util import apply_mask


class BiModalAttention(TransformerModule, FromParams):

	def __init__(
		self,
		hidden_size1: int,
		hidden_size2: int,
		combined_hidden_size: int,
		num_attention_heads: int,
		dropout1: float = 0.0,
		dropout2: float = 0.0,
		scoring_func1: str = "scaled_dot_product",
		scoring_func2: str = "scaled_dot_product",
	):
		super().__init__()
		if combined_hidden_size % num_attention_heads != 0:
			raise ValueError(
				"The hidden size (%d) is not a multiple of the number of attention "
				"heads (%d)" % (combined_hidden_size, num_attention_heads)
			)

		self.num_attention_heads = num_attention_heads
		self.attention_head_size = int(combined_hidden_size / num_attention_heads)

		self.all_head_size = self.num_attention_heads * self.attention_head_size


		self.query1 = torch.nn.Linear(hidden_size1, self.all_head_size)
		self.key1 = torch.nn.Linear(hidden_size1, self.all_head_size)
		self.value1 = torch.nn.Linear(hidden_size1, self.all_head_size)

		self.scoring_func1 = scoring_func1
		self.attn1 = MatrixAttention.by_name(self.scoring_func1)()
		self.dropout1 = torch.nn.Dropout(dropout1)


		self.query2 = torch.nn.Linear(hidden_size2, self.all_head_size)
		self.key2 = torch.nn.Linear(hidden_size2, self.all_head_size)
		self.value2 = torch.nn.Linear(hidden_size2, self.all_head_size)

		self.scoring_func2 = scoring_func2
		self.attn2 = MatrixAttention.by_name(self.scoring_func2)()
		self.dropout2 = torch.nn.Dropout(dropout2)

	def _transpose_for_scores(self, x):
		new_x_shape = x.size()[:-1] + (
			self.num_attention_heads,
			self.attention_head_size,
		)
		x = x.view(*new_x_shape)
		return x.permute(0, 2, 1, 3)

	def forward(
		self,
		input_tensor1,
		input_tensor2,
		attention_mask1=None,
		attention_mask2=None,
		co_attention_mask=None,
	):

		mixed_query_layer1 = self.query1(input_tensor1)
		mixed_key_layer1 = self.key1(input_tensor1)
		mixed_value_layer1 = self.value1(input_tensor1)

		query_layer1 = self._transpose_for_scores(mixed_query_layer1)
		key_layer1 = self._transpose_for_scores(mixed_key_layer1)
		value_layer1 = self._transpose_for_scores(mixed_value_layer1)

		mixed_query_layer2 = self.query2(input_tensor2)
		mixed_key_layer2 = self.key2(input_tensor2)
		mixed_value_layer2 = self.value2(input_tensor2)

		query_layer2 = self._transpose_for_scores(mixed_query_layer2)
		key_layer2 = self._transpose_for_scores(mixed_key_layer2)
		value_layer2 = self._transpose_for_scores(mixed_value_layer2)

		attention_scores1 = self.attn1(query_layer2, key_layer1)
		if attention_mask1 is not None:
			attention_scores1 = apply_mask(attention_scores1, attention_mask1)
		if co_attention_mask is not None:
			attention_scores1 = apply_mask(attention_scores1, co_attention_mask.permute(0, 1, 3, 2))

		attention_probs1 = torch.nn.Softmax(dim=-1)(attention_scores1)

		attention_probs1 = self.dropout1(attention_probs1)

		context_layer1 = torch.matmul(attention_probs1, value_layer1)
		context_layer1 = context_layer1.permute(0, 2, 1, 3).contiguous()
		new_context_layer_shape1 = context_layer1.size()[:-2] + (self.all_head_size,)
		context_layer1 = context_layer1.view(*new_context_layer_shape1)

		attention_scores2 = self.attn2(query_layer1, key_layer2)
		if attention_mask2 is not None:
			attention_scores2 = apply_mask(attention_scores2, attention_mask2)
		if co_attention_mask is not None:
			attention_scores2 = apply_mask(attention_scores2, co_attention_mask)

		attention_probs2 = torch.nn.Softmax(dim=-1)(attention_scores2)

		attention_probs2 = self.dropout2(attention_probs2)

		context_layer2 = torch.matmul(attention_probs2, value_layer2)
		context_layer2 = context_layer2.permute(0, 2, 1, 3).contiguous()
		new_context_layer_shape2 = context_layer2.size()[:-2] + (self.all_head_size,)
		context_layer2 = context_layer2.view(*new_context_layer_shape2)

		return context_layer1, context_layer2

import os
import sys
import torch
import torch.nn as nn

from torch.distributed._tensor import Shard

from torch.distributed.tensor.parallel import (
	parallelize_module,
	ColwiseParallel,
	RowwiseParallel,
)

from log_utils import rank_log, get_logger, verify_min_gpu_count


_min_gpu_count = 2

if not verify_min_gpu_count(min_gpus=_min_gpu_count):
	print(f"Unable to locate sufficient {_min_gpu_count} gpus to run this example. Exiting.")
	sys.exit()


from torch.distributed._tensor.device_mesh import init_device_mesh





class ToyModel(nn.Module):

	def __init__(self):
		super().__init__()
		self.in_proj = nn.Linear(10, 32)
		self.relu = nn.ReLU()
		self.out_proj = nn.Linear(32, 5)

	def forward(self, x):
		return self.out_proj(self.relu(self.in_proj(x)))


logger = get_logger()

device_mesh = init_device_mesh(
	device_type="cuda", mesh_shape=(int(os.environ["WORLD_SIZE"]),)
)

_rank = device_mesh.get_rank()

print(f"Starting PyTorch Sequence Parallel example on rank {_rank}.")

rank_log(_rank, logger, f"Device Mesh created: {device_mesh=}")

model = ToyModel().to("cuda")

sp_model = parallelize_module(
	module=model,
	device_mesh=device_mesh,
	parallelize_plan={
		"in_proj": ColwiseParallel(input_layouts=Shard(0)),
		"out_proj": RowwiseParallel(output_layouts=Shard(0)),
	},
)


lr = 0.25
optimizer = torch.optim.AdamW(sp_model.parameters(), lr=lr, foreach=True)


num_iters = 10
rank_log(_rank, logger, "Sequence Parallel training starting...")

for i in range(num_iters):
	inp = torch.rand(20, 10, device="cuda")
	output = sp_model(inp)
	output.sum().backward()
	optimizer.step()
	rank_log(_rank, logger, f"Sequence Parallel iter {i} completed")

rank_log(_rank, logger, "Sequence Parallel training completed!")

import argparse
import gym
import os
import threading
import time

import torch
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributed.rpc import RRef, rpc_sync, rpc_async, remote
from torch.distributions import Categorical


NUM_STEPS = 500
AGENT_NAME = "agent"
OBSERVER_NAME = "observer{}"

parser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example')
parser.add_argument('--gamma', type=float, default=1.0, metavar='G',
					help='discount factor (default: 1.0)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
					help='random seed (default: 543)')
parser.add_argument('--num-episode', type=int, default=10, metavar='E',
					help='number of episodes (default: 10)')
args = parser.parse_args()

torch.manual_seed(args.seed)


class Policy(nn.Module):
	def __init__(self, batch=True):
		super(Policy, self).__init__()
		self.affine1 = nn.Linear(4, 128)
		self.dropout = nn.Dropout(p=0.6)
		self.affine2 = nn.Linear(128, 2)
		self.dim = 2 if batch else 1

	def forward(self, x):
		x = self.affine1(x)
		x = self.dropout(x)
		x = F.relu(x)
		action_scores = self.affine2(x)
		return F.softmax(action_scores, dim=self.dim)


class Observer:
	def __init__(self, batch=True):
		self.id = rpc.get_worker_info().id - 1
		self.env = gym.make('CartPole-v1')
		self.env.seed(args.seed)
		self.select_action = Agent.select_action_batch if batch else Agent.select_action

	def run_episode(self, agent_rref, n_steps):
		state, ep_reward = self.env.reset(), NUM_STEPS
		rewards = torch.zeros(n_steps)
		start_step = 0
		for step in range(n_steps):
			state = torch.from_numpy(state).float().unsqueeze(0)
			action = rpc.rpc_sync(
				agent_rref.owner(),
				self.select_action,
				args=(agent_rref, self.id, state)
			)

			state, reward, done, _ = self.env.step(action)
			rewards[step] = reward

			if done or step + 1 >= n_steps:
				curr_rewards = rewards[start_step:(step + 1)]
				R = 0
				for i in range(curr_rewards.numel() -1, -1, -1):
					R = curr_rewards[i] + args.gamma * R
					curr_rewards[i] = R
				state = self.env.reset()
				if start_step == 0:
					ep_reward = min(ep_reward, step - start_step + 1)
				start_step = step + 1

		return [rewards, ep_reward]


class Agent:
	def __init__(self, world_size, batch=True):
		self.ob_rrefs = []
		self.agent_rref = RRef(self)
		self.rewards = {}
		self.policy = Policy(batch).cuda()
		self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)
		self.running_reward = 0

		for ob_rank in range(1, world_size):
			ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))
			self.ob_rrefs.append(remote(ob_info, Observer, args=(batch,)))
			self.rewards[ob_info.id] = []

		self.states = torch.zeros(len(self.ob_rrefs), 1, 4)
		self.batch = batch
		self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}
		self.future_actions = torch.futures.Future()
		self.lock = threading.Lock()
		self.pending_states = len(self.ob_rrefs)

	@staticmethod
	@rpc.functions.async_execution
	def select_action_batch(agent_rref, ob_id, state):
		self = agent_rref.local_value()
		self.states[ob_id].copy_(state)
		future_action = self.future_actions.then(
			lambda future_actions: future_actions.wait()[ob_id].item()
		)

		with self.lock:
			self.pending_states -= 1
			if self.pending_states == 0:
				self.pending_states = len(self.ob_rrefs)
				probs = self.policy(self.states.cuda())
				m = Categorical(probs)
				actions = m.sample()
				self.saved_log_probs.append(m.log_prob(actions).t()[0])
				future_actions = self.future_actions
				self.future_actions = torch.futures.Future()
				future_actions.set_result(actions.cpu())
		return future_action

	@staticmethod
	def select_action(agent_rref, ob_id, state):
		self = agent_rref.local_value()
		probs = self.policy(state.cuda())
		m = Categorical(probs)
		action = m.sample()
		self.saved_log_probs[ob_id].append(m.log_prob(action))
		return action.item()

	def run_episode(self, n_steps=0):
		futs = []
		for ob_rref in self.ob_rrefs:
			futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps))

		rets = torch.futures.wait_all(futs)
		rewards = torch.stack([ret[0] for ret in rets]).cuda().t()
		ep_rewards = sum([ret[1] for ret in rets]) / len(rets)

		if self.batch:
			probs = torch.stack(self.saved_log_probs)
		else:
			probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))]
			probs = torch.stack(probs)

		policy_loss = -probs * rewards / len(rets)
		policy_loss.sum().backward()
		self.optimizer.step()
		self.optimizer.zero_grad()

		self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}
		self.states = torch.zeros(len(self.ob_rrefs), 1, 4)

		self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward
		return ep_rewards, self.running_reward


def run_worker(rank, world_size, n_episode, batch, print_log=True):
	os.environ['MASTER_ADDR'] = 'localhost'
	os.environ['MASTER_PORT'] = '29500'
	if rank == 0:
		rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)

		agent = Agent(world_size, batch)
		for i_episode in range(n_episode):
			last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS)

			if print_log:
				print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
					i_episode, last_reward, running_reward))
	else:
		rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)
	rpc.shutdown()


def main():
	for world_size in range(2, 12):
		delays = []
		for batch in [True, False]:
			tik = time.time()
			mp.spawn(
				run_worker,
				args=(world_size, args.num_episode, batch),
				nprocs=world_size,
				join=True
			)
			tok = time.time()
			delays.append(tok - tik)

		print(f"{world_size}, {delays[0]}, {delays[1]}")


if __name__ == '__main__':
	main()


import argparse
import json
import logging
import math
import os
import random
from pathlib import Path

import datasets
import evaluate
import numpy as np
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from utils_qa import postprocess_qa_predictions

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_MAPPING,
	AutoConfig,
	AutoModelForQuestionAnswering,
	AutoTokenizer,
	DataCollatorWithPadding,
	EvalPrediction,
	SchedulerType,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/question-answering/requirements.txt")

logger = get_logger(__name__)
MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def save_prefixed_metrics(results, output_dir, file_name: str = "all_results.json", metric_key_prefix: str = "eval"):
	for key in list(results.keys()):
		if not key.startswith(f"{metric_key_prefix}_"):
			results[f"{metric_key_prefix}_{key}"] = results.pop(key)

	with open(os.path.join(output_dir, file_name), "w") as f:
		json.dump(results, f, indent=4)


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a Question Answering task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)
	parser.add_argument(
		"--preprocessing_num_workers", type=int, default=1, help="A csv or a json file containing the training data."
	)
	parser.add_argument("--do_predict", action="store_true", help="To do prediction on the question answering model")
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--test_file", type=str, default=None, help="A csv or a json file containing the Prediction data."
	)
	parser.add_argument(
		"--max_seq_length",
		type=int,
		default=384,
		help=(
			"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
			" sequences shorter will be padded if `--pad_to_max_lengh` is passed."
		),
	)
	parser.add_argument(
		"--pad_to_max_length",
		action="store_true",
		help="If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=False,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--tokenizer_name",
		type=str,
		default=None,
		help="Pretrained tokenizer name or path if not the same as model_name",
	)
	parser.add_argument(
		"--use_slow_tokenizer",
		action="store_true",
		help="If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--doc_stride",
		type=int,
		default=128,
		help="When splitting up a long document into chunks how much stride to take between chunks.",
	)
	parser.add_argument(
		"--n_best_size",
		type=int,
		default=20,
		help="The total number of n-best predictions to generate when looking for an answer.",
	)
	parser.add_argument(
		"--null_score_diff_threshold",
		type=float,
		default=0.0,
		help=(
			"The threshold used to select the null answer: if the best answer has a score that is less than "
			"the score of the null answer minus this threshold, the null answer is selected for this example. "
			"Only useful when `version_2_with_negative=True`."
		),
	)
	parser.add_argument(
		"--version_2_with_negative",
		action="store_true",
		help="If true, some of the examples do not have an answer.",
	)
	parser.add_argument(
		"--max_answer_length",
		type=int,
		default=30,
		help=(
			"The maximum length of an answer that can be generated. This is needed because the start "
			"and end predictions are not conditioned on one another."
		),
	)
	parser.add_argument(
		"--max_train_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of training examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--max_eval_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
	)
	parser.add_argument(
		"--max_predict_samples",
		type=int,
		default=None,
		help="For debugging purposes or quicker training, truncate the number of prediction examples to this",
	)
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="Model type to use if training from scratch.",
		choices=MODEL_TYPES,
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	args = parser.parse_args()

	if (
		args.dataset_name is None
		and args.train_file is None
		and args.validation_file is None
		and args.test_file is None
	):
		raise ValueError("Need either a dataset name or a training/validation/test file.")
	else:
		if args.train_file is not None:
			extension = args.train_file.split(".")[-1]
			assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
		if args.validation_file is not None:
			extension = args.validation_file.split(".")[-1]
			assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
		if args.test_file is not None:
			extension = args.test_file.split(".")[-1]
			assert extension in ["csv", "json"], "`test_file` should be a csv or a json file."

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_qa_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		if args.test_file is not None:
			data_files["test"] = args.test_file
		extension = args.train_file.split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files, field="data")


	if args.config_name:
		config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")

	if args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(
			args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code
		)
	elif args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(
			args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code
		)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if args.model_name_or_path:
		model = AutoModelForQuestionAnswering.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)


	column_names = raw_datasets["train"].column_names

	question_column_name = "question" if "question" in column_names else column_names[0]
	context_column_name = "context" if "context" in column_names else column_names[1]
	answer_column_name = "answers" if "answers" in column_names else column_names[2]

	pad_on_right = tokenizer.padding_side == "right"

	if args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)

	max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)

	def prepare_train_features(examples):
		examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]

		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			padding="max_length" if args.pad_to_max_length else False,
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
		offset_mapping = tokenized_examples.pop("offset_mapping")

		tokenized_examples["start_positions"] = []
		tokenized_examples["end_positions"] = []

		for i, offsets in enumerate(offset_mapping):
			input_ids = tokenized_examples["input_ids"][i]
			cls_index = input_ids.index(tokenizer.cls_token_id)

			sequence_ids = tokenized_examples.sequence_ids(i)

			sample_index = sample_mapping[i]
			answers = examples[answer_column_name][sample_index]
			if len(answers["answer_start"]) == 0:
				tokenized_examples["start_positions"].append(cls_index)
				tokenized_examples["end_positions"].append(cls_index)
			else:
				start_char = answers["answer_start"][0]
				end_char = start_char + len(answers["text"][0])

				token_start_index = 0
				while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
					token_start_index += 1

				token_end_index = len(input_ids) - 1
				while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
					token_end_index -= 1

				if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
					tokenized_examples["start_positions"].append(cls_index)
					tokenized_examples["end_positions"].append(cls_index)
				else:
					while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
						token_start_index += 1
					tokenized_examples["start_positions"].append(token_start_index - 1)
					while offsets[token_end_index][1] >= end_char:
						token_end_index -= 1
					tokenized_examples["end_positions"].append(token_end_index + 1)

		return tokenized_examples

	if "train" not in raw_datasets:
		raise ValueError("--do_train requires a train dataset")
	train_dataset = raw_datasets["train"]
	if args.max_train_samples is not None:
		train_dataset = train_dataset.select(range(args.max_train_samples))

	with accelerator.main_process_first():
		train_dataset = train_dataset.map(
			prepare_train_features,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on train dataset",
		)
		if args.max_train_samples is not None:
			train_dataset = train_dataset.select(range(args.max_train_samples))

	def prepare_validation_features(examples):
		examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]

		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			padding="max_length" if args.pad_to_max_length else False,
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

		tokenized_examples["example_id"] = []

		for i in range(len(tokenized_examples["input_ids"])):
			sequence_ids = tokenized_examples.sequence_ids(i)
			context_index = 1 if pad_on_right else 0

			sample_index = sample_mapping[i]
			tokenized_examples["example_id"].append(examples["id"][sample_index])

			tokenized_examples["offset_mapping"][i] = [
				(o if sequence_ids[k] == context_index else None)
				for k, o in enumerate(tokenized_examples["offset_mapping"][i])
			]

		return tokenized_examples

	if "validation" not in raw_datasets:
		raise ValueError("--do_eval requires a validation dataset")
	eval_examples = raw_datasets["validation"]
	if args.max_eval_samples is not None:
		eval_examples = eval_examples.select(range(args.max_eval_samples))
	with accelerator.main_process_first():
		eval_dataset = eval_examples.map(
			prepare_validation_features,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on validation dataset",
		)

	if args.max_eval_samples is not None:
		eval_dataset = eval_dataset.select(range(args.max_eval_samples))

	if args.do_predict:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_examples = raw_datasets["test"]
		if args.max_predict_samples is not None:
			predict_examples = predict_examples.select(range(args.max_predict_samples))
		with accelerator.main_process_first():
			predict_dataset = predict_examples.map(
				prepare_validation_features,
				batched=True,
				num_proc=args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)
			if args.max_predict_samples is not None:
				predict_dataset = predict_dataset.select(range(args.max_predict_samples))

	for index in random.sample(range(len(train_dataset)), 3):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if args.pad_to_max_length:
		data_collator = default_data_collator
	else:
		data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)

	eval_dataset_for_model = eval_dataset.remove_columns(["example_id", "offset_mapping"])
	eval_dataloader = DataLoader(
		eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
	)

	if args.do_predict:
		predict_dataset_for_model = predict_dataset.remove_columns(["example_id", "offset_mapping"])
		predict_dataloader = DataLoader(
			predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
		)

	def post_processing_function(examples, features, predictions, stage="eval"):
		predictions = postprocess_qa_predictions(
			examples=examples,
			features=features,
			predictions=predictions,
			version_2_with_negative=args.version_2_with_negative,
			n_best_size=args.n_best_size,
			max_answer_length=args.max_answer_length,
			null_score_diff_threshold=args.null_score_diff_threshold,
			output_dir=args.output_dir,
			prefix=stage,
		)
		if args.version_2_with_negative:
			formatted_predictions = [
				{"id": k, "prediction_text": v, "no_answer_probability": 0.0} for k, v in predictions.items()
			]
		else:
			formatted_predictions = [{"id": k, "prediction_text": v} for k, v in predictions.items()]

		references = [{"id": ex["id"], "answers": ex[answer_column_name]} for ex in examples]
		return EvalPrediction(predictions=formatted_predictions, label_ids=references)

	metric = evaluate.load("squad_v2" if args.version_2_with_negative else "squad")

	def create_and_fill_np_array(start_or_end_logits, dataset, max_len):

		step = 0
		logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)
		for i, output_logit in enumerate(start_or_end_logits):  # populate columns

			batch_size = output_logit.shape[0]
			cols = output_logit.shape[1]

			if step + batch_size < len(dataset):
				logits_concat[step : step + batch_size, :cols] = output_logit
			else:
				logits_concat[step:, :cols] = output_logit[: len(dataset) - step]

			step += batch_size

		return logits_concat

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("qa_no_trainer", experiment_config)

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")

	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()

				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

	logger.info("***** Running Evaluation *****")
	logger.info(f"  Num examples = {len(eval_dataset)}")
	logger.info(f"  Batch size = {args.per_device_eval_batch_size}")

	all_start_logits = []
	all_end_logits = []

	model.eval()

	for step, batch in enumerate(eval_dataloader):
		with torch.no_grad():
			outputs = model(**batch)
			start_logits = outputs.start_logits
			end_logits = outputs.end_logits

			if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered
				start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)
				end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)

			all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())
			all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())

	max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor

	start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)
	end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)

	del all_start_logits
	del all_end_logits

	outputs_numpy = (start_logits_concat, end_logits_concat)
	prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)
	eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)
	logger.info(f"Evaluation metrics: {eval_metric}")

	if args.do_predict:
		logger.info("***** Running Prediction *****")
		logger.info(f"  Num examples = {len(predict_dataset)}")
		logger.info(f"  Batch size = {args.per_device_eval_batch_size}")

		all_start_logits = []
		all_end_logits = []

		model.eval()

		for step, batch in enumerate(predict_dataloader):
			with torch.no_grad():
				outputs = model(**batch)
				start_logits = outputs.start_logits
				end_logits = outputs.end_logits

				if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered
					start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)
					end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)

				all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())
				all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())

		max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor
		start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)
		end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)

		del all_start_logits
		del all_end_logits

		outputs_numpy = (start_logits_concat, end_logits_concat)
		prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)
		predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)
		logger.info(f"Predict metrics: {predict_metric}")

	if args.with_tracking:
		log = {
			"squad_v2" if args.version_2_with_negative else "squad": eval_metric,
			"train_loss": total_loss.item() / len(train_dataloader),
			"epoch": epoch,
			"step": completed_steps,
		}
	if args.do_predict:
		log["squad_v2_predict" if args.version_2_with_negative else "squad_predict"] = predict_metric

		accelerator.log(log, step=completed_steps)

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			logger.info(json.dumps(eval_metric, indent=4))
			save_prefixed_metrics(eval_metric, args.output_dir)


if __name__ == "__main__":
	main()

import torch.nn as nn
import torch.nn.functional as F
import torch

import math


class Attention(nn.Module):

	def forward(self, query, key, value, mask=None, dropout=None):
		scores = torch.matmul(query, key.transpose(-2, -1)) \
				 / math.sqrt(query.size(-1))

		if mask is not None:
			scores = scores.masked_fill(mask == 0, -1e9)

		p_attn = F.softmax(scores, dim=-1)

		if dropout is not None:
			p_attn = dropout(p_attn)

		return torch.matmul(p_attn, value), p_attn



import argparse
import logging

from accelerate import PartialState
from accelerate.utils import set_seed

from transformers import AutoModelForCausalLM, AutoTokenizer


logging.basicConfig(
	format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
	datefmt="%m/%d/%Y %H:%M:%S",
	level=logging.INFO,
)
logger = logging.getLogger(__name__)


def main():
	parser = argparse.ArgumentParser()
	parser.add_argument(
		"--model_name_or_path",
		default=None,
		type=str,
		required=True,
	)
	parser.add_argument("--prompt", type=str, default="")
	parser.add_argument("--length", type=int, default=20)
	parser.add_argument("--stop_token", type=str, default=None, help="Token at which text generation is stopped")
	parser.add_argument(
		"--temperature",
		type=float,
		default=1.0,
		help="temperature of 1.0 has no effect, lower tend toward greedy sampling",
	)
	parser.add_argument(
		"--repetition_penalty", type=float, default=1.0, help="primarily useful for CTRL model; in that case, use 1.2"
	)
	parser.add_argument("--k", type=int, default=0)
	parser.add_argument("--penalty_alpha", type=float, default=0.0)
	parser.add_argument("--p", type=float, default=0.9)

	parser.add_argument("--prefix", type=str, default="", help="Text added prior to input.")
	parser.add_argument("--padding_text", type=str, default="", help="Deprecated, the use of `--prefix` is preferred.")
	parser.add_argument("--xlm_language", type=str, default="", help="Optional language when used with the XLM model.")

	parser.add_argument("--seed", type=int, default=42, help="random seed for initialization")
	parser.add_argument(
		"--use_cpu",
		action="store_true",
		help="Whether or not to use cpu. If set to False, " "we will use gpu/npu or mps device if available",
	)
	parser.add_argument(
		"--fp16",
		action="store_true",
		help="Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit",
	)
	args = parser.parse_args()

	distributed_state = PartialState(cpu=args.use_cpu)

	logger.warning(f"device: {distributed_state.device}, 16-bits inference: {args.fp16}")

	if args.seed is not None:
		set_seed(args.seed)

	tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)
	model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)

	model.to(distributed_state.device)

	if args.fp16:
		model.half()

	logger.info(args)
	prompt_text = args.prompt if args.prompt else input("Model prompt >>> ")

	inputs = tokenizer(prompt_text, return_tensors="pt", add_special_tokens=False)
	inputs = {key: value.to(distributed_state.device) for key, value in inputs.items()}

	output_sequences = model.generate(
		**inputs,
		max_length=args.length + len(inputs["input_ids"][0]),
		penalty_alpha=args.penalty_alpha,
		top_k=args.k,
	)

	generated_sequences = []
	for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
		print(f"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===")
		generated_sequence = generated_sequence.tolist()

		text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, add_special_tokens=False)

		text = text[: text.find(args.stop_token) if args.stop_token else None]

		total_sequence = (
			prompt_text + text[len(tokenizer.decode(inputs["input_ids"][0], clean_up_tokenization_spaces=True)) :]
		)

		generated_sequences.append(total_sequence)
		print(total_sequence)

	return generated_sequences


if __name__ == "__main__":
	main()

from os import PathLike
from typing import Union, Sequence, Tuple, List, cast


import torch
import torchvision
from torch import FloatTensor, IntTensor

from allennlp.common.file_utils import cached_path
from allennlp.common.registrable import Registrable

OnePath = Union[str, PathLike]
ManyPaths = Sequence[OnePath]
ImagesWithSize = Tuple[FloatTensor, IntTensor]


class ImageLoader(Registrable):

	default_implementation = "torch"

	def __init__(
		self,
		*,
		size_divisibility: int = 0,
		pad_value: float = 0.0,
		device: Union[str, torch.device] = "cpu",
	) -> None:
		self.size_divisibility = size_divisibility
		self.pad_value = pad_value
		self.device = device

	def __call__(self, filename_or_filenames: Union[OnePath, ManyPaths]) -> ImagesWithSize:
		if not isinstance(filename_or_filenames, (list, tuple)):
			image, size = self([filename_or_filenames])  # type: ignore[list-item]
			return cast(FloatTensor, image.squeeze(0)), cast(IntTensor, size.squeeze(0))

		images: List[FloatTensor] = []
		sizes: List[IntTensor] = []
		for filename in filename_or_filenames:
			image = self.load(cached_path(filename)).to(self.device)
			size = cast(
				IntTensor,
				torch.tensor(
					[image.shape[-2], image.shape[-1]], dtype=torch.int32, device=self.device
				),
			)
			images.append(image)
			sizes.append(size)
		return self._pack_image_list(images, sizes)

	def load(self, filename: OnePath) -> FloatTensor:
		raise NotImplementedError()

	def _pack_image_list(
		self,
		images: List[FloatTensor],
		sizes: List[IntTensor],
	) -> ImagesWithSize:

		size_tensor = torch.stack(sizes)  # type: ignore[arg-type]

		max_size = size_tensor.max(0).values

		if self.size_divisibility > 1:
			max_size = (
				(max_size + self.size_divisibility - 1) // self.size_divisibility
			) * self.size_divisibility

		batched_shape = [len(images)] + list(images[0].shape[:-2]) + list(max_size)

		batched_images = images[0].new_full(batched_shape, self.pad_value)

		for image, batch_slice, size in zip(images, batched_images, size_tensor):
			batch_slice[..., : image.shape[-2], : image.shape[-1]].copy_(image)

		return cast(FloatTensor, batched_images), cast(IntTensor, size_tensor)


@ImageLoader.register("torch")
class TorchImageLoader(ImageLoader):

	def __init__(
		self,
		*,
		image_backend: str = None,
		resize: bool = True,
		normalize: bool = True,
		min_size: int = 800,
		max_size: int = 1333,
		pixel_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),
		pixel_std: Tuple[float, float, float] = (0.229, 0.224, 0.225),
		size_divisibility: int = 32,
		**kwargs,
	) -> None:
		super().__init__(size_divisibility=size_divisibility, **kwargs)
		if image_backend is not None:
			torchvision.set_image_backend(image_backend)
		self.resize = resize
		self.normalize = normalize
		self.min_size = min_size
		self.max_size = max_size
		self.pixel_mean = pixel_mean
		self.pixel_std = pixel_std

	def load(self, filename: OnePath) -> FloatTensor:
		image = torchvision.io.read_image(filename).float().to(self.device) / 256
		if self.normalize:
			mean = torch.as_tensor(self.pixel_mean, dtype=image.dtype, device=self.device).view(
				-1, 1, 1
			)
			std = torch.as_tensor(self.pixel_std, dtype=image.dtype, device=self.device).view(
				-1, 1, 1
			)
			image = (image - mean) / std
		if self.resize:
			min_size = min(image.shape[-2:])
			max_size = max(image.shape[-2:])
			scale_factor = self.min_size / min_size
			if max_size * scale_factor > self.max_size:
				scale_factor = self.max_size / max_size
			image = torch.nn.functional.interpolate(
				image[None],
				scale_factor=scale_factor,
				mode="bilinear",
				recompute_scale_factor=True,
				align_corners=False,
			)[0]
		return image

from .EncoderRNN import EncoderRNN
from .DecoderRNN import DecoderRNN
from .TopKDecoder import TopKDecoder
from .seq2seq import Seq2seq

import torch

from allennlp.common.registrable import Registrable


class MatrixAttention(torch.nn.Module, Registrable):

	def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:
		raise NotImplementedError

import torch

from allennlp.common import Registrable


class Regularizer(Registrable):

	default_implementation = "l2"

	def __call__(self, parameter: torch.Tensor) -> torch.Tensor:
		raise NotImplementedError

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler


@LearningRateScheduler.register("polynomial_decay")
class PolynomialDecay(LearningRateScheduler):

	def __init__(
		self,
		optimizer: torch.optim.Optimizer,
		num_epochs: int,
		num_steps_per_epoch: int,
		power=1.0,
		warmup_steps=0,
		end_learning_rate=0.0,
		last_epoch: int = -1,
	):
		super().__init__(optimizer, last_epoch)

		if num_steps_per_epoch is None:
			raise ConfigurationError(
				"'num_steps_per_epoch' is required for this LR scheduler.\n\n"
				"If you know how many batches per epoch for your training data, you can set this value "
				"directly in your config. Otherwise you'll need to use compatible settings with your data loader "
				"so that it can report an accurate number of batches per epoch. "
				"If you're using the MultiProcessDataLoader, "
				"this means you either need to set 'batches_per_epoch' "
				"or leave 'max_instances_in_memory' as None (if your entire dataset can fit into memory)."
			)

		self.power = power
		self.warmup_steps = warmup_steps
		self.total_steps = num_epochs * num_steps_per_epoch
		self.end_learning_rate = end_learning_rate

		self.steps = 0

		self.step_batch(0)

	def get_values(self):
		if self.warmup_steps > 0 and self.steps < self.warmup_steps:
			f = self.steps / self.warmup_steps
			return [f * lr for lr in self.base_values]

		if self.steps >= self.total_steps:
			return [self.end_learning_rate for _ in self.base_values]

		current_decay_steps = self.total_steps - self.steps
		total_decay_steps = self.total_steps - self.warmup_steps
		f = (current_decay_steps / total_decay_steps) ** self.power
		return [
			f * (lr - self.end_learning_rate) + self.end_learning_rate for lr in self.base_values
		]

	def step(self, metric: float = None) -> None:
		pass

	def step_batch(self, batch_num_total: int = None) -> None:
		if batch_num_total is None:
			self.steps += 1
		else:
			self.steps = batch_num_total

		for param_group, lr in zip(self.optimizer.param_groups, self.get_values()):
			param_group[self.param_group_field] = lr

import torch


from allennlp.modules.feedforward import FeedForward
from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder


@Seq2SeqEncoder.register("feedforward")
class FeedForwardEncoder(Seq2SeqEncoder):

	def __init__(self, feedforward: FeedForward) -> None:
		super().__init__()
		self._feedforward = feedforward

	def get_input_dim(self) -> int:
		return self._feedforward.get_input_dim()

	def get_output_dim(self) -> int:
		return self._feedforward.get_output_dim()

	def is_bidirectional(self) -> bool:
		return False

	def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor = None) -> torch.Tensor:
		if mask is None:
			return self._feedforward(inputs)
		else:
			outputs = self._feedforward(inputs)
			return outputs * mask.unsqueeze(dim=-1)

import os
import torch
from torch.utils.data import random_split
from torch.distributed import init_process_group, destroy_process_group
from model import GPT, GPTConfig, OptimizerConfig, create_optimizer
from trainer import Trainer, TrainerConfig
from char_dataset import CharDataset, DataConfig
from omegaconf import DictConfig
import hydra


def ddp_setup():
	init_process_group(backend="nccl")
	torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))

def get_train_objs(gpt_cfg: GPTConfig, opt_cfg: OptimizerConfig, data_cfg: DataConfig):
	dataset = CharDataset(data_cfg)
	train_len = int(len(dataset) * data_cfg.train_split)
	train_set, test_set = random_split(dataset, [train_len, len(dataset) - train_len])

	gpt_cfg.vocab_size = dataset.vocab_size
	gpt_cfg.block_size = dataset.block_size
	model = GPT(gpt_cfg)
	optimizer = create_optimizer(model, opt_cfg)
	
	return model, optimizer, train_set, test_set

@hydra.main(version_base=None, config_path=".", config_name="gpt2_train_cfg")
def main(cfg: DictConfig):
	ddp_setup()

	gpt_cfg = GPTConfig(**cfg['gpt_config'])
	opt_cfg = OptimizerConfig(**cfg['optimizer_config'])
	data_cfg = DataConfig(**cfg['data_config'])
	trainer_cfg = TrainerConfig(**cfg['trainer_config'])

	model, optimizer, train_data, test_data = get_train_objs(gpt_cfg, opt_cfg, data_cfg)
	trainer = Trainer(trainer_cfg, model, optimizer, train_data, test_data)
	trainer.train()

	destroy_process_group()


if __name__ == "__main__":
	main()

from typing import Any, Dict, List

import torch

from allennlp.common import Registrable
from allennlp.common.util import pad_sequence_to_length
from allennlp.data.tokenizers import Token
from allennlp.data.vocabulary import Vocabulary

IndexedTokenList = Dict[str, List[Any]]


class TokenIndexer(Registrable):

	default_implementation = "single_id"
	has_warned_for_as_padded_tensor = False

	def __init__(self, token_min_padding_length: int = 0) -> None:
		self._token_min_padding_length: int = token_min_padding_length

	def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
		raise NotImplementedError

	def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:
		raise NotImplementedError

	def indices_to_tokens(
		self, indexed_tokens: IndexedTokenList, vocabulary: Vocabulary
	) -> List[Token]:
		raise NotImplementedError

	def get_empty_token_list(self) -> IndexedTokenList:
		raise NotImplementedError

	def get_padding_lengths(self, indexed_tokens: IndexedTokenList) -> Dict[str, int]:
		padding_lengths = {}
		for key, token_list in indexed_tokens.items():
			padding_lengths[key] = max(len(token_list), self._token_min_padding_length)
		return padding_lengths

	def as_padded_tensor_dict(
		self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]
	) -> Dict[str, torch.Tensor]:
		tensor_dict = {}
		for key, val in tokens.items():
			if val and isinstance(val[0], bool):
				tensor = torch.BoolTensor(
					pad_sequence_to_length(val, padding_lengths[key], default_value=lambda: False)
				)
			else:
				tensor = torch.LongTensor(pad_sequence_to_length(val, padding_lengths[key]))
			tensor_dict[key] = tensor
		return tensor_dict

	def __eq__(self, other) -> bool:
		if isinstance(self, other.__class__):
			return self.__dict__ == other.__dict__
		return NotImplemented

from typing import List

from transformers.data.data_collator import DataCollatorForLanguageModeling
from allennlp.common import Registrable
from allennlp.data.batch import Batch
from allennlp.data.data_loaders.data_loader import TensorDict
from allennlp.data.instance import Instance


def allennlp_collate(instances: List[Instance]) -> TensorDict:
	batch = Batch(instances)
	return batch.as_tensor_dict()


class DataCollator(Registrable):

	default_implementation = "allennlp"

	def __call__(self, instances: List[Instance]) -> TensorDict:
		raise NotImplementedError


@DataCollator.register("allennlp")
class DefaultDataCollator(DataCollator):
	def __call__(self, instances: List[Instance]) -> TensorDict:
		return allennlp_collate(instances)


@DataCollator.register("language_model")
class LanguageModelingDataCollator(DataCollator):

	def __init__(
		self,
		model_name: str,
		mlm: bool = True,
		mlm_probability: float = 0.15,
		filed_name: str = "source",
		namespace: str = "tokens",
	):
		self._field_name = filed_name
		self._namespace = namespace
		from allennlp.common import cached_transformers

		tokenizer = cached_transformers.get_tokenizer(model_name)
		self._collator = DataCollatorForLanguageModeling(tokenizer, mlm, mlm_probability)
		if hasattr(self._collator, "mask_tokens"):
			self._mask_tokens = self._collator.mask_tokens
		else:
			self._mask_tokens = self._collator.torch_mask_tokens

	def __call__(self, instances: List[Instance]) -> TensorDict:
		tensor_dicts = allennlp_collate(instances)
		tensor_dicts = self.process_tokens(tensor_dicts)
		return tensor_dicts

	def process_tokens(self, tensor_dicts: TensorDict) -> TensorDict:
		inputs = tensor_dicts[self._field_name][self._namespace]["token_ids"]
		inputs, labels = self._mask_tokens(inputs)
		tensor_dicts[self._field_name][self._namespace]["token_ids"] = inputs
		tensor_dicts[self._field_name][self._namespace]["labels"] = labels
		return tensor_dicts

import torch

from allennlp.nn import Activation


class GatedSum(torch.nn.Module):

	def __init__(self, input_dim: int, activation: Activation = torch.nn.Sigmoid()) -> None:
		super().__init__()
		self.input_dim = input_dim
		self._gate = torch.nn.Linear(input_dim * 2, 1)
		self._activation = activation

	def get_input_dim(self):
		return self.input_dim

	def get_output_dim(self):
		return self.input_dim

	def forward(self, input_a: torch.Tensor, input_b: torch.Tensor) -> torch.Tensor:
		if input_a.size() != input_b.size():
			raise ValueError("The input must have the same size.")
		if input_a.size(-1) != self.input_dim:
			raise ValueError("Input size must match `input_dim`.")
		gate_value = self._activation(self._gate(torch.cat([input_a, input_b], -1)))
		return gate_value * input_a + (1 - gate_value) * input_b

from typing import Sequence, Dict, List, Callable

import torch
import numpy as np

from allennlp.common.checks import ConfigurationError
from allennlp.modules.layer_norm import LayerNorm
from allennlp.modules.highway import Highway
from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder

_VALID_PROJECTION_LOCATIONS = {"after_cnn", "after_highway", None}


@Seq2VecEncoder.register("cnn-highway")
class CnnHighwayEncoder(Seq2VecEncoder):

	def __init__(
		self,
		embedding_dim: int,
		filters: Sequence[Sequence[int]],
		num_highway: int,
		projection_dim: int,
		activation: str = "relu",
		projection_location: str = "after_highway",
		do_layer_norm: bool = False,
	) -> None:
		super().__init__()

		if projection_location not in _VALID_PROJECTION_LOCATIONS:
			raise ConfigurationError(f"unknown projection location: {projection_location}")

		self.input_dim = embedding_dim
		self.output_dim = projection_dim
		self._projection_location = projection_location

		if activation == "tanh":
			self._activation = torch.nn.functional.tanh
		elif activation == "relu":
			self._activation = torch.nn.functional.relu
		else:
			raise ConfigurationError(f"unknown activation {activation}")

		self._convolutions: List[torch.nn.Module] = []
		for i, (width, num) in enumerate(filters):
			conv = torch.nn.Conv1d(
				in_channels=embedding_dim, out_channels=num, kernel_size=width, bias=True
			)
			conv.weight.data.uniform_(-0.05, 0.05)
			conv.bias.data.fill_(0.0)
			self.add_module(f"char_conv_{i}", conv)  # needs to match the old ELMo name
			self._convolutions.append(conv)

		num_filters = sum(num for _, num in filters)
		if projection_location == "after_cnn":
			highway_dim = projection_dim
		else:
			highway_dim = num_filters
		self._highways = Highway(highway_dim, num_highway, activation=torch.nn.functional.relu)
		for highway_layer in self._highways._layers:
			highway_layer.weight.data.normal_(mean=0.0, std=np.sqrt(1.0 / highway_dim))
			highway_layer.bias[:highway_dim].data.fill_(0.0)
			highway_layer.bias[highway_dim:].data.fill_(2.0)

		self._projection = torch.nn.Linear(num_filters, projection_dim, bias=True)
		self._projection.weight.data.normal_(mean=0.0, std=np.sqrt(1.0 / num_filters))
		self._projection.bias.data.fill_(0.0)

		if do_layer_norm:
			self._layer_norm: Callable = LayerNorm(self.output_dim)
		else:
			self._layer_norm = lambda tensor: tensor

	def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor) -> Dict[str, torch.Tensor]:
		inputs = inputs.transpose(1, 2)

		convolutions = []
		for i in range(len(self._convolutions)):
			char_conv_i = getattr(self, f"char_conv_{i}")
			convolved = char_conv_i(inputs)

			convolved, _ = torch.max(convolved, dim=-1)
			convolved = self._activation(convolved)
			convolutions.append(convolved)

		token_embedding = torch.cat(convolutions, dim=-1)

		if self._projection_location == "after_cnn":
			token_embedding = self._projection(token_embedding)

		token_embedding = self._highways(token_embedding)

		if self._projection_location == "after_highway":
			token_embedding = self._projection(token_embedding)

		token_embedding = self._layer_norm(token_embedding)

		return token_embedding

	def get_input_dim(self) -> int:
		return self.input_dim

	def get_output_dim(self) -> int:
		return self.output_dim

from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention
from allennlp.modules.matrix_attention.bilinear_matrix_attention import BilinearMatrixAttention
from allennlp.modules.matrix_attention.cosine_matrix_attention import CosineMatrixAttention
from allennlp.modules.matrix_attention.dot_product_matrix_attention import DotProductMatrixAttention
from allennlp.modules.matrix_attention.scaled_dot_product_matrix_attention import (
	ScaledDotProductMatrixAttention,
)
from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention

from typing import Dict, List


import torch

from allennlp.common.util import pad_sequence_to_length
from allennlp.data.tokenizers import Token
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList
from allennlp.data.vocabulary import Vocabulary


def _make_bos_eos(
	character: int,
	padding_character: int,
	beginning_of_word_character: int,
	end_of_word_character: int,
	max_word_length: int,
):
	char_ids = [padding_character] * max_word_length
	char_ids[0] = beginning_of_word_character
	char_ids[1] = character
	char_ids[2] = end_of_word_character
	return char_ids


class ELMoCharacterMapper:

	max_word_length = 50

	beginning_of_sentence_character = 256  # <begin sentence>
	end_of_sentence_character = 257  # <end sentence>
	beginning_of_word_character = 258  # <begin word>
	end_of_word_character = 259  # <end word>
	padding_character = 260  # <padding>

	beginning_of_sentence_characters = _make_bos_eos(
		beginning_of_sentence_character,
		padding_character,
		beginning_of_word_character,
		end_of_word_character,
		max_word_length,
	)
	end_of_sentence_characters = _make_bos_eos(
		end_of_sentence_character,
		padding_character,
		beginning_of_word_character,
		end_of_word_character,
		max_word_length,
	)

	bos_token = "<S>"
	eos_token = "</S>"

	def __init__(self, tokens_to_add: Dict[str, int] = None) -> None:
		self.tokens_to_add = tokens_to_add or {}

	def convert_word_to_char_ids(self, word: str) -> List[int]:
		if word in self.tokens_to_add:
			char_ids = [ELMoCharacterMapper.padding_character] * ELMoCharacterMapper.max_word_length
			char_ids[0] = ELMoCharacterMapper.beginning_of_word_character
			char_ids[1] = self.tokens_to_add[word]
			char_ids[2] = ELMoCharacterMapper.end_of_word_character
		elif word == ELMoCharacterMapper.bos_token:
			char_ids = ELMoCharacterMapper.beginning_of_sentence_characters
		elif word == ELMoCharacterMapper.eos_token:
			char_ids = ELMoCharacterMapper.end_of_sentence_characters
		else:
			word_encoded = word.encode("utf-8", "ignore")[
				: (ELMoCharacterMapper.max_word_length - 2)
			]
			char_ids = [ELMoCharacterMapper.padding_character] * ELMoCharacterMapper.max_word_length
			char_ids[0] = ELMoCharacterMapper.beginning_of_word_character
			for k, chr_id in enumerate(word_encoded, start=1):
				char_ids[k] = chr_id
			char_ids[len(word_encoded) + 1] = ELMoCharacterMapper.end_of_word_character

		return [c + 1 for c in char_ids]

	def __eq__(self, other) -> bool:
		if isinstance(self, other.__class__):
			return self.__dict__ == other.__dict__
		return NotImplemented


@TokenIndexer.register("elmo_characters")
class ELMoTokenCharactersIndexer(TokenIndexer):

	def __init__(
		self,
		namespace: str = "elmo_characters",
		tokens_to_add: Dict[str, int] = None,
		token_min_padding_length: int = 0,
	) -> None:
		super().__init__(token_min_padding_length)
		self._namespace = namespace
		self._mapper = ELMoCharacterMapper(tokens_to_add)

	def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
		pass

	def get_empty_token_list(self) -> IndexedTokenList:
		return {"elmo_tokens": []}

	def tokens_to_indices(
		self, tokens: List[Token], vocabulary: Vocabulary
	) -> Dict[str, List[List[int]]]:


		return {
			"elmo_tokens": [self._mapper.convert_word_to_char_ids(t.ensure_text()) for t in tokens]
		}

	def as_padded_tensor_dict(
		self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]
	) -> Dict[str, torch.Tensor]:
		tensor_dict = {}

		def padding_token():
			return [0] * ELMoCharacterMapper.max_word_length

		tensor_dict["elmo_tokens"] = torch.LongTensor(
			pad_sequence_to_length(
				tokens["elmo_tokens"], padding_lengths["elmo_tokens"], default_value=padding_token
			)
		)
		return tensor_dict

from collections import deque, defaultdict
import logging
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union, Deque, Set

import torch

from allennlp.data import TensorDict
from allennlp.nn.util import tiny_value_of_dtype
from allennlp.training.callbacks.callback import TrainerCallback
from allennlp.training.util import get_train_and_validation_metrics, get_batch_size

if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


logger = logging.getLogger(__name__)


class LogWriterCallback(TrainerCallback):

	def __init__(
		self,
		serialization_dir: str,
		summary_interval: int = 100,
		distribution_interval: Optional[int] = None,
		batch_size_interval: Optional[int] = None,
		should_log_parameter_statistics: bool = True,
		should_log_learning_rate: bool = False,
		batch_loss_moving_average_count: int = 100,
	) -> None:
		super().__init__(serialization_dir)
		self._summary_interval = summary_interval
		self._distribution_interval = distribution_interval
		self._batch_size_interval = batch_size_interval
		self._should_log_parameter_statistics = should_log_parameter_statistics
		self._should_log_learning_rate = should_log_learning_rate
		self._cumulative_batch_group_size = 0
		self._distribution_parameters: Optional[Set[str]] = None
		self._module_hook_handles: List[torch.utils.hooks.RemovableHandle] = []
		self._batch_loss_moving_average_count = batch_loss_moving_average_count
		self._batch_loss_moving_sum: Dict[str, float] = defaultdict(float)
		self._batch_loss_moving_items: Dict[str, Deque[float]] = defaultdict(deque)
		self._param_updates: Optional[Dict[str, torch.Tensor]] = None

	def log_scalars(
		self,
		scalars: Dict[str, Union[int, float]],
		log_prefix: str = "",
		epoch: Optional[int] = None,
	) -> None:
		raise NotImplementedError

	def log_tensors(
		self, tensors: Dict[str, torch.Tensor], log_prefix: str = "", epoch: Optional[int] = None
	) -> None:
		raise NotImplementedError

	def log_inputs(self, inputs: List[TensorDict], log_prefix: str = "") -> None:
		pass

	def close(self) -> None:
		for handle in self._module_hook_handles:
			handle.remove()

	def on_start(
		self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
	) -> None:
		self.trainer = trainer
		if is_primary:
			self._enable_activation_logging()

	def on_batch(
		self,
		trainer: "GradientDescentTrainer",
		batch_inputs: List[TensorDict],
		batch_outputs: List[Dict[str, Any]],
		batch_metrics: Dict[str, Any],
		epoch: int,
		batch_number: int,
		is_training: bool,
		is_primary: bool = True,
		batch_grad_norm: Optional[float] = None,
		**kwargs,
	) -> None:
		if not is_training or not is_primary:
			return None
		assert self.trainer is not None

		if self._should_log_distributions_this_batch():
			assert self._param_updates is not None
			for name, param in trainer.model.named_parameters():
				self._param_updates[name].sub_(param.detach().cpu())
		else:
			self._param_updates = None

		self.log_batch(
			batch_grad_norm,
			batch_metrics,
			batch_inputs,
			self._param_updates,
			batch_number,
		)

		if self._should_log_distributions_next_batch():
			self._param_updates = {
				name: param.detach().cpu().clone()
				for name, param in trainer.model.named_parameters()
			}

	def on_epoch(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any],
		epoch: int,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		if not is_primary:
			return None
		assert self.trainer is not None

		train_metrics, val_metrics = get_train_and_validation_metrics(metrics)
		self.log_epoch(
			train_metrics,
			val_metrics,
			epoch,
		)

	def on_end(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any] = None,
		epoch: int = None,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		if is_primary:
			self.close()

	def log_batch(
		self,
		batch_grad_norm: Optional[float],
		metrics: Dict[str, float],
		batch_group: List[TensorDict],
		param_updates: Optional[Dict[str, torch.Tensor]],
		batch_number: int,
	) -> None:
		if batch_number <= 1:  # batch_number is usually 1-indexed
			self._cumulative_batch_group_size = 0
			self.log_inputs(batch_group)

		if self._should_log_this_batch():
			if self._should_log_parameter_statistics:
				self._log_parameter_and_gradient_statistics(batch_grad_norm)

			if self._should_log_learning_rate:
				self._log_learning_rates()

			metrics_to_log: Dict[str, float] = {}
			batch_loss_metrics = {"batch_loss", "batch_reg_loss"}
			for key in batch_loss_metrics:
				if key not in metrics:
					continue
				value = metrics[key]
				metrics_to_log[key] = value
				self._batch_loss_moving_sum[key] += value
				self._batch_loss_moving_items[key].append(value)
				if len(self._batch_loss_moving_items[key]) > self._batch_loss_moving_average_count:
					self._batch_loss_moving_sum[key] -= self._batch_loss_moving_items[key].popleft()
				metrics_to_log[f"{key}_mov_avg"] = self._batch_loss_moving_sum[key] / len(
					self._batch_loss_moving_items[key]
				)

			for key, value in metrics.items():
				if key in batch_loss_metrics:
					continue
				key = "batch_" + key
				if key not in metrics_to_log:
					metrics_to_log[key] = value

			self.log_scalars(
				metrics_to_log,
				log_prefix="train",
			)

		if self._should_log_distributions_this_batch():
			assert param_updates is not None
			self._log_distributions()
			self._log_gradient_updates(param_updates)

		if self._batch_size_interval:
			batch_group_size = sum(get_batch_size(batch) for batch in batch_group)  # type: ignore
			self._cumulative_batch_group_size += batch_group_size
			if batch_number % self._batch_size_interval == 0:
				average = self._cumulative_batch_group_size / batch_number
				self.log_scalars(
					{"batch_size": batch_group_size, "mean_batch_size": average}, log_prefix="train"
				)

	def log_epoch(
		self,
		train_metrics: Dict[str, Any],
		val_metrics: Dict[str, Any],
		epoch: int,
	) -> None:
		self.log_scalars(
			{
				k: v
				for k, v in train_metrics.items()
				if isinstance(v, (int, float))
				if "_memory_MB" not in k  # W&B gives us much better system metrics
			},
			log_prefix="train",
			epoch=epoch,
		)
		self.log_scalars(
			{k: v for k, v in val_metrics.items() if isinstance(v, (int, float))},
			log_prefix="validation",
			epoch=epoch,
		)

	def _should_log_distributions_next_batch(self) -> bool:
		assert self.trainer is not None
		return (
			self._distribution_interval is not None
			and (self.trainer._total_batches_completed + 1) % self._distribution_interval == 0
		)

	def _should_log_distributions_this_batch(self) -> bool:
		assert self.trainer is not None
		return (
			self._distribution_interval is not None
			and self.trainer._total_batches_completed % self._distribution_interval == 0
		)

	def _enable_activation_logging(self) -> None:
		if self._distribution_interval is not None:
			for _, module in self.trainer.model.named_modules():  # type: ignore[union-attr]
				if not getattr(module, "should_log_activations", False):
					continue

				def hook(module_, inputs, outputs):
					if self._should_log_distributions_this_batch():
						self._log_activation_distribution(outputs, str(module_.__class__))

				self._module_hook_handles.append(module.register_forward_hook(hook))

	def _should_log_this_batch(self) -> bool:
		return self.trainer._total_batches_completed % self._summary_interval == 0  # type: ignore[union-attr]

	def _log_activation_distribution(self, outputs: Any, module_name: str) -> None:
		activations_to_log: Dict[str, torch.Tensor] = {}
		if isinstance(outputs, torch.Tensor):
			log_name = module_name
			activations_to_log[log_name] = outputs
		elif isinstance(outputs, (list, tuple)):
			for i, output in enumerate(outputs):
				if isinstance(output, torch.Tensor):
					log_name = "{0}_{1}".format(module_name, i)
					activations_to_log[log_name] = output
		elif isinstance(outputs, dict):
			for k, output in outputs.items():
				log_name = "{0}_{1}".format(module_name, k)
				if isinstance(output, torch.Tensor):
					activations_to_log[log_name] = output

		if activations_to_log:
			self.log_tensors(activations_to_log, log_prefix="activation_histogram")

	def _log_parameter_and_gradient_statistics(self, batch_grad_norm: float = None) -> None:
		parameter_mean_scalars: Dict[str, float] = {}
		parameter_std_scalars: Dict[str, float] = {}
		gradient_mean_scalars: Dict[str, float] = {}
		gradient_std_scalars: Dict[str, float] = {}
		for name, param in self.trainer.model.named_parameters():  # type: ignore[union-attr]
			if param.data.numel() > 0:
				parameter_mean_scalars[name] = param.data.mean().item()
			if param.data.numel() > 1:
				parameter_std_scalars[name] = param.data.std().item()
			if param.grad is not None:
				if param.grad.is_sparse:
					grad_data = param.grad.data._values()
				else:
					grad_data = param.grad.data

				if torch.prod(torch.tensor(grad_data.shape)).item() > 0:
					gradient_mean_scalars[name] = grad_data.mean().item()
					if grad_data.numel() > 1:
						gradient_std_scalars[name] = grad_data.std().item()
				else:
					logger.info("No gradient for %s, skipping logging.", name)
		self.log_scalars(parameter_mean_scalars, log_prefix="parameter_mean")
		self.log_scalars(parameter_std_scalars, log_prefix="parameter_std")
		self.log_scalars(gradient_mean_scalars, log_prefix="gradient_mean")
		self.log_scalars(gradient_std_scalars, log_prefix="gradient_std")
		if batch_grad_norm is not None:
			self.log_scalars({"gradient_norm": batch_grad_norm})

	def _log_learning_rates(self):
		lr_scalars: Dict[str, float] = {}
		names = {param: name for name, param in self.trainer.model.named_parameters()}  # type: ignore[union-attr]
		for group in self.trainer.optimizer.param_groups:  # type: ignore[union-attr]
			if "lr" not in group:
				continue
			rate = group["lr"]
			for param in group["params"]:
				effective_rate = rate * float(param.requires_grad)
				lr_scalars[names[param]] = effective_rate
		self.log_scalars(lr_scalars, log_prefix="learning_rate")

	def _log_distributions(self) -> None:
		if not self._distribution_parameters:
			self._distribution_parameters = set(
				self.trainer.model.get_parameters_for_histogram_logging()  # type: ignore[union-attr]
			)
		parameters_to_log: Dict[str, torch.Tensor] = {}
		for name, param in self.trainer.model.named_parameters():  # type: ignore[union-attr]
			if name in self._distribution_parameters:
				parameters_to_log[name] = param
		self.log_tensors(parameters_to_log, log_prefix="parameter_histogram")

	def _log_gradient_updates(self, param_updates: Dict[str, torch.Tensor]) -> None:
		gradient_update_scalars: Dict[str, float] = {}
		for name, param in self.trainer.model.named_parameters():  # type: ignore[union-attr]
			update_norm = torch.norm(param_updates[name].view(-1))
			param_norm = torch.norm(param.view(-1)).cpu()
			gradient_update_scalars[name] = (
				update_norm / (param_norm + tiny_value_of_dtype(param_norm.dtype))
			).item()
		self.log_scalars(gradient_update_scalars, log_prefix="gradient_update")


import math

import torch

from allennlp.common import Registrable


class Activation(torch.nn.Module, Registrable):

	def forward(self, x: torch.Tensor) -> torch.Tensor:
		raise NotImplementedError


Registrable._registry[Activation] = {
	"relu": (torch.nn.ReLU, None),
	"relu6": (torch.nn.ReLU6, None),
	"elu": (torch.nn.ELU, None),
	"gelu": (torch.nn.GELU, None),
	"prelu": (torch.nn.PReLU, None),
	"leaky_relu": (torch.nn.LeakyReLU, None),
	"threshold": (torch.nn.Threshold, None),
	"hardtanh": (torch.nn.Hardtanh, None),
	"sigmoid": (torch.nn.Sigmoid, None),
	"tanh": (torch.nn.Tanh, None),
	"log_sigmoid": (torch.nn.LogSigmoid, None),
	"softplus": (torch.nn.Softplus, None),
	"softshrink": (torch.nn.Softshrink, None),
	"softsign": (torch.nn.Softsign, None),
	"tanhshrink": (torch.nn.Tanhshrink, None),
	"selu": (torch.nn.SELU, None),
}


@Activation.register("linear")
class LinearActivation(Activation):
	def forward(self, x: torch.Tensor) -> torch.Tensor:
		return x


@Activation.register("mish")
class MishActivation(Activation):
	def forward(self, x: torch.Tensor) -> torch.Tensor:
		return x * torch.tanh(torch.nn.functional.softplus(x))


@Activation.register("swish")
class SwishActivation(Activation):
	def forward(self, x: torch.Tensor) -> torch.Tensor:
		return x * torch.sigmoid(x)


@Activation.register("gelu_new")
class GeluNew(Activation):

	def forward(self, x: torch.Tensor) -> torch.Tensor:
		return (
			0.5
			* x
			* (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))
		)


@Activation.register("gelu_fast")
class GeluFast(Activation):
	def forward(self, x: torch.Tensor) -> torch.Tensor:
		return 0.5 * x * (1.0 + torch.tanh(x * 0.7978845608 * (1.0 + 0.044715 * x * x)))

from __future__ import print_function
import argparse
import torch
import torch.utils.data
from torch import nn, optim
from torch.nn import functional as F
from torchvision import datasets, transforms
from torchvision.utils import save_image


parser = argparse.ArgumentParser(description='VAE MNIST Example')
parser.add_argument('--batch-size', type=int, default=128, metavar='N',
					help='input batch size for training (default: 128)')
parser.add_argument('--epochs', type=int, default=10, metavar='N',
					help='number of epochs to train (default: 10)')
parser.add_argument('--no-cuda', action='store_true', default=False,
					help='disables CUDA training')
parser.add_argument('--no-mps', action='store_true', default=False,
						help='disables macOS GPU training')
parser.add_argument('--seed', type=int, default=1, metavar='S',
					help='random seed (default: 1)')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='how many batches to wait before logging training status')
args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()
use_mps = not args.no_mps and torch.backends.mps.is_available()

torch.manual_seed(args.seed)

if args.cuda:
	device = torch.device("cuda")
elif use_mps:
	device = torch.device("mps")
else:
	device = torch.device("cpu")

kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}
train_loader = torch.utils.data.DataLoader(
	datasets.MNIST('../data', train=True, download=True,
				   transform=transforms.ToTensor()),
	batch_size=args.batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(
	datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),
	batch_size=args.batch_size, shuffle=False, **kwargs)


class VAE(nn.Module):
	def __init__(self):
		super(VAE, self).__init__()

		self.fc1 = nn.Linear(784, 400)
		self.fc21 = nn.Linear(400, 20)
		self.fc22 = nn.Linear(400, 20)
		self.fc3 = nn.Linear(20, 400)
		self.fc4 = nn.Linear(400, 784)

	def encode(self, x):
		h1 = F.relu(self.fc1(x))
		return self.fc21(h1), self.fc22(h1)

	def reparameterize(self, mu, logvar):
		std = torch.exp(0.5*logvar)
		eps = torch.randn_like(std)
		return mu + eps*std

	def decode(self, z):
		h3 = F.relu(self.fc3(z))
		return torch.sigmoid(self.fc4(h3))

	def forward(self, x):
		mu, logvar = self.encode(x.view(-1, 784))
		z = self.reparameterize(mu, logvar)
		return self.decode(z), mu, logvar


model = VAE().to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)


def loss_function(recon_x, x, mu, logvar):
	BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')

	KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

	return BCE + KLD


def train(epoch):
	model.train()
	train_loss = 0
	for batch_idx, (data, _) in enumerate(train_loader):
		data = data.to(device)
		optimizer.zero_grad()
		recon_batch, mu, logvar = model(data)
		loss = loss_function(recon_batch, data, mu, logvar)
		loss.backward()
		train_loss += loss.item()
		optimizer.step()
		if batch_idx % args.log_interval == 0:
			print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
				epoch, batch_idx * len(data), len(train_loader.dataset),
				100. * batch_idx / len(train_loader),
				loss.item() / len(data)))

	print('====> Epoch: {} Average loss: {:.4f}'.format(
		  epoch, train_loss / len(train_loader.dataset)))


def test(epoch):
	model.eval()
	test_loss = 0
	with torch.no_grad():
		for i, (data, _) in enumerate(test_loader):
			data = data.to(device)
			recon_batch, mu, logvar = model(data)
			test_loss += loss_function(recon_batch, data, mu, logvar).item()
			if i == 0:
				n = min(data.size(0), 8)
				comparison = torch.cat([data[:n],
									  recon_batch.view(args.batch_size, 1, 28, 28)[:n]])
				save_image(comparison.cpu(),
						 'results/reconstruction_' + str(epoch) + '.png', nrow=n)

	test_loss /= len(test_loader.dataset)
	print('====> Test set loss: {:.4f}'.format(test_loss))

if __name__ == "__main__":
	for epoch in range(1, args.epochs + 1):
		train(epoch)
		test(epoch)
		with torch.no_grad():
			sample = torch.randn(64, 20).to(device)
			sample = model.decode(sample).cpu()
			save_image(sample.view(64, 1, 28, 28),
					   'results/sample_' + str(epoch) + '.png')

import collections
import json
import logging
import os
from typing import Optional, Tuple

import numpy as np
from tqdm.auto import tqdm


logger = logging.getLogger(__name__)


def postprocess_qa_predictions(
	examples,
	features,
	predictions: Tuple[np.ndarray, np.ndarray],
	version_2_with_negative: bool = False,
	n_best_size: int = 20,
	max_answer_length: int = 30,
	null_score_diff_threshold: float = 0.0,
	output_dir: Optional[str] = None,
	prefix: Optional[str] = None,
	log_level: Optional[int] = logging.WARNING,
):
	if len(predictions) != 2:
		raise ValueError("`predictions` should be a tuple with two elements (start_logits, end_logits).")
	all_start_logits, all_end_logits = predictions

	if len(predictions[0]) != len(features):
		raise ValueError(f"Got {len(predictions[0])} predictions and {len(features)} features.")

	example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
	features_per_example = collections.defaultdict(list)
	for i, feature in enumerate(features):
		features_per_example[example_id_to_index[feature["example_id"]]].append(i)

	all_predictions = collections.OrderedDict()
	all_nbest_json = collections.OrderedDict()
	if version_2_with_negative:
		scores_diff_json = collections.OrderedDict()

	logger.setLevel(log_level)
	logger.info(f"Post-processing {len(examples)} example predictions split into {len(features)} features.")

	for example_index, example in enumerate(tqdm(examples)):
		feature_indices = features_per_example[example_index]

		min_null_prediction = None
		prelim_predictions = []

		for feature_index in feature_indices:
			start_logits = all_start_logits[feature_index]
			end_logits = all_end_logits[feature_index]
			offset_mapping = features[feature_index]["offset_mapping"]
			token_is_max_context = features[feature_index].get("token_is_max_context", None)

			feature_null_score = start_logits[0] + end_logits[0]
			if min_null_prediction is None or min_null_prediction["score"] > feature_null_score:
				min_null_prediction = {
					"offsets": (0, 0),
					"score": feature_null_score,
					"start_logit": start_logits[0],
					"end_logit": end_logits[0],
				}

			start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
			end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
			for start_index in start_indexes:
				for end_index in end_indexes:
					if (
						start_index >= len(offset_mapping)
						or end_index >= len(offset_mapping)
						or offset_mapping[start_index] is None
						or len(offset_mapping[start_index]) < 2
						or offset_mapping[end_index] is None
						or len(offset_mapping[end_index]) < 2
					):
						continue
					if end_index < start_index or end_index - start_index + 1 > max_answer_length:
						continue
					if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):
						continue

					prelim_predictions.append(
						{
							"offsets": (offset_mapping[start_index][0], offset_mapping[end_index][1]),
							"score": start_logits[start_index] + end_logits[end_index],
							"start_logit": start_logits[start_index],
							"end_logit": end_logits[end_index],
						}
					)
		if version_2_with_negative and min_null_prediction is not None:
			prelim_predictions.append(min_null_prediction)
			null_score = min_null_prediction["score"]

		predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]

		if (
			version_2_with_negative
			and min_null_prediction is not None
			and not any(p["offsets"] == (0, 0) for p in predictions)
		):
			predictions.append(min_null_prediction)

		context = example["context"]
		for pred in predictions:
			offsets = pred.pop("offsets")
			pred["text"] = context[offsets[0] : offsets[1]]

		if len(predictions) == 0 or (len(predictions) == 1 and predictions[0]["text"] == ""):
			predictions.insert(0, {"text": "empty", "start_logit": 0.0, "end_logit": 0.0, "score": 0.0})

		scores = np.array([pred.pop("score") for pred in predictions])
		exp_scores = np.exp(scores - np.max(scores))
		probs = exp_scores / exp_scores.sum()

		for prob, pred in zip(probs, predictions):
			pred["probability"] = prob

		if not version_2_with_negative:
			all_predictions[example["id"]] = predictions[0]["text"]
		else:
			i = 0
			while predictions[i]["text"] == "":
				i += 1
			best_non_null_pred = predictions[i]

			score_diff = null_score - best_non_null_pred["start_logit"] - best_non_null_pred["end_logit"]
			scores_diff_json[example["id"]] = float(score_diff)  # To be JSON-serializable.
			if score_diff > null_score_diff_threshold:
				all_predictions[example["id"]] = ""
			else:
				all_predictions[example["id"]] = best_non_null_pred["text"]

		all_nbest_json[example["id"]] = [
			{k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}
			for pred in predictions
		]

	if output_dir is not None:
		if not os.path.isdir(output_dir):
			raise EnvironmentError(f"{output_dir} is not a directory.")

		prediction_file = os.path.join(
			output_dir, "predictions.json" if prefix is None else f"{prefix}_predictions.json"
		)
		nbest_file = os.path.join(
			output_dir, "nbest_predictions.json" if prefix is None else f"{prefix}_nbest_predictions.json"
		)
		if version_2_with_negative:
			null_odds_file = os.path.join(
				output_dir, "null_odds.json" if prefix is None else f"{prefix}_null_odds.json"
			)

		logger.info(f"Saving predictions to {prediction_file}.")
		with open(prediction_file, "w") as writer:
			writer.write(json.dumps(all_predictions, indent=4) + "\n")
		logger.info(f"Saving nbest_preds to {nbest_file}.")
		with open(nbest_file, "w") as writer:
			writer.write(json.dumps(all_nbest_json, indent=4) + "\n")
		if version_2_with_negative:
			logger.info(f"Saving null_odds to {null_odds_file}.")
			with open(null_odds_file, "w") as writer:
				writer.write(json.dumps(scores_diff_json, indent=4) + "\n")

	return all_predictions


def postprocess_qa_predictions_with_beam_search(
	examples,
	features,
	predictions: Tuple[np.ndarray, np.ndarray],
	version_2_with_negative: bool = False,
	n_best_size: int = 20,
	max_answer_length: int = 30,
	start_n_top: int = 5,
	end_n_top: int = 5,
	output_dir: Optional[str] = None,
	prefix: Optional[str] = None,
	log_level: Optional[int] = logging.WARNING,
):
	if len(predictions) != 5:
		raise ValueError("`predictions` should be a tuple with five elements.")
	start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits = predictions

	if len(predictions[0]) != len(features):
		raise ValueError(f"Got {len(predictions[0])} predictions and {len(features)} features.")

	example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
	features_per_example = collections.defaultdict(list)
	for i, feature in enumerate(features):
		features_per_example[example_id_to_index[feature["example_id"]]].append(i)

	all_predictions = collections.OrderedDict()
	all_nbest_json = collections.OrderedDict()
	scores_diff_json = collections.OrderedDict() if version_2_with_negative else None

	logger.setLevel(log_level)
	logger.info(f"Post-processing {len(examples)} example predictions split into {len(features)} features.")

	for example_index, example in enumerate(tqdm(examples)):
		feature_indices = features_per_example[example_index]

		min_null_score = None
		prelim_predictions = []

		for feature_index in feature_indices:
			start_log_prob = start_top_log_probs[feature_index]
			start_indexes = start_top_index[feature_index]
			end_log_prob = end_top_log_probs[feature_index]
			end_indexes = end_top_index[feature_index]
			feature_null_score = cls_logits[feature_index]
			offset_mapping = features[feature_index]["offset_mapping"]
			token_is_max_context = features[feature_index].get("token_is_max_context", None)

			if min_null_score is None or feature_null_score < min_null_score:
				min_null_score = feature_null_score

			for i in range(start_n_top):
				for j in range(end_n_top):
					start_index = int(start_indexes[i])
					j_index = i * end_n_top + j
					end_index = int(end_indexes[j_index])
					if (
						start_index >= len(offset_mapping)
						or end_index >= len(offset_mapping)
						or offset_mapping[start_index] is None
						or len(offset_mapping[start_index]) < 2
						or offset_mapping[end_index] is None
						or len(offset_mapping[end_index]) < 2
					):
						continue

					if end_index < start_index or end_index - start_index + 1 > max_answer_length:
						continue
					if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):
						continue
					prelim_predictions.append(
						{
							"offsets": (offset_mapping[start_index][0], offset_mapping[end_index][1]),
							"score": start_log_prob[i] + end_log_prob[j_index],
							"start_log_prob": start_log_prob[i],
							"end_log_prob": end_log_prob[j_index],
						}
					)

		predictions = sorted(prelim_predictions, key=lambda x: x["score"], reverse=True)[:n_best_size]

		context = example["context"]
		for pred in predictions:
			offsets = pred.pop("offsets")
			pred["text"] = context[offsets[0] : offsets[1]]

		if len(predictions) == 0:
			min_null_score = -2e-6
			predictions.insert(0, {"text": "", "start_logit": -1e-6, "end_logit": -1e-6, "score": min_null_score})

		scores = np.array([pred.pop("score") for pred in predictions])
		exp_scores = np.exp(scores - np.max(scores))
		probs = exp_scores / exp_scores.sum()

		for prob, pred in zip(probs, predictions):
			pred["probability"] = prob

		all_predictions[example["id"]] = predictions[0]["text"]
		if version_2_with_negative:
			scores_diff_json[example["id"]] = float(min_null_score)

		all_nbest_json[example["id"]] = [
			{k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}
			for pred in predictions
		]

	if output_dir is not None:
		if not os.path.isdir(output_dir):
			raise EnvironmentError(f"{output_dir} is not a directory.")

		prediction_file = os.path.join(
			output_dir, "predictions.json" if prefix is None else f"{prefix}_predictions.json"
		)
		nbest_file = os.path.join(
			output_dir, "nbest_predictions.json" if prefix is None else f"{prefix}_nbest_predictions.json"
		)
		if version_2_with_negative:
			null_odds_file = os.path.join(
				output_dir, "null_odds.json" if prefix is None else f"{prefix}_null_odds.json"
			)

		logger.info(f"Saving predictions to {prediction_file}.")
		with open(prediction_file, "w") as writer:
			writer.write(json.dumps(all_predictions, indent=4) + "\n")
		logger.info(f"Saving nbest_preds to {nbest_file}.")
		with open(nbest_file, "w") as writer:
			writer.write(json.dumps(all_nbest_json, indent=4) + "\n")
		if version_2_with_negative:
			logger.info(f"Saving null_odds to {null_odds_file}.")
			with open(null_odds_file, "w") as writer:
				writer.write(json.dumps(scores_diff_json, indent=4) + "\n")

	return all_predictions, scores_diff_json

from collections import Counter
import math
from typing import Iterable, Tuple, Dict, Set, Optional

import torch
import torch.distributed as dist

from allennlp.common.util import is_distributed
from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("bleu")
class BLEU(Metric):

	def __init__(
		self,
		ngram_weights: Iterable[float] = (0.25, 0.25, 0.25, 0.25),
		exclude_indices: Set[int] = None,
	) -> None:
		self._ngram_weights = ngram_weights
		self._exclude_indices = exclude_indices or set()
		self._precision_matches: Dict[int, int] = Counter()
		self._precision_totals: Dict[int, int] = Counter()
		self._prediction_lengths = 0
		self._reference_lengths = 0

	def reset(self) -> None:
		self._precision_matches = Counter()
		self._precision_totals = Counter()
		self._prediction_lengths = 0
		self._reference_lengths = 0

	def _get_modified_precision_counts(
		self,
		predicted_tokens: torch.LongTensor,
		reference_tokens: torch.LongTensor,
		ngram_size: int,
	) -> Tuple[int, int]:
		clipped_matches = 0
		total_predicted = 0
		from allennlp.training.util import ngrams

		for predicted_row, reference_row in zip(predicted_tokens, reference_tokens):
			predicted_ngram_counts = ngrams(predicted_row, ngram_size, self._exclude_indices)
			reference_ngram_counts = ngrams(reference_row, ngram_size, self._exclude_indices)
			for ngram, count in predicted_ngram_counts.items():
				clipped_matches += min(count, reference_ngram_counts[ngram])
				total_predicted += count
		return clipped_matches, total_predicted

	def _get_brevity_penalty(self) -> float:
		if self._prediction_lengths > self._reference_lengths:
			return 1.0
		if self._reference_lengths == 0 or self._prediction_lengths == 0:
			return 0.0
		return math.exp(1.0 - self._reference_lengths / self._prediction_lengths)

	def __call__(
		self,  # type: ignore
		predictions: torch.LongTensor,
		gold_targets: torch.LongTensor,
		mask: Optional[torch.BoolTensor] = None,
	) -> None:
		if mask is not None:
			raise NotImplementedError("This metric does not support a mask.")

		predictions, gold_targets = self.detach_tensors(predictions, gold_targets)
		if is_distributed():
			world_size = dist.get_world_size()
		else:
			world_size = 1

		for ngram_size, _ in enumerate(self._ngram_weights, start=1):
			precision_matches, precision_totals = self._get_modified_precision_counts(
				predictions, gold_targets, ngram_size
			)

			self._precision_matches[ngram_size] += dist_reduce_sum(precision_matches) / world_size
			self._precision_totals[ngram_size] += dist_reduce_sum(precision_totals) / world_size

		if not self._exclude_indices:
			_prediction_lengths = predictions.size(0) * predictions.size(1)
			_reference_lengths = gold_targets.size(0) * gold_targets.size(1)

		else:
			from allennlp.training.util import get_valid_tokens_mask

			valid_predictions_mask = get_valid_tokens_mask(predictions, self._exclude_indices)
			valid_gold_targets_mask = get_valid_tokens_mask(gold_targets, self._exclude_indices)
			_prediction_lengths = valid_predictions_mask.sum().item()
			_reference_lengths = valid_gold_targets_mask.sum().item()

		self._prediction_lengths += dist_reduce_sum(_prediction_lengths)
		self._reference_lengths += dist_reduce_sum(_reference_lengths)

	def get_metric(self, reset: bool = False) -> Dict[str, float]:

		brevity_penalty = self._get_brevity_penalty()
		ngram_scores = (
			weight
			* (
				math.log(self._precision_matches[n] + 1e-13)
				- math.log(self._precision_totals[n] + 1e-13)
			)
			for n, weight in enumerate(self._ngram_weights, start=1)
		)
		bleu = brevity_penalty * math.exp(sum(ngram_scores))

		if reset:
			self.reset()
		return {"BLEU": bleu}

from __future__ import print_function
from __future__ import unicode_literals

import argparse

import matplotlib.pyplot as plt
import torch


parser = argparse.ArgumentParser()
parser.add_argument("-i", "--sample-file", required=True)
parser.add_argument("-o", "--out-file", default="out.png")
parser.add_argument("-d", "--dimension", type=int, default=3)
options = parser.parse_args()

module = torch.jit.load(options.sample_file)
images = list(module.parameters())[0]

for index in range(options.dimension * options.dimension):
	image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)
	array = image.numpy()
	axis = plt.subplot(options.dimension, options.dimension, 1 + index)
	plt.imshow(array, cmap="gray")
	axis.get_xaxis().set_visible(False)
	axis.get_yaxis().set_visible(False)

plt.savefig(options.out_file)
print("Saved ", options.out_file)

import torch


from allennlp.common.registrable import Registrable


class SpanExtractor(torch.nn.Module, Registrable):

	def forward(
		self,
		sequence_tensor: torch.FloatTensor,
		span_indices: torch.LongTensor,
		sequence_mask: torch.BoolTensor = None,
		span_indices_mask: torch.BoolTensor = None,
	):
		raise NotImplementedError

	def get_input_dim(self) -> int:
		raise NotImplementedError

	def get_output_dim(self) -> int:
		raise NotImplementedError

from collections import defaultdict
from copy import deepcopy
from typing import Dict, List, Optional, Iterator
import textwrap


from spacy.tokens import Token as SpacyToken
import torch

from allennlp.common.checks import ConfigurationError
from allennlp.data.fields.sequence_field import SequenceField
from allennlp.data.tokenizers import Token
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList
from allennlp.data.vocabulary import Vocabulary
from allennlp.nn import util

TextFieldTensors = Dict[str, Dict[str, torch.Tensor]]


class TextField(SequenceField[TextFieldTensors]):

	__slots__ = ["tokens", "_token_indexers", "_indexed_tokens"]

	def __init__(
		self, tokens: List[Token], token_indexers: Optional[Dict[str, TokenIndexer]] = None
	) -> None:
		self.tokens = tokens
		self._token_indexers = token_indexers
		self._indexed_tokens: Optional[Dict[str, IndexedTokenList]] = None

		if not all(isinstance(x, (Token, SpacyToken)) for x in tokens):
			raise ConfigurationError(
				"TextFields must be passed Tokens. "
				"Found: {} with types {}.".format(tokens, [type(x) for x in tokens])
			)

	@property
	def token_indexers(self) -> Dict[str, TokenIndexer]:
		if self._token_indexers is None:
			raise ValueError(
				"TextField's token_indexers have not been set.\n"
				"Did you forget to call DatasetReader.apply_token_indexers(instance) "
				"on your instance?\n"
				"If apply_token_indexers() is being called but "
				"you're still seeing this error, it may not be implemented correctly."
			)
		return self._token_indexers

	@token_indexers.setter
	def token_indexers(self, token_indexers: Dict[str, TokenIndexer]) -> None:
		self._token_indexers = token_indexers

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		for indexer in self.token_indexers.values():
			for token in self.tokens:
				indexer.count_vocab_items(token, counter)

	def index(self, vocab: Vocabulary):
		self._indexed_tokens = {}
		for indexer_name, indexer in self.token_indexers.items():
			self._indexed_tokens[indexer_name] = indexer.tokens_to_indices(self.tokens, vocab)

	def get_padding_lengths(self) -> Dict[str, int]:
		if self._indexed_tokens is None:
			raise ConfigurationError(
				"You must call .index(vocabulary) on a field before determining padding lengths."
			)

		padding_lengths = {}
		for indexer_name, indexer in self.token_indexers.items():
			indexer_lengths = indexer.get_padding_lengths(self._indexed_tokens[indexer_name])
			for key, length in indexer_lengths.items():
				padding_lengths[f"{indexer_name}___{key}"] = length
		return padding_lengths

	def sequence_length(self) -> int:
		return len(self.tokens)

	def as_tensor(self, padding_lengths: Dict[str, int]) -> TextFieldTensors:
		if self._indexed_tokens is None:
			raise ConfigurationError(
				"You must call .index(vocabulary) on a field before calling .as_tensor()"
			)

		tensors = {}

		indexer_lengths: Dict[str, Dict[str, int]] = defaultdict(dict)
		for key, value in padding_lengths.items():
			indexer_name, padding_key = key.split("___")
			indexer_lengths[indexer_name][padding_key] = value

		for indexer_name, indexer in self.token_indexers.items():
			tensors[indexer_name] = indexer.as_padded_tensor_dict(
				self._indexed_tokens[indexer_name], indexer_lengths[indexer_name]
			)
		return tensors

	def empty_field(self):
		text_field = TextField([], self._token_indexers)
		text_field._indexed_tokens = {}
		if self._token_indexers is not None:
			for indexer_name, indexer in self.token_indexers.items():
				text_field._indexed_tokens[indexer_name] = indexer.get_empty_token_list()
		return text_field

	def batch_tensors(self, tensor_list: List[TextFieldTensors]) -> TextFieldTensors:
		indexer_lists: Dict[str, List[Dict[str, torch.Tensor]]] = defaultdict(list)
		for tensor_dict in tensor_list:
			for indexer_name, indexer_output in tensor_dict.items():
				indexer_lists[indexer_name].append(indexer_output)
		batched_tensors = {
			indexer_name: util.batch_tensor_dicts(indexer_outputs)
			for indexer_name, indexer_outputs in indexer_lists.items()
		}
		return batched_tensors

	def __str__(self) -> str:
		formatted_text = "".join(
			"\t\t" + text + "\n" for text in textwrap.wrap(repr(self.tokens), 100)
		)
		if self._token_indexers is not None:
			indexers = {
				name: indexer.__class__.__name__ for name, indexer in self._token_indexers.items()
			}
			return (
				f"TextField of length {self.sequence_length()} with "
				f"text: \n {formatted_text} \t\tand TokenIndexers : {indexers}"
			)
		else:
			return f"TextField of length {self.sequence_length()} with text: \n {formatted_text}"

	def __iter__(self) -> Iterator[Token]:
		return iter(self.tokens)

	def __getitem__(self, idx: int) -> Token:
		return self.tokens[idx]

	def __len__(self) -> int:
		return len(self.tokens)

	def duplicate(self):
		if self._token_indexers is not None:
			new = TextField(deepcopy(self.tokens), {k: v for k, v in self._token_indexers.items()})
		else:
			new = TextField(deepcopy(self.tokens))
		new._indexed_tokens = deepcopy(self._indexed_tokens)
		return new

	def human_readable_repr(self) -> List[str]:
		return [str(t) for t in self.tokens]

from typing import List

import numpy
import torch

from allennlp.common import Registrable
from allennlp.common.util import JsonDict
from allennlp.nn import util
from allennlp.predictors import Predictor


class SaliencyInterpreter(Registrable):

	def __init__(self, predictor: Predictor) -> None:
		self.predictor = predictor

	def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:
		raise NotImplementedError("Implement this for saliency interpretations")

	@staticmethod
	def _aggregate_token_embeddings(
		embeddings_list: List[torch.Tensor], token_offsets: List[torch.Tensor]
	) -> List[numpy.ndarray]:
		if len(token_offsets) == 0:
			return [embeddings.detach().cpu().numpy() for embeddings in embeddings_list]
		aggregated_embeddings = []
		for embeddings, offsets in zip(embeddings_list, token_offsets):
			span_embeddings, span_mask = util.batched_span_select(embeddings.contiguous(), offsets)
			span_mask = span_mask.unsqueeze(-1)
			span_embeddings *= span_mask  # zero out paddings

			span_embeddings_sum = span_embeddings.sum(2)
			span_embeddings_len = span_mask.sum(2)
			embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)

			embeddings[(span_embeddings_len == 0).expand(embeddings.shape)] = 0
			aggregated_embeddings.append(embeddings.detach().cpu().numpy())
		return aggregated_embeddings

import torch.nn as nn
import torch
import math


class GELU(nn.Module):

	def forward(self, x):
		return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))



import torch.distributed as dist
import torch.nn as nn
import torch

from transformers.models.t5.modeling_t5 import T5Block

from torch.distributed.fsdp.fully_sharded_data_parallel import (
	FullyShardedDataParallel as FSDP,
	CPUOffload,
	BackwardPrefetch,
	MixedPrecision,
)
from torch.distributed.fsdp.wrap import (
	transformer_auto_wrap_policy,
	size_based_auto_wrap_policy,
	enable_wrap,
	wrap,
)

import functools
from typing import Type


def get_size_policy(min_params=1e8):
	num_wrap_policy = functools.partial(
		size_based_auto_wrap_policy, min_num_params=min_params
	)
	return num_wrap_policy


def get_t5_wrapper():

	t5_auto_wrap_policy = functools.partial(
		transformer_auto_wrap_policy,
		transformer_layer_cls={
			T5Block,
		},
	)

	return t5_auto_wrap_policy

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datautils import MyTrainDataset

import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import os


def ddp_setup():
	init_process_group(backend="nccl")
	torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))

class Trainer:
	def __init__(
		self,
		model: torch.nn.Module,
		train_data: DataLoader,
		optimizer: torch.optim.Optimizer,
		save_every: int,
		snapshot_path: str,
	) -> None:
		self.local_rank = int(os.environ["LOCAL_RANK"])
		self.global_rank = int(os.environ["RANK"])
		self.model = model.to(self.local_rank)
		self.train_data = train_data
		self.optimizer = optimizer
		self.save_every = save_every
		self.epochs_run = 0
		self.snapshot_path = snapshot_path
		if os.path.exists(snapshot_path):
			print("Loading snapshot")
			self._load_snapshot(snapshot_path)

		self.model = DDP(self.model, device_ids=[self.local_rank])

	def _load_snapshot(self, snapshot_path):
		loc = f"cuda:{self.local_rank}"
		snapshot = torch.load(snapshot_path, map_location=loc)
		self.model.load_state_dict(snapshot["MODEL_STATE"])
		self.epochs_run = snapshot["EPOCHS_RUN"]
		print(f"Resuming training from snapshot at Epoch {self.epochs_run}")

	def _run_batch(self, source, targets):
		self.optimizer.zero_grad()
		output = self.model(source)
		loss = F.cross_entropy(output, targets)
		loss.backward()
		self.optimizer.step()

	def _run_epoch(self, epoch):
		b_sz = len(next(iter(self.train_data))[0])
		print(f"[GPU{self.global_rank}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}")
		self.train_data.sampler.set_epoch(epoch)
		for source, targets in self.train_data:
			source = source.to(self.local_rank)
			targets = targets.to(self.local_rank)
			self._run_batch(source, targets)

	def _save_snapshot(self, epoch):
		snapshot = {
			"MODEL_STATE": self.model.module.state_dict(),
			"EPOCHS_RUN": epoch,
		}
		torch.save(snapshot, self.snapshot_path)
		print(f"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}")

	def train(self, max_epochs: int):
		for epoch in range(self.epochs_run, max_epochs):
			self._run_epoch(epoch)
			if self.local_rank == 0 and epoch % self.save_every == 0:
				self._save_snapshot(epoch)


def load_train_objs():
	train_set = MyTrainDataset(2048)  # load your dataset
	model = torch.nn.Linear(20, 1)  # load your model
	optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
	return train_set, model, optimizer


def prepare_dataloader(dataset: Dataset, batch_size: int):
	return DataLoader(
		dataset,
		batch_size=batch_size,
		pin_memory=True,
		shuffle=False,
		sampler=DistributedSampler(dataset)
	)


def main(save_every: int, total_epochs: int, batch_size: int, snapshot_path: str = "snapshot.pt"):
	ddp_setup()
	dataset, model, optimizer = load_train_objs()
	train_data = prepare_dataloader(dataset, batch_size)
	trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)
	trainer.train(total_epochs)
	destroy_process_group()


if __name__ == "__main__":
	import argparse
	parser = argparse.ArgumentParser(description='simple distributed training job')
	parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')
	parser.add_argument('save_every', type=int, help='How often to save a snapshot')
	parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')
	args = parser.parse_args()
	
	main(args.save_every, args.total_epochs, args.batch_size)

from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("average")
class Average(Metric):

	def __init__(self) -> None:
		self._total_value = 0.0
		self._count = 0

	def __call__(self, value):
		self._count += dist_reduce_sum(1)
		self._total_value += dist_reduce_sum(float(list(self.detach_tensors(value))[0]))

	def get_metric(self, reset: bool = False):

		average_value = self._total_value / self._count if self._count > 0 else 0.0
		if reset:
			self.reset()
		return float(average_value)

	def reset(self):
		self._total_value = 0.0
		self._count = 0

from typing import List, Tuple

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.modules.conditional_random_field.conditional_random_field import (
	ConditionalRandomField,
)


class ConditionalRandomFieldWeightEmission(ConditionalRandomField):

	def __init__(
		self,
		num_tags: int,
		label_weights: List[float],
		constraints: List[Tuple[int, int]] = None,
		include_start_end_transitions: bool = True,
	) -> None:
		super().__init__(num_tags, constraints, include_start_end_transitions)

		if label_weights is None:
			raise ConfigurationError("label_weights must be given")

		self.register_buffer("label_weights", torch.Tensor(label_weights))

	def forward(
		self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor = None
	) -> torch.Tensor:
		if mask is None:
			mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
		else:
			mask = mask.to(torch.bool)

		label_weights = self.label_weights

		inputs = inputs * label_weights.view(1, 1, -1)

		log_denominator = self._input_likelihood(inputs, self.transitions, mask)
		log_numerator = self._joint_likelihood(inputs, self.transitions, tags, mask)

		return torch.sum(log_numerator - log_denominator)

from typing import Dict, List, Optional, Tuple, Any
import logging
import torch
from allennlp.common.util import pad_sequence_to_length


from allennlp.data.vocabulary import Vocabulary
from allennlp.data.tokenizers import Token, PretrainedTransformerTokenizer
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList


logger = logging.getLogger(__name__)


@TokenIndexer.register("pretrained_transformer")
class PretrainedTransformerIndexer(TokenIndexer):

	def __init__(
		self,
		model_name: str,
		namespace: str = "tags",
		max_length: int = None,
		tokenizer_kwargs: Optional[Dict[str, Any]] = None,
		**kwargs,
	) -> None:
		super().__init__(**kwargs)
		self._namespace = namespace
		self._allennlp_tokenizer = PretrainedTransformerTokenizer(
			model_name, tokenizer_kwargs=tokenizer_kwargs
		)
		self._tokenizer = self._allennlp_tokenizer.tokenizer
		self._added_to_vocabulary = False

		self._num_added_start_tokens = len(self._allennlp_tokenizer.single_sequence_start_tokens)
		self._num_added_end_tokens = len(self._allennlp_tokenizer.single_sequence_end_tokens)

		self._max_length = max_length
		if self._max_length is not None:
			num_added_tokens = len(self._allennlp_tokenizer.tokenize("a")) - 1
			self._effective_max_length = (  # we need to take into account special tokens
				self._max_length - num_added_tokens
			)
			if self._effective_max_length <= 0:
				raise ValueError(
					"max_length needs to be greater than the number of special tokens inserted."
				)

	def _add_encoding_to_vocabulary_if_needed(self, vocab: Vocabulary) -> None:
		if self._added_to_vocabulary:
			return

		vocab.add_transformer_vocab(self._tokenizer, self._namespace)

		self._added_to_vocabulary = True

	def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
		pass

	def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:
		self._add_encoding_to_vocabulary_if_needed(vocabulary)

		indices, type_ids = self._extract_token_and_type_ids(tokens)
		output: IndexedTokenList = {
			"token_ids": indices,
			"mask": [True] * len(indices),
			"type_ids": type_ids or [0] * len(indices),
		}

		return self._postprocess_output(output)

	def indices_to_tokens(
		self, indexed_tokens: IndexedTokenList, vocabulary: Vocabulary
	) -> List[Token]:
		self._add_encoding_to_vocabulary_if_needed(vocabulary)

		token_ids = indexed_tokens["token_ids"]
		type_ids = indexed_tokens.get("type_ids")

		return [
			Token(
				text=vocabulary.get_token_from_index(token_ids[i], self._namespace),
				text_id=token_ids[i],
				type_id=type_ids[i] if type_ids is not None else None,
			)
			for i in range(len(token_ids))
		]

	def _extract_token_and_type_ids(self, tokens: List[Token]) -> Tuple[List[int], List[int]]:
		indices: List[int] = []
		type_ids: List[int] = []
		for token in tokens:
			indices.append(
				token.text_id
				if token.text_id is not None
				else self._tokenizer.convert_tokens_to_ids(token.text)
			)
			type_ids.append(token.type_id if token.type_id is not None else 0)
		return indices, type_ids

	def _postprocess_output(self, output: IndexedTokenList) -> IndexedTokenList:
		if self._max_length is not None:

			indices = output["token_ids"]
			type_ids = output.get("type_ids", [0] * len(indices))

			indices = indices[
				self._num_added_start_tokens : len(indices) - self._num_added_end_tokens
			]
			type_ids = type_ids[
				self._num_added_start_tokens : len(type_ids) - self._num_added_end_tokens
			]

			folded_indices = [
				indices[i : i + self._effective_max_length]
				for i in range(0, len(indices), self._effective_max_length)
			]
			folded_type_ids = [
				type_ids[i : i + self._effective_max_length]
				for i in range(0, len(type_ids), self._effective_max_length)
			]

			folded_indices = [
				self._tokenizer.build_inputs_with_special_tokens(segment)
				for segment in folded_indices
			]
			single_sequence_start_type_ids = [
				t.type_id for t in self._allennlp_tokenizer.single_sequence_start_tokens
			]
			single_sequence_end_type_ids = [
				t.type_id for t in self._allennlp_tokenizer.single_sequence_end_tokens
			]
			folded_type_ids = [
				single_sequence_start_type_ids + segment + single_sequence_end_type_ids
				for segment in folded_type_ids
			]
			assert all(
				len(segment_indices) == len(segment_type_ids)
				for segment_indices, segment_type_ids in zip(folded_indices, folded_type_ids)
			)

			indices = [i for segment in folded_indices for i in segment]
			type_ids = [i for segment in folded_type_ids for i in segment]

			output["token_ids"] = indices
			output["type_ids"] = type_ids
			output["segment_concat_mask"] = [True] * len(indices)

		return output

	def get_empty_token_list(self) -> IndexedTokenList:
		output: IndexedTokenList = {"token_ids": [], "mask": [], "type_ids": []}
		if self._max_length is not None:
			output["segment_concat_mask"] = []
		return output

	def as_padded_tensor_dict(
		self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]
	) -> Dict[str, torch.Tensor]:
		tensor_dict = {}
		for key, val in tokens.items():
			if key == "type_ids":
				padding_value = 0
				mktensor = torch.LongTensor
			elif key == "mask" or key == "wordpiece_mask":
				padding_value = False
				mktensor = torch.BoolTensor
			elif len(val) > 0 and isinstance(val[0], bool):
				padding_value = False
				mktensor = torch.BoolTensor
			else:
				padding_value = self._tokenizer.pad_token_id
				if padding_value is None:
					padding_value = (
						0  # Some tokenizers don't have padding tokens and rely on the mask only.
					)
				mktensor = torch.LongTensor

			tensor = mktensor(
				pad_sequence_to_length(
					val, padding_lengths[key], default_value=lambda: padding_value
				)
			)

			tensor_dict[key] = tensor
		return tensor_dict

	def __eq__(self, other):
		if isinstance(other, PretrainedTransformerIndexer):
			for key in self.__dict__:
				if key == "_tokenizer":
					continue
				if self.__dict__[key] != other.__dict__[key]:
					return False
			return True
		return NotImplemented

import torch


from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention


@MatrixAttention.register("dot_product")
class DotProductMatrixAttention(MatrixAttention):

	def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:
		return matrix_1.matmul(matrix_2.transpose(-1, -2))


import logging
import math
import os
import sys
import warnings
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional

import datasets
from datasets import load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoTokenizer,
	DataCollatorForPermutationLanguageModeling,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	XLNetConfig,
	XLNetLMHeadModel,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/language-modeling/requirements.txt")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:

	model_name_or_path: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch."
			)
		},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	config_overrides: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"Override some existing default config settings when a model is trained from scratch. Example: "
				"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
			)
		},
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	low_cpu_mem_usage: bool = field(
		default=False,
		metadata={
			"help": (
				"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. "
				"set True will benefit LLM loading time and RAM consumption."
			)
		},
	)

	def __post_init__(self):
		if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
			raise ValueError(
				"--config_overrides can't be used in combination with --config_name or --model_name_or_path"
			)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	validation_split_percentage: Optional[int] = field(
		default=5,
		metadata={
			"help": "The percentage of the train set used as validation set in case there's no validation split"
		},
	)
	max_seq_length: int = field(
		default=512,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated."
			)
		},
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	plm_probability: float = field(
		default=1 / 6,
		metadata={
			"help": (
				"Ratio of length of a span of masked tokens to surrounding context length for "
				"permutation language modeling."
			)
		},
	)
	max_span_length: int = field(
		default=5, metadata={"help": "Maximum length of a span of masked tokens for permutation language modeling."}
	)
	line_by_line: bool = field(
		default=False,
		metadata={"help": "Whether distinct lines of text in the dataset are to be handled as distinct sequences."},
	)
	pad_to_max_length: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)

	def __post_init__(self):
		if self.dataset_name is None and self.train_file is None and self.validation_file is None:
			raise ValueError("Need either a dataset name or a training/validation file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json", "txt"], "`train_file` should be a csv, a json or a txt file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json", "txt"], "`validation_file` should be a csv, a json or a txt file."


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_plm", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				data_args.dataset_name,
				data_args.dataset_config_name,
				split=f"train[:{data_args.validation_split_percentage}%]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
			raw_datasets["train"] = load_dataset(
				data_args.dataset_name,
				data_args.dataset_config_name,
				split=f"train[{data_args.validation_split_percentage}%:]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
		extension = data_args.train_file.split(".")[-1]
		if extension == "txt":
			extension = "text"
		raw_datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[:{data_args.validation_split_percentage}%]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
			raw_datasets["train"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[{data_args.validation_split_percentage}%:]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)


	config_kwargs = {
		"cache_dir": model_args.cache_dir,
		"revision": model_args.model_revision,
		"token": model_args.token,
	}
	if model_args.config_name:
		config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
	elif model_args.model_name_or_path:
		config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
	else:
		config = XLNetConfig()
		logger.warning("You are instantiating a new config instance from scratch.")
		if model_args.config_overrides is not None:
			logger.info(f"Overriding config: {model_args.config_overrides}")
			config.update_from_string(model_args.config_overrides)
			logger.info(f"New config: {config}")

	tokenizer_kwargs = {
		"cache_dir": model_args.cache_dir,
		"use_fast": model_args.use_fast_tokenizer,
		"revision": model_args.model_revision,
		"token": model_args.token,
	}
	if model_args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
	elif model_args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if model_args.model_name_or_path:
		model = XLNetLMHeadModel.from_pretrained(
			model_args.model_name_or_path,
			from_tf=bool(".ckpt" in model_args.model_name_or_path),
			config=config,
			cache_dir=model_args.cache_dir,
			revision=model_args.model_revision,
			token=model_args.token,
			low_cpu_mem_usage=model_args.low_cpu_mem_usage,
		)
	else:
		logger.info("Training new model from scratch")
		model = XLNetLMHeadModel(config)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if training_args.do_train:
		column_names = raw_datasets["train"].column_names
	else:
		column_names = raw_datasets["validation"].column_names
	text_column_name = "text" if "text" in column_names else column_names[0]

	if data_args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)
	max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	if data_args.line_by_line:
		padding = "max_length" if data_args.pad_to_max_length else False

		def tokenize_function(examples):
			examples["text"] = [line for line in examples["text"] if len(line) > 0 and not line.isspace()]
			return tokenizer(examples["text"], padding=padding, truncation=True, max_length=max_seq_length)

		with training_args.main_process_first(desc="dataset map tokenization"):
			tokenized_datasets = raw_datasets.map(
				tokenize_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=[text_column_name],
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on dataset line_by_line",
			)
	else:
		def tokenize_function(examples):
			return tokenizer(examples[text_column_name])

		with training_args.main_process_first(desc="dataset map tokenization"):
			tokenized_datasets = raw_datasets.map(
				tokenize_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on every text in dataset",
			)

		def group_texts(examples):
			concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
			total_length = len(concatenated_examples[list(examples.keys())[0]])
			total_length = (total_length // max_seq_length) * max_seq_length
			result = {
				k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]
				for k, t in concatenated_examples.items()
			}
			return result


		with training_args.main_process_first(desc="grouping texts together"):
			tokenized_datasets = tokenized_datasets.map(
				group_texts,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				load_from_cache_file=not data_args.overwrite_cache,
				desc=f"Grouping texts in chunks of {max_seq_length}",
			)

	if training_args.do_train:
		if "train" not in tokenized_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = tokenized_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	if training_args.do_eval:
		if "validation" not in tokenized_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_dataset = tokenized_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

	data_collator = DataCollatorForPermutationLanguageModeling(
		tokenizer=tokenizer,
		plm_probability=data_args.plm_probability,
		max_span_length=data_args.max_span_length,
	)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload
		metrics = train_result.metrics

		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")

		metrics = trainer.evaluate()

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
		try:
			perplexity = math.exp(metrics["eval_loss"])
		except OverflowError:
			perplexity = float("inf")
		metrics["perplexity"] = perplexity

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "language-modeling"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from typing import Any, Dict, Iterable, Iterator, Union, Optional
import itertools
import math

import torch


from allennlp.common import util
from allennlp.data.batch import Batch
from allennlp.data.data_loaders.data_loader import DataLoader, TensorDict
from allennlp.data.data_loaders.multiprocess_data_loader import MultiProcessDataLoader
from allennlp.data.data_loaders.multitask_scheduler import MultiTaskScheduler
from allennlp.data.data_loaders.multitask_epoch_sampler import MultiTaskEpochSampler
from allennlp.data.dataset_readers.multitask import MultiTaskDatasetReader
from allennlp.data.instance import Instance
from allennlp.data.vocabulary import Vocabulary
import allennlp.nn.util as nn_util


def maybe_shuffle_instances(loader: DataLoader, shuffle: bool) -> Iterable[Instance]:
	if shuffle:
		return util.shuffle_iterable(loader.iter_instances())
	else:
		return loader.iter_instances()


@DataLoader.register("multitask")
class MultiTaskDataLoader(DataLoader):

	def __init__(
		self,
		reader: MultiTaskDatasetReader,
		data_path: Dict[str, str],
		scheduler: MultiTaskScheduler,
		*,
		sampler: MultiTaskEpochSampler = None,
		instances_per_epoch: int = None,
		num_workers: Dict[str, int] = None,
		max_instances_in_memory: Dict[str, int] = None,
		start_method: Dict[str, str] = None,
		instance_queue_size: Dict[str, int] = None,
		instance_chunk_size: Dict[str, int] = None,
		shuffle: bool = True,
		cuda_device: Optional[Union[int, str, torch.device]] = None,
	) -> None:
		self.readers = reader.readers
		self.data_paths = data_path
		self.scheduler = scheduler
		self.sampler = sampler
		self.cuda_device: Optional[torch.device] = None
		if cuda_device is not None:
			if not isinstance(cuda_device, torch.device):
				self.cuda_device = torch.device(cuda_device)
			else:
				self.cuda_device = cuda_device

		self._instances_per_epoch = instances_per_epoch
		self._shuffle = shuffle

		if instances_per_epoch is not None and sampler is None:
			raise ValueError(
				"You must provide an EpochSampler if you want to not use all instances every epoch."
			)

		self._num_workers = num_workers or {}
		self._max_instances_in_memory = max_instances_in_memory or {}
		self._start_method = start_method or {}
		self._instance_queue_size = instance_queue_size or {}
		self._instance_chunk_size = instance_chunk_size or {}

		if self.readers.keys() != self.data_paths.keys():
			raise ValueError(
				f"Mismatch between readers ({self.readers.keys()}) and data paths "
				f"({self.data_paths.keys()})"
			)
		self._loaders = {key: self._make_data_loader(key) for key in self.readers}

		self._iterators: Dict[str, Iterator[Instance]] = {
			key: util.cycle_iterator_function(
				lambda l=loader: maybe_shuffle_instances(l, self._shuffle)  # type: ignore
			)
			for key, loader in self._loaders.items()
		}

	def __len__(self) -> int:
		if self._instances_per_epoch is None:
			return self.scheduler.count_batches(
				{dataset: len(loader) for dataset, loader in self._loaders.items()}
			)
		else:
			return self.scheduler.count_batches(
				{dataset: self._instances_per_epoch for dataset in self._loaders.keys()}
			)

	def __iter__(self) -> Iterator[TensorDict]:
		epoch_instances = self._get_instances_for_epoch()
		return (
			nn_util.move_to_device(
				Batch(instances).as_tensor_dict(),
				-1 if self.cuda_device is None else self.cuda_device,
			)
			for instances in self.scheduler.batch_instances(epoch_instances)
		)

	def iter_instances(self) -> Iterator[Instance]:
		for loader in self._loaders.values():
			yield from loader.iter_instances()

	def index_with(self, vocab: Vocabulary) -> None:
		for loader in self._loaders.values():
			loader.index_with(vocab)

	def set_target_device(self, device: torch.device) -> None:
		self.cuda_device = device

	def _get_instances_for_epoch(self) -> Dict[str, Iterable[Instance]]:
		if self._instances_per_epoch is None:
			return {
				key: maybe_shuffle_instances(loader, self._shuffle)
				for key, loader in self._loaders.items()
			}
		if self.sampler is None:
			raise ValueError(
				"You must specify an EpochSampler if self._instances_per_epoch is not None."
			)
		dataset_proportions = self.sampler.get_task_proportions(self._loaders)
		proportion_sum = sum(dataset_proportions.values())
		num_instances_per_dataset = {
			key: math.floor(proportion * self._instances_per_epoch / proportion_sum)
			for key, proportion in dataset_proportions.items()
		}
		return {
			key: itertools.islice(self._iterators[key], num_instances)
			for key, num_instances in num_instances_per_dataset.items()
		}

	def _make_data_loader(self, key: str) -> MultiProcessDataLoader:
		kwargs: Dict[str, Any] = {
			"reader": self.readers[key],
			"data_path": self.data_paths[key],
			"batch_size": 1,
		}
		if key in self._num_workers:
			kwargs["num_workers"] = self._num_workers[key]
		if key in self._max_instances_in_memory:
			kwargs["max_instances_in_memory"] = self._max_instances_in_memory[key]
		if key in self._start_method:
			kwargs["start_method"] = self._start_method[key]
		return MultiProcessDataLoader(**kwargs)

from __future__ import division
from __future__ import print_function

import argparse
import gzip
import os
import sys
import urllib

try:
	from urllib.error import URLError
	from urllib.request import urlretrieve
except ImportError:
	from urllib2 import URLError
	from urllib import urlretrieve

RESOURCES = [
	'train-images-idx3-ubyte.gz',
	'train-labels-idx1-ubyte.gz',
	't10k-images-idx3-ubyte.gz',
	't10k-labels-idx1-ubyte.gz',
]


def report_download_progress(chunk_number, chunk_size, file_size):
	if file_size != -1:
		percent = min(1, (chunk_number * chunk_size) / file_size)
		bar = '#' * int(64 * percent)
		sys.stdout.write('\r0% |{:<64}| {}%'.format(bar, int(percent * 100)))


def download(destination_path, url, quiet):
	if os.path.exists(destination_path):
		if not quiet:
			print('{} already exists, skipping ...'.format(destination_path))
	else:
		print('Downloading {} ...'.format(url))
		try:
			hook = None if quiet else report_download_progress
			urlretrieve(url, destination_path, reporthook=hook)
		except URLError:
			raise RuntimeError('Error downloading resource!')
		finally:
			if not quiet:
				print()


def unzip(zipped_path, quiet):
	unzipped_path = os.path.splitext(zipped_path)[0]
	if os.path.exists(unzipped_path):
		if not quiet:
			print('{} already exists, skipping ... '.format(unzipped_path))
		return
	with gzip.open(zipped_path, 'rb') as zipped_file:
		with open(unzipped_path, 'wb') as unzipped_file:
			unzipped_file.write(zipped_file.read())
			if not quiet:
				print('Unzipped {} ...'.format(zipped_path))


def main():
	parser = argparse.ArgumentParser(
		description='Download the MNIST dataset from the internet')
	parser.add_argument(
		'-d', '--destination', default='.', help='Destination directory')
	parser.add_argument(
		'-q',
		'--quiet',
		action='store_true',
		help="Don't report about progress")
	options = parser.parse_args()

	if not os.path.exists(options.destination):
		os.makedirs(options.destination)

	try:
		for resource in RESOURCES:
			path = os.path.join(options.destination, resource)
			url = 'http://yann.lecun.com/exdb/mnist/{}'.format(resource)
			download(path, url, options.quiet)
			unzip(path, options.quiet)
	except KeyboardInterrupt:
		print('Interrupted')


if __name__ == '__main__':
	main()

from typing import Callable, List, Set, Tuple, TypeVar, Optional
import warnings

from allennlp.common.checks import ConfigurationError
from allennlp.data.tokenizers import Token


TypedSpan = Tuple[int, Tuple[int, int]]
TypedStringSpan = Tuple[str, Tuple[int, int]]


class InvalidTagSequence(Exception):
	def __init__(self, tag_sequence=None):
		super().__init__()
		self.tag_sequence = tag_sequence

	def __str__(self):
		return " ".join(self.tag_sequence)


T = TypeVar("T", str, Token)


def enumerate_spans(
	sentence: List[T],
	offset: int = 0,
	max_span_width: int = None,
	min_span_width: int = 1,
	filter_function: Callable[[List[T]], bool] = None,
) -> List[Tuple[int, int]]:
	max_span_width = max_span_width or len(sentence)
	filter_function = filter_function or (lambda x: True)
	spans: List[Tuple[int, int]] = []

	for start_index in range(len(sentence)):
		last_end_index = min(start_index + max_span_width, len(sentence))
		first_end_index = min(start_index + min_span_width - 1, len(sentence))
		for end_index in range(first_end_index, last_end_index):
			start = offset + start_index
			end = offset + end_index
			if filter_function(sentence[slice(start_index, end_index + 1)]):
				spans.append((start, end))
	return spans


def bio_tags_to_spans(
	tag_sequence: List[str], classes_to_ignore: List[str] = None
) -> List[TypedStringSpan]:
	classes_to_ignore = classes_to_ignore or []
	spans: Set[Tuple[str, Tuple[int, int]]] = set()
	span_start = 0
	span_end = 0
	active_conll_tag = None
	for index, string_tag in enumerate(tag_sequence):
		bio_tag = string_tag[0]
		if bio_tag not in ["B", "I", "O"]:
			raise InvalidTagSequence(tag_sequence)
		conll_tag = string_tag[2:]
		if bio_tag == "O" or conll_tag in classes_to_ignore:
			if active_conll_tag is not None:
				spans.add((active_conll_tag, (span_start, span_end)))
			active_conll_tag = None
			continue
		elif bio_tag == "B":
			if active_conll_tag is not None:
				spans.add((active_conll_tag, (span_start, span_end)))
			active_conll_tag = conll_tag
			span_start = index
			span_end = index
		elif bio_tag == "I" and conll_tag == active_conll_tag:
			span_end += 1
		else:
			if active_conll_tag is not None:
				spans.add((active_conll_tag, (span_start, span_end)))
			active_conll_tag = conll_tag
			span_start = index
			span_end = index
	if active_conll_tag is not None:
		spans.add((active_conll_tag, (span_start, span_end)))
	return list(spans)


def iob1_tags_to_spans(
	tag_sequence: List[str], classes_to_ignore: List[str] = None
) -> List[TypedStringSpan]:
	classes_to_ignore = classes_to_ignore or []
	spans: Set[Tuple[str, Tuple[int, int]]] = set()
	span_start = 0
	span_end = 0
	active_conll_tag = None
	prev_bio_tag = None
	prev_conll_tag = None
	for index, string_tag in enumerate(tag_sequence):
		curr_bio_tag = string_tag[0]
		curr_conll_tag = string_tag[2:]

		if curr_bio_tag not in ["B", "I", "O"]:
			raise InvalidTagSequence(tag_sequence)
		if curr_bio_tag == "O" or curr_conll_tag in classes_to_ignore:
			if active_conll_tag is not None:
				spans.add((active_conll_tag, (span_start, span_end)))
			active_conll_tag = None
		elif _iob1_start_of_chunk(prev_bio_tag, prev_conll_tag, curr_bio_tag, curr_conll_tag):
			if active_conll_tag is not None:
				spans.add((active_conll_tag, (span_start, span_end)))
			active_conll_tag = curr_conll_tag
			span_start = index
			span_end = index
		else:
			span_end += 1

		prev_bio_tag = string_tag[0]
		prev_conll_tag = string_tag[2:]
	if active_conll_tag is not None:
		spans.add((active_conll_tag, (span_start, span_end)))
	return list(spans)


def _iob1_start_of_chunk(
	prev_bio_tag: Optional[str],
	prev_conll_tag: Optional[str],
	curr_bio_tag: str,
	curr_conll_tag: str,
) -> bool:
	if curr_bio_tag == "B":
		return True
	if curr_bio_tag == "I" and prev_bio_tag == "O":
		return True
	if curr_bio_tag != "O" and prev_conll_tag != curr_conll_tag:
		return True
	return False


def bioul_tags_to_spans(
	tag_sequence: List[str], classes_to_ignore: List[str] = None
) -> List[TypedStringSpan]:
	spans = []
	classes_to_ignore = classes_to_ignore or []
	index = 0
	while index < len(tag_sequence):
		label = tag_sequence[index]
		if label[0] == "U":
			spans.append((label.partition("-")[2], (index, index)))
		elif label[0] == "B":
			start = index
			while label[0] != "L":
				index += 1
				if index >= len(tag_sequence):
					raise InvalidTagSequence(tag_sequence)
				label = tag_sequence[index]
				if not (label[0] == "I" or label[0] == "L"):
					raise InvalidTagSequence(tag_sequence)
			spans.append((label.partition("-")[2], (start, index)))
		else:
			if label != "O":
				raise InvalidTagSequence(tag_sequence)
		index += 1
	return [span for span in spans if span[0] not in classes_to_ignore]


def iob1_to_bioul(tag_sequence: List[str]) -> List[str]:
	warnings.warn(
		"iob1_to_bioul has been replaced with 'to_bioul' to allow more encoding options.",
		FutureWarning,
	)
	return to_bioul(tag_sequence)


def to_bioul(tag_sequence: List[str], encoding: str = "IOB1") -> List[str]:
	if encoding not in {"IOB1", "BIO"}:
		raise ConfigurationError(f"Invalid encoding {encoding} passed to 'to_bioul'.")

	def replace_label(full_label, new_label):
		parts = list(full_label.partition("-"))
		parts[0] = new_label
		return "".join(parts)

	def pop_replace_append(in_stack, out_stack, new_label):
		tag = in_stack.pop()
		new_tag = replace_label(tag, new_label)
		out_stack.append(new_tag)

	def process_stack(stack, out_stack):
		if len(stack) == 1:
			pop_replace_append(stack, out_stack, "U")
		else:
			recoded_stack = []
			pop_replace_append(stack, recoded_stack, "L")
			while len(stack) >= 2:
				pop_replace_append(stack, recoded_stack, "I")
			pop_replace_append(stack, recoded_stack, "B")
			recoded_stack.reverse()
			out_stack.extend(recoded_stack)

	bioul_sequence = []
	stack: List[str] = []

	for label in tag_sequence:

		if label == "O" and len(stack) == 0:
			bioul_sequence.append(label)
		elif label == "O" and len(stack) > 0:
			process_stack(stack, bioul_sequence)
			bioul_sequence.append(label)
		elif label[0] == "I":
			if len(stack) == 0:
				if encoding == "BIO":
					raise InvalidTagSequence(tag_sequence)
				stack.append(label)
			else:
				this_type = label.partition("-")[2]
				prev_type = stack[-1].partition("-")[2]
				if this_type == prev_type:
					stack.append(label)
				else:
					if encoding == "BIO":
						raise InvalidTagSequence(tag_sequence)
					process_stack(stack, bioul_sequence)
					stack.append(label)
		elif label[0] == "B":
			if len(stack) > 0:
				process_stack(stack, bioul_sequence)
			stack.append(label)
		else:
			raise InvalidTagSequence(tag_sequence)

	if len(stack) > 0:
		process_stack(stack, bioul_sequence)

	return bioul_sequence


def bmes_tags_to_spans(
	tag_sequence: List[str], classes_to_ignore: List[str] = None
) -> List[TypedStringSpan]:

	def extract_bmes_tag_label(text):
		bmes_tag = text[0]
		label = text[2:]
		return bmes_tag, label

	spans: List[Tuple[str, List[int]]] = []
	prev_bmes_tag: Optional[str] = None
	for index, tag in enumerate(tag_sequence):
		bmes_tag, label = extract_bmes_tag_label(tag)
		if bmes_tag in ("B", "S"):
			spans.append((label, [index, index]))
		elif bmes_tag in ("M", "E") and prev_bmes_tag in ("B", "M") and spans[-1][0] == label:
			spans[-1][1][1] = index
		else:
			spans.append((label, [index, index]))
		prev_bmes_tag = bmes_tag

	classes_to_ignore = classes_to_ignore or []
	return [
		(span[0], (span[1][0], span[1][1]))
		for span in spans
		if span[0] not in classes_to_ignore
	]

import torch
import json
from os import PathLike
from typing import List, Tuple, Union, Optional

from allennlp.common.file_utils import cached_path
from allennlp.data import Vocabulary
from allennlp.data.tokenizers.tokenizer import Tokenizer


def _convert_word_to_ids_tensor(word, tokenizer, vocab, namespace, all_cases):
	if all_cases:
		words_list = [word.lower(), word.title(), word.upper()]
	else:
		words_list = [word]
	ids = []
	for w in words_list:
		if vocab:
			tokens = tokenizer.tokenize(w)
			ids.append(torch.tensor([vocab.get_token_index(t.text, namespace) for t in tokens]))
		else:
			ids.append(torch.tensor(tokenizer.tokenizer(w)["input_ids"]))
	return ids


def load_words(
	fname: Union[str, PathLike],
	tokenizer: Tokenizer,
	vocab: Optional[Vocabulary] = None,
	namespace: str = "tokens",
	all_cases: bool = True,
) -> List[torch.Tensor]:
	word_ids = []
	with open(cached_path(fname)) as f:
		words = json.load(f)
		for w in words:
			word_ids.extend(_convert_word_to_ids_tensor(w, tokenizer, vocab, namespace, all_cases))
	return word_ids


def load_word_pairs(
	fname: Union[str, PathLike],
	tokenizer: Tokenizer,
	vocab: Optional[Vocabulary] = None,
	namespace: str = "token",
	all_cases: bool = True,
) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
	word_ids1 = []
	word_ids2 = []
	with open(cached_path(fname)) as f:
		words = json.load(f)
		for w1, w2 in words:
			word_ids1.extend(
				_convert_word_to_ids_tensor(w1, tokenizer, vocab, namespace, all_cases)
			)
			word_ids2.extend(
				_convert_word_to_ids_tensor(w2, tokenizer, vocab, namespace, all_cases)
			)
	return word_ids1, word_ids2

from typing import List, Tuple

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.modules.conditional_random_field.conditional_random_field import (
	ConditionalRandomField,
)


class ConditionalRandomFieldWeightTrans(ConditionalRandomField):

	def __init__(
		self,
		num_tags: int,
		label_weights: List[float],
		constraints: List[Tuple[int, int]] = None,
		include_start_end_transitions: bool = True,
	) -> None:
		super().__init__(num_tags, constraints, include_start_end_transitions)

		if label_weights is None:
			raise ConfigurationError("label_weights must be given")

		self.register_buffer("label_weights", torch.Tensor(label_weights))

	def forward(
		self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor = None
	) -> torch.Tensor:
		if mask is None:
			mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
		else:
			mask = mask.to(torch.bool)

		label_weights = self.label_weights

		transitions = self.transitions * label_weights.view(-1, 1)

		inputs = inputs * label_weights.view(1, 1, -1)

		log_denominator = self._input_likelihood(inputs, transitions, mask)
		log_numerator = self._joint_likelihood(inputs, transitions, tags, mask)

		return torch.sum(log_numerator - log_denominator)

from typing import List

import torch
from torch.nn import ParameterList, Parameter

from allennlp.common.checks import ConfigurationError
from allennlp.nn import util


class ScalarMix(torch.nn.Module):

	def __init__(
		self,
		mixture_size: int,
		do_layer_norm: bool = False,
		initial_scalar_parameters: List[float] = None,
		trainable: bool = True,
	) -> None:
		super().__init__()
		self.mixture_size = mixture_size
		self.do_layer_norm = do_layer_norm

		if initial_scalar_parameters is None:
			initial_scalar_parameters = [0.0] * mixture_size
		elif len(initial_scalar_parameters) != mixture_size:
			raise ConfigurationError(
				"Length of initial_scalar_parameters {} differs "
				"from mixture_size {}".format(initial_scalar_parameters, mixture_size)
			)

		self.scalar_parameters = ParameterList(
			[
				Parameter(
					torch.FloatTensor([initial_scalar_parameters[i]]), requires_grad=trainable
				)
				for i in range(mixture_size)
			]
		)
		self.gamma = Parameter(torch.FloatTensor([1.0]), requires_grad=trainable)

	def forward(self, tensors: List[torch.Tensor], mask: torch.BoolTensor = None) -> torch.Tensor:
		if len(tensors) != self.mixture_size:
			raise ConfigurationError(
				"{} tensors were passed, but the module was initialized to "
				"mix {} tensors.".format(len(tensors), self.mixture_size)
			)

		def _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked):
			tensor_masked = tensor * broadcast_mask
			mean = torch.sum(tensor_masked) / num_elements_not_masked
			variance = (
				torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2) / num_elements_not_masked
			)
			return (tensor - mean) / torch.sqrt(variance + util.tiny_value_of_dtype(variance.dtype))

		normed_weights = torch.nn.functional.softmax(
			torch.cat([parameter for parameter in self.scalar_parameters]), dim=0
		)
		normed_weights = torch.split(normed_weights, split_size_or_sections=1)

		if not self.do_layer_norm:
			pieces = []
			for weight, tensor in zip(normed_weights, tensors):
				pieces.append(weight * tensor)
			return self.gamma * sum(pieces)

		else:
			assert mask is not None
			broadcast_mask = mask.unsqueeze(-1)
			input_dim = tensors[0].size(-1)
			num_elements_not_masked = torch.sum(mask) * input_dim

			pieces = []
			for weight, tensor in zip(normed_weights, tensors):
				pieces.append(
					weight * _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked)
				)
			return self.gamma * sum(pieces)

import argparse

from torch.utils.data import DataLoader

from .model import BERT
from .trainer import BERTTrainer
from .dataset import BERTDataset, WordVocab


def train():
	parser = argparse.ArgumentParser()

	parser.add_argument("-c", "--train_dataset", required=True, type=str, help="train dataset for train bert")
	parser.add_argument("-t", "--test_dataset", type=str, default=None, help="test set for evaluate train set")
	parser.add_argument("-v", "--vocab_path", required=True, type=str, help="built vocab model path with bert-vocab")
	parser.add_argument("-o", "--output_path", required=True, type=str, help="ex)output/bert.model")

	parser.add_argument("-hs", "--hidden", type=int, default=256, help="hidden size of transformer model")
	parser.add_argument("-l", "--layers", type=int, default=8, help="number of layers")
	parser.add_argument("-a", "--attn_heads", type=int, default=8, help="number of attention heads")
	parser.add_argument("-s", "--seq_len", type=int, default=20, help="maximum sequence len")

	parser.add_argument("-b", "--batch_size", type=int, default=64, help="number of batch_size")
	parser.add_argument("-e", "--epochs", type=int, default=10, help="number of epochs")
	parser.add_argument("-w", "--num_workers", type=int, default=5, help="dataloader worker size")

	parser.add_argument("--with_cuda", type=bool, default=True, help="training with CUDA: true, or false")
	parser.add_argument("--log_freq", type=int, default=10, help="printing loss every n iter: setting n")
	parser.add_argument("--corpus_lines", type=int, default=None, help="total number of lines in corpus")
	parser.add_argument("--cuda_devices", type=int, nargs='+', default=None, help="CUDA device ids")
	parser.add_argument("--on_memory", type=bool, default=True, help="Loading on memory: true or false")

	parser.add_argument("--lr", type=float, default=1e-3, help="learning rate of adam")
	parser.add_argument("--adam_weight_decay", type=float, default=0.01, help="weight_decay of adam")
	parser.add_argument("--adam_beta1", type=float, default=0.9, help="adam first beta value")
	parser.add_argument("--adam_beta2", type=float, default=0.999, help="adam first beta value")

	args = parser.parse_args()

	print("Loading Vocab", args.vocab_path)
	vocab = WordVocab.load_vocab(args.vocab_path)
	print("Vocab Size: ", len(vocab))

	print("Loading Train Dataset", args.train_dataset)
	train_dataset = BERTDataset(args.train_dataset, vocab, seq_len=args.seq_len,
								corpus_lines=args.corpus_lines, on_memory=args.on_memory)

	print("Loading Test Dataset", args.test_dataset)
	test_dataset = BERTDataset(args.test_dataset, vocab, seq_len=args.seq_len, on_memory=args.on_memory) \
		if args.test_dataset is not None else None

	print("Creating Dataloader")
	train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers)
	test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers) \
		if test_dataset is not None else None

	print("Building BERT model")
	bert = BERT(len(vocab), hidden=args.hidden, n_layers=args.layers, attn_heads=args.attn_heads)

	print("Creating BERT Trainer")
	trainer = BERTTrainer(bert, len(vocab), train_dataloader=train_data_loader, test_dataloader=test_data_loader,
						  lr=args.lr, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay,
						  with_cuda=args.with_cuda, cuda_devices=args.cuda_devices, log_freq=args.log_freq)

	print("Training Start")
	for epoch in range(args.epochs):
		trainer.train(epoch)
		trainer.save(epoch, args.output_path)

		if test_data_loader is not None:
			trainer.test(epoch)

import os
from typing import Tuple, Union, Optional, TYPE_CHECKING, List, Any, Dict, Sequence

from fairscale.nn import FullyShardedDataParallel as FS_FSDP
from fairscale.nn.wrap import enable_wrap, wrap
from fairscale.nn.misc import FlattenParamsWrapper
from fairscale.optim.grad_scaler import GradScaler

import torch
from torch.cuda import amp

from allennlp.nn.parallel.sharded_module_mixin import ShardedModuleMixin
from allennlp.nn.parallel.ddp_accelerator import (
	DdpAccelerator,
	DdpWrappedModel,
	StateDictType,
	LoadStateDictReturnType,
)

if TYPE_CHECKING:
	from allennlp.models import Model


class _FSDP(FS_FSDP, ShardedModuleMixin):

	def get_original_module(self) -> torch.nn.Module:
		module = self.module
		if isinstance(module, FlattenParamsWrapper):
			module = module.module
		return module


class FairScaleFsdpWrappedModel(DdpWrappedModel):

	@staticmethod
	def consolidate_sharded_state(
		sharded_state_files: Sequence[Union[str, os.PathLike]]
	) -> StateDictType:
		shard_weights: List[StateDictType] = []
		shard_metadata: List[Dict[str, Any]] = []
		for path in sharded_state_files:
			shard_state = torch.load(path, map_location="cpu")
			shard_weights.append(shard_state["weights"])
			shard_metadata.append(shard_state["metadata"])
		return _FSDP.consolidate_shard_weights(shard_weights, shard_metadata)

	def load_state_dict(
		self, state_dict: StateDictType, strict: bool = True
	) -> LoadStateDictReturnType:
		return self.model.load_local_state_dict(state_dict["weights"], strict=strict)  # type: ignore[operator]

	def state_dict(self, *args, **kwargs) -> StateDictType:
		weights = self.model.local_state_dict(*args, **kwargs)  # type: ignore[operator]
		metadata = self.model.local_metadata_dict()
		return {"weights": weights, "metadata": metadata}

	def clip_grad_norm_(self, max_norm: Union[float, int]) -> torch.Tensor:
		return self.model.clip_grad_norm_(max_norm)  # type: ignore[operator]

	def init_grad_scaler(self) -> amp.GradScaler:
		return GradScaler()


@DdpAccelerator.register("fairscale_fsdp")
class FairScaleFsdpAccelerator(DdpAccelerator):

	def __init__(
		self,
		*,
		mixed_precision: bool = False,
		reshard_after_forward: bool = True,
		flatten_parameters: bool = True,
		local_rank: Optional[int] = None,
		world_size: Optional[int] = None,
		cuda_device: Union[torch.device, int] = -1,
	) -> None:
		super().__init__(local_rank=local_rank, world_size=world_size, cuda_device=cuda_device)
		self._fsdp_kwargs = {
			"compute_device": self.cuda_device,
			"mixed_precision": mixed_precision,
			"reshard_after_forward": reshard_after_forward,
			"flatten_parameters": flatten_parameters,
		}
		if mixed_precision:
			self._fsdp_kwargs["move_params_to_cpu"] = True
			self._fsdp_kwargs["clear_autocast_cache"] = True

	def wrap_model(self, model: "Model") -> Tuple["Model", DdpWrappedModel]:
		wrapped_model = _FSDP(
			model,
			**self._fsdp_kwargs,
		)
		if not self._fsdp_kwargs["mixed_precision"] and self.cuda_device != torch.device("cpu"):
			wrapped_model = wrapped_model.cuda()
		for module in wrapped_model.modules():
			if isinstance(module, _FSDP):
				module._reset_lazy_init()
		return model, FairScaleFsdpWrappedModel(
			wrapped_model,
			local_rank=self.local_rank,
			world_size=self.world_size,
		)

	def wrap_module(self, module: torch.nn.Module) -> torch.nn.Module:
		with enable_wrap(wrapper_cls=_FSDP, **self._fsdp_kwargs):
			wrapped_module = wrap(module)
		return wrapped_module

from __future__ import print_function
import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils


parser = argparse.ArgumentParser()
parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')
parser.add_argument('--dataroot', required=False, help='path to dataset')
parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)
parser.add_argument('--batchSize', type=int, default=64, help='input batch size')
parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')
parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')
parser.add_argument('--ngf', type=int, default=64)
parser.add_argument('--ndf', type=int, default=64)
parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')
parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')
parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')
parser.add_argument('--cuda', action='store_true', default=False, help='enables cuda')
parser.add_argument('--dry-run', action='store_true', help='check a single training cycle works')
parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')
parser.add_argument('--netG', default='', help="path to netG (to continue training)")
parser.add_argument('--netD', default='', help="path to netD (to continue training)")
parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')
parser.add_argument('--manualSeed', type=int, help='manual seed')
parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')
parser.add_argument('--mps', action='store_true', default=False, help='enables macOS GPU training')

opt = parser.parse_args()
print(opt)

try:
	os.makedirs(opt.outf)
except OSError:
	pass

if opt.manualSeed is None:
	opt.manualSeed = random.randint(1, 10000)
print("Random Seed: ", opt.manualSeed)
random.seed(opt.manualSeed)
torch.manual_seed(opt.manualSeed)

cudnn.benchmark = True

if torch.cuda.is_available() and not opt.cuda:
	print("WARNING: You have a CUDA device, so you should probably run with --cuda")

if torch.backends.mps.is_available() and not opt.mps:
	print("WARNING: You have mps device, to enable macOS GPU run with --mps")
  
if opt.dataroot is None and str(opt.dataset).lower() != 'fake':
	raise ValueError("`dataroot` parameter is required for dataset \"%s\"" % opt.dataset)

if opt.dataset in ['imagenet', 'folder', 'lfw']:
	dataset = dset.ImageFolder(root=opt.dataroot,
							   transform=transforms.Compose([
								   transforms.Resize(opt.imageSize),
								   transforms.CenterCrop(opt.imageSize),
								   transforms.ToTensor(),
								   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
							   ]))
	nc=3
elif opt.dataset == 'lsun':
	classes = [ c + '_train' for c in opt.classes.split(',')]
	dataset = dset.LSUN(root=opt.dataroot, classes=classes,
						transform=transforms.Compose([
							transforms.Resize(opt.imageSize),
							transforms.CenterCrop(opt.imageSize),
							transforms.ToTensor(),
							transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
						]))
	nc=3
elif opt.dataset == 'cifar10':
	dataset = dset.CIFAR10(root=opt.dataroot, download=True,
						   transform=transforms.Compose([
							   transforms.Resize(opt.imageSize),
							   transforms.ToTensor(),
							   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
						   ]))
	nc=3

elif opt.dataset == 'mnist':
		dataset = dset.MNIST(root=opt.dataroot, download=True,
						   transform=transforms.Compose([
							   transforms.Resize(opt.imageSize),
							   transforms.ToTensor(),
							   transforms.Normalize((0.5,), (0.5,)),
						   ]))
		nc=1

elif opt.dataset == 'fake':
	dataset = dset.FakeData(image_size=(3, opt.imageSize, opt.imageSize),
							transform=transforms.ToTensor())
	nc=3

assert dataset
dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,
										 shuffle=True, num_workers=int(opt.workers))
use_mps = opt.mps and torch.backends.mps.is_available()
if opt.cuda:
	device = torch.device("cuda:0")
elif use_mps:
	device = torch.device("mps")
else:
	device = torch.device("cpu")

ngpu = int(opt.ngpu)
nz = int(opt.nz)
ngf = int(opt.ngf)
ndf = int(opt.ndf)


def weights_init(m):
	classname = m.__class__.__name__
	if classname.find('Conv') != -1:
		torch.nn.init.normal_(m.weight, 0.0, 0.02)
	elif classname.find('BatchNorm') != -1:
		torch.nn.init.normal_(m.weight, 1.0, 0.02)
		torch.nn.init.zeros_(m.bias)


class Generator(nn.Module):
	def __init__(self, ngpu):
		super(Generator, self).__init__()
		self.ngpu = ngpu
		self.main = nn.Sequential(
			nn.ConvTranspose2d(	 nz, ngf * 8, 4, 1, 0, bias=False),
			nn.BatchNorm2d(ngf * 8),
			nn.ReLU(True),
			nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ngf * 4),
			nn.ReLU(True),
			nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ngf * 2),
			nn.ReLU(True),
			nn.ConvTranspose2d(ngf * 2,	 ngf, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ngf),
			nn.ReLU(True),
			nn.ConvTranspose2d(	ngf,	  nc, 4, 2, 1, bias=False),
			nn.Tanh()
		)

	def forward(self, input):
		if input.is_cuda and self.ngpu > 1:
			output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
		else:
			output = self.main(input)
		return output


netG = Generator(ngpu).to(device)
netG.apply(weights_init)
if opt.netG != '':
	netG.load_state_dict(torch.load(opt.netG))
print(netG)


class Discriminator(nn.Module):
	def __init__(self, ngpu):
		super(Discriminator, self).__init__()
		self.ngpu = ngpu
		self.main = nn.Sequential(
			nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
			nn.LeakyReLU(0.2, inplace=True),
			nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ndf * 2),
			nn.LeakyReLU(0.2, inplace=True),
			nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ndf * 4),
			nn.LeakyReLU(0.2, inplace=True),
			nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
			nn.BatchNorm2d(ndf * 8),
			nn.LeakyReLU(0.2, inplace=True),
			nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
			nn.Sigmoid()
		)

	def forward(self, input):
		if input.is_cuda and self.ngpu > 1:
			output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
		else:
			output = self.main(input)

		return output.view(-1, 1).squeeze(1)


netD = Discriminator(ngpu).to(device)
netD.apply(weights_init)
if opt.netD != '':
	netD.load_state_dict(torch.load(opt.netD))
print(netD)

criterion = nn.BCELoss()

fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)
real_label = 1
fake_label = 0

optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))

if opt.dry_run:
	opt.niter = 1

for epoch in range(opt.niter):
	for i, data in enumerate(dataloader, 0):
		netD.zero_grad()
		real_cpu = data[0].to(device)
		batch_size = real_cpu.size(0)
		label = torch.full((batch_size,), real_label,
						   dtype=real_cpu.dtype, device=device)

		output = netD(real_cpu)
		errD_real = criterion(output, label)
		errD_real.backward()
		D_x = output.mean().item()

		noise = torch.randn(batch_size, nz, 1, 1, device=device)
		fake = netG(noise)
		label.fill_(fake_label)
		output = netD(fake.detach())
		errD_fake = criterion(output, label)
		errD_fake.backward()
		D_G_z1 = output.mean().item()
		errD = errD_real + errD_fake
		optimizerD.step()

		netG.zero_grad()
		label.fill_(real_label)  # fake labels are real for generator cost
		output = netD(fake)
		errG = criterion(output, label)
		errG.backward()
		D_G_z2 = output.mean().item()
		optimizerG.step()

		print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'
			  % (epoch, opt.niter, i, len(dataloader),
				 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))
		if i % 100 == 0:
			vutils.save_image(real_cpu,
					'%s/real_samples.png' % opt.outf,
					normalize=True)
			fake = netG(fixed_noise)
			vutils.save_image(fake.detach(),
					'%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),
					normalize=True)

		if opt.dry_run:
			break
	torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))
	torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))

import copy
from itertools import chain
import json
import logging
import os
import zlib
from collections import OrderedDict
from collections.abc import MutableMapping
from os import PathLike
from typing import Any, Dict, List, Union, Optional, TypeVar, Iterable, Set


try:
	from _jsonnet import evaluate_file, evaluate_snippet
except ImportError:

	def evaluate_file(filename: str, **_kwargs) -> str:
		logger.warning(
			f"error loading _jsonnet (this is expected on Windows), treating {filename} as plain json"
		)
		with open(filename, "r") as evaluation_file:
			return evaluation_file.read()

	def evaluate_snippet(_filename: str, expr: str, **_kwargs) -> str:
		logger.warning(
			"error loading _jsonnet (this is expected on Windows), treating snippet as plain json"
		)
		return expr


from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path

logger = logging.getLogger(__name__)


def infer_and_cast(value: Any):

	if isinstance(value, (int, float, bool)):
		return value
	elif isinstance(value, list):
		return [infer_and_cast(item) for item in value]
	elif isinstance(value, dict):
		return {key: infer_and_cast(item) for key, item in value.items()}
	elif isinstance(value, str):
		if value.lower() == "true":
			return True
		elif value.lower() == "false":
			return False
		else:
			try:
				return int(value)
			except ValueError:
				pass
			try:
				return float(value)
			except ValueError:
				return value
	else:
		raise ValueError(f"cannot infer type of {value}")


def _is_encodable(value: str) -> bool:
	return (value == "") or (value.encode("utf-8", "ignore") != b"")


def _environment_variables() -> Dict[str, str]:
	return {key: value for key, value in os.environ.items() if _is_encodable(value)}


T = TypeVar("T", dict, list)


def with_overrides(original: T, overrides_dict: Dict[str, Any], prefix: str = "") -> T:
	merged: T
	keys: Union[Iterable[str], Iterable[int]]
	if isinstance(original, list):
		merged = [None] * len(original)
		keys = range(len(original))
	elif isinstance(original, dict):
		merged = {}
		keys = chain(
			original.keys(), (k for k in overrides_dict if "." not in k and k not in original)
		)
	else:
		if prefix:
			raise ValueError(
				f"overrides for '{prefix[:-1]}.*' expected list or dict in original, "
				f"found {type(original)} instead"
			)
		else:
			raise ValueError(f"expected list or dict, found {type(original)} instead")

	used_override_keys: Set[str] = set()
	for key in keys:
		if str(key) in overrides_dict:
			merged[key] = copy.deepcopy(overrides_dict[str(key)])
			used_override_keys.add(str(key))
		else:
			overrides_subdict = {}
			for o_key in overrides_dict:
				if o_key.startswith(f"{key}."):
					overrides_subdict[o_key[len(f"{key}.") :]] = overrides_dict[o_key]
					used_override_keys.add(o_key)
			if overrides_subdict:
				merged[key] = with_overrides(
					original[key], overrides_subdict, prefix=prefix + f"{key}."
				)
			else:
				merged[key] = copy.deepcopy(original[key])

	unused_override_keys = [prefix + key for key in set(overrides_dict.keys()) - used_override_keys]
	if unused_override_keys:
		raise ValueError(f"overrides dict contains unused keys: {unused_override_keys}")

	return merged


def parse_overrides(
	serialized_overrides: str, ext_vars: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
	if serialized_overrides:
		ext_vars = {**_environment_variables(), **(ext_vars or {})}

		return json.loads(evaluate_snippet("", serialized_overrides, ext_vars=ext_vars))
	else:
		return {}


def _is_dict_free(obj: Any) -> bool:
	if isinstance(obj, dict):
		return False
	elif isinstance(obj, list):
		return all(_is_dict_free(item) for item in obj)
	else:
		return True


class Params(MutableMapping):

	DEFAULT = object()

	def __init__(self, params: Dict[str, Any], history: str = "") -> None:
		self.params = _replace_none(params)
		self.history = history

	def pop(self, key: str, default: Any = DEFAULT, keep_as_dict: bool = False) -> Any:

		if default is self.DEFAULT:
			try:
				value = self.params.pop(key)
			except KeyError:
				msg = f'key "{key}" is required'
				if self.history:
					msg += f' at location "{self.history}"'
				raise ConfigurationError(msg)
		else:
			value = self.params.pop(key, default)

		if keep_as_dict or _is_dict_free(value):
			logger.info(f"{self.history}{key} = {value}")
			return value
		else:
			return self._check_is_dict(key, value)

	def pop_int(self, key: str, default: Any = DEFAULT) -> Optional[int]:
		value = self.pop(key, default)
		if value is None:
			return None
		else:
			return int(value)

	def pop_float(self, key: str, default: Any = DEFAULT) -> Optional[float]:
		value = self.pop(key, default)
		if value is None:
			return None
		else:
			return float(value)

	def pop_bool(self, key: str, default: Any = DEFAULT) -> Optional[bool]:
		value = self.pop(key, default)
		if value is None:
			return None
		elif isinstance(value, bool):
			return value
		elif value == "true":
			return True
		elif value == "false":
			return False
		else:
			raise ValueError("Cannot convert variable to bool: " + value)

	def get(self, key: str, default: Any = DEFAULT):
		default = None if default is self.DEFAULT else default
		value = self.params.get(key, default)
		return self._check_is_dict(key, value)

	def pop_choice(
		self,
		key: str,
		choices: List[Any],
		default_to_first_choice: bool = False,
		allow_class_names: bool = True,
	) -> Any:
		default = choices[0] if default_to_first_choice else self.DEFAULT
		value = self.pop(key, default)
		ok_because_class_name = allow_class_names and "." in value
		if value not in choices and not ok_because_class_name:
			key_str = self.history + key
			message = (
				f"{value} not in acceptable choices for {key_str}: {choices}. "
				"You should either use the --include-package flag to make sure the correct module "
				"is loaded, or use a fully qualified class name in your config file like "
			)
			raise ConfigurationError(message)
		return value

	def as_dict(self, quiet: bool = False, infer_type_and_cast: bool = False):
		if infer_type_and_cast:
			params_as_dict = infer_and_cast(self.params)
		else:
			params_as_dict = self.params

		if quiet:
			return params_as_dict

		def log_recursively(parameters, history):
			for key, value in parameters.items():
				if isinstance(value, dict):
					new_local_history = history + key + "."
					log_recursively(value, new_local_history)
				else:
					logger.info(f"{history}{key} = {value}")

		log_recursively(self.params, self.history)
		return params_as_dict

	def as_flat_dict(self) -> Dict[str, Any]:
		flat_params = {}

		def recurse(parameters, path):
			for key, value in parameters.items():
				newpath = path + [key]
				if isinstance(value, dict):
					recurse(value, newpath)
				else:
					flat_params[".".join(newpath)] = value

		recurse(self.params, [])
		return flat_params

	def duplicate(self) -> "Params":
		return copy.deepcopy(self)

	def assert_empty(self, class_name: str):
		if self.params:
			raise ConfigurationError(
				"Extra parameters passed to {}: {}".format(class_name, self.params)
			)

	def __getitem__(self, key):
		if key in self.params:
			return self._check_is_dict(key, self.params[key])
		else:
			raise KeyError(str(key))

	def __setitem__(self, key, value):
		self.params[key] = value

	def __delitem__(self, key):
		del self.params[key]

	def __iter__(self):
		return iter(self.params)

	def __len__(self):
		return len(self.params)

	def _check_is_dict(self, new_history, value):
		if isinstance(value, dict):
			new_history = self.history + new_history + "."
			return Params(value, history=new_history)
		if isinstance(value, list):
			value = [self._check_is_dict(f"{new_history}.{i}", v) for i, v in enumerate(value)]
		return value

	@classmethod
	def from_file(
		cls,
		params_file: Union[str, PathLike],
		params_overrides: Union[str, Dict[str, Any]] = "",
		ext_vars: dict = None,
	) -> "Params":
		if ext_vars is None:
			ext_vars = {}

		params_file = cached_path(params_file)
		ext_vars = {**_environment_variables(), **ext_vars}

		file_dict = json.loads(evaluate_file(params_file, ext_vars=ext_vars))

		if isinstance(params_overrides, dict):
			params_overrides = json.dumps(params_overrides)
		overrides_dict = parse_overrides(params_overrides, ext_vars=ext_vars)

		if overrides_dict:
			param_dict = with_overrides(file_dict, overrides_dict)
		else:
			param_dict = file_dict

		return cls(param_dict)

	def to_file(self, params_file: str, preference_orders: List[List[str]] = None) -> None:
		with open(params_file, "w") as handle:
			json.dump(self.as_ordered_dict(preference_orders), handle, indent=4)

	def as_ordered_dict(self, preference_orders: List[List[str]] = None) -> OrderedDict:
		params_dict = self.as_dict(quiet=True)
		if not preference_orders:
			preference_orders = []
			preference_orders.append(
				[
					"dataset_reader",
					"iterator",
					"model",
					"train_data_path",
					"validation_data_path",
					"test_data_path",
					"trainer",
					"vocabulary",
				]
			)
			preference_orders.append(["type"])

		def order_func(key):
			order_tuple = [
				order.index(key) if key in order else len(order) for order in preference_orders
			]
			return order_tuple + [key]

		def order_dict(dictionary, order_func):
			result = OrderedDict()
			for key, val in sorted(dictionary.items(), key=lambda item: order_func(item[0])):
				result[key] = order_dict(val, order_func) if isinstance(val, dict) else val
			return result

		return order_dict(params_dict, order_func)

	def get_hash(self) -> str:
		dumped = json.dumps(self.params, sort_keys=True)
		hashed = zlib.adler32(dumped.encode())
		return str(hashed)

	def __str__(self) -> str:
		return f"{self.history}Params({self.params})"


def pop_choice(
	params: Dict[str, Any],
	key: str,
	choices: List[Any],
	default_to_first_choice: bool = False,
	history: str = "?.",
	allow_class_names: bool = True,
) -> Any:
	value = Params(params, history).pop_choice(
		key, choices, default_to_first_choice, allow_class_names=allow_class_names
	)
	return value


def _replace_none(params: Any) -> Any:
	if params == "None":
		return None
	elif isinstance(params, dict):
		for key, value in params.items():
			params[key] = _replace_none(value)
		return params
	elif isinstance(params, list):
		return [_replace_none(value) for value in params]
	return params


def remove_keys_from_params(params: Params, keys: List[str] = ["pretrained_file", "initializer"]):
	if isinstance(params, Params):  # The model could possibly be a string, for example.
		param_keys = params.keys()
		for key in keys:
			if key in param_keys:
				del params[key]
		for value in params.values():
			if isinstance(value, Params):
				remove_keys_from_params(value, keys)

import torch.nn as nn
from .layer_norm import LayerNorm


class SublayerConnection(nn.Module):

	def __init__(self, size, dropout):
		super(SublayerConnection, self).__init__()
		self.norm = LayerNorm(size)
		self.dropout = nn.Dropout(dropout)

	def forward(self, x, sublayer):
		"Apply residual connection to any sublayer with the same size."
		return x + self.dropout(sublayer(self.norm(x)))

from typing import Any, Dict

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.common.registrable import Registrable
from allennlp.training.optimizers import Optimizer
from allennlp.training.scheduler import Scheduler

from transformers.optimization import (
	get_constant_schedule,
	get_constant_schedule_with_warmup,
	get_cosine_schedule_with_warmup,
	get_cosine_with_hard_restarts_schedule_with_warmup,
)


class LearningRateScheduler(Scheduler, Registrable):
	def __init__(self, optimizer: torch.optim.Optimizer, last_epoch: int = -1) -> None:
		super().__init__(optimizer, "lr", last_epoch)

	def get_values(self):
		raise NotImplementedError


class _PyTorchLearningRateSchedulerWrapper(LearningRateScheduler):
	def __init__(self, lr_scheduler: torch.optim.lr_scheduler._LRScheduler) -> None:
		self.lr_scheduler = lr_scheduler

	def get_values(self):
		return self.lr_scheduler.get_last_lr()

	def step(self, metric: float = None) -> None:
		self.lr_scheduler.step()

	def state_dict(self) -> Dict[str, Any]:
		return self.lr_scheduler.state_dict()

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		self.lr_scheduler.load_state_dict(state_dict)


class _PyTorchLearningRateSchedulerWithMetricsWrapper(_PyTorchLearningRateSchedulerWrapper):
	def step(self, metric: float = None) -> None:
		if metric is None:
			raise ConfigurationError(
				"This learning rate scheduler requires "
				"a validation metric to compute the schedule and therefore "
				"must be used with a validation dataset."
			)
		self.lr_scheduler.step(metric)


@LearningRateScheduler.register("constant")
class ConstantLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):

	def __init__(self, optimizer: Optimizer, last_epoch: int = -1) -> None:
		lr_scheduler = get_constant_schedule(optimizer=optimizer, last_epoch=last_epoch)
		super().__init__(lr_scheduler)


@LearningRateScheduler.register("constant_with_warmup")
class ConstantWithWarmupLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):

	def __init__(self, optimizer: Optimizer, num_warmup_steps: int, last_epoch: int = -1) -> None:
		lr_scheduler = get_constant_schedule_with_warmup(
			optimizer=optimizer, num_warmup_steps=num_warmup_steps, last_epoch=last_epoch
		)
		super().__init__(lr_scheduler)


@LearningRateScheduler.register("cosine_with_warmup")
class CosineWithWarmupLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):

	def __init__(
		self,
		optimizer: Optimizer,
		num_warmup_steps: int,
		num_training_steps: int,
		num_cycles: float = 0.5,
		last_epoch: int = -1,
	) -> None:
		lr_scheduler = get_cosine_schedule_with_warmup(
			optimizer=optimizer,
			num_warmup_steps=num_warmup_steps,
			num_training_steps=num_training_steps,
			num_cycles=num_cycles,
			last_epoch=last_epoch,
		)
		super().__init__(lr_scheduler)


@LearningRateScheduler.register("cosine_hard_restarts_with_warmup")
class CosineHardRestartsWithWarmupLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):

	def __init__(
		self,
		optimizer: Optimizer,
		num_warmup_steps: int,
		num_training_steps: int,
		num_cycles: int = 1,
		last_epoch: int = -1,
	) -> None:
		lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(
			optimizer=optimizer,
			num_warmup_steps=num_warmup_steps,
			num_training_steps=num_training_steps,
			num_cycles=num_cycles,
			last_epoch=last_epoch,
		)
		super().__init__(lr_scheduler)

import torch
import torch.fx

class Foo(torch.nn.Module):
  def forward(self, x):
	with torch.profiler.record_function('foo'):
	  return torch.relu(x)

f = Foo()
x = torch.randn(5, 3, 2)
with torch.autograd.profiler.profile() as prof:
  f(x)

print(prof)


traced = torch.fx.symbolic_trace(f)
with torch.autograd.profiler.profile() as prof:
  traced(x)

print(prof)


class ProfilerTracer(torch.fx.Tracer):
  def trace(self, root, concrete_args=None):
	orig_record_function_enter = torch.autograd.profiler.record_function.__enter__
	orig_record_function_exit = torch.autograd.profiler.record_function.__exit__

	def fake_profiler_enter(_self):
	  nonlocal self
	  handle_proxy = self.create_proxy(
		  kind='call_function',
		  target=torch.ops.profiler._record_function_enter,
		  args=(_self.name,),
		  kwargs={})
	  
	  assert getattr(_self, '_fx_profiler_ctx', None) is None
	  setattr(_self, '_fx_profiler_ctx', handle_proxy)
	  return handle_proxy

	def fake_profiler_exit(_self, exc_type, exc_value, traceback):
	  assert hasattr(_self, '_fx_profiler_ctx')
	  handle_proxy = _self._fx_profiler_ctx
	  torch.ops.profiler._record_function_exit(handle_proxy)
	  setattr(_self, '_fx_profiler_ctx', None)

	torch.autograd.profiler.record_function.__enter__ = fake_profiler_enter
	torch.autograd.profiler.record_function.__exit__ = fake_profiler_exit

	try:
	  return super().trace(root, concrete_args)
	finally:
	  torch.autograd.profiler.record_function.__enter__ = orig_record_function_enter
	  torch.autograd.profiler.record_function.__exit__ = orig_record_function_exit

pt = ProfilerTracer()

graph_with_profiler = pt.trace(f)
traced_with_profiler = torch.fx.GraphModule(pt.root, graph_with_profiler)

with torch.autograd.profiler.profile() as prof:
  traced_with_profiler(x)

print(prof)

import logging
import re
import math
from typing import Callable, List, Tuple, Dict
import itertools

import tarfile

import torch
import torch.nn.init

from allennlp.common import FromParams, Registrable
from allennlp.common.checks import ConfigurationError

logger = logging.getLogger(__name__)


class Initializer(Registrable):

	default_implementation = "normal"

	def __call__(self, tensor: torch.Tensor, **kwargs) -> None:
		raise NotImplementedError


def uniform_unit_scaling(tensor: torch.Tensor, nonlinearity: str = "linear"):
	size = 1.0
	for dimension in list(tensor.size())[:-1]:
		size *= dimension

	activation_scaling = torch.nn.init.calculate_gain(nonlinearity, tensor)
	max_value = math.sqrt(3 / size) * activation_scaling

	return tensor.data.uniform_(-max_value, max_value)


def block_orthogonal(tensor: torch.Tensor, split_sizes: List[int], gain: float = 1.0) -> None:
	data = tensor.data
	sizes = list(tensor.size())
	if any(a % b != 0 for a, b in zip(sizes, split_sizes)):
		raise ConfigurationError(
			"tensor dimensions must be divisible by their respective "
			"split_sizes. Found size: {} and split_sizes: {}".format(sizes, split_sizes)
		)
	indexes = [list(range(0, max_size, split)) for max_size, split in zip(sizes, split_sizes)]
	for block_start_indices in itertools.product(*indexes):
		index_and_step_tuples = zip(block_start_indices, split_sizes)
		block_slice = tuple(
			slice(start_index, start_index + step) for start_index, step in index_and_step_tuples
		)
		data[block_slice] = torch.nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)


def zero(tensor: torch.Tensor) -> None:
	return tensor.data.zero_()


def lstm_hidden_bias(tensor: torch.Tensor) -> None:
	tensor.data.zero_()
	hidden_size = tensor.shape[0] // 4
	tensor.data[hidden_size : (2 * hidden_size)] = 1.0


class _InitializerWrapper(Initializer):
	def __init__(self, init_function: Callable[..., None], **kwargs):
		self._init_function = init_function
		self._kwargs = kwargs

	def __call__(self, tensor: torch.Tensor, **kwargs) -> None:
		self._init_function(tensor, **self._kwargs)

	def __repr__(self):
		return "Init: %s, with params: %s" % (self._init_function, self._kwargs)


@Initializer.register("normal")
class NormalInitializer(_InitializerWrapper):

	def __init__(self, mean: float = 0.0, std: float = 0.1):
		super().__init__(init_function=torch.nn.init.normal_, mean=mean, std=std)


@Initializer.register("orthogonal")
class OrthogonalInitializer(_InitializerWrapper):

	def __init__(self, gain: float = 1.0):
		super().__init__(init_function=torch.nn.init.orthogonal_, gain=gain)


@Initializer.register("uniform")
class UniformInitializer(_InitializerWrapper):

	def __init__(self, a: float = 0.0, b: float = 1.0):
		super().__init__(init_function=torch.nn.init.uniform_, a=a, b=b)


@Initializer.register("constant")
class ConstantInitializer(_InitializerWrapper):

	def __init__(self, val: float):
		super().__init__(init_function=torch.nn.init.constant_, val=val)


@Initializer.register("dirac")
class DiracInitializer(_InitializerWrapper):

	def __init__(self):
		super().__init__(init_function=torch.nn.init.dirac_)


@Initializer.register("xavier_uniform")
class XavierUniformInitializer(_InitializerWrapper):

	def __init__(self, gain: float = 1.0):
		super().__init__(init_function=torch.nn.init.xavier_uniform_, gain=gain)


@Initializer.register("xavier_normal")
class XavierNormalInitializer(_InitializerWrapper):

	def __init__(self, gain: float = 1.0):
		super().__init__(init_function=torch.nn.init.xavier_normal_, gain=gain)


@Initializer.register("kaiming_uniform")
class KaimingUniformInitializer(_InitializerWrapper):

	def __init__(self, a: float = 0.0, mode: str = "fan_in", nonlinearity: str = "leaky_relu"):
		super().__init__(
			init_function=torch.nn.init.kaiming_uniform_, a=a, mode=mode, nonlinearity=nonlinearity
		)


@Initializer.register("kaiming_normal")
class KaimingNormalInitializer(_InitializerWrapper):

	def __init__(self, a: float = 0.0, mode: str = "fan_in", nonlinearity: str = "leaky_relu"):
		super().__init__(
			init_function=torch.nn.init.kaiming_normal_, a=a, mode=mode, nonlinearity=nonlinearity
		)


@Initializer.register("sparse")
class SparseInitializer(_InitializerWrapper):

	def __init__(self, sparsity: float, std: float = 0.01):
		super().__init__(init_function=torch.nn.init.sparse_, sparsity=sparsity, std=std)


@Initializer.register("eye")
class EyeInitializer(_InitializerWrapper):

	def __init__(self):
		super().__init__(init_function=torch.nn.init.eye_)


@Initializer.register("block_orthogonal")
class BlockOrthogonalInitializer(_InitializerWrapper):

	def __init__(self, split_sizes: List[int], gain: float = 1.0):
		super().__init__(init_function=block_orthogonal, split_sizes=split_sizes, gain=gain)


@Initializer.register("uniform_unit_scaling")
class UniformUnitScalingInitializer(_InitializerWrapper):

	def __init__(self, nonlinearity: str = "linear"):
		super().__init__(init_function=uniform_unit_scaling, nonlinearity=nonlinearity)


@Initializer.register("zero")
class ZeroInitializer(_InitializerWrapper):

	def __init__(self):
		super().__init__(init_function=zero)


@Initializer.register("lstm_hidden_bias")
class LstmHiddenBiasInitializer(_InitializerWrapper):

	def __init__(self):
		super().__init__(init_function=lstm_hidden_bias)


@Initializer.register("pretrained")
class PretrainedModelInitializer(Initializer):

	def __init__(
		self, weights_file_path: str, parameter_name_overrides: Dict[str, str] = None
	) -> None:
		from allennlp.models.archival import (
			extracted_archive,
			get_weights_path,
		)  # import here to avoid circular imports

		self.weights: Dict[str, torch.Tensor]
		if tarfile.is_tarfile(weights_file_path):
			with extracted_archive(weights_file_path) as extraction_path:
				self.weights = torch.load(get_weights_path(extraction_path), map_location="cpu")
		else:
			self.weights = torch.load(weights_file_path, map_location="cpu")

		self.parameter_name_overrides = parameter_name_overrides or {}

	def __call__(self, tensor: torch.Tensor, parameter_name: str, **kwargs) -> None:  # type: ignore
		if parameter_name in self.parameter_name_overrides:
			parameter_name = self.parameter_name_overrides[parameter_name]

		source_weights = self.weights[parameter_name]
		if tensor.data.size() != source_weights.size():
			raise ConfigurationError(
				"Incompatible sizes found for parameter %s. "
				"Found %s and %s" % (parameter_name, tensor.data.size(), source_weights.size())
			)

		tensor.data.copy_(source_weights.data)


class InitializerApplicator(FromParams):

	def __init__(
		self, regexes: List[Tuple[str, Initializer]] = None, prevent_regexes: List[str] = None
	) -> None:
		self._initializers = regexes or []
		self._prevent_regex = None
		if prevent_regexes:
			self._prevent_regex = "(" + ")|(".join(prevent_regexes) + ")"

	def __call__(self, module: torch.nn.Module) -> None:
		logger.info("Initializing parameters")
		unused_regexes = {initializer[0] for initializer in self._initializers}
		uninitialized_parameters = set()
		for name, parameter in module.named_parameters():
			for initializer_regex, initializer in self._initializers:
				allow = self._prevent_regex is None or not bool(
					re.search(self._prevent_regex, name)
				)
				if allow and re.search(initializer_regex, name):
					logger.info("Initializing %s using %s initializer", name, initializer_regex)
					initializer(parameter, parameter_name=name)
					unused_regexes.discard(initializer_regex)
					break
			else:  # no break
				uninitialized_parameters.add(name)
		for regex in unused_regexes:
			logger.warning("Did not use initialization regex that was passed: %s", regex)
		logger.info(
			"Done initializing parameters; the following parameters are using their "
			"default initialization from their code"
		)
		uninitialized_parameter_list = list(uninitialized_parameters)
		uninitialized_parameter_list.sort()
		for name in uninitialized_parameter_list:
			logger.info("   %s", name)


import torch
from torch import nn as nn
from typing import Tuple, List, Callable
from allennlp.confidence_checks.verification_base import VerificationBase
import logging

logger = logging.getLogger(__name__)


class NormalizationBiasVerification(VerificationBase):

	normalization_layers = (
		nn.BatchNorm1d,
		nn.BatchNorm2d,
		nn.BatchNorm3d,
		nn.SyncBatchNorm,
		nn.InstanceNorm1d,
		nn.InstanceNorm2d,
		nn.InstanceNorm3d,
		nn.GroupNorm,
		nn.LayerNorm,
	)

	def __init__(self, model: nn.Module):
		super().__init__(model)
		self._hook_handles: List[Callable] = []
		self._module_sequence: List[Tuple[str, nn.Module]] = []
		self._detected_pairs: List[Tuple] = []

	@property
	def detected_pairs(self) -> List[Tuple]:
		return self._detected_pairs

	def check(self, inputs) -> bool:  # type: ignore
		inputs = self._get_inputs_copy(inputs)
		self.register_hooks()
		self._model_forward(inputs)
		self.destroy_hooks()
		self.collect_detections()
		return not self._detected_pairs

	def collect_detections(self):
		detected_pairs = []
		for (name0, mod0), (name1, mod1) in zip(
			self._module_sequence[:-1], self._module_sequence[1:]
		):
			bias = getattr(mod0, "bias", None)
			detected = (
				isinstance(mod1, self.normalization_layers)
				and mod1.training
				and isinstance(bias, torch.Tensor)
				and bias.requires_grad
			)
			if detected:
				detected_pairs.append((name0, name1))
		self._detected_pairs = detected_pairs
		if detected_pairs:
			logger.warning(self._verification_message())
		return detected_pairs

	def _verification_message(self):
		if self._detected_pairs:
			message = "\n\nThe model failed the NormalizationBiasVerification check:"
			for pair in self._detected_pairs:
				message += (
					f"\n  * Detected a layer '{pair[0]}' with bias followed by"
					f" a normalization layer '{pair[1]}'."
				)
			message += (
				"\n\nThis makes the normalization ineffective and can lead to unstable training. "
				"Either remove the normalization or turn off the bias.\n\n"
			)
		else:
			message = "\nThe model passed the NormalizationBiasVerification check."
		return message

	def register_hooks(self):
		hook_handles = []
		for name, module in self.model.named_modules():
			handle = module.register_forward_hook(self._create_hook(name))
			hook_handles.append(handle)
		self._hook_handles = hook_handles

	def _create_hook(self, module_name) -> Callable:
		def hook(module, inp_, out_):
			self._module_sequence.append((module_name, module))

		return hook

	def destroy_hooks(self):
		for hook in self._hook_handles:
			hook.remove()
		self._hook_handles = []

from os import PathLike
from pathlib import Path
from typing import Tuple, NamedTuple, Union, Dict, Any, List, Optional
import logging
import os
import tempfile
import tarfile
import shutil
from contextlib import contextmanager
import glob
import warnings

from torch.nn import Module

from allennlp.version import VERSION, _MAJOR, _MINOR, _PATCH
from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path
from allennlp.common.meta import Meta, META_NAME
from allennlp.common.params import Params
from allennlp.data.dataset_readers import DatasetReader
from allennlp.models.model import Model, _DEFAULT_WEIGHTS

logger = logging.getLogger(__name__)


class Archive(NamedTuple):

	model: Model
	config: Params
	dataset_reader: DatasetReader
	validation_dataset_reader: DatasetReader
	meta: Optional[Meta]

	def extract_module(self, path: str, freeze: bool = True) -> Module:
		modules_dict = {path: module for path, module in self.model.named_modules()}
		module = modules_dict.get(path)

		if not module:
			raise ConfigurationError(
				f"You asked to transfer module at path {path} from "
				f"the model {type(self.model)}. But it's not present."
			)
		if not isinstance(module, Module):
			raise ConfigurationError(
				f"The transferred object from model {type(self.model)} at path "
				f"{path} is not a PyTorch Module."
			)

		for parameter in module.parameters():  # type: ignore
			parameter.requires_grad_(not freeze)
		return module


CONFIG_NAME = "config.json"
_WEIGHTS_NAME = "weights.th"
_VERSION_TUPLE = (_MAJOR, _MINOR, _PATCH)


def verify_include_in_archive(include_in_archive: Optional[List[str]] = None):
	if include_in_archive is None:
		return
	saved_names = [CONFIG_NAME, _WEIGHTS_NAME, _DEFAULT_WEIGHTS, META_NAME, "vocabulary"]
	for archival_target in include_in_archive:
		if archival_target in saved_names:
			raise ConfigurationError(
				f"{', '.join(saved_names)} are saved names and cannot be used for include_in_archive."
			)


def archive_model(
	serialization_dir: Union[str, PathLike],
	weights: str = _DEFAULT_WEIGHTS,
	archive_path: Union[str, PathLike] = None,
	include_in_archive: Optional[List[str]] = None,
) -> str:
	extra_copy_of_weights_just_for_mypy = Path(weights)
	if extra_copy_of_weights_just_for_mypy.is_absolute():
		weights_file = extra_copy_of_weights_just_for_mypy
	else:
		weights_file = Path(serialization_dir) / extra_copy_of_weights_just_for_mypy
	if not os.path.exists(weights_file):
		err_msg = f"weights file '{weights_file}' does not exist, unable to archive model"
		logger.error(err_msg)
		raise RuntimeError(err_msg)

	config_file = os.path.join(serialization_dir, CONFIG_NAME)
	if not os.path.exists(config_file):
		err_msg = f"config file '{config_file}' does not exist, unable to archive model"
		logger.error(err_msg)
		raise RuntimeError(err_msg)

	meta_file = os.path.join(serialization_dir, META_NAME)

	if archive_path is not None:
		archive_file = archive_path
		if os.path.isdir(archive_file):
			archive_file = os.path.join(archive_file, "model.tar.gz")
	else:
		archive_file = os.path.join(serialization_dir, "model.tar.gz")

	logger.info("archiving weights and vocabulary to %s", archive_file)
	with tarfile.open(archive_file, "w:gz") as archive:
		archive.add(config_file, arcname=CONFIG_NAME)
		archive.add(weights_file, arcname=_WEIGHTS_NAME)
		archive.add(os.path.join(serialization_dir, "vocabulary"), arcname="vocabulary")
		if os.path.exists(meta_file):
			archive.add(meta_file, arcname=META_NAME)
		else:
			logger.warning("meta file %s does not exist", meta_file)

		if include_in_archive is not None:
			for archival_target in include_in_archive:
				archival_target_path = os.path.join(serialization_dir, archival_target)
				for path in glob.glob(archival_target_path):
					if os.path.exists(path):
						arcname = path[len(os.path.join(serialization_dir, "")) :]
						archive.add(path, arcname=arcname)

	return str(archive_file)


def load_archive(
	archive_file: Union[str, PathLike],
	cuda_device: int = -1,
	overrides: Union[str, Dict[str, Any]] = "",
	weights_file: str = None,
) -> Archive:
	resolved_archive_file = cached_path(archive_file)

	if resolved_archive_file == archive_file:
		logger.info(f"loading archive file {archive_file}")
	else:
		logger.info(f"loading archive file {archive_file} from cache at {resolved_archive_file}")

	meta: Optional[Meta] = None

	tempdir = None
	try:
		if os.path.isdir(resolved_archive_file):
			serialization_dir = resolved_archive_file
		else:
			with extracted_archive(resolved_archive_file, cleanup=False) as tempdir:
				serialization_dir = tempdir

		if weights_file:
			weights_path = weights_file
		else:
			weights_path = get_weights_path(serialization_dir)

		config = Params.from_file(os.path.join(serialization_dir, CONFIG_NAME), overrides)

		dataset_reader, validation_dataset_reader = _load_dataset_readers(
			config.duplicate(), serialization_dir
		)
		model = _load_model(config.duplicate(), weights_path, serialization_dir, cuda_device)

		meta_path = os.path.join(serialization_dir, META_NAME)
		if os.path.exists(meta_path):
			meta = Meta.from_path(meta_path)
	finally:
		if tempdir is not None:
			logger.info(f"removing temporary unarchived model dir at {tempdir}")
			shutil.rmtree(tempdir, ignore_errors=True)

	if meta is not None:
		_check_version_compatibility(archive_file, meta)

	return Archive(
		model=model,
		config=config,
		dataset_reader=dataset_reader,
		validation_dataset_reader=validation_dataset_reader,
		meta=meta,
	)


def _load_dataset_readers(config, serialization_dir):
	dataset_reader_params = config.get("dataset_reader")

	validation_dataset_reader_params = config.get(
		"validation_dataset_reader", dataset_reader_params.duplicate()
	)

	dataset_reader = DatasetReader.from_params(
		dataset_reader_params, serialization_dir=serialization_dir
	)
	validation_dataset_reader = DatasetReader.from_params(
		validation_dataset_reader_params, serialization_dir=serialization_dir
	)

	return dataset_reader, validation_dataset_reader


def _load_model(config, weights_path, serialization_dir, cuda_device):
	return Model.load(
		config,
		weights_file=weights_path,
		serialization_dir=serialization_dir,
		cuda_device=cuda_device,
	)


def get_weights_path(serialization_dir):
	weights_path = os.path.join(serialization_dir, _WEIGHTS_NAME)
	if not os.path.exists(weights_path):
		weights_path = os.path.join(serialization_dir, _DEFAULT_WEIGHTS)
	return weights_path


@contextmanager
def extracted_archive(resolved_archive_file, cleanup=True):
	tempdir = None
	try:
		tempdir = tempfile.mkdtemp()
		logger.info(f"extracting archive file {resolved_archive_file} to temp dir {tempdir}")
		with tarfile.open(resolved_archive_file, "r:gz") as archive:
			archive.extractall(tempdir)
		yield tempdir
	finally:
		if tempdir is not None and cleanup:
			logger.info(f"removing temporary unarchived model dir at {tempdir}")
			shutil.rmtree(tempdir, ignore_errors=True)


def _parse_version(version: str) -> Tuple[str, str, str]:
	try:
		major, minor, patch = version.split(".")[:3]
	except ValueError:
		raise ValueError(f"Invalid version '{version}', unable to parse")
	return (major, minor, patch)


def _check_version_compatibility(archive_file: Union[PathLike, str], meta: Meta):
	meta_version_tuple = _parse_version(meta.version)
	if _VERSION_TUPLE < meta_version_tuple:
		warnings.warn(
			f"The model {archive_file} was trained on a newer version of AllenNLP (v{meta.version}), "
			f"but you're using version {VERSION}.",
			UserWarning,
		)
	elif _VERSION_TUPLE[0] != meta_version_tuple[0]:
		warnings.warn(
			f"The model {archive_file} was trained on version {meta.version} of AllenNLP, "
			f"but you're using {VERSION} which may not be compatible.",
			UserWarning,
		)

from typing import Dict, List, Union
import logging
import json

from allennlp.common.file_utils import cached_path
from allennlp.data.dataset_readers.dataset_reader import DatasetReader
from allennlp.data.fields import LabelField, TextField, Field, ListField
from allennlp.data.instance import Instance
from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer
from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer
from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter

logger = logging.getLogger(__name__)


@DatasetReader.register("text_classification_json")
class TextClassificationJsonReader(DatasetReader):

	def __init__(
		self,
		token_indexers: Dict[str, TokenIndexer] = None,
		tokenizer: Tokenizer = None,
		segment_sentences: bool = False,
		max_sequence_length: int = None,
		skip_label_indexing: bool = False,
		text_key: str = "text",
		label_key: str = "label",
		**kwargs,
	) -> None:
		super().__init__(
			manual_distributed_sharding=True, manual_multiprocess_sharding=True, **kwargs
		)
		self._tokenizer = tokenizer or SpacyTokenizer()
		self._segment_sentences = segment_sentences
		self._max_sequence_length = max_sequence_length
		self._skip_label_indexing = skip_label_indexing
		self._token_indexers = token_indexers or {"tokens": SingleIdTokenIndexer()}
		self._text_key = text_key
		self._label_key = label_key
		if self._segment_sentences:
			self._sentence_segmenter = SpacySentenceSplitter()

	def _read(self, file_path):
		with open(cached_path(file_path), "r") as data_file:
			for line in self.shard_iterable(data_file.readlines()):
				if not line:
					continue
				items = json.loads(line)
				text = items[self._text_key]
				label = items.get(self._label_key)
				if label is not None:
					if self._skip_label_indexing:
						try:
							label = int(label)
						except ValueError:
							raise ValueError(
								"Labels must be integers if skip_label_indexing is True."
							)
					else:
						label = str(label)
				yield self.text_to_instance(text=text, label=label)

	def _truncate(self, tokens):
		if len(tokens) > self._max_sequence_length:
			tokens = tokens[: self._max_sequence_length]
		return tokens

	def text_to_instance(  # type: ignore
		self, text: str, label: Union[str, int] = None
	) -> Instance:

		fields: Dict[str, Field] = {}
		if self._segment_sentences:
			sentences: List[Field] = []
			sentence_splits = self._sentence_segmenter.split_sentences(text)
			for sentence in sentence_splits:
				word_tokens = self._tokenizer.tokenize(sentence)
				if self._max_sequence_length is not None:
					word_tokens = self._truncate(word_tokens)
				sentences.append(TextField(word_tokens))
			fields["tokens"] = ListField(sentences)
		else:
			tokens = self._tokenizer.tokenize(text)
			if self._max_sequence_length is not None:
				tokens = self._truncate(tokens)
			fields["tokens"] = TextField(tokens)
		if label is not None:
			fields["label"] = LabelField(label, skip_indexing=self._skip_label_indexing)
		return Instance(fields)

	def apply_token_indexers(self, instance: Instance) -> None:
		if self._segment_sentences:
			for text_field in instance.fields["tokens"]:  # type: ignore
				text_field._token_indexers = self._token_indexers
		else:
			instance.fields["tokens"]._token_indexers = self._token_indexers  # type: ignore

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datautils import MyTrainDataset


class Trainer:
	def __init__(
		self,
		model: torch.nn.Module,
		train_data: DataLoader,
		optimizer: torch.optim.Optimizer,
		gpu_id: int,
		save_every: int, 
	) -> None:
		self.gpu_id = gpu_id
		self.model = model.to(gpu_id)
		self.train_data = train_data
		self.optimizer = optimizer
		self.save_every = save_every

	def _run_batch(self, source, targets):
		self.optimizer.zero_grad()
		output = self.model(source)
		loss = F.cross_entropy(output, targets)
		loss.backward()
		self.optimizer.step()

	def _run_epoch(self, epoch):
		b_sz = len(next(iter(self.train_data))[0])
		print(f"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}")
		for source, targets in self.train_data:
			source = source.to(self.gpu_id)
			targets = targets.to(self.gpu_id)
			self._run_batch(source, targets)

	def _save_checkpoint(self, epoch):
		ckp = self.model.state_dict()
		PATH = "checkpoint.pt"
		torch.save(ckp, PATH)
		print(f"Epoch {epoch} | Training checkpoint saved at {PATH}")

	def train(self, max_epochs: int):
		for epoch in range(max_epochs):
			self._run_epoch(epoch)
			if epoch % self.save_every == 0:
				self._save_checkpoint(epoch)


def load_train_objs():
	train_set = MyTrainDataset(2048)  # load your dataset
	model = torch.nn.Linear(20, 1)  # load your model
	optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
	return train_set, model, optimizer


def prepare_dataloader(dataset: Dataset, batch_size: int):
	return DataLoader(
		dataset,
		batch_size=batch_size,
		pin_memory=True,
		shuffle=True
	)


def main(device, total_epochs, save_every, batch_size):
	dataset, model, optimizer = load_train_objs()
	train_data = prepare_dataloader(dataset, batch_size)
	trainer = Trainer(model, train_data, optimizer, device, save_every)
	trainer.train(total_epochs)


if __name__ == "__main__":
	import argparse
	parser = argparse.ArgumentParser(description='simple distributed training job')
	parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')
	parser.add_argument('save_every', type=int, help='How often to save a snapshot')
	parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')
	args = parser.parse_args()
	
	device = 0  # shorthand for cuda:0
	main(device, args.total_epochs, args.save_every, args.batch_size)

from collections import OrderedDict
from typing import Tuple

from torch import nn, FloatTensor, IntTensor
import torchvision

from allennlp.common.registrable import Registrable


class GridEmbedder(nn.Module, Registrable):

	def forward(self, images: FloatTensor, sizes: IntTensor) -> "OrderedDict[str, FloatTensor]":
		raise NotImplementedError()

	def get_feature_names(self) -> Tuple[str, ...]:
		raise NotImplementedError()


@GridEmbedder.register("null")
class NullGridEmbedder(GridEmbedder):

	def forward(self, images: FloatTensor, sizes: IntTensor) -> "OrderedDict[str, FloatTensor]":
		out = OrderedDict()
		out["0"] = images
		return out

	def get_feature_names(self) -> Tuple[str, ...]:
		return ("0",)


@GridEmbedder.register("resnet_backbone")
class ResnetBackbone(GridEmbedder):

	def __init__(self) -> None:
		super().__init__()
		detection_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
		self.backbone = detection_model.backbone
		del detection_model
		self.feature_names = tuple(
			[
				self.backbone.body.return_layers[key]
				for key in self.backbone.body.keys()
				if key in self.backbone.body.return_layers
			]
			+ ["pool"]
		)

	def forward(self, images: FloatTensor, sizes: IntTensor) -> "OrderedDict[str, FloatTensor]":
		return self.backbone(images)

	def get_feature_names(self) -> Tuple[str, ...]:
		return self.feature_names

from typing import Dict, Union, Set
import logging


import torch

from allennlp.data.fields.field import Field
from allennlp.data.vocabulary import Vocabulary
from allennlp.common.checks import ConfigurationError

logger = logging.getLogger(__name__)


class LabelField(Field[torch.Tensor]):

	__slots__ = ["label", "_label_namespace", "_label_id", "_skip_indexing"]

	_already_warned_namespaces: Set[str] = set()

	def __init__(
		self, label: Union[str, int], label_namespace: str = "labels", skip_indexing: bool = False
	) -> None:
		self.label = label
		self._label_namespace = label_namespace
		self._label_id = None
		self._maybe_warn_for_namespace(label_namespace)
		self._skip_indexing = skip_indexing

		if skip_indexing:
			if not isinstance(label, int):
				raise ConfigurationError(
					"In order to skip indexing, your labels must be integers. "
					"Found label = {}".format(label)
				)
			self._label_id = label
		elif not isinstance(label, str):
			raise ConfigurationError(
				"LabelFields must be passed a string label if skip_indexing=False. "
				"Found label: {} with type: {}.".format(label, type(label))
			)

	def _maybe_warn_for_namespace(self, label_namespace: str) -> None:
		if not (self._label_namespace.endswith("labels") or self._label_namespace.endswith("tags")):
			if label_namespace not in self._already_warned_namespaces:
				logger.warning(
					"Your label namespace was '%s'. We recommend you use a namespace "
					"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by "
					"default to your vocabulary.  See documentation for "
					"`non_padded_namespaces` parameter in Vocabulary.",
					self._label_namespace,
				)
				self._already_warned_namespaces.add(label_namespace)

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		if self._label_id is None:
			counter[self._label_namespace][self.label] += 1  # type: ignore

	def index(self, vocab: Vocabulary):
		if not self._skip_indexing:
			self._label_id = vocab.get_token_index(
				self.label, self._label_namespace  # type: ignore
			)

	def get_padding_lengths(self) -> Dict[str, int]:
		return {}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:
		tensor = torch.tensor(self._label_id, dtype=torch.long)
		return tensor

	def empty_field(self):
		return LabelField(-1, self._label_namespace, skip_indexing=True)

	def human_readable_repr(self) -> Union[str, int]:
		return self.label

	def __str__(self) -> str:
		return f"LabelField with label: {self.label} in namespace: '{self._label_namespace}'."

	def __len__(self):
		return 1

import logging
import re
import warnings
from typing import Dict, NamedTuple, Optional, Tuple, Union, cast

import transformers
from allennlp.common.checks import ConfigurationError
from transformers import AutoConfig, AutoModel

logger = logging.getLogger(__name__)


class TransformerSpec(NamedTuple):
	model_name: str
	override_weights_file: Optional[str] = None
	override_weights_strip_prefix: Optional[str] = None
	reinit_modules: Optional[Union[int, Tuple[int, ...], Tuple[str, ...]]] = None


_model_cache: Dict[TransformerSpec, transformers.PreTrainedModel] = {}


def get(
	model_name: str,
	make_copy: bool,
	override_weights_file: Optional[str] = None,
	override_weights_strip_prefix: Optional[str] = None,
	reinit_modules: Optional[Union[int, Tuple[int, ...], Tuple[str, ...]]] = None,
	load_weights: bool = True,
	**kwargs,
) -> transformers.PreTrainedModel:
	global _model_cache
	spec = TransformerSpec(
		model_name,
		override_weights_file,
		override_weights_strip_prefix,
		reinit_modules,
	)
	transformer = _model_cache.get(spec, None)
	if transformer is None:
		if not load_weights:
			if override_weights_file is not None:
				warnings.warn(
					"You specified an 'override_weights_file' in allennlp.common.cached_transformers.get(), "
					"but 'load_weights' is set to False, so 'override_weights_file' will be ignored.",
					UserWarning,
				)
			if reinit_modules is not None:
				warnings.warn(
					"You specified 'reinit_modules' in allennlp.common.cached_transformers.get(), "
					"but 'load_weights' is set to False, so 'reinit_modules' will be ignored.",
					UserWarning,
				)
			transformer = AutoModel.from_config(
				AutoConfig.from_pretrained(
					model_name,
					**kwargs,
				)
			)
		elif override_weights_file is not None:
			if reinit_modules is not None:
				warnings.warn(
					"You specified 'reinit_modules' in allennlp.common.cached_transformers.get(), "
					"but 'override_weights_file' is not None, so 'reinit_modules' will be ignored.",
					UserWarning,
				)
			import torch
			from allennlp.common.file_utils import cached_path

			override_weights_file = cached_path(override_weights_file)
			override_weights = torch.load(override_weights_file)
			if override_weights_strip_prefix is not None:

				def strip_prefix(s):
					if s.startswith(override_weights_strip_prefix):
						return s[len(override_weights_strip_prefix) :]
					else:
						return s

				valid_keys = {
					k
					for k in override_weights.keys()
					if k.startswith(override_weights_strip_prefix)
				}
				if len(valid_keys) > 0:
					logger.info(
						"Loading %d tensors from %s", len(valid_keys), override_weights_file
					)
				else:
					raise ValueError(
						f"Specified prefix of '{override_weights_strip_prefix}' means no tensors "
						f"will be loaded from {override_weights_file}."
					)
				override_weights = {strip_prefix(k): override_weights[k] for k in valid_keys}

			transformer = AutoModel.from_config(
				AutoConfig.from_pretrained(
					model_name,
					**kwargs,
				)
			)
			if hasattr(transformer, "module"):
				transformer.module.load_state_dict(override_weights)
			else:
				transformer.load_state_dict(override_weights)
		elif reinit_modules is not None:
			transformer = AutoModel.from_pretrained(
				model_name,
				**kwargs,
			)
			num_layers = transformer.config.num_hidden_layers
			if isinstance(reinit_modules, int):
				reinit_modules = tuple(range(num_layers - reinit_modules, num_layers))
			if all(isinstance(x, int) for x in reinit_modules):
				reinit_modules = cast(Tuple[int], reinit_modules)
				if any(layer_idx < 0 or layer_idx > num_layers for layer_idx in reinit_modules):
					raise ValueError(
						f"A layer index in reinit_modules ({reinit_modules}) is invalid."
						f" Must be between 0 and the maximum layer index ({num_layers - 1}.)"
					)
				try:
					for layer_idx in reinit_modules:
						transformer.encoder.layer[layer_idx].apply(transformer._init_weights)
				except AttributeError:
					raise ConfigurationError(
						f"Unable to re-initialize the layers of transformer model"
						f" {model_name} using layer indices. Please provide a tuple of"
						" strings corresponding to the names of the layers to re-initialize."
					)
			elif all(isinstance(x, str) for x in reinit_modules):
				for regex in reinit_modules:
					for name, module in transformer.named_modules():
						if re.search(str(regex), name):
							module.apply(transformer._init_weights)
			else:
				raise ValueError(
					"reinit_modules must be either an integer, a tuple of strings, or a tuple of integers."
				)
		else:
			transformer = AutoModel.from_pretrained(
				model_name,
				**kwargs,
			)
		_model_cache[spec] = transformer
	if make_copy:
		import copy

		return copy.deepcopy(transformer)
	else:
		return transformer


_tokenizer_cache: Dict[Tuple[str, str], transformers.PreTrainedTokenizer] = {}


def get_tokenizer(model_name: str, **kwargs) -> transformers.PreTrainedTokenizer:
	from allennlp.common.util import hash_object

	cache_key = (model_name, hash_object(kwargs))

	global _tokenizer_cache
	tokenizer = _tokenizer_cache.get(cache_key, None)
	if tokenizer is None:
		tokenizer = transformers.AutoTokenizer.from_pretrained(
			model_name,
			**kwargs,
		)
		_tokenizer_cache[cache_key] = tokenizer
	return tokenizer


def _clear_caches():
	global _model_cache
	global _tokenizer_cache
	_model_cache.clear()
	_tokenizer_cache.clear()

import torch


class InputVariationalDropout(torch.nn.Dropout):

	def forward(self, input_tensor):

		ones = input_tensor.data.new_ones(input_tensor.shape[0], input_tensor.shape[-1])
		dropout_mask = torch.nn.functional.dropout(ones, self.p, self.training, inplace=False)
		if self.inplace:
			input_tensor *= dropout_mask.unsqueeze(1)
			return None
		else:
			return dropout_mask.unsqueeze(1) * input_tensor

import copy
import dataclasses
import logging
from typing import Any, Dict, List, Optional, Tuple, Iterable


from transformers import PreTrainedTokenizer

from allennlp.common.util import sanitize_wordpiece
from allennlp.data.tokenizers.token_class import Token
from allennlp.data.tokenizers.tokenizer import Tokenizer

logger = logging.getLogger(__name__)


@Tokenizer.register("pretrained_transformer")
class PretrainedTransformerTokenizer(Tokenizer):

	def __init__(
		self,
		model_name: str,
		add_special_tokens: bool = True,
		max_length: Optional[int] = None,
		tokenizer_kwargs: Optional[Dict[str, Any]] = None,
		verification_tokens: Optional[Tuple[str, str]] = None,
	) -> None:
		if tokenizer_kwargs is None:
			tokenizer_kwargs = {}
		else:
			tokenizer_kwargs = tokenizer_kwargs.copy()
		tokenizer_kwargs.setdefault("use_fast", True)

		self._tokenizer_kwargs = tokenizer_kwargs
		self._model_name = model_name

		from allennlp.common import cached_transformers

		self.tokenizer = cached_transformers.get_tokenizer(
			self._model_name, add_special_tokens=False, **self._tokenizer_kwargs
		)

		self._add_special_tokens = add_special_tokens
		self._max_length = max_length

		self._tokenizer_lowercases = self.tokenizer_lowercases(self.tokenizer)

		if verification_tokens is None:
			try:
				self._reverse_engineer_special_tokens("a", "b", model_name, tokenizer_kwargs)
			except AssertionError:
				self._reverse_engineer_special_tokens("1", "2", model_name, tokenizer_kwargs)
		else:
			token_a, token_b = verification_tokens
			self._reverse_engineer_special_tokens(token_a, token_b, model_name, tokenizer_kwargs)

	def _reverse_engineer_special_tokens(
		self,
		token_a: str,
		token_b: str,
		model_name: str,
		tokenizer_kwargs: Optional[Dict[str, Any]],
	):
		self.sequence_pair_start_tokens = []
		self.sequence_pair_mid_tokens = []
		self.sequence_pair_end_tokens = []
		self.sequence_pair_first_token_type_id = None
		self.sequence_pair_second_token_type_id = None

		self.single_sequence_start_tokens = []
		self.single_sequence_end_tokens = []
		self.single_sequence_token_type_id = None

		from allennlp.common import cached_transformers

		tokenizer_with_special_tokens = cached_transformers.get_tokenizer(
			model_name, add_special_tokens=True, **(tokenizer_kwargs or {})
		)
		dummy_output = tokenizer_with_special_tokens.encode_plus(
			token_a,
			token_b,
			add_special_tokens=True,
			return_token_type_ids=True,
			return_attention_mask=False,
		)
		if len(dummy_output["token_type_ids"]) != len(dummy_output["input_ids"]):
			logger.warning(
				"Tokenizer library did not return valid token type ids. We will assume they are all zero."
			)
			dummy_output["token_type_ids"] = [0] * len(dummy_output["input_ids"])

		dummy_a = self.tokenizer.encode(token_a, add_special_tokens=False)[0]
		assert dummy_a in dummy_output["input_ids"]
		dummy_b = self.tokenizer.encode(token_b, add_special_tokens=False)[0]
		assert dummy_b in dummy_output["input_ids"]
		assert dummy_a != dummy_b

		seen_dummy_a = False
		seen_dummy_b = False
		for token_id, token_type_id in zip(
			dummy_output["input_ids"], dummy_output["token_type_ids"]
		):
			if token_id == dummy_a:
				if seen_dummy_a or seen_dummy_b:  # seeing a twice or b before a
					raise ValueError("Cannot auto-determine the number of special tokens added.")
				seen_dummy_a = True
				assert (
					self.sequence_pair_first_token_type_id is None
					or self.sequence_pair_first_token_type_id == token_type_id
				), "multiple different token type ids found for the first sequence"
				self.sequence_pair_first_token_type_id = token_type_id
				continue

			if token_id == dummy_b:
				if seen_dummy_b:  # seeing b twice
					raise ValueError("Cannot auto-determine the number of special tokens added.")
				seen_dummy_b = True
				assert (
					self.sequence_pair_second_token_type_id is None
					or self.sequence_pair_second_token_type_id == token_type_id
				), "multiple different token type ids found for the second sequence"
				self.sequence_pair_second_token_type_id = token_type_id
				continue

			token = Token(
				tokenizer_with_special_tokens.convert_ids_to_tokens(token_id),
				text_id=token_id,
				type_id=token_type_id,
			)
			if not seen_dummy_a:
				self.sequence_pair_start_tokens.append(token)
			elif not seen_dummy_b:
				self.sequence_pair_mid_tokens.append(token)
			else:
				self.sequence_pair_end_tokens.append(token)

		assert (
			len(self.sequence_pair_start_tokens)
			+ len(self.sequence_pair_mid_tokens)
			+ len(self.sequence_pair_end_tokens)
		) == self.tokenizer.num_special_tokens_to_add(pair=True)

		dummy_output = tokenizer_with_special_tokens.encode_plus(
			token_a,
			add_special_tokens=True,
			return_token_type_ids=True,
			return_attention_mask=False,
		)
		if len(dummy_output["token_type_ids"]) != len(dummy_output["input_ids"]):
			logger.warning(
				"Tokenizer library did not return valid token type ids. We will assume they are all zero."
			)
			dummy_output["token_type_ids"] = [0] * len(dummy_output["input_ids"])

		seen_dummy_a = False
		for token_id, token_type_id in zip(
			dummy_output["input_ids"], dummy_output["token_type_ids"]
		):
			if token_id == dummy_a:
				if seen_dummy_a:
					raise ValueError("Cannot auto-determine the number of special tokens added.")
				seen_dummy_a = True
				assert (
					self.single_sequence_token_type_id is None
					or self.single_sequence_token_type_id == token_type_id
				), "multiple different token type ids found for the sequence"
				self.single_sequence_token_type_id = token_type_id
				continue

			token = Token(
				tokenizer_with_special_tokens.convert_ids_to_tokens(token_id),
				text_id=token_id,
				type_id=token_type_id,
			)
			if not seen_dummy_a:
				self.single_sequence_start_tokens.append(token)
			else:
				self.single_sequence_end_tokens.append(token)

		assert (
			len(self.single_sequence_start_tokens) + len(self.single_sequence_end_tokens)
		) == self.tokenizer.num_special_tokens_to_add(pair=False)

	@staticmethod
	def tokenizer_lowercases(tokenizer: PreTrainedTokenizer) -> bool:
		tokenized = tokenizer.tokenize(
			"A"
		)  # Use a single character that won't be cut into word pieces.
		detokenized = " ".join(tokenized)
		return "a" in detokenized

	def tokenize(self, text: str) -> List[Token]:
		max_length = self._max_length
		if max_length is not None and not self._add_special_tokens:
			max_length += self.num_special_tokens_for_sequence()

		encoded_tokens = self.tokenizer.encode_plus(
			text=text,
			add_special_tokens=True,
			max_length=max_length,
			truncation=True if max_length is not None else False,
			return_tensors=None,
			return_offsets_mapping=self.tokenizer.is_fast,
			return_attention_mask=False,
			return_token_type_ids=True,
			return_special_tokens_mask=True,
		)
		token_ids, token_type_ids, special_tokens_mask, token_offsets = (
			encoded_tokens["input_ids"],
			encoded_tokens["token_type_ids"],
			encoded_tokens["special_tokens_mask"],
			encoded_tokens.get("offset_mapping"),
		)

		if token_offsets is None:
			token_offsets = self._estimate_character_indices(text, token_ids)

		tokens = []
		for token_id, token_type_id, special_token_mask, offsets in zip(
			token_ids, token_type_ids, special_tokens_mask, token_offsets
		):
			if not self._add_special_tokens and special_token_mask == 1:
				continue

			if offsets is None or offsets[0] >= offsets[1]:
				start = None
				end = None
			else:
				start, end = offsets

			tokens.append(
				Token(
					text=self.tokenizer.convert_ids_to_tokens(token_id, skip_special_tokens=False),
					text_id=token_id,
					type_id=token_type_id,
					idx=start,
					idx_end=end,
				)
			)

		return tokens

	def _estimate_character_indices(
		self, text: str, token_ids: List[int]
	) -> List[Optional[Tuple[int, int]]]:

		token_texts = [
			sanitize_wordpiece(t) for t in self.tokenizer.convert_ids_to_tokens(token_ids)
		]
		token_offsets: List[Optional[Tuple[int, int]]] = [None] * len(token_ids)
		if self._tokenizer_lowercases:
			text = text.lower()
			token_texts = [t.lower() for t in token_texts]

		min_allowed_skipped_whitespace = 3
		allowed_skipped_whitespace = min_allowed_skipped_whitespace

		text_index = 0
		token_index = 0
		while text_index < len(text) and token_index < len(token_ids):
			token_text = token_texts[token_index]
			token_start_index = text.find(token_text, text_index)

			if token_start_index < 0:
				token_index += 1
				allowed_skipped_whitespace += 1 + min_allowed_skipped_whitespace
				continue

			non_whitespace_chars_skipped = sum(
				1 for c in text[text_index:token_start_index] if not c.isspace()
			)
			if non_whitespace_chars_skipped > allowed_skipped_whitespace:
				token_index += 1
				allowed_skipped_whitespace += 1 + min_allowed_skipped_whitespace
				continue
			allowed_skipped_whitespace = min_allowed_skipped_whitespace

			token_offsets[token_index] = (
				token_start_index,
				token_start_index + len(token_text),
			)
			text_index = token_start_index + len(token_text)
			token_index += 1
		return token_offsets

	def _intra_word_tokenize(
		self, string_tokens: List[str]
	) -> Tuple[List[Token], List[Optional[Tuple[int, int]]]]:
		tokens: List[Token] = []
		offsets: List[Optional[Tuple[int, int]]] = []
		for token_string in string_tokens:
			wordpieces = self.tokenizer.encode_plus(
				token_string,
				add_special_tokens=False,
				return_tensors=None,
				return_offsets_mapping=False,
				return_attention_mask=False,
			)
			wp_ids = wordpieces["input_ids"]

			if len(wp_ids) > 0:
				offsets.append((len(tokens), len(tokens) + len(wp_ids) - 1))
				tokens.extend(
					Token(text=wp_text, text_id=wp_id)
					for wp_id, wp_text in zip(wp_ids, self.tokenizer.convert_ids_to_tokens(wp_ids))
				)
			else:
				offsets.append(None)
		return tokens, offsets

	@staticmethod
	def _increment_offsets(
		offsets: Iterable[Optional[Tuple[int, int]]], increment: int
	) -> List[Optional[Tuple[int, int]]]:
		return [
			None if offset is None else (offset[0] + increment, offset[1] + increment)
			for offset in offsets
		]

	def intra_word_tokenize(
		self, string_tokens: List[str]
	) -> Tuple[List[Token], List[Optional[Tuple[int, int]]]]:
		tokens, offsets = self._intra_word_tokenize(string_tokens)
		tokens = self.add_special_tokens(tokens)
		offsets = self._increment_offsets(offsets, len(self.single_sequence_start_tokens))
		return tokens, offsets

	def intra_word_tokenize_sentence_pair(
		self, string_tokens_a: List[str], string_tokens_b: List[str]
	) -> Tuple[List[Token], List[Optional[Tuple[int, int]]], List[Optional[Tuple[int, int]]]]:
		tokens_a, offsets_a = self._intra_word_tokenize(string_tokens_a)
		tokens_b, offsets_b = self._intra_word_tokenize(string_tokens_b)
		offsets_b = self._increment_offsets(
			offsets_b,
			(
				len(self.sequence_pair_start_tokens)
				+ len(tokens_a)
				+ len(self.sequence_pair_mid_tokens)
			),
		)
		tokens_a = self.add_special_tokens(tokens_a, tokens_b)
		offsets_a = self._increment_offsets(offsets_a, len(self.sequence_pair_start_tokens))

		return tokens_a, offsets_a, offsets_b

	def add_special_tokens(
		self, tokens1: List[Token], tokens2: Optional[List[Token]] = None
	) -> List[Token]:
		def with_new_type_id(tokens: List[Token], type_id: int) -> List[Token]:
			return [dataclasses.replace(t, type_id=type_id) for t in tokens]

		tokens2 = copy.deepcopy(tokens2)

		if tokens2 is None:
			return (
				self.single_sequence_start_tokens
				+ with_new_type_id(tokens1, self.single_sequence_token_type_id)  # type: ignore
				+ self.single_sequence_end_tokens
			)
		else:
			return (
				self.sequence_pair_start_tokens
				+ with_new_type_id(tokens1, self.sequence_pair_first_token_type_id)  # type: ignore
				+ self.sequence_pair_mid_tokens
				+ with_new_type_id(tokens2, self.sequence_pair_second_token_type_id)  # type: ignore
				+ self.sequence_pair_end_tokens
			)

	def num_special_tokens_for_sequence(self) -> int:
		return len(self.single_sequence_start_tokens) + len(self.single_sequence_end_tokens)

	def num_special_tokens_for_pair(self) -> int:
		return (
			len(self.sequence_pair_start_tokens)
			+ len(self.sequence_pair_mid_tokens)
			+ len(self.sequence_pair_end_tokens)
		)

	def _to_params(self) -> Dict[str, Any]:
		return {
			"type": "pretrained_transformer",
			"model_name": self._model_name,
			"add_special_tokens": self._add_special_tokens,
			"max_length": self._max_length,
			"tokenizer_kwargs": self._tokenizer_kwargs,
		}

import argparse
import os
from threading import Lock

import torch
import torch.distributed.autograd as dist_autograd
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.distributed.optim import DistributedOptimizer
from torchvision import datasets, transforms



class Net(nn.Module):
	def __init__(self, num_gpus=0):
		super(Net, self).__init__()
		print(f"Using {num_gpus} GPUs to train")
		self.num_gpus = num_gpus
		device = torch.device(
			"cuda:0" if torch.cuda.is_available() and self.num_gpus > 0 else "cpu")
		print(f"Putting first 2 convs on {str(device)}")
		self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)
		self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)
		if "cuda" in str(device) and num_gpus > 1:
			device = torch.device("cuda:1")

		print(f"Putting rest of layers on {str(device)}")
		self.dropout1 = nn.Dropout2d(0.25).to(device)
		self.dropout2 = nn.Dropout2d(0.5).to(device)
		self.fc1 = nn.Linear(9216, 128).to(device)
		self.fc2 = nn.Linear(128, 10).to(device)

	def forward(self, x):
		x = self.conv1(x)
		x = F.relu(x)
		x = self.conv2(x)
		x = F.max_pool2d(x, 2)

		x = self.dropout1(x)
		x = torch.flatten(x, 1)
		next_device = next(self.fc1.parameters()).device
		x = x.to(next_device)

		x = self.fc1(x)
		x = F.relu(x)
		x = self.dropout2(x)
		x = self.fc2(x)
		output = F.log_softmax(x, dim=1)
		return output



def call_method(method, rref, *args, **kwargs):
	return method(rref.local_value(), *args, **kwargs)



def remote_method(method, rref, *args, **kwargs):
	args = [method, rref] + list(args)
	return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs)


class ParameterServer(nn.Module):
	def __init__(self, num_gpus=0):
		super().__init__()
		model = Net(num_gpus=num_gpus)
		self.model = model
		self.input_device = torch.device(
			"cuda:0" if torch.cuda.is_available() and num_gpus > 0 else "cpu")

	def forward(self, inp):
		inp = inp.to(self.input_device)
		out = self.model(inp)
		out = out.to("cpu")
		return out

	def get_dist_gradients(self, cid):
		grads = dist_autograd.get_gradients(cid)
		cpu_grads = {}
		for k, v in grads.items():
			k_cpu, v_cpu = k.to("cpu"), v.to("cpu")
			cpu_grads[k_cpu] = v_cpu
		return cpu_grads

	def get_param_rrefs(self):
		param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]
		return param_rrefs


param_server = None
global_lock = Lock()


def get_parameter_server(num_gpus=0):
	global param_server
	with global_lock:
		if not param_server:
			param_server = ParameterServer(num_gpus=num_gpus)
		return param_server


def run_parameter_server(rank, world_size):
	print("PS master initializing RPC")
	rpc.init_rpc(name="parameter_server", rank=rank, world_size=world_size)
	print("RPC initialized! Running parameter server...")
	rpc.shutdown()
	print("RPC shutdown on parameter server.")



class TrainerNet(nn.Module):
	def __init__(self, num_gpus=0):
		super().__init__()
		self.num_gpus = num_gpus
		self.param_server_rref = rpc.remote(
			"parameter_server", get_parameter_server, args=(num_gpus,))

	def get_global_param_rrefs(self):
		remote_params = remote_method(
			ParameterServer.get_param_rrefs,
			self.param_server_rref)
		return remote_params

	def forward(self, x):
		model_output = remote_method(
			ParameterServer.forward, self.param_server_rref, x)
		return model_output


def run_training_loop(rank, num_gpus, train_loader, test_loader):
	net = TrainerNet(num_gpus=num_gpus)
	param_rrefs = net.get_global_param_rrefs()
	opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03)
	for i, (data, target) in enumerate(train_loader):
		with dist_autograd.context() as cid:
			model_output = net(data)
			target = target.to(model_output.device)
			loss = F.nll_loss(model_output, target)
			if i % 5 == 0:
				print(f"Rank {rank} training batch {i} loss {loss.item()}")
			dist_autograd.backward(cid, [loss])
			assert remote_method(
				ParameterServer.get_dist_gradients,
				net.param_server_rref,
				cid) != {}
			opt.step(cid)

	print("Training complete!")
	print("Getting accuracy....")
	get_accuracy(test_loader, net)


def get_accuracy(test_loader, model):
	model.eval()
	correct_sum = 0
	device = torch.device("cuda:0" if model.num_gpus > 0
		and torch.cuda.is_available() else "cpu")
	with torch.no_grad():
		for i, (data, target) in enumerate(test_loader):
			out = model(data)
			pred = out.argmax(dim=1, keepdim=True)
			pred, target = pred.to(device), target.to(device)
			correct = pred.eq(target.view_as(pred)).sum().item()
			correct_sum += correct

	print(f"Accuracy {correct_sum / len(test_loader.dataset)}")


def run_worker(rank, world_size, num_gpus, train_loader, test_loader):
	print(f"Worker rank {rank} initializing RPC")
	rpc.init_rpc(
		name=f"trainer_{rank}",
		rank=rank,
		world_size=world_size)

	print(f"Worker {rank} done initializing RPC")

	run_training_loop(rank, num_gpus, train_loader, test_loader)
	rpc.shutdown()



if __name__ == '__main__':
	parser = argparse.ArgumentParser(
		description="Parameter-Server RPC based training")
	parser.add_argument(
		"--world_size",
		type=int,
		default=4,
	parser.add_argument(
		"--rank",
		type=int,
		default=None,
		help="Global rank of this process. Pass in 0 for master.")
	parser.add_argument(
		"--num_gpus",
		type=int,
		default=0,
	parser.add_argument(
		"--master_addr",
		type=str,
		default="localhost",
	parser.add_argument(
		"--master_port",
		type=str,
		default="29500",

	args = parser.parse_args()
	assert args.rank is not None, "must provide rank argument."
	assert args.num_gpus <= 3, f"Only 0-2 GPUs currently supported (got {args.num_gpus})."
	os.environ['MASTER_ADDR'] = args.master_addr
	os.environ['MASTER_PORT'] = args.master_port
	processes = []
	world_size = args.world_size

	mp.set_start_method("spawn")

	if args.rank == 0:
		p = mp.Process(target=run_parameter_server, args=(0, world_size))
		p.start()
		processes.append(p)
	else:
		train_loader = torch.utils.data.DataLoader(
			datasets.MNIST('../data', train=True, download=True,
						   transform=transforms.Compose([
							   transforms.ToTensor(),
							   transforms.Normalize((0.1307,), (0.3081,))
						   ])),
			batch_size=32, shuffle=True)
		test_loader = torch.utils.data.DataLoader(
			datasets.MNIST('../data', train=False,
						   transform=transforms.Compose([
							   transforms.ToTensor(),
							   transforms.Normalize((0.1307,), (0.3081,))
						   ])),
			batch_size=32, shuffle=True)
		p = mp.Process(
			target=run_worker,
			args=(
				args.rank,
				world_size, args.num_gpus,
				train_loader,
				test_loader))
		p.start()
		processes.append(p)

	for p in processes:
		p.join()


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import evaluate
import numpy as np
import torch
from datasets import load_dataset
from PIL import Image
from torchvision.transforms import (
	CenterCrop,
	Compose,
	Lambda,
	Normalize,
	RandomHorizontalFlip,
	RandomResizedCrop,
	Resize,
	ToTensor,
)

import transformers
from transformers import (
	MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,
	AutoConfig,
	AutoImageProcessor,
	AutoModelForImageClassification,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version



logger = logging.getLogger(__name__)

check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/image-classification/requirements.txt")

MODEL_CONFIG_CLASSES = list(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def pil_loader(path: str):
	with open(path, "rb") as f:
		im = Image.open(f)
		return im.convert("RGB")


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None,
		metadata={
			"help": "Name of a dataset from the hub (could be your own, possibly private dataset hosted on the hub)."
		},
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_dir: Optional[str] = field(default=None, metadata={"help": "A folder containing the training data."})
	validation_dir: Optional[str] = field(default=None, metadata={"help": "A folder containing the validation data."})
	train_val_split: Optional[float] = field(
		default=0.15, metadata={"help": "Percent to split off of train for validation."}
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	image_column_name: str = field(
		default="image",
		metadata={"help": "The name of the dataset column containing the image data. Defaults to 'image'."},
	)
	label_column_name: str = field(
		default="label",
		metadata={"help": "The name of the dataset column containing the labels. Defaults to 'label'."},
	)

	def __post_init__(self):
		if self.dataset_name is None and (self.train_dir is None and self.validation_dir is None):
			raise ValueError(
				"You must specify either a dataset name from the hub or a train and/or validation directory."
			)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		default="google/vit-base-patch16-224-in21k",
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"},
	)
	model_type: Optional[str] = field(
		default=None,
		metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	image_processor_name: str = field(default=None, metadata={"help": "Name or path of preprocessor config."})
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	ignore_mismatched_sizes: bool = field(
		default=False,
		metadata={"help": "Will enable to load a pretrained model whose head dimensions are different."},
	)


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_image_classification", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		dataset = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_dir is not None:
			data_files["train"] = os.path.join(data_args.train_dir, "**")
		if data_args.validation_dir is not None:
			data_files["validation"] = os.path.join(data_args.validation_dir, "**")
		dataset = load_dataset(
			"imagefolder",
			data_files=data_files,
			cache_dir=model_args.cache_dir,
		)

	dataset_column_names = dataset["train"].column_names if "train" in dataset else dataset["validation"].column_names
	if data_args.image_column_name not in dataset_column_names:
		raise ValueError(
			f"--image_column_name {data_args.image_column_name} not found in dataset '{data_args.dataset_name}'. "
			"Make sure to set `--image_column_name` to the correct audio column - one of "
			f"{', '.join(dataset_column_names)}."
		)
	if data_args.label_column_name not in dataset_column_names:
		raise ValueError(
			f"--label_column_name {data_args.label_column_name} not found in dataset '{data_args.dataset_name}'. "
			"Make sure to set `--label_column_name` to the correct text column - one of "
			f"{', '.join(dataset_column_names)}."
		)

	def collate_fn(examples):
		pixel_values = torch.stack([example["pixel_values"] for example in examples])
		labels = torch.tensor([example[data_args.label_column_name] for example in examples])
		return {"pixel_values": pixel_values, "labels": labels}

	data_args.train_val_split = None if "validation" in dataset.keys() else data_args.train_val_split
	if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:
		split = dataset["train"].train_test_split(data_args.train_val_split)
		dataset["train"] = split["train"]
		dataset["validation"] = split["test"]

	labels = dataset["train"].features[data_args.label_column_name].names
	label2id, id2label = {}, {}
	for i, label in enumerate(labels):
		label2id[label] = str(i)
		id2label[str(i)] = label

	metric = evaluate.load("accuracy", cache_dir=model_args.cache_dir)

	def compute_metrics(p):
		return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)

	config = AutoConfig.from_pretrained(
		model_args.config_name or model_args.model_name_or_path,
		num_labels=len(labels),
		label2id=label2id,
		id2label=id2label,
		finetuning_task="image-classification",
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForImageClassification.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
		ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
	)
	image_processor = AutoImageProcessor.from_pretrained(
		model_args.image_processor_name or model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	if "shortest_edge" in image_processor.size:
		size = image_processor.size["shortest_edge"]
	else:
		size = (image_processor.size["height"], image_processor.size["width"])
	normalize = (
		Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
		if hasattr(image_processor, "image_mean") and hasattr(image_processor, "image_std")
		else Lambda(lambda x: x)
	)
	_train_transforms = Compose(
		[
			RandomResizedCrop(size),
			RandomHorizontalFlip(),
			ToTensor(),
			normalize,
		]
	)
	_val_transforms = Compose(
		[
			Resize(size),
			CenterCrop(size),
			ToTensor(),
			normalize,
		]
	)

	def train_transforms(example_batch):
		example_batch["pixel_values"] = [
			_train_transforms(pil_img.convert("RGB")) for pil_img in example_batch[data_args.image_column_name]
		]
		return example_batch

	def val_transforms(example_batch):
		example_batch["pixel_values"] = [
			_val_transforms(pil_img.convert("RGB")) for pil_img in example_batch[data_args.image_column_name]
		]
		return example_batch

	if training_args.do_train:
		if "train" not in dataset:
			raise ValueError("--do_train requires a train dataset")
		if data_args.max_train_samples is not None:
			dataset["train"] = (
				dataset["train"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))
			)
		dataset["train"].set_transform(train_transforms)

	if training_args.do_eval:
		if "validation" not in dataset:
			raise ValueError("--do_eval requires a validation dataset")
		if data_args.max_eval_samples is not None:
			dataset["validation"] = (
				dataset["validation"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))
			)
		dataset["validation"].set_transform(val_transforms)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=dataset["train"] if training_args.do_train else None,
		eval_dataset=dataset["validation"] if training_args.do_eval else None,
		compute_metrics=compute_metrics,
		tokenizer=image_processor,
		data_collator=collate_fn,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()
		trainer.log_metrics("train", train_result.metrics)
		trainer.save_metrics("train", train_result.metrics)
		trainer.save_state()

	if training_args.do_eval:
		metrics = trainer.evaluate()
		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {
		"finetuned_from": model_args.model_name_or_path,
		"tasks": "image-classification",
		"dataset": data_args.dataset_name,
		"tags": ["image-classification", "vision"],
	}
	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


if __name__ == "__main__":
	main()

import logging

import torchtext

class SourceField(torchtext.data.Field):

	def __init__(self, **kwargs):
		logger = logging.getLogger(__name__)

		if kwargs.get('batch_first') is False:
			logger.warning("Option batch_first has to be set to use pytorch-seq2seq.  Changed to True.")
		kwargs['batch_first'] = True
		if kwargs.get('include_lengths') is False:
			logger.warning("Option include_lengths has to be set to use pytorch-seq2seq.  Changed to True.")
		kwargs['include_lengths'] = True

		super(SourceField, self).__init__(**kwargs)

class TargetField(torchtext.data.Field):

	SYM_SOS = '<sos>'
	SYM_EOS = '<eos>'

	def __init__(self, **kwargs):
		logger = logging.getLogger(__name__)

		if kwargs.get('batch_first') == False:
			logger.warning("Option batch_first has to be set to use pytorch-seq2seq.  Changed to True.")
		kwargs['batch_first'] = True
		if kwargs.get('preprocessing') is None:
			kwargs['preprocessing'] = lambda seq: [self.SYM_SOS] + seq + [self.SYM_EOS]
		else:
			func = kwargs['preprocessing']
			kwargs['preprocessing'] = lambda seq: [self.SYM_SOS] + func(seq) + [self.SYM_EOS]

		self.sos_id = None
		self.eos_id = None
		super(TargetField, self).__init__(**kwargs)

	def build_vocab(self, *args, **kwargs):
		super(TargetField, self).build_vocab(*args, **kwargs)
		self.sos_id = self.vocab.stoi[self.SYM_SOS]
		self.eos_id = self.vocab.stoi[self.SYM_EOS]



import sys
import warnings
from os.path import abspath, dirname, join


git_repo_path = abspath(join(dirname(dirname(dirname(__file__))), "src"))
sys.path.insert(1, git_repo_path)


warnings.simplefilter(action="ignore", category=FutureWarning)


def pytest_addoption(parser):
	from transformers.testing_utils import pytest_addoption_shared

	pytest_addoption_shared(parser)


def pytest_terminal_summary(terminalreporter):
	from transformers.testing_utils import pytest_terminal_summary_main

	make_reports = terminalreporter.config.getoption("--make-reports")
	if make_reports:
		pytest_terminal_summary_main(terminalreporter, id=make_reports)

import logging
import re
import subprocess
from typing import Any, List, Tuple, Union

import torch
from torch import cuda

logger = logging.getLogger(__name__)


class ConfigurationError(Exception):

	def __reduce__(self) -> Union[str, Tuple[Any, ...]]:
		return type(self), (self.message,)

	def __init__(self, message: str):
		super().__init__()
		self.message = message

	def __str__(self):
		return self.message


class ExperimentalFeatureWarning(RuntimeWarning):

	pass


def log_pytorch_version_info():
	import torch

	logger.info("Pytorch version: %s", torch.__version__)


def check_dimensions_match(
	dimension_1: int, dimension_2: int, dim_1_name: str, dim_2_name: str
) -> None:
	if dimension_1 != dimension_2:
		raise ConfigurationError(
			f"{dim_1_name} must match {dim_2_name}, but got {dimension_1} "
			f"and {dimension_2} instead"
		)


def parse_cuda_device(cuda_device: Union[str, int, List[int]]) -> int:


	def from_list(strings):
		if len(strings) > 1:
			raise ConfigurationError(message)
		elif len(strings) == 1:
			return int(strings[0])
		else:
			return -1

	if isinstance(cuda_device, str):
		return from_list(re.split(r",\s*", cuda_device))
	elif isinstance(cuda_device, int):
		return cuda_device
	elif isinstance(cuda_device, list):
		return from_list(cuda_device)
	else:
		return int(cuda_device)  # type: ignore


def check_for_gpu(device: Union[int, torch.device, List[Union[int, torch.device]]]):
	if isinstance(device, list):
		for did in device:
			check_for_gpu(did)
	elif device is None:
		return
	else:
		from allennlp.common.util import int_to_device

		device = int_to_device(device)
		if device != torch.device("cpu"):
			num_devices_available = cuda.device_count()
			if num_devices_available == 0:
				raise ConfigurationError(
					"Experiment specified a GPU but none is available;"
					" if you want to run on CPU use the override"
					" 'trainer.cuda_device=-1' in the json config file."
				)
			elif device.index >= num_devices_available:
				raise ConfigurationError(
					f"Experiment specified GPU device {device.index}"
					f" but there are only {num_devices_available} devices "
					f" available."
				)


def check_for_java() -> bool:
	try:
		java_version = subprocess.check_output(["java", "-version"], stderr=subprocess.STDOUT)
		return "version" in java_version.decode()
	except FileNotFoundError:
		return False

from dataclasses import dataclass
import itertools
from os import PathLike
from typing import Iterable, Iterator, Optional, Union, TypeVar, Dict, List
import logging
import warnings

import torch.distributed as dist

from allennlp.data.instance import Instance
from allennlp.common import util
from allennlp.common.registrable import Registrable


logger = logging.getLogger(__name__)


@dataclass
class WorkerInfo:

	num_workers: int

	id: int


@dataclass
class DistributedInfo:

	world_size: int

	global_rank: int


_T = TypeVar("_T")

PathOrStr = Union[PathLike, str]
DatasetReaderInput = Union[PathOrStr, List[PathOrStr], Dict[str, PathOrStr]]


class DatasetReader(Registrable):

	def __init__(
		self,
		max_instances: Optional[int] = None,
		manual_distributed_sharding: bool = False,
		manual_multiprocess_sharding: bool = False,
		serialization_dir: Optional[str] = None,
	) -> None:
		if max_instances is not None and max_instances < 0:
			raise ValueError("If specified, max_instances should be a positive int")

		self.max_instances = max_instances
		self.manual_distributed_sharding = manual_distributed_sharding
		self.manual_multiprocess_sharding = manual_multiprocess_sharding
		self.serialization_dir = serialization_dir
		self._worker_info: Optional[WorkerInfo] = None
		self._distributed_info: Optional[DistributedInfo] = None
		if util.is_distributed():
			self._distributed_info = DistributedInfo(dist.get_world_size(), dist.get_rank())

	def read(self, file_path: DatasetReaderInput) -> Iterator[Instance]:
		for instance in self._multi_worker_islice(self._read(file_path)):  # type: ignore
			if self._worker_info is None:
				self.apply_token_indexers(instance)
			yield instance

	def _read(self, file_path) -> Iterable[Instance]:
		raise NotImplementedError

	def text_to_instance(self, *inputs) -> Instance:
		raise NotImplementedError

	def apply_token_indexers(self, instance: Instance) -> None:
		pass

	def get_worker_info(self) -> Optional[WorkerInfo]:
		return self._worker_info

	def get_distributed_info(self) -> Optional[DistributedInfo]:
		return self._distributed_info

	def _set_worker_info(self, info: Optional[WorkerInfo]) -> None:
		self._worker_info = info

	def _set_distributed_info(self, info: Optional[DistributedInfo]) -> None:
		self._distributed_info = info

	def shard_iterable(self, iterable: Iterable[_T]) -> Iterator[_T]:
		if not self.manual_distributed_sharding or not self.manual_multiprocess_sharding:
			raise ValueError(
				"self.shard_iterable() was called but self.manual_distributed_sharding and "
				"self.manual_multiprocess_sharding was not set to True. Did you forget to call "
				"super().__init__(manual_distributed_sharding=True, manual_multiprocess_sharding=True) "
				"in your constructor?"
			)

		sharded_slice: Iterator[_T] = iter(iterable)

		if util.is_distributed():
			sharded_slice = itertools.islice(
				sharded_slice, dist.get_rank(), None, dist.get_world_size()
			)

		if self._worker_info is not None:
			sharded_slice = itertools.islice(
				sharded_slice, self._worker_info.id, None, self._worker_info.num_workers
			)

		if self.max_instances is not None:
			sharded_slice = itertools.islice(sharded_slice, self.max_instances)

		return sharded_slice

	def _multi_worker_islice(
		self,
		iterable: Iterable[_T],
	) -> Iterator[_T]:

		sharded_slice: Iterator[_T] = iter(iterable)

		max_instances = self.max_instances

		if self._distributed_info is not None:
			if max_instances is not None:
				if self._distributed_info.global_rank < (
					max_instances % self._distributed_info.world_size
				):
					max_instances = max_instances // self._distributed_info.world_size + 1
				else:
					max_instances = max_instances // self._distributed_info.world_size

			if not self.manual_distributed_sharding:
				sharded_slice = itertools.islice(
					sharded_slice,
					self._distributed_info.global_rank,
					None,
					self._distributed_info.world_size,
				)

		if self._worker_info is not None:
			if max_instances is not None:
				if self._worker_info.id < (max_instances % self._worker_info.num_workers):
					max_instances = max_instances // self._worker_info.num_workers + 1
				else:
					max_instances = max_instances // self._worker_info.num_workers

			if not self.manual_multiprocess_sharding:
				warnings.warn(
					"Using multi-process data loading without setting "
					"DatasetReader.manual_multiprocess_sharding to True.\n"
					"Did you forget to set this?\n"
					"If you're not handling the multi-process sharding logic within your "
					"_read() method, there is probably no benefit to using more than one "
					"worker.",
					UserWarning,
				)
				sharded_slice = itertools.islice(
					sharded_slice, self._worker_info.id, None, self._worker_info.num_workers
				)

		if max_instances is not None:
			sharded_slice = itertools.islice(sharded_slice, max_instances)

		return sharded_slice


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
import torch
from datasets import load_dataset
from torchvision.transforms import Compose, Lambda, Normalize, RandomHorizontalFlip, RandomResizedCrop, ToTensor

import transformers
from transformers import (
	CONFIG_MAPPING,
	IMAGE_PROCESSOR_MAPPING,
	MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING,
	AutoConfig,
	AutoImageProcessor,
	AutoModelForMaskedImageModeling,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version



logger = logging.getLogger(__name__)

check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/image-pretraining/requirements.txt")

MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default="cifar10", metadata={"help": "Name of a dataset from the datasets package"}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	image_column_name: Optional[str] = field(
		default=None,
		metadata={"help": "The column name of the images in the files. If not set, will try to use 'image' or 'img'."},
	)
	train_dir: Optional[str] = field(default=None, metadata={"help": "A folder containing the training data."})
	validation_dir: Optional[str] = field(default=None, metadata={"help": "A folder containing the validation data."})
	train_val_split: Optional[float] = field(
		default=0.15, metadata={"help": "Percent to split off of train for validation."}
	)
	mask_patch_size: int = field(default=32, metadata={"help": "The size of the square patches to use for masking."})
	mask_ratio: float = field(
		default=0.6,
		metadata={"help": "Percentage of patches to mask."},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)

	def __post_init__(self):
		data_files = {}
		if self.train_dir is not None:
			data_files["train"] = self.train_dir
		if self.validation_dir is not None:
			data_files["val"] = self.validation_dir
		self.data_files = data_files if data_files else None


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		default=None,
		metadata={
			"help": (
				"The model checkpoint for weights initialization. Can be a local path to a pytorch_model.bin or a "
				"checkpoint identifier on the hub. "
				"Don't set if you want to train a model from scratch."
			)
		},
	)
	model_type: Optional[str] = field(
		default=None,
		metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
	)
	config_name_or_path: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	config_overrides: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"Override some existing default config settings when a model is trained from scratch. Example: "
				"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
			)
		},
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store (cache) the pretrained models/datasets downloaded from the hub"},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	image_processor_name: str = field(default=None, metadata={"help": "Name or path of preprocessor config."})
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	image_size: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"The size (resolution) of each image. If not specified, will use `image_size` of the configuration."
			)
		},
	)
	patch_size: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"The size (resolution) of each patch. If not specified, will use `patch_size` of the configuration."
			)
		},
	)
	encoder_stride: Optional[int] = field(
		default=None,
		metadata={"help": "Stride to use for the encoder."},
	)


class MaskGenerator:

	def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):
		self.input_size = input_size
		self.mask_patch_size = mask_patch_size
		self.model_patch_size = model_patch_size
		self.mask_ratio = mask_ratio

		if self.input_size % self.mask_patch_size != 0:
			raise ValueError("Input size must be divisible by mask patch size")
		if self.mask_patch_size % self.model_patch_size != 0:
			raise ValueError("Mask patch size must be divisible by model patch size")

		self.rand_size = self.input_size // self.mask_patch_size
		self.scale = self.mask_patch_size // self.model_patch_size

		self.token_count = self.rand_size**2
		self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))

	def __call__(self):
		mask_idx = np.random.permutation(self.token_count)[: self.mask_count]
		mask = np.zeros(self.token_count, dtype=int)
		mask[mask_idx] = 1

		mask = mask.reshape((self.rand_size, self.rand_size))
		mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)

		return torch.tensor(mask.flatten())


def collate_fn(examples):
	pixel_values = torch.stack([example["pixel_values"] for example in examples])
	mask = torch.stack([example["mask"] for example in examples])
	return {"pixel_values": pixel_values, "bool_masked_pos": mask}


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_mim", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	ds = load_dataset(
		data_args.dataset_name,
		data_args.dataset_config_name,
		data_files=data_args.data_files,
		cache_dir=model_args.cache_dir,
		token=model_args.token,
	)

	data_args.train_val_split = None if "validation" in ds.keys() else data_args.train_val_split
	if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:
		split = ds["train"].train_test_split(data_args.train_val_split)
		ds["train"] = split["train"]
		ds["validation"] = split["test"]

	config_kwargs = {
		"cache_dir": model_args.cache_dir,
		"revision": model_args.model_revision,
		"token": model_args.token,
		"trust_remote_code": model_args.trust_remote_code,
	}
	if model_args.config_name_or_path:
		config = AutoConfig.from_pretrained(model_args.config_name_or_path, **config_kwargs)
	elif model_args.model_name_or_path:
		config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
	else:
		config = CONFIG_MAPPING[model_args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")
		if model_args.config_overrides is not None:
			logger.info(f"Overriding config: {model_args.config_overrides}")
			config.update_from_string(model_args.config_overrides)
			logger.info(f"New config: {config}")

	if hasattr(config, "decoder_type"):
		config.decoder_type = "simmim"

	model_args.image_size = model_args.image_size if model_args.image_size is not None else config.image_size
	model_args.patch_size = model_args.patch_size if model_args.patch_size is not None else config.patch_size
	model_args.encoder_stride = (
		model_args.encoder_stride if model_args.encoder_stride is not None else config.encoder_stride
	)

	config.update(
		{
			"image_size": model_args.image_size,
			"patch_size": model_args.patch_size,
			"encoder_stride": model_args.encoder_stride,
		}
	)

	if model_args.image_processor_name:
		image_processor = AutoImageProcessor.from_pretrained(model_args.image_processor_name, **config_kwargs)
	elif model_args.model_name_or_path:
		image_processor = AutoImageProcessor.from_pretrained(model_args.model_name_or_path, **config_kwargs)
	else:
		IMAGE_PROCESSOR_TYPES = {
			conf.model_type: image_processor_class for conf, image_processor_class in IMAGE_PROCESSOR_MAPPING.items()
		}
		image_processor = IMAGE_PROCESSOR_TYPES[model_args.model_type]()

	if model_args.model_name_or_path:
		model = AutoModelForMaskedImageModeling.from_pretrained(
			model_args.model_name_or_path,
			from_tf=bool(".ckpt" in model_args.model_name_or_path),
			config=config,
			cache_dir=model_args.cache_dir,
			revision=model_args.model_revision,
			token=model_args.token,
			trust_remote_code=model_args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForMaskedImageModeling.from_config(config, trust_remote_code=model_args.trust_remote_code)

	if training_args.do_train:
		column_names = ds["train"].column_names
	else:
		column_names = ds["validation"].column_names

	if data_args.image_column_name is not None:
		image_column_name = data_args.image_column_name
	elif "image" in column_names:
		image_column_name = "image"
	elif "img" in column_names:
		image_column_name = "img"
	else:
		image_column_name = column_names[0]

	transforms = Compose(
		[
			Lambda(lambda img: img.convert("RGB") if img.mode != "RGB" else img),
			RandomResizedCrop(model_args.image_size, scale=(0.67, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),
			RandomHorizontalFlip(),
			ToTensor(),
			Normalize(mean=image_processor.image_mean, std=image_processor.image_std),
		]
	)

	mask_generator = MaskGenerator(
		input_size=model_args.image_size,
		mask_patch_size=data_args.mask_patch_size,
		model_patch_size=model_args.patch_size,
		mask_ratio=data_args.mask_ratio,
	)

	def preprocess_images(examples):

		examples["pixel_values"] = [transforms(image) for image in examples[image_column_name]]
		examples["mask"] = [mask_generator() for i in range(len(examples[image_column_name]))]

		return examples

	if training_args.do_train:
		if "train" not in ds:
			raise ValueError("--do_train requires a train dataset")
		if data_args.max_train_samples is not None:
			ds["train"] = ds["train"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))
		ds["train"].set_transform(preprocess_images)

	if training_args.do_eval:
		if "validation" not in ds:
			raise ValueError("--do_eval requires a validation dataset")
		if data_args.max_eval_samples is not None:
			ds["validation"] = (
				ds["validation"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))
			)
		ds["validation"].set_transform(preprocess_images)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=ds["train"] if training_args.do_train else None,
		eval_dataset=ds["validation"] if training_args.do_eval else None,
		tokenizer=image_processor,
		data_collator=collate_fn,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()
		trainer.log_metrics("train", train_result.metrics)
		trainer.save_metrics("train", train_result.metrics)
		trainer.save_state()

	if training_args.do_eval:
		metrics = trainer.evaluate()
		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {
		"finetuned_from": model_args.model_name_or_path,
		"tasks": "masked-image-modeling",
		"dataset": data_args.dataset_name,
		"tags": ["masked-image-modeling"],
	}
	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


if __name__ == "__main__":
	main()


import torch
import sklearn
import numpy as np

from allennlp.common.checks import ConfigurationError


class BiasDirection:

	def __init__(self, requires_grad: bool = False):
		self.requires_grad = requires_grad

	def _normalize_bias_direction(self, bias_direction: torch.Tensor):
		return bias_direction / torch.linalg.norm(bias_direction)


class PCABiasDirection(BiasDirection):

	def __call__(self, seed_embeddings: torch.Tensor):

		if seed_embeddings.ndim < 2:
			raise ConfigurationError("seed_embeddings1 must have at least two dimensions.")

		with torch.set_grad_enabled(self.requires_grad):
			_, _, V = torch.pca_lowrank(seed_embeddings, q=2)
			bias_direction = V[:, 0]
			return self._normalize_bias_direction(bias_direction)


class PairedPCABiasDirection(BiasDirection):

	def __call__(self, seed_embeddings1: torch.Tensor, seed_embeddings2: torch.Tensor):

		if seed_embeddings1.size() != seed_embeddings2.size():
			raise ConfigurationError("seed_embeddings1 and seed_embeddings2 must be the same size.")
		if seed_embeddings1.ndim < 2:
			raise ConfigurationError(
				"seed_embeddings1 and seed_embeddings2 must have at least two dimensions."
			)

		with torch.set_grad_enabled(self.requires_grad):
			paired_embeddings = seed_embeddings1 - seed_embeddings2
			_, _, V = torch.pca_lowrank(
				paired_embeddings,
				q=min(paired_embeddings.size(0), paired_embeddings.size(1)) - 1,
				center=False,
			)
			bias_direction = V[:, 0]
			return self._normalize_bias_direction(bias_direction)


class TwoMeansBiasDirection(BiasDirection):

	def __call__(self, seed_embeddings1: torch.Tensor, seed_embeddings2: torch.Tensor):
		if seed_embeddings1.ndim < 2 or seed_embeddings2.ndim < 2:
			raise ConfigurationError(
				"seed_embeddings1 and seed_embeddings2 must have at least two dimensions."
			)
		if seed_embeddings1.size(-1) != seed_embeddings2.size(-1):
			raise ConfigurationError("All seed embeddings must have same dimensionality.")

		with torch.set_grad_enabled(self.requires_grad):
			seed_embeddings1_mean = torch.mean(seed_embeddings1, dim=0)
			seed_embeddings2_mean = torch.mean(seed_embeddings2, dim=0)
			bias_direction = seed_embeddings1_mean - seed_embeddings2_mean
			return self._normalize_bias_direction(bias_direction)


class ClassificationNormalBiasDirection(BiasDirection):

	def __init__(self):
		super().__init__()

	def __call__(self, seed_embeddings1: torch.Tensor, seed_embeddings2: torch.Tensor):

		if seed_embeddings1.ndim < 2 or seed_embeddings2.ndim < 2:
			raise ConfigurationError(
				"seed_embeddings1 and seed_embeddings2 must have at least two dimensions."
			)
		if seed_embeddings1.size(-1) != seed_embeddings2.size(-1):
			raise ConfigurationError("All seed embeddings must have same dimensionality.")

		device = seed_embeddings1.device
		seed_embeddings1 = seed_embeddings1.flatten(end_dim=-2).detach().cpu().numpy()
		seed_embeddings2 = seed_embeddings2.flatten(end_dim=-2).detach().cpu().numpy()

		X = np.vstack([seed_embeddings1, seed_embeddings2])
		Y = np.concatenate([[0] * seed_embeddings1.shape[0], [1] * seed_embeddings2.shape[0]])

		classifier = sklearn.svm.SVC(kernel="linear").fit(X, Y)
		bias_direction = torch.Tensor(classifier.coef_[0]).to(device)

		return self._normalize_bias_direction(bias_direction)

from copy import deepcopy
from typing import Dict, Generic, List, TypeVar, Any

import torch

from allennlp.data.vocabulary import Vocabulary


DataArray = TypeVar(
	"DataArray", torch.Tensor, Dict[str, torch.Tensor], Dict[str, Dict[str, torch.Tensor]]
)


class Field(Generic[DataArray]):

	__slots__ = []  # type: ignore

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		pass

	def human_readable_repr(self) -> Any:
		raise NotImplementedError

	def index(self, vocab: Vocabulary):
		pass

	def get_padding_lengths(self) -> Dict[str, int]:
		raise NotImplementedError

	def as_tensor(self, padding_lengths: Dict[str, int]) -> DataArray:
		raise NotImplementedError

	def empty_field(self) -> "Field":
		raise NotImplementedError

	def batch_tensors(self, tensor_list: List[DataArray]) -> DataArray:  # type: ignore

		return torch.stack(tensor_list)

	def __eq__(self, other) -> bool:
		if isinstance(self, other.__class__):
			for class_ in self.__class__.mro():
				for attr in getattr(class_, "__slots__", []):
					if getattr(self, attr) != getattr(other, attr):
						return False
			if hasattr(self, "__dict__"):
				return self.__dict__ == other.__dict__
			return True
		return NotImplemented

	def __len__(self):
		raise NotImplementedError

	def duplicate(self):
		return deepcopy(self)

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

class Sequence(nn.Module):
	def __init__(self):
		super(Sequence, self).__init__()
		self.lstm1 = nn.LSTMCell(1, 51)
		self.lstm2 = nn.LSTMCell(51, 51)
		self.linear = nn.Linear(51, 1)

	def forward(self, input, future = 0):
		outputs = []
		h_t = torch.zeros(input.size(0), 51, dtype=torch.double)
		c_t = torch.zeros(input.size(0), 51, dtype=torch.double)
		h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)
		c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)

		for input_t in input.split(1, dim=1):
			h_t, c_t = self.lstm1(input_t, (h_t, c_t))
			h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))
			output = self.linear(h_t2)
			outputs += [output]
		for i in range(future):# if we should predict the future
			h_t, c_t = self.lstm1(output, (h_t, c_t))
			h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))
			output = self.linear(h_t2)
			outputs += [output]
		outputs = torch.cat(outputs, dim=1)
		return outputs


if __name__ == '__main__':
	parser = argparse.ArgumentParser()
	parser.add_argument('--steps', type=int, default=15, help='steps to run')
	opt = parser.parse_args()
	np.random.seed(0)
	torch.manual_seed(0)
	data = torch.load('traindata.pt')
	input = torch.from_numpy(data[3:, :-1])
	target = torch.from_numpy(data[3:, 1:])
	test_input = torch.from_numpy(data[:3, :-1])
	test_target = torch.from_numpy(data[:3, 1:])
	seq = Sequence()
	seq.double()
	criterion = nn.MSELoss()
	optimizer = optim.LBFGS(seq.parameters(), lr=0.8)
	for i in range(opt.steps):
		print('STEP: ', i)
		def closure():
			optimizer.zero_grad()
			out = seq(input)
			loss = criterion(out, target)
			print('loss:', loss.item())
			loss.backward()
			return loss
		optimizer.step(closure)
		with torch.no_grad():
			future = 1000
			pred = seq(test_input, future=future)
			loss = criterion(pred[:, :-future], test_target)
			print('test loss:', loss.item())
			y = pred.detach().numpy()
		plt.figure(figsize=(30,10))
		plt.title('Predict future values for time sequences\n(Dashlines are predicted values)', fontsize=30)
		plt.xlabel('x', fontsize=20)
		plt.ylabel('y', fontsize=20)
		plt.xticks(fontsize=20)
		plt.yticks(fontsize=20)
		def draw(yi, color):
			plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)
			plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)
		draw(y[0], 'r')
		draw(y[1], 'g')
		draw(y[2], 'b')
		plt.savefig('predict%d.pdf'%i)
		plt.close()

import logging
import os
import pathlib
import shutil
import tempfile

from allennlp.common.checks import log_pytorch_version_info

TEST_DIR = tempfile.mkdtemp(prefix="allennlp_tests")


class AllenNlpTestCase:

	PROJECT_ROOT = (pathlib.Path(__file__).parent / ".." / ".." / "..").resolve()
	MODULE_ROOT = PROJECT_ROOT / "allennlp"
	TOOLS_ROOT = MODULE_ROOT / "tools"
	PROJECT_ROOT_FALLBACK = (
		pathlib.Path(os.environ["ALLENNLP_SRC_DIR"])
		if "ALLENNLP_SRC_DIR" in os.environ
		else (
			pathlib.Path(os.environ["SRC_DIR"])
			if "CONDA_BUILD" in os.environ
			else PROJECT_ROOT
		)
	)
	TESTS_ROOT = PROJECT_ROOT_FALLBACK / "tests"
	FIXTURES_ROOT = PROJECT_ROOT_FALLBACK / "test_fixtures"

	def setup_method(self):
		logging.basicConfig(
			format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", level=logging.DEBUG
		)
		logging.getLogger("allennlp.common.params").disabled = True
		logging.getLogger("allennlp.nn.initializers").disabled = True
		logging.getLogger("allennlp.modules.token_embedders.embedding").setLevel(logging.INFO)
		logging.getLogger("urllib3.connectionpool").disabled = True
		log_pytorch_version_info()
		self.TEST_DIR = pathlib.Path(TEST_DIR)

		os.makedirs(self.TEST_DIR, exist_ok=True)

	def teardown_method(self):
		shutil.rmtree(self.TEST_DIR)

import torch
from torch.nn.parameter import Parameter

from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention
from allennlp.nn import Activation


@MatrixAttention.register("bilinear")
class BilinearMatrixAttention(MatrixAttention):

	def __init__(
		self,
		matrix_1_dim: int,
		matrix_2_dim: int,
		activation: Activation = None,
		use_input_biases: bool = False,
		label_dim: int = 1,
	) -> None:
		super().__init__()
		if use_input_biases:
			matrix_1_dim += 1
			matrix_2_dim += 1

		if label_dim == 1:
			self._weight_matrix = Parameter(torch.Tensor(matrix_1_dim, matrix_2_dim))
		else:
			self._weight_matrix = Parameter(torch.Tensor(label_dim, matrix_1_dim, matrix_2_dim))

		self._bias = Parameter(torch.Tensor(1))
		self._activation = activation or Activation.by_name("linear")()
		self._use_input_biases = use_input_biases
		self.reset_parameters()

	def reset_parameters(self):
		torch.nn.init.xavier_uniform_(self._weight_matrix)
		self._bias.data.fill_(0)

	def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:

		if self._use_input_biases:
			bias1 = matrix_1.new_ones(matrix_1.size()[:-1] + (1,))
			bias2 = matrix_2.new_ones(matrix_2.size()[:-1] + (1,))

			matrix_1 = torch.cat([matrix_1, bias1], -1)
			matrix_2 = torch.cat([matrix_2, bias2], -1)

		weight = self._weight_matrix
		if weight.dim() == 2:
			weight = weight.unsqueeze(0)
		intermediate = torch.matmul(matrix_1.unsqueeze(1), weight)
		final = torch.matmul(intermediate, matrix_2.unsqueeze(1).transpose(2, 3))
		return self._activation(final.squeeze(1) + self._bias)


import copy
from collections import defaultdict, OrderedDict
from itertools import chain
import json
import logging
from os import PathLike
import re
from typing import Any, Dict, List, Optional, Sequence, Tuple, TypeVar, Union, NamedTuple

import math
import numpy
import torch
import torch.distributed as dist

from allennlp.common.checks import ConfigurationError
from allennlp.common.util import int_to_device, is_distributed, is_global_primary

logger = logging.getLogger(__name__)

T = TypeVar("T")
StateDictType = Union[Dict[str, torch.Tensor], "OrderedDict[str, torch.Tensor]"]


def move_to_device(obj, device: Union[torch.device, int]):
	device = int_to_device(device)

	if isinstance(obj, torch.Tensor):
		return obj if obj.device == device else obj.to(device=device)
	elif isinstance(obj, dict):
		for key, value in obj.items():
			obj[key] = move_to_device(value, device)
		return obj
	elif isinstance(obj, list):
		for i, item in enumerate(obj):
			obj[i] = move_to_device(item, device)
		return obj
	elif isinstance(obj, tuple) and hasattr(obj, "_fields"):
		return obj.__class__(*(move_to_device(item, device) for item in obj))
	elif isinstance(obj, tuple):
		return tuple(move_to_device(item, device) for item in obj)
	else:
		return obj


def clamp_tensor(tensor, minimum, maximum):
	if tensor.is_sparse:
		coalesced_tensor = tensor.coalesce()

		coalesced_tensor._values().clamp_(minimum, maximum)
		return coalesced_tensor
	else:
		return tensor.clamp(minimum, maximum)


def batch_tensor_dicts(
	tensor_dicts: List[Dict[str, torch.Tensor]], remove_trailing_dimension: bool = False
) -> Dict[str, torch.Tensor]:
	key_to_tensors: Dict[str, List[torch.Tensor]] = defaultdict(list)
	for tensor_dict in tensor_dicts:
		for key, tensor in tensor_dict.items():
			key_to_tensors[key].append(tensor)
	batched_tensors = {}
	for key, tensor_list in key_to_tensors.items():
		batched_tensor = torch.stack(tensor_list)
		if remove_trailing_dimension and all(tensor.size(-1) == 1 for tensor in tensor_list):
			batched_tensor = batched_tensor.squeeze(-1)
		batched_tensors[key] = batched_tensor
	return batched_tensors


def get_lengths_from_binary_sequence_mask(mask: torch.BoolTensor) -> torch.LongTensor:
	return mask.sum(-1)


def get_mask_from_sequence_lengths(
	sequence_lengths: torch.Tensor, max_length: int
) -> torch.BoolTensor:
	ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)
	range_tensor = ones.cumsum(dim=1)
	return sequence_lengths.unsqueeze(1) >= range_tensor


def sort_batch_by_length(tensor: torch.Tensor, sequence_lengths: torch.Tensor):

	if not isinstance(tensor, torch.Tensor) or not isinstance(sequence_lengths, torch.Tensor):
		raise ConfigurationError("Both the tensor and sequence lengths must be torch.Tensors.")

	sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)
	sorted_tensor = tensor.index_select(0, permutation_index)

	index_range = torch.arange(0, len(sequence_lengths), device=sequence_lengths.device)
	_, reverse_mapping = permutation_index.sort(0, descending=False)
	restoration_indices = index_range.index_select(0, reverse_mapping)
	return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index


def get_final_encoder_states(
	encoder_outputs: torch.Tensor, mask: torch.BoolTensor, bidirectional: bool = False
) -> torch.Tensor:
	last_word_indices = mask.sum(1) - 1
	batch_size, _, encoder_output_dim = encoder_outputs.size()
	expanded_indices = last_word_indices.view(-1, 1, 1).expand(batch_size, 1, encoder_output_dim)
	final_encoder_output = encoder_outputs.gather(1, expanded_indices)
	final_encoder_output = final_encoder_output.squeeze(1)  # (batch_size, encoder_output_dim)
	if bidirectional:
		final_forward_output = final_encoder_output[:, : (encoder_output_dim // 2)]
		final_backward_output = encoder_outputs[:, 0, (encoder_output_dim // 2) :]
		final_encoder_output = torch.cat([final_forward_output, final_backward_output], dim=-1)
	return final_encoder_output


def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.Tensor):
	binary_mask = (torch.rand(tensor_for_masking.size()) > dropout_probability).to(
		tensor_for_masking.device
	)
	dropout_mask = binary_mask.float().div(1.0 - dropout_probability)
	return dropout_mask


def masked_softmax(
	vector: torch.Tensor,
	mask: torch.BoolTensor,
	dim: int = -1,
	memory_efficient: bool = False,
) -> torch.Tensor:
	if mask is None:
		result = torch.nn.functional.softmax(vector, dim=dim)
	else:
		while mask.dim() < vector.dim():
			mask = mask.unsqueeze(1)
		if not memory_efficient:
			result = torch.nn.functional.softmax(vector * mask, dim=dim)
			result = result * mask
			result = result / (
				result.sum(dim=dim, keepdim=True) + tiny_value_of_dtype(result.dtype)
			)
		else:
			masked_vector = vector.masked_fill(~mask, min_value_of_dtype(vector.dtype))
			result = torch.nn.functional.softmax(masked_vector, dim=dim)
	return result


def masked_log_softmax(vector: torch.Tensor, mask: torch.BoolTensor, dim: int = -1) -> torch.Tensor:
	if mask is not None:
		while mask.dim() < vector.dim():
			mask = mask.unsqueeze(1)
		vector = vector + (mask + tiny_value_of_dtype(vector.dtype)).log()
	return torch.nn.functional.log_softmax(vector, dim=dim)


def masked_max(
	vector: torch.Tensor,
	mask: torch.BoolTensor,
	dim: int,
	keepdim: bool = False,
) -> torch.Tensor:
	replaced_vector = vector.masked_fill(~mask, min_value_of_dtype(vector.dtype))
	max_value, _ = replaced_vector.max(dim=dim, keepdim=keepdim)
	return max_value


def masked_mean(
	vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False
) -> torch.Tensor:
	replaced_vector = vector.masked_fill(~mask, 0.0)

	value_sum = torch.sum(replaced_vector, dim=dim, keepdim=keepdim)
	value_count = torch.sum(mask, dim=dim, keepdim=keepdim)
	return value_sum / value_count.float().clamp(min=tiny_value_of_dtype(torch.float))


def masked_flip(padded_sequence: torch.Tensor, sequence_lengths: List[int]) -> torch.Tensor:
	assert padded_sequence.size(0) == len(
		sequence_lengths
	), f"sequence_lengths length ${len(sequence_lengths)} does not match batch size ${padded_sequence.size(0)}"
	num_timesteps = padded_sequence.size(1)
	flipped_padded_sequence = torch.flip(padded_sequence, [1])
	sequences = [
		flipped_padded_sequence[i, num_timesteps - length :]
		for i, length in enumerate(sequence_lengths)
	]
	return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)


def viterbi_decode(
	tag_sequence: torch.Tensor,
	transition_matrix: torch.Tensor,
	tag_observations: Optional[List[int]] = None,
	allowed_start_transitions: torch.Tensor = None,
	allowed_end_transitions: torch.Tensor = None,
	top_k: int = None,
):
	if top_k is None:
		top_k = 1
		flatten_output = True
	elif top_k >= 1:
		flatten_output = False
	else:
		raise ValueError(f"top_k must be either None or an integer >=1. Instead received {top_k}")

	sequence_length, num_tags = list(tag_sequence.size())

	has_start_end_restrictions = (
		allowed_end_transitions is not None or allowed_start_transitions is not None
	)

	if has_start_end_restrictions:

		if allowed_end_transitions is None:
			allowed_end_transitions = torch.zeros(num_tags)
		if allowed_start_transitions is None:
			allowed_start_transitions = torch.zeros(num_tags)

		num_tags = num_tags + 2
		new_transition_matrix = torch.zeros(num_tags, num_tags)
		new_transition_matrix[:-2, :-2] = transition_matrix


		allowed_start_transitions = torch.cat(
			[allowed_start_transitions, torch.tensor([-math.inf, -math.inf])]
		)
		allowed_end_transitions = torch.cat(
			[allowed_end_transitions, torch.tensor([-math.inf, -math.inf])]
		)

		new_transition_matrix[-2, :] = allowed_start_transitions
		new_transition_matrix[-1, :] = -math.inf

		new_transition_matrix[:, -1] = allowed_end_transitions
		new_transition_matrix[:, -2] = -math.inf

		transition_matrix = new_transition_matrix

	if tag_observations:
		if len(tag_observations) != sequence_length:
			raise ConfigurationError(
				"Observations were provided, but they were not the same length "
				"as the sequence. Found sequence of length: {} and evidence: {}".format(
					sequence_length, tag_observations
				)
			)
	else:
		tag_observations = [-1 for _ in range(sequence_length)]

	if has_start_end_restrictions:
		tag_observations = [num_tags - 2] + tag_observations + [num_tags - 1]
		zero_sentinel = torch.zeros(1, num_tags)
		extra_tags_sentinel = torch.ones(sequence_length, 2) * -math.inf
		tag_sequence = torch.cat([tag_sequence, extra_tags_sentinel], -1)
		tag_sequence = torch.cat([zero_sentinel, tag_sequence, zero_sentinel], 0)
		sequence_length = tag_sequence.size(0)

	path_scores = []
	path_indices = []

	if tag_observations[0] != -1:
		one_hot = torch.zeros(num_tags)
		one_hot[tag_observations[0]] = 100000.0
		path_scores.append(one_hot.unsqueeze(0))
	else:
		path_scores.append(tag_sequence[0, :].unsqueeze(0))

	for timestep in range(1, sequence_length):
		summed_potentials = path_scores[timestep - 1].unsqueeze(2) + transition_matrix
		summed_potentials = summed_potentials.view(-1, num_tags)

		max_k = min(summed_potentials.size()[0], top_k)
		scores, paths = torch.topk(summed_potentials, k=max_k, dim=0)

		observation = tag_observations[timestep]
		if tag_observations[timestep - 1] != -1 and observation != -1:
			if transition_matrix[tag_observations[timestep - 1], observation] < -10000:
				logger.warning(
					"The pairwise potential between tags you have passed as "
					"observations is extremely unlikely. Double check your evidence "
					"or transition potentials!"
				)
		if observation != -1:
			one_hot = torch.zeros(num_tags)
			one_hot[observation] = 100000.0
			path_scores.append(one_hot.unsqueeze(0))
		else:
			path_scores.append(tag_sequence[timestep, :] + scores)
		path_indices.append(paths.squeeze())

	path_scores_v = path_scores[-1].view(-1)
	max_k = min(path_scores_v.size()[0], top_k)
	viterbi_scores, best_paths = torch.topk(path_scores_v, k=max_k, dim=0)
	viterbi_paths = []
	for i in range(max_k):
		viterbi_path = [best_paths[i]]
		for backward_timestep in reversed(path_indices):
			viterbi_path.append(int(backward_timestep.view(-1)[viterbi_path[-1]]))
		viterbi_path.reverse()

		if has_start_end_restrictions:
			viterbi_path = viterbi_path[1:-1]

		viterbi_path = [j % num_tags for j in viterbi_path]
		viterbi_paths.append(viterbi_path)

	if flatten_output:
		return viterbi_paths[0], viterbi_scores[0]

	return viterbi_paths, viterbi_scores


def get_text_field_mask(
	text_field_tensors: Dict[str, Dict[str, torch.Tensor]],
	num_wrapping_dims: int = 0,
	padding_id: int = 0,
) -> torch.BoolTensor:
	masks = []
	for indexer_name, indexer_tensors in text_field_tensors.items():
		if "mask" in indexer_tensors:
			masks.append(indexer_tensors["mask"].bool())
	if len(masks) == 1:
		return masks[0]
	elif len(masks) > 1:
		raise ValueError("found two mask outputs; not sure which to use!")

	tensor_dims = [
		(tensor.dim(), tensor)
		for indexer_output in text_field_tensors.values()
		for tensor in indexer_output.values()
	]
	tensor_dims.sort(key=lambda x: x[0])

	smallest_dim = tensor_dims[0][0] - num_wrapping_dims
	if smallest_dim == 2:
		token_tensor = tensor_dims[0][1]
		return token_tensor != padding_id
	elif smallest_dim == 3:
		character_tensor = tensor_dims[0][1]
		return (character_tensor != padding_id).any(dim=-1)
	else:
		raise ValueError("Expected a tensor with dimension 2 or 3, found {}".format(smallest_dim))


def get_token_ids_from_text_field_tensors(
	text_field_tensors: Dict[str, Dict[str, torch.Tensor]],
) -> torch.Tensor:
	for indexer_name, indexer_tensors in text_field_tensors.items():
		for argument_name, tensor in indexer_tensors.items():
			if argument_name in ["tokens", "token_ids", "input_ids"]:
				return tensor
	raise NotImplementedError(
		"Our heuristic for guessing the right token ids failed. Please open an issue on "
		"github with more detail on how you got this error, so we can implement more robust "
		"logic in this method."
	)


def weighted_sum(matrix: torch.Tensor, attention: torch.Tensor) -> torch.Tensor:
	if attention.dim() == 2 and matrix.dim() == 3:
		return attention.unsqueeze(1).bmm(matrix).squeeze(1)
	if attention.dim() == 3 and matrix.dim() == 3:
		return attention.bmm(matrix)
	if matrix.dim() - 1 < attention.dim():
		expanded_size = list(matrix.size())
		for i in range(attention.dim() - matrix.dim() + 1):
			matrix = matrix.unsqueeze(1)
			expanded_size.insert(i + 1, attention.size(i + 1))
		matrix = matrix.expand(*expanded_size)
	intermediate = attention.unsqueeze(-1).expand_as(matrix) * matrix
	return intermediate.sum(dim=-2)


def sequence_cross_entropy_with_logits(
	logits: torch.FloatTensor,
	targets: torch.LongTensor,
	weights: Union[torch.FloatTensor, torch.BoolTensor],
	average: str = "batch",
	label_smoothing: float = None,
	gamma: float = None,
	alpha: Union[float, List[float], torch.FloatTensor] = None,
) -> torch.FloatTensor:
	if average not in {None, "token", "batch"}:
		raise ValueError(f"Got average f{average}, expected one of None, 'token', or 'batch'")

	weights = weights.to(logits.dtype)
	non_batch_dims = tuple(range(1, len(weights.shape)))
	weights_batch_sum = weights.sum(dim=non_batch_dims)
	logits_flat = logits.view(-1, logits.size(-1))
	log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)
	targets_flat = targets.view(-1, 1).long()
	if gamma:
		probs_flat = log_probs_flat.exp()
		probs_flat = torch.gather(probs_flat, dim=1, index=targets_flat)
		focal_factor = (1.0 - probs_flat) ** gamma
		focal_factor = focal_factor.view(*targets.size())
		weights = weights * focal_factor

	if alpha is not None:
		if isinstance(alpha, (float, int)):

			alpha_factor = torch.tensor(
				[1.0 - float(alpha), float(alpha)], dtype=weights.dtype, device=weights.device
			)

		elif isinstance(alpha, (list, numpy.ndarray, torch.Tensor)):

			alpha_factor = torch.tensor(alpha, dtype=weights.dtype, device=weights.device)

			if not alpha_factor.size():
				alpha_factor = alpha_factor.view(1)
				alpha_factor = torch.cat([1 - alpha_factor, alpha_factor])
		else:
			raise TypeError(
				("alpha must be float, list of float, or torch.FloatTensor, {} provided.").format(
					type(alpha)
				)
			)
		alpha_factor = torch.gather(alpha_factor, dim=0, index=targets_flat.view(-1)).view(
			*targets.size()
		)
		weights = weights * alpha_factor

	if label_smoothing is not None and label_smoothing > 0.0:
		num_classes = logits.size(-1)
		smoothing_value = label_smoothing / num_classes
		smoothed_targets = torch.full_like(log_probs_flat, smoothing_value).scatter_(
			-1, targets_flat, 1.0 - label_smoothing + smoothing_value
		)
		negative_log_likelihood_flat = -log_probs_flat * smoothed_targets
		negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)
	else:
		negative_log_likelihood_flat = -torch.gather(log_probs_flat, dim=1, index=targets_flat)
	negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())
	negative_log_likelihood = negative_log_likelihood * weights

	if average == "batch":
		per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (
			weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype)
		)
		num_non_empty_sequences = (weights_batch_sum > 0).sum() + tiny_value_of_dtype(
			negative_log_likelihood.dtype
		)
		return per_batch_loss.sum() / num_non_empty_sequences
	elif average == "token":
		return negative_log_likelihood.sum() / (
			weights_batch_sum.sum() + tiny_value_of_dtype(negative_log_likelihood.dtype)
		)
	else:
		per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (
			weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype)
		)
		return per_batch_loss


def replace_masked_values(
	tensor: torch.Tensor, mask: torch.BoolTensor, replace_with: float
) -> torch.Tensor:
	if tensor.dim() != mask.dim():
		raise ConfigurationError(
			"tensor.dim() (%d) != mask.dim() (%d)" % (tensor.dim(), mask.dim())
		)
	return tensor.masked_fill(~mask, replace_with)


def tensors_equal(tensor1: torch.Tensor, tensor2: torch.Tensor, tolerance: float = 1e-12) -> bool:

	if isinstance(tensor1, (list, tuple)):
		if not isinstance(tensor2, (list, tuple)) or len(tensor1) != len(tensor2):
			return False
		return all(tensors_equal(t1, t2, tolerance) for t1, t2 in zip(tensor1, tensor2))
	elif isinstance(tensor1, dict):
		if not isinstance(tensor2, dict):
			return False
		if tensor1.keys() != tensor2.keys():
			return False
		return all(tensors_equal(tensor1[key], tensor2[key], tolerance) for key in tensor1)
	elif isinstance(tensor1, torch.Tensor):
		if not isinstance(tensor2, torch.Tensor):
			return False
		if tensor1.size() != tensor2.size():
			return False
		if tensor1.dtype == torch.bool or tensor2.dtype == torch.bool:
			return (tensor1 == tensor2).all()
		return ((tensor1 - tensor2).abs().float() < tolerance).all()
	else:
		try:
			return tensor1 == tensor2
		except RuntimeError:
			print(type(tensor1), type(tensor2))
			raise


def device_mapping(cuda_device: int):

	def inner_device_mapping(storage: torch.Storage, location) -> torch.Storage:
		if cuda_device >= 0:
			return storage.cuda(cuda_device)
		else:
			return storage

	return inner_device_mapping


def read_state_dict(
	path: Union[PathLike, str],
	strip_prefix: Optional[str] = None,
	ignore: Optional[List[str]] = None,
	strict: bool = True,
	cuda_device: int = -1,
) -> Dict[str, torch.Tensor]:
	state = torch.load(path, map_location=device_mapping(cuda_device))
	out: Dict[str, torch.Tensor] = OrderedDict()

	if ignore is not None and not isinstance(ignore, list):
		raise ValueError("'ignore' parameter should be a list")

	strip_prefix_used: Optional[bool] = None
	ignore_used: Optional[List[bool]] = None
	if strict and strip_prefix is not None:
		strip_prefix_used = False
	if strict and ignore:
		ignore_used = [False] * len(ignore)

	for key in state.keys():
		ignore_key = False
		if ignore:
			for i, pattern in enumerate(ignore):
				if re.match(pattern, key):
					if ignore_used:
						ignore_used[i] = True
					logger.warning("ignoring %s from state dict", key)
					ignore_key = True
					break

		if ignore_key:
			continue

		new_key = key

		if strip_prefix and key.startswith(strip_prefix):
			strip_prefix_used = True
			new_key = key[len(strip_prefix) :]
			if not new_key:
				raise ValueError("'strip_prefix' resulted in an empty string for a key")

		out[new_key] = state[key]

	if strip_prefix_used is False:
		raise ValueError(f"'strip_prefix' of '{strip_prefix}' was never used")
	if ignore is not None and ignore_used is not None:
		for pattern, used in zip(ignore, ignore_used):
			if not used:
				raise ValueError(f"'ignore' pattern '{pattern}' didn't have any matches")

	return out


def combine_tensors(combination: str, tensors: List[torch.Tensor]) -> torch.Tensor:
	if len(tensors) > 9:
		raise ConfigurationError("Double-digit tensor lists not currently supported")
	combination = combination.replace("x", "1").replace("y", "2")
	to_concatenate = [_get_combination(piece, tensors) for piece in combination.split(",")]
	return torch.cat(to_concatenate, dim=-1)


def _rindex(sequence: Sequence[T], obj: T) -> int:
	for i in range(len(sequence) - 1, -1, -1):
		if sequence[i] == obj:
			return i

	raise ValueError(f"Unable to find {obj} in sequence {sequence}.")


def _get_combination(combination: str, tensors: List[torch.Tensor]) -> torch.Tensor:
	if combination.isdigit():
		index = int(combination) - 1
		return tensors[index]
	else:
		if len(combination) != 3:
			raise ConfigurationError("Invalid combination: " + combination)
		first_tensor = _get_combination(combination[0], tensors)
		second_tensor = _get_combination(combination[2], tensors)
		operation = combination[1]
		if operation == "*":
			return first_tensor * second_tensor
		elif operation == "/":
			return first_tensor / second_tensor
		elif operation == "+":
			return first_tensor + second_tensor
		elif operation == "-":
			return first_tensor - second_tensor
		else:
			raise ConfigurationError("Invalid operation: " + operation)


def combine_tensors_and_multiply(
	combination: str, tensors: List[torch.Tensor], weights: torch.nn.Parameter
) -> torch.Tensor:
	if len(tensors) > 9:
		raise ConfigurationError("Double-digit tensor lists not currently supported")
	combination = combination.replace("x", "1").replace("y", "2")
	pieces = combination.split(",")
	tensor_dims = [tensor.size(-1) for tensor in tensors]
	combination_dims = [_get_combination_dim(piece, tensor_dims) for piece in pieces]
	dims_so_far = 0
	to_sum = []
	for piece, combination_dim in zip(pieces, combination_dims):
		weight = weights[dims_so_far : (dims_so_far + combination_dim)]
		dims_so_far += combination_dim
		to_sum.append(_get_combination_and_multiply(piece, tensors, weight))
	result = to_sum[0]
	for result_piece in to_sum[1:]:
		result = result + result_piece
	return result


def _get_combination_and_multiply(
	combination: str, tensors: List[torch.Tensor], weight: torch.nn.Parameter
) -> torch.Tensor:
	if combination.isdigit():
		index = int(combination) - 1
		return torch.matmul(tensors[index], weight)
	else:
		if len(combination) != 3:
			raise ConfigurationError("Invalid combination: " + combination)
		first_tensor = _get_combination(combination[0], tensors)
		second_tensor = _get_combination(combination[2], tensors)
		operation = combination[1]
		if operation == "*":
			if first_tensor.dim() > 4 or second_tensor.dim() > 4:
				raise ValueError("Tensors with dim > 4 not currently supported")
			desired_dim = max(first_tensor.dim(), second_tensor.dim()) - 1
			if first_tensor.dim() == 4:
				expanded_dim = _rindex(first_tensor.size(), 1)
				first_tensor = first_tensor.squeeze(expanded_dim)
			if second_tensor.dim() == 4:
				expanded_dim = _rindex(second_tensor.size(), 1)
				second_tensor = second_tensor.squeeze(expanded_dim)
			intermediate = first_tensor * weight
			result = torch.matmul(intermediate, second_tensor.transpose(-1, -2))
			if result.dim() == desired_dim + 1:
				result = result.squeeze(-1)
			return result
		elif operation == "/":
			if first_tensor.dim() > 4 or second_tensor.dim() > 4:
				raise ValueError("Tensors with dim > 4 not currently supported")
			desired_dim = max(first_tensor.dim(), second_tensor.dim()) - 1
			if first_tensor.dim() == 4:
				expanded_dim = _rindex(first_tensor.size(), 1)
				first_tensor = first_tensor.squeeze(expanded_dim)
			if second_tensor.dim() == 4:
				expanded_dim = _rindex(second_tensor.size(), 1)
				second_tensor = second_tensor.squeeze(expanded_dim)
			intermediate = first_tensor * weight
			result = torch.matmul(intermediate, second_tensor.pow(-1).transpose(-1, -2))
			if result.dim() == desired_dim + 1:
				result = result.squeeze(-1)
			return result
		elif operation == "+":
			return torch.matmul(first_tensor, weight) + torch.matmul(second_tensor, weight)
		elif operation == "-":
			return torch.matmul(first_tensor, weight) - torch.matmul(second_tensor, weight)
		else:
			raise ConfigurationError("Invalid operation: " + operation)


def get_combined_dim(combination: str, tensor_dims: List[int]) -> int:
	if len(tensor_dims) > 9:
		raise ConfigurationError("Double-digit tensor lists not currently supported")
	combination = combination.replace("x", "1").replace("y", "2")
	return sum(_get_combination_dim(piece, tensor_dims) for piece in combination.split(","))


def _get_combination_dim(combination: str, tensor_dims: List[int]) -> int:
	if combination.isdigit():
		index = int(combination) - 1
		return tensor_dims[index]
	else:
		if len(combination) != 3:
			raise ConfigurationError("Invalid combination: " + combination)
		first_tensor_dim = _get_combination_dim(combination[0], tensor_dims)
		second_tensor_dim = _get_combination_dim(combination[2], tensor_dims)
		operation = combination[1]
		if first_tensor_dim != second_tensor_dim:
			raise ConfigurationError('Tensor dims must match for operation "{}"'.format(operation))
		return first_tensor_dim


def logsumexp(tensor: torch.Tensor, dim: int = -1, keepdim: bool = False) -> torch.Tensor:
	max_score, _ = tensor.max(dim, keepdim=keepdim)
	if keepdim:
		stable_vec = tensor - max_score
	else:
		stable_vec = tensor - max_score.unsqueeze(dim)
	return max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()


def get_device_of(tensor: torch.Tensor) -> int:
	if not tensor.is_cuda:
		return -1
	else:
		return tensor.get_device()


def flatten_and_batch_shift_indices(indices: torch.Tensor, sequence_length: int) -> torch.Tensor:
	if torch.max(indices) >= sequence_length or torch.min(indices) < 0:
		raise ConfigurationError(
			f"All elements in indices should be in range (0, {sequence_length - 1})"
		)
	offsets = get_range_vector(indices.size(0), get_device_of(indices)) * sequence_length
	for _ in range(len(indices.size()) - 1):
		offsets = offsets.unsqueeze(1)

	offset_indices = indices + offsets

	offset_indices = offset_indices.view(-1)
	return offset_indices


def batched_index_select(
	target: torch.Tensor,
	indices: torch.LongTensor,
	flattened_indices: Optional[torch.LongTensor] = None,
) -> torch.Tensor:
	if flattened_indices is None:
		flattened_indices = flatten_and_batch_shift_indices(indices, target.size(1))

	flattened_target = target.view(-1, target.size(-1))

	flattened_selected = flattened_target.index_select(0, flattened_indices)
	selected_shape = list(indices.size()) + [target.size(-1)]
	selected_targets = flattened_selected.view(*selected_shape)
	return selected_targets


def masked_index_fill(
	target: torch.Tensor, indices: torch.LongTensor, mask: torch.BoolTensor, fill_value: int = 1
) -> torch.Tensor:
	mask = mask.bool()
	prev_shape = target.size()
	flattened_indices = flatten_and_batch_shift_indices(indices * mask, target.size(1))
	mask = mask.view(-1)
	flattened_target = target.view(-1, 1)
	unmasked_indices = flattened_indices[mask].unsqueeze(-1)

	flattened_target = flattened_target.scatter(0, unmasked_indices, fill_value)

	filled_target = flattened_target.reshape(prev_shape)

	return filled_target


def masked_index_replace(
	target: torch.Tensor,
	indices: torch.LongTensor,
	mask: torch.BoolTensor,
	replace: torch.Tensor,
) -> torch.Tensor:
	target = target.clone()
	mask = mask.bool()
	prev_shape = target.size()
	flattened_indices = flatten_and_batch_shift_indices(indices * mask, target.size(1))
	flattened_target = target.view(-1, target.size(-1))
	mask = mask.view(-1)
	flattened_target[flattened_indices[mask]] = replace.view(-1, replace.size(-1))[mask]
	replaced_target = flattened_target.reshape(prev_shape)
	return replaced_target


def batched_span_select(target: torch.Tensor, spans: torch.LongTensor) -> torch.Tensor:
	span_starts, span_ends = spans.split(1, dim=-1)

	span_widths = span_ends - span_starts

	max_batch_span_width = span_widths.max().item() + 1

	max_span_range_indices = get_range_vector(max_batch_span_width, get_device_of(target)).view(
		1, 1, -1
	)
	span_mask = max_span_range_indices <= span_widths
	raw_span_indices = span_starts + max_span_range_indices
	span_mask = span_mask & (raw_span_indices < target.size(1)) & (0 <= raw_span_indices)
	span_indices = raw_span_indices * span_mask

	span_embeddings = batched_index_select(target, span_indices)

	return span_embeddings, span_mask


def flattened_index_select(target: torch.Tensor, indices: torch.LongTensor) -> torch.Tensor:
	if indices.dim() != 2:
		raise ConfigurationError(
			"Indices passed to flattened_index_select had shape {} but "
			"only 2 dimensional inputs are supported.".format(indices.size())
		)
	flattened_selected = target.index_select(1, indices.view(-1))

	selected = flattened_selected.view(target.size(0), indices.size(0), indices.size(1), -1)
	return selected


def get_range_vector(size: int, device: int) -> torch.Tensor:
	if device > -1:
		return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1
	else:
		return torch.arange(0, size, dtype=torch.long)


def bucket_values(
	distances: torch.Tensor, num_identity_buckets: int = 4, num_total_buckets: int = 10
) -> torch.Tensor:
	logspace_index = (distances.float().log() / math.log(2)).floor().long() + (
		num_identity_buckets - 1
	)
	use_identity_mask = (distances <= num_identity_buckets).long()
	use_buckets_mask = 1 + (-1 * use_identity_mask)
	combined_index = use_identity_mask * distances + use_buckets_mask * logspace_index
	return combined_index.clamp(0, num_total_buckets - 1)


def add_sentence_boundary_token_ids(
	tensor: torch.Tensor, mask: torch.BoolTensor, sentence_begin_token: Any, sentence_end_token: Any
) -> Tuple[torch.Tensor, torch.BoolTensor]:
	sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()
	tensor_shape = list(tensor.data.shape)
	new_shape = list(tensor_shape)
	new_shape[1] = tensor_shape[1] + 2
	tensor_with_boundary_tokens = tensor.new_zeros(*new_shape, device=tensor.device)
	if len(tensor_shape) == 2:
		tensor_with_boundary_tokens[:, 1:-1] = tensor
		tensor_with_boundary_tokens[:, 0] = sentence_begin_token
		for i, j in enumerate(sequence_lengths):
			tensor_with_boundary_tokens[i, j + 1] = sentence_end_token
		new_mask = tensor_with_boundary_tokens != 0
	elif len(tensor_shape) == 3:
		tensor_with_boundary_tokens[:, 1:-1, :] = tensor
		sentence_begin_token = sentence_begin_token.detach().to(tensor.device)
		sentence_end_token = sentence_end_token.detach().to(tensor.device)
		for i, j in enumerate(sequence_lengths):
			tensor_with_boundary_tokens[i, 0, :] = sentence_begin_token
			tensor_with_boundary_tokens[i, j + 1, :] = sentence_end_token
		new_mask = (tensor_with_boundary_tokens > 0).sum(dim=-1) > 0
	else:
		raise ValueError("add_sentence_boundary_token_ids only accepts 2D and 3D input")

	return tensor_with_boundary_tokens, new_mask


def remove_sentence_boundaries(
	tensor: torch.Tensor, mask: torch.BoolTensor
) -> Tuple[torch.Tensor, torch.Tensor]:
	sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()
	tensor_shape = list(tensor.data.shape)
	new_shape = list(tensor_shape)
	new_shape[1] = tensor_shape[1] - 2
	tensor_without_boundary_tokens = tensor.new_zeros(*new_shape)
	new_mask = tensor.new_zeros((new_shape[0], new_shape[1]), dtype=torch.bool)
	for i, j in enumerate(sequence_lengths):
		if j > 2:
			tensor_without_boundary_tokens[i, : (j - 2), :] = tensor[i, 1 : (j - 1), :]
			new_mask[i, : (j - 2)] = True

	return tensor_without_boundary_tokens, new_mask


def add_positional_features(
	tensor: torch.Tensor, min_timescale: float = 1.0, max_timescale: float = 1.0e4
):

	_, timesteps, hidden_dim = tensor.size()

	timestep_range = get_range_vector(timesteps, get_device_of(tensor)).data.float()
	num_timescales = hidden_dim // 2
	timescale_range = get_range_vector(num_timescales, get_device_of(tensor)).data.float()

	log_timescale_increments = math.log(float(max_timescale) / float(min_timescale)) / float(
		num_timescales - 1
	)
	inverse_timescales = min_timescale * torch.exp(timescale_range * -log_timescale_increments)

	scaled_time = timestep_range.unsqueeze(1) * inverse_timescales.unsqueeze(0)
	sinusoids = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 1)
	if hidden_dim % 2 != 0:
		sinusoids = torch.cat([sinusoids, sinusoids.new_zeros(timesteps, 1)], 1)
	return tensor + sinusoids.unsqueeze(0)


def clone(module: torch.nn.Module, num_copies: int) -> torch.nn.ModuleList:
	return torch.nn.ModuleList(copy.deepcopy(module) for _ in range(num_copies))


def combine_initial_dims(tensor: torch.Tensor) -> torch.Tensor:
	if tensor.dim() <= 2:
		return tensor
	else:
		return tensor.view(-1, tensor.size(-1))


def uncombine_initial_dims(tensor: torch.Tensor, original_size: torch.Size) -> torch.Tensor:
	if len(original_size) <= 2:
		return tensor
	else:
		view_args = list(original_size) + [tensor.size(-1)]
		return tensor.view(*view_args)


def inspect_parameters(module: torch.nn.Module, quiet: bool = False) -> Dict[str, Any]:
	results: Dict[str, Any] = {}
	for name, param in sorted(module.named_parameters()):
		keys = name.split(".")
		write_to = results
		for key in keys[:-1]:
			if key not in write_to:
				write_to[key] = {}
			write_to = write_to[key]
		write_to[keys[-1]] = "tunable" if param.requires_grad else "frozen"
	if not quiet:
		print(json.dumps(results, indent=4))
	return results


def find_text_field_embedder(model: torch.nn.Module) -> torch.nn.Module:
	from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder

	for module in model.modules():
		if isinstance(module, TextFieldEmbedder):
			return module
	raise ValueError("Couldn't find TextFieldEmbedder!")


def find_embedding_layer(model: torch.nn.Module) -> torch.nn.Module:
	from transformers.models.gpt2.modeling_gpt2 import GPT2Model
	from transformers.models.bert.modeling_bert import BertEmbeddings
	from transformers.models.albert.modeling_albert import AlbertEmbeddings
	from transformers.models.roberta.modeling_roberta import RobertaEmbeddings
	from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder
	from allennlp.modules.text_field_embedders.basic_text_field_embedder import (
		BasicTextFieldEmbedder,
	)
	from allennlp.modules.token_embedders.embedding import Embedding

	for module in model.modules():
		if isinstance(module, BertEmbeddings):
			return module.word_embeddings
		if isinstance(module, RobertaEmbeddings):
			return module.word_embeddings
		if isinstance(module, AlbertEmbeddings):
			return module.word_embeddings
		if isinstance(module, GPT2Model):
			return module.wte

	for module in model.modules():
		if isinstance(module, TextFieldEmbedder):

			if isinstance(module, BasicTextFieldEmbedder):
				if len(module._token_embedders) == 1:
					embedder = list(module._token_embedders.values())[0]
					if isinstance(embedder, Embedding):
						if embedder._projection is None:
							return embedder
			return module
	raise RuntimeError("No embedding module found!")


def get_token_offsets_from_text_field_inputs(
	text_field_inputs: List[Any],
) -> Optional[torch.Tensor]:
	for input_index, text_field_input in enumerate(text_field_inputs):
		if not isinstance(text_field_input, dict):
			continue
		for input_value in text_field_input.values():
			if not isinstance(input_value, dict):
				continue
			for embedder_arg_name, embedder_arg_value in input_value.items():
				if embedder_arg_name == "offsets":
					return embedder_arg_value
	return None


def extend_layer(layer: torch.nn.Module, new_dim: int) -> None:
	valid_layers = [torch.nn.Linear, torch.nn.Bilinear]
	if not any([isinstance(layer, i) for i in valid_layers]):
		raise ConfigurationError("Inappropriate layer type")

	extend_dim = new_dim - layer.out_features
	if not extend_dim:
		return layer

	if isinstance(layer, torch.nn.Linear):
		new_weight = torch.FloatTensor(extend_dim, layer.in_features)
	elif isinstance(layer, torch.nn.Bilinear):
		new_weight = torch.FloatTensor(extend_dim, layer.in1_features, layer.in2_features)

	new_bias = torch.FloatTensor(extend_dim)
	torch.nn.init.xavier_uniform_(new_weight)
	torch.nn.init.zeros_(new_bias)

	device = layer.weight.device
	layer.weight = torch.nn.Parameter(
		torch.cat([layer.weight.data, new_weight.to(device)], dim=0),
		requires_grad=layer.weight.requires_grad,
	)
	layer.bias = torch.nn.Parameter(
		torch.cat([layer.bias.data, new_bias.to(device)], dim=0),
		requires_grad=layer.bias.requires_grad,
	)
	layer.out_features = new_dim


def masked_topk(
	input_: torch.FloatTensor,
	mask: torch.BoolTensor,
	k: Union[int, torch.LongTensor],
	dim: int = -1,
) -> Tuple[torch.LongTensor, torch.LongTensor, torch.FloatTensor]:
	if input_.size() != mask.size():
		raise ValueError("`input_` and `mask` must have the same shape.")
	if not -input_.dim() <= dim < input_.dim():
		raise ValueError("`dim` must be in `[-input_.dim(), input_.dim())`")
	dim = (dim + input_.dim()) % input_.dim()

	max_k = k if isinstance(k, int) else k.max()


	permutation = list(range(input_.dim()))
	permutation.pop(dim)
	permutation += [dim]

	reverse_permutation = list(range(input_.dim() - 1))
	reverse_permutation.insert(dim, -1)

	other_dims_size = list(input_.size())
	other_dims_size.pop(dim)
	permuted_size = other_dims_size + [max_k]  # for restoration

	if isinstance(k, int):
		k = k * torch.ones(*other_dims_size, dtype=torch.long, device=mask.device)
	else:
		if list(k.size()) != other_dims_size:
			raise ValueError(
				"`k` must have the same shape as `input_` with dimension `dim` removed."
			)

	num_items = input_.size(dim)
	input_ = input_.permute(*permutation).reshape(-1, num_items)
	mask = mask.permute(*permutation).reshape(-1, num_items)
	k = k.reshape(-1)

	input_ = replace_masked_values(input_, mask, min_value_of_dtype(input_.dtype))

	_, top_indices = input_.topk(max_k, 1)

	top_indices_mask = get_mask_from_sequence_lengths(k, max_k).bool()

	fill_value, _ = top_indices.max(dim=1, keepdim=True)
	top_indices = torch.where(top_indices_mask, top_indices, fill_value)

	top_indices, _ = top_indices.sort(1)

	sequence_mask = mask.gather(1, top_indices)
	top_mask = top_indices_mask & sequence_mask

	top_input = input_.gather(1, top_indices)

	return (
		top_input.reshape(*permuted_size).permute(*reverse_permutation),
		top_mask.reshape(*permuted_size).permute(*reverse_permutation),
		top_indices.reshape(*permuted_size).permute(*reverse_permutation),
	)


def info_value_of_dtype(dtype: torch.dtype):
	if dtype == torch.bool:
		raise TypeError("Does not support torch.bool")
	elif dtype.is_floating_point:
		return torch.finfo(dtype)
	else:
		return torch.iinfo(dtype)


def min_value_of_dtype(dtype: torch.dtype):
	return info_value_of_dtype(dtype).min


def max_value_of_dtype(dtype: torch.dtype):
	return info_value_of_dtype(dtype).max


def tiny_value_of_dtype(dtype: torch.dtype):
	if not dtype.is_floating_point:
		raise TypeError("Only supports floating point dtypes.")
	if dtype == torch.float or dtype == torch.double:
		return 1e-13
	elif dtype == torch.half:
		return 1e-4
	else:
		raise TypeError("Does not support dtype " + str(dtype))


_V = TypeVar("_V", int, float, torch.Tensor)


def distributed_device() -> torch.device:
	if not is_distributed():
		raise RuntimeError(
			"'distributed_device()' can only be called within a distributed process group"
		)
	return int_to_device(-1 if dist.get_backend() != "nccl" else torch.cuda.current_device())


def dist_reduce(value: _V, reduce_op) -> _V:
	if not is_distributed():
		return value
	device = distributed_device()
	if isinstance(value, torch.Tensor):
		value_tensor = value.clone().to(device)
	else:
		value_tensor = torch.tensor(value, device=device)
	dist.all_reduce(value_tensor, op=reduce_op)

	if isinstance(value, torch.Tensor):
		return value_tensor
	return value_tensor.item()  # type: ignore[return-value]


def dist_reduce_sum(value: _V) -> _V:
	if not is_distributed():
		return value
	return dist_reduce(value, dist.ReduceOp.SUM)


def _collect_state_dict(
	module: torch.nn.Module,
	state_dict: Optional[StateDictType],
	recurse: bool = True,
	prefix: str = "",
) -> Tuple[StateDictType, List[str], List[str]]:
	dist_device = distributed_device()
	state_dict_device = int_to_device(-1)

	missing_keys: List[str] = []
	unexpected_keys: List[str] = []


	if recurse:
		current_state_dict = module.state_dict()
	else:
		current_state_dict = OrderedDict(
			chain(
				((n, p.data) for (n, p) in module.named_parameters(recurse=False)),
				module.named_buffers(recurse=False),
			)
		)

	keys = list(current_state_dict.keys())

	if is_global_primary():
		assert state_dict is not None
		for key in state_dict:
			if key not in keys:
				unexpected_keys.append(key)

	for key in keys:
		tensor = current_state_dict[key]
		if is_global_primary():
			assert state_dict is not None
			if key in state_dict:
				tensor = state_dict[key]
			else:
				missing_keys.append(key)
		logger.debug("Broadcasting distributed parameter '%s'", prefix + key)
		tensor = tensor.to(dist_device).contiguous()
		dist.broadcast(tensor, 0)
		current_state_dict[key] = tensor.to(state_dict_device)

	return current_state_dict, missing_keys, unexpected_keys


class _IncompatibleKeys(NamedTuple):
	missing_keys: List[str]
	unexpected_keys: List[str]

	def __repr__(self):
		if not self.missing_keys and not self.unexpected_keys:
			return "<All keys matched successfully>"
		return f"(missing_keys = {self.missing_keys}, unexpected_keys = {self.unexpected_keys})"


def _check_incompatible_keys(
	module, missing_keys: List[str], unexpected_keys: List[str], strict: bool
):
	error_msgs: List[str] = []
	if missing_keys:
		error_msgs.append(
			"Missing key(s) in state_dict: {}".format(", ".join(f'"{k}"' for k in missing_keys))
		)
	if unexpected_keys:
		error_msgs.append(
			"Unexpected key(s) in state_dict: {}".format(
				", ".join(f'"{k}"' for k in unexpected_keys)
			)
		)
	if error_msgs and strict:
		raise RuntimeError(
			"Error(s) in loading state_dict for {}:\n\t{}".format(
				module.__class__.__name__, "\n\t".join(error_msgs)
			)
		)


def load_state_dict_distributed(
	module: torch.nn.Module,
	state_dict: Optional[StateDictType],
	strict: bool = True,
	prefix: str = "",
) -> _IncompatibleKeys:
	if not is_distributed():
		return module.load_state_dict(state_dict, strict=strict)

	if is_global_primary():
		assert state_dict is not None
	else:
		assert state_dict is None

	missing_keys: List[str] = []
	unexpected_keys: List[str] = []

	submodules = dict(module.named_children())

	def update_key_list(original, updates):
		for key in updates:
			if key not in original:
				original.append(key)

	from allennlp.nn.parallel.sharded_module_mixin import ShardedModuleMixin

	if isinstance(module, ShardedModuleMixin) or not submodules:
		state_dict, _missing_keys, _unexpected_keys = _collect_state_dict(
			module, state_dict, prefix=prefix
		)
		assert state_dict is not None
		update_key_list(missing_keys, _missing_keys)
		update_key_list(unexpected_keys, _unexpected_keys)
		_missing_keys, _unexpected_keys = module.load_state_dict(state_dict, strict=False)
		update_key_list(missing_keys, _missing_keys)
		update_key_list(unexpected_keys, _unexpected_keys)
	else:
		direct_member_state_dict, _missing_keys, _unexpected_keys = _collect_state_dict(
			module,
			state_dict,
			recurse=False,
			prefix=prefix,
		)
		_unexpected_keys = [
			k for k in _unexpected_keys if "." not in k or k.split(".")[0] not in submodules.keys()
		]
		update_key_list(missing_keys, _missing_keys)
		update_key_list(unexpected_keys, _unexpected_keys)

		_missing_keys, _unexpected_keys = module.load_state_dict(
			direct_member_state_dict, strict=False
		)
		update_key_list(missing_keys, _missing_keys)
		update_key_list(unexpected_keys, _unexpected_keys)

		for name, submodule in submodules.items():
			missing_keys = [k for k in missing_keys if not k.startswith(name + ".")]
			submodule_state_dict: Optional[StateDictType] = None
			if is_global_primary():
				assert state_dict is not None
				submodule_state_dict = {
					key.replace(name + ".", "", 1): value
					for key, value in state_dict.items()
					if key.startswith(name + ".")
				}
			_missing_keys, _unexpected_keys = load_state_dict_distributed(
				submodule,
				submodule_state_dict,
				strict=False,
				prefix=prefix + name + ".",
			)
			update_key_list(missing_keys, [f"{name}.{key}" for key in _missing_keys])
			update_key_list(unexpected_keys, [f"{name}.{key}" for key in _unexpected_keys])

	_check_incompatible_keys(module, missing_keys, unexpected_keys, strict)

	return _IncompatibleKeys(missing_keys, unexpected_keys)

from allennlp.data.data_loaders import (
	DataLoader,
	TensorDict,
	allennlp_collate,
)
from allennlp.data.dataset_readers.dataset_reader import DatasetReader, DatasetReaderInput
from allennlp.data.fields.field import DataArray, Field
from allennlp.data.fields.text_field import TextFieldTensors
from allennlp.data.instance import Instance
from allennlp.data.samplers import BatchSampler
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList
from allennlp.data.tokenizers import Token, Tokenizer
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.batch import Batch
from allennlp.data.image_loader import ImageLoader, TorchImageLoader

import torch.nn as nn
from .token import TokenEmbedding
from .position import PositionalEmbedding
from .segment import SegmentEmbedding


class BERTEmbedding(nn.Module):

	def __init__(self, vocab_size, embed_size, dropout=0.1):
		super().__init__()
		self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)
		self.position = PositionalEmbedding(d_model=self.token.embedding_dim)
		self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)
		self.dropout = nn.Dropout(p=dropout)
		self.embed_size = embed_size

	def forward(self, sequence, segment_label):
		x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)
		return self.dropout(x)


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
import numpy as np
from datasets import load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoModelForSeq2SeqLM,
	AutoTokenizer,
	DataCollatorForSeq2Seq,
	HfArgumentParser,
	M2M100Tokenizer,
	MBart50Tokenizer,
	MBart50TokenizerFast,
	MBartTokenizer,
	MBartTokenizerFast,
	Seq2SeqTrainer,
	Seq2SeqTrainingArguments,
	default_data_collator,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/translation/requirements.txt")

logger = logging.getLogger(__name__)

MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast, M2M100Tokenizer]


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)


@dataclass
class DataTrainingArguments:

	source_lang: str = field(default=None, metadata={"help": "Source language id for translation."})
	target_lang: str = field(default=None, metadata={"help": "Target language id for translation."})

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a jsonlines)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={
			"help": "An optional input evaluation data file to evaluate the metrics (sacrebleu) on a jsonlines file."
		},
	)
	test_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input test data file to evaluate the metrics (sacrebleu) on a jsonlines file."},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_source_length: Optional[int] = field(
		default=1024,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	max_target_length: Optional[int] = field(
		default=128,
		metadata={
			"help": (
				"The maximum total sequence length for target text after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	val_max_target_length: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"The maximum total sequence length for validation target text after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`. "
				"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used "
				"during ``evaluate`` and ``predict``."
			)
		},
	)
	pad_to_max_length: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to pad all samples to model maximum sentence length. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
				"efficient on GPU but very bad for TPU."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	num_beams: Optional[int] = field(
		default=1,
		metadata={
			"help": (
				"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, "
				"which is used during ``evaluate`` and ``predict``."
			)
		},
	)
	ignore_pad_token_for_loss: bool = field(
		default=True,
		metadata={
			"help": "Whether to ignore the tokens corresponding to padded labels in the loss computation or not."
		},
	)
	source_prefix: Optional[str] = field(
		default=None, metadata={"help": "A prefix to add before every source text (useful for T5 models)."}
	)
	forced_bos_token: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The token to force as the first generated token after the :obj:`decoder_start_token_id`.Useful for"
				" multilingual models like :doc:`mBART <../model_doc/mbart>` where the first generated token needs to"
				" be the target language token.(Usually it is the target language token)"
			)
		},
	)

	def __post_init__(self):
		if self.dataset_name is None and self.train_file is None and self.validation_file is None:
			raise ValueError("Need either a dataset name or a training/validation file.")
		elif self.source_lang is None or self.target_lang is None:
			raise ValueError("Need to specify the source language and the target language.")

		valid_extensions = ["json", "jsonl"]

		if self.train_file is not None:
			extension = self.train_file.split(".")[-1]
			assert extension in valid_extensions, "`train_file` should be a jsonlines file."
		if self.validation_file is not None:
			extension = self.validation_file.split(".")[-1]
			assert extension in valid_extensions, "`validation_file` should be a jsonlines file."
		if self.val_max_target_length is None:
			self.val_max_target_length = self.max_target_length


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_translation", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	if data_args.source_prefix is None and model_args.model_name_or_path in [
		"t5-small",
		"t5-base",
		"t5-large",
		"t5-3b",
		"t5-11b",
	]:
		logger.warning(
			"You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with "
			"`--source_prefix 'translate English to German: ' `"
		)

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
			extension = data_args.train_file.split(".")[-1]
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
			extension = data_args.validation_file.split(".")[-1]
		if data_args.test_file is not None:
			data_files["test"] = data_args.test_file
			extension = data_args.test_file.split(".")[-1]
		if extension == "jsonl":
			builder_name = "json"  # the "json" builder reads both .json and .jsonl files
		else:
			builder_name = extension  # e.g. "parquet"
		raw_datasets = load_dataset(
			builder_name,
			data_files=data_files,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSeq2SeqLM.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):
		if isinstance(tokenizer, MBartTokenizer):
			model.config.decoder_start_token_id = tokenizer.lang_code_to_id[data_args.target_lang]
		else:
			model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(data_args.target_lang)

	if model.config.decoder_start_token_id is None:
		raise ValueError("Make sure that `config.decoder_start_token_id` is correctly defined")

	prefix = data_args.source_prefix if data_args.source_prefix is not None else ""

	if training_args.do_train:
		column_names = raw_datasets["train"].column_names
	elif training_args.do_eval:
		column_names = raw_datasets["validation"].column_names
	elif training_args.do_predict:
		column_names = raw_datasets["test"].column_names
	else:
		logger.info("There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.")
		return

	if isinstance(tokenizer, tuple(MULTILINGUAL_TOKENIZERS)):
		assert data_args.target_lang is not None and data_args.source_lang is not None, (
			f"{tokenizer.__class__.__name__} is a multilingual tokenizer which requires --source_lang and "
			"--target_lang arguments."
		)

		tokenizer.src_lang = data_args.source_lang
		tokenizer.tgt_lang = data_args.target_lang

		forced_bos_token_id = (
			tokenizer.lang_code_to_id[data_args.forced_bos_token] if data_args.forced_bos_token is not None else None
		)
		model.config.forced_bos_token_id = forced_bos_token_id

	source_lang = data_args.source_lang.split("_")[0]
	target_lang = data_args.target_lang.split("_")[0]

	max_target_length = data_args.max_target_length
	padding = "max_length" if data_args.pad_to_max_length else False

	if training_args.label_smoothing_factor > 0 and not hasattr(model, "prepare_decoder_input_ids_from_labels"):
		logger.warning(
			"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for "
			f"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory"
		)

	def preprocess_function(examples):
		inputs = [ex[source_lang] for ex in examples["translation"]]
		targets = [ex[target_lang] for ex in examples["translation"]]
		inputs = [prefix + inp for inp in inputs]
		model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)

		labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)

		if padding == "max_length" and data_args.ignore_pad_token_for_loss:
			labels["input_ids"] = [
				[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
			]

		model_inputs["labels"] = labels["input_ids"]
		return model_inputs

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on train dataset",
			)

	if training_args.do_eval:
		max_target_length = data_args.val_max_target_length
		if "validation" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_dataset = raw_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on validation dataset",
			)

	if training_args.do_predict:
		max_target_length = data_args.val_max_target_length
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_dataset = raw_datasets["test"]
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))
		with training_args.main_process_first(desc="prediction dataset map pre-processing"):
			predict_dataset = predict_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)

	label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id
	if data_args.pad_to_max_length:
		data_collator = default_data_collator
	else:
		data_collator = DataCollatorForSeq2Seq(
			tokenizer,
			model=model,
			label_pad_token_id=label_pad_token_id,
			pad_to_multiple_of=8 if training_args.fp16 else None,
		)

	metric = evaluate.load("sacrebleu", cache_dir=model_args.cache_dir)

	def postprocess_text(preds, labels):
		preds = [pred.strip() for pred in preds]
		labels = [[label.strip()] for label in labels]

		return preds, labels

	def compute_metrics(eval_preds):
		preds, labels = eval_preds
		if isinstance(preds, tuple):
			preds = preds[0]
		preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
		decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
		labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
		decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

		decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

		result = metric.compute(predictions=decoded_preds, references=decoded_labels)
		result = {"bleu": result["score"]}

		prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
		result["gen_len"] = np.mean(prediction_lens)
		result = {k: round(v, 4) for k, v in result.items()}
		return result

	trainer = Seq2SeqTrainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		compute_metrics=compute_metrics if training_args.predict_with_generate else None,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload

		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	results = {}
	max_length = (
		training_args.generation_max_length
		if training_args.generation_max_length is not None
		else data_args.val_max_target_length
	)
	num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams
	if training_args.do_eval:
		logger.info("*** Evaluate ***")

		metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix="eval")
		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")

		predict_results = trainer.predict(
			predict_dataset, metric_key_prefix="predict", max_length=max_length, num_beams=num_beams
		)
		metrics = predict_results.metrics
		max_predict_samples = (
			data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
		)
		metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))

		trainer.log_metrics("predict", metrics)
		trainer.save_metrics("predict", metrics)

		if trainer.is_world_process_zero():
			if training_args.predict_with_generate:
				predictions = predict_results.predictions
				predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)
				predictions = tokenizer.batch_decode(
					predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True
				)
				predictions = [pred.strip() for pred in predictions]
				output_prediction_file = os.path.join(training_args.output_dir, "generated_predictions.txt")
				with open(output_prediction_file, "w", encoding="utf-8") as writer:
					writer.write("\n".join(predictions))

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "translation"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	languages = [l for l in [data_args.source_lang, data_args.target_lang] if l is not None]
	if len(languages) > 0:
		kwargs["language"] = languages

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)

	return results


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from .feed_forward import PositionwiseFeedForward
from .layer_norm import LayerNorm
from .sublayer import SublayerConnection
from .gelu import GELU

import torch
from allennlp.data.vocabulary import Vocabulary
from allennlp.models.model import Model


class FakeModelForTestingNormalizationBiasVerification(Model):
	def __init__(self, use_bias=True):
		super().__init__(vocab=Vocabulary())
		self.conv = torch.nn.Conv2d(3, 5, kernel_size=1, bias=use_bias)
		self.bn = torch.nn.BatchNorm2d(5)

	def forward(self, x):
		out = self.bn(self.conv(x))
		return {"loss": out.sum()}

from __future__ import print_function
import argparse
from math import log10

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from model import Net
from data import get_training_set, get_test_set

parser = argparse.ArgumentParser(description='PyTorch Super Res Example')
parser.add_argument('--upscale_factor', type=int, required=True, help="super resolution upscale factor")
parser.add_argument('--batchSize', type=int, default=64, help='training batch size')
parser.add_argument('--testBatchSize', type=int, default=10, help='testing batch size')
parser.add_argument('--nEpochs', type=int, default=2, help='number of epochs to train for')
parser.add_argument('--lr', type=float, default=0.01, help='Learning Rate. Default=0.01')
parser.add_argument('--cuda', action='store_true', help='use cuda?')
parser.add_argument('--mps', action='store_true', default=False, help='enables macOS GPU training')
parser.add_argument('--threads', type=int, default=4, help='number of threads for data loader to use')
parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')
opt = parser.parse_args()

print(opt)

if opt.cuda and not torch.cuda.is_available():
	raise Exception("No GPU found, please run without --cuda")
if not opt.mps and torch.backends.mps.is_available():
	raise Exception("Found mps device, please run with --mps to enable macOS GPU")

torch.manual_seed(opt.seed)
use_mps = opt.mps and torch.backends.mps.is_available()

if opt.cuda:
	device = torch.device("cuda")
elif use_mps:
	device = torch.device("mps")
else:
	device = torch.device("cpu")

print('===> Loading datasets')
train_set = get_training_set(opt.upscale_factor)
test_set = get_test_set(opt.upscale_factor)
training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)
testing_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.testBatchSize, shuffle=False)

print('===> Building model')
model = Net(upscale_factor=opt.upscale_factor).to(device)
criterion = nn.MSELoss()

optimizer = optim.Adam(model.parameters(), lr=opt.lr)


def train(epoch):
	epoch_loss = 0
	for iteration, batch in enumerate(training_data_loader, 1):
		input, target = batch[0].to(device), batch[1].to(device)

		optimizer.zero_grad()
		loss = criterion(model(input), target)
		epoch_loss += loss.item()
		loss.backward()
		optimizer.step()

		print("===> Epoch[{}]({}/{}): Loss: {:.4f}".format(epoch, iteration, len(training_data_loader), loss.item()))

	print("===> Epoch {} Complete: Avg. Loss: {:.4f}".format(epoch, epoch_loss / len(training_data_loader)))


def test():
	avg_psnr = 0
	with torch.no_grad():
		for batch in testing_data_loader:
			input, target = batch[0].to(device), batch[1].to(device)

			prediction = model(input)
			mse = criterion(prediction, target)
			psnr = 10 * log10(1 / mse.item())
			avg_psnr += psnr
	print("===> Avg. PSNR: {:.4f} dB".format(avg_psnr / len(testing_data_loader)))


def checkpoint(epoch):
	model_out_path = "model_epoch_{}.pth".format(epoch)
	torch.save(model, model_out_path)
	print("Checkpoint saved to {}".format(model_out_path))

for epoch in range(1, opt.nEpochs + 1):
	train(epoch)
	test()
	checkpoint(epoch)

import argparse
import json
import logging
import math
import os
from pathlib import Path

import datasets
import evaluate
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from torchvision.transforms import (
	CenterCrop,
	Compose,
	Lambda,
	Normalize,
	RandomHorizontalFlip,
	RandomResizedCrop,
	Resize,
	ToTensor,
)
from tqdm.auto import tqdm

import transformers
from transformers import AutoConfig, AutoImageProcessor, AutoModelForImageClassification, SchedulerType, get_scheduler
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)

require_version("datasets>=2.0.0", "To fix: pip install -r examples/pytorch/image-classification/requirements.txt")


def parse_args():
	parser = argparse.ArgumentParser(description="Fine-tune a Transformers model on an image classification dataset")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default="cifar10",
		help=(
			"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,"
			" dataset)."
		),
	)
	parser.add_argument("--train_dir", type=str, default=None, help="A folder containing the training data.")
	parser.add_argument("--validation_dir", type=str, default=None, help="A folder containing the validation data.")
	parser.add_argument(
		"--max_train_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of training examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--max_eval_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--train_val_split",
		type=float,
		default=0.15,
		help="Percent to split off of train for validation",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		default="google/vit-base-patch16-224-in21k",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	parser.add_argument(
		"--ignore_mismatched_sizes",
		action="store_true",
		help="Whether or not to enable to load a pretrained model whose head dimensions are different.",
	)
	parser.add_argument(
		"--image_column_name",
		type=str,
		default="image",
		help="The name of the dataset column containing the image data. Defaults to 'image'.",
	)
	parser.add_argument(
		"--label_column_name",
		type=str,
		default="label",
		help="The name of the dataset column containing the labels. Defaults to 'label'.",
	)
	args = parser.parse_args()

	if args.dataset_name is None and args.train_dir is None and args.validation_dir is None:
		raise ValueError("Need either a dataset name or a training/validation folder.")

	if args.push_to_hub or args.with_tracking:
		if args.output_dir is None:
			raise ValueError(
				"Need an `output_dir` to create a repo when `--push_to_hub` or `with_tracking` is specified."
			)

	if args.output_dir is not None:
		os.makedirs(args.output_dir, exist_ok=True)

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_image_classification_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

	logger.info(accelerator.state)
	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()


	if args.dataset_name is not None:
		dataset = load_dataset(args.dataset_name)
	else:
		data_files = {}
		if args.train_dir is not None:
			data_files["train"] = os.path.join(args.train_dir, "**")
		if args.validation_dir is not None:
			data_files["validation"] = os.path.join(args.validation_dir, "**")
		dataset = load_dataset(
			"imagefolder",
			data_files=data_files,
		)

	dataset_column_names = dataset["train"].column_names if "train" in dataset else dataset["validation"].column_names
	if args.image_column_name not in dataset_column_names:
		raise ValueError(
			f"--image_column_name {args.image_column_name} not found in dataset '{args.dataset_name}'. "
			"Make sure to set `--image_column_name` to the correct audio column - one of "
			f"{', '.join(dataset_column_names)}."
		)
	if args.label_column_name not in dataset_column_names:
		raise ValueError(
			f"--label_column_name {args.label_column_name} not found in dataset '{args.dataset_name}'. "
			"Make sure to set `--label_column_name` to the correct text column - one of "
			f"{', '.join(dataset_column_names)}."
		)

	args.train_val_split = None if "validation" in dataset.keys() else args.train_val_split
	if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:
		split = dataset["train"].train_test_split(args.train_val_split)
		dataset["train"] = split["train"]
		dataset["validation"] = split["test"]

	labels = dataset["train"].features[args.label_column_name].names
	label2id = {label: str(i) for i, label in enumerate(labels)}
	id2label = {str(i): label for i, label in enumerate(labels)}

	config = AutoConfig.from_pretrained(
		args.model_name_or_path,
		num_labels=len(labels),
		i2label=id2label,
		label2id=label2id,
		finetuning_task="image-classification",
		trust_remote_code=args.trust_remote_code,
	)
	image_processor = AutoImageProcessor.from_pretrained(
		args.model_name_or_path,
		trust_remote_code=args.trust_remote_code,
	)
	model = AutoModelForImageClassification.from_pretrained(
		args.model_name_or_path,
		from_tf=bool(".ckpt" in args.model_name_or_path),
		config=config,
		ignore_mismatched_sizes=args.ignore_mismatched_sizes,
		trust_remote_code=args.trust_remote_code,
	)


	if "shortest_edge" in image_processor.size:
		size = image_processor.size["shortest_edge"]
	else:
		size = (image_processor.size["height"], image_processor.size["width"])
	normalize = (
		Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
		if hasattr(image_processor, "image_mean") and hasattr(image_processor, "image_std")
		else Lambda(lambda x: x)
	)
	train_transforms = Compose(
		[
			RandomResizedCrop(size),
			RandomHorizontalFlip(),
			ToTensor(),
			normalize,
		]
	)
	val_transforms = Compose(
		[
			Resize(size),
			CenterCrop(size),
			ToTensor(),
			normalize,
		]
	)

	def preprocess_train(example_batch):
		example_batch["pixel_values"] = [
			train_transforms(image.convert("RGB")) for image in example_batch[args.image_column_name]
		]
		return example_batch

	def preprocess_val(example_batch):
		example_batch["pixel_values"] = [
			val_transforms(image.convert("RGB")) for image in example_batch[args.image_column_name]
		]
		return example_batch

	with accelerator.main_process_first():
		if args.max_train_samples is not None:
			dataset["train"] = dataset["train"].shuffle(seed=args.seed).select(range(args.max_train_samples))
		train_dataset = dataset["train"].with_transform(preprocess_train)
		if args.max_eval_samples is not None:
			dataset["validation"] = dataset["validation"].shuffle(seed=args.seed).select(range(args.max_eval_samples))
		eval_dataset = dataset["validation"].with_transform(preprocess_val)

	def collate_fn(examples):
		pixel_values = torch.stack([example["pixel_values"] for example in examples])
		labels = torch.tensor([example[args.label_column_name] for example in examples])
		return {"pixel_values": pixel_values, "labels": labels}

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(eval_dataset, collate_fn=collate_fn, batch_size=args.per_device_eval_batch_size)

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("image_classification_no_trainer", experiment_config)

	metric = evaluate.load("accuracy")

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0
	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()
				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

					if args.push_to_hub and epoch < args.num_train_epochs - 1:
						accelerator.wait_for_everyone()
						unwrapped_model = accelerator.unwrap_model(model)
						unwrapped_model.save_pretrained(
							args.output_dir,
							is_main_process=accelerator.is_main_process,
							save_function=accelerator.save,
						)
						if accelerator.is_main_process:
							image_processor.save_pretrained(args.output_dir)
							repo.push_to_hub(
								commit_message=f"Training in progress {completed_steps} steps",
								blocking=False,
								auto_lfs_prune=True,
							)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				outputs = model(**batch)
			predictions = outputs.logits.argmax(dim=-1)
			predictions, references = accelerator.gather_for_metrics((predictions, batch["labels"]))
			metric.add_batch(
				predictions=predictions,
				references=references,
			)

		eval_metric = metric.compute()
		logger.info(f"epoch {epoch}: {eval_metric}")

		if args.with_tracking:
			accelerator.log(
				{
					"accuracy": eval_metric,
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				image_processor.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			image_processor.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			all_results = {f"eval_{k}": v for k, v in eval_metric.items()}
			with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
				json.dump(all_results, f)


if __name__ == "__main__":
	main()


import argparse
import logging
import math
import os
import warnings
from pathlib import Path

import datasets
import numpy as np
import torch
from accelerate import Accelerator, DistributedType
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from torchvision.transforms import Compose, Lambda, Normalize, RandomHorizontalFlip, RandomResizedCrop, ToTensor
from tqdm.auto import tqdm

import transformers
from transformers import (
	CONFIG_MAPPING,
	IMAGE_PROCESSOR_MAPPING,
	MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING,
	AutoConfig,
	AutoImageProcessor,
	AutoModelForMaskedImageModeling,
	SchedulerType,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version



logger = logging.getLogger(__name__)

check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/image-pretraining/requirements.txt")

MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def parse_args():
	parser = argparse.ArgumentParser(
		description="Finetune a transformers model on a simple Masked Image Modeling task"
	)
	parser.add_argument(
		"--dataset_name",
		type=str,
		default="cifar10",
		help="Name of a dataset from the datasets package",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--image_column_name",
		type=str,
		default=None,
		help="The column name of the images in the files. If not set, will try to use 'image' or 'img'.",
	)
	parser.add_argument(
		"--train_dir",
		type=str,
		default=None,
		help="A folder containing the training data.",
	)
	parser.add_argument(
		"--validation_dir",
		type=None,
		default=None,
		help="A folder containing the validation data.",
	)
	parser.add_argument(
		"--train_val_split",
		type=float,
		default=0.15,
		help="Percent to split off of train for validation.",
	)
	parser.add_argument(
		"--mask_patch_size",
		type=int,
		default=32,
		help="The size of the square patches to use for masking.",
	)
	parser.add_argument(
		"--mask_ratio",
		type=float,
		default=0.6,
		help="Percentage of patches to mask.",
	)
	parser.add_argument(
		"--max_train_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of training examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--max_eval_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		default=None,
		help=(
			"The model checkpoint for weights initialization. Can be a local path to a pytorch_model.bin or a "
			"checkpoint identifier on the hub. "
			"Don't set if you want to train a model from scratch."
		),
	)
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES),
	)
	parser.add_argument(
		"--config_name_or_path",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--config_overrides",
		type=str,
		default=None,
		help=(
			"Override some existing default config settings when a model is trained from scratch. Example: "
			"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
		),
	)
	parser.add_argument(
		"--cache_dir",
		type=str,
		default=None,
		help="Where do you want to store (cache) the pretrained models/datasets downloaded from the hub",
	)
	parser.add_argument(
		"--model_revision",
		type=str,
		default="main",
		help="The specific model version to use (can be a branch name, tag name or commit id).",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--image_processor_name",
		type=str,
		default=None,
		help="Name or path of preprocessor config.",
	)
	parser.add_argument(
		"--token",
		type=str,
		default=None,
		help=(
			"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
			"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
		),
	)
	parser.add_argument(
		"--use_auth_token",
		type=bool,
		default=None,
		help="The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
	)
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--image_size",
		type=int,
		default=None,
		help="The size (resolution) of each image. If not specified, will use `image_size` of the configuration.",
	)
	parser.add_argument(
		"--patch_size",
		type=int,
		default=None,
		help="The size (resolution) of each patch. If not specified, will use `patch_size` of the configuration.",
	)
	parser.add_argument(
		"--encoder_stride",
		type=int,
		default=None,
		help={"help": "Stride to use for the encoder."},
	)
	parser.add_argument(
		"--push_to_hub",
		action="store_true",
		help="Whether or not to push the model to the Hub.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	parser.add_argument(
		"--seed",
		type=int,
		default=None,
		help="A seed for reproducible training.",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="The initial learning rate for [`AdamW`] optimizer.",
	)
	parser.add_argument(
		"--weight_decay",
		type=float,
		default=0.0,
		help="Weight decay to use.",
	)
	parser.add_argument(
		"--num_train_epochs",
		type=float,
		default=3.0,
		help="Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).",
	)
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps",
		type=int,
		default=0,
		help="Number of steps for the warmup in the lr scheduler.",
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--output_dir",
		type=str,
		default=None,
		help="Where to store the final model.",
	)
	args = parser.parse_args()

	data_files = {}
	if args.train_dir is not None:
		data_files["train"] = args.train_dir
	if args.validation_dir is not None:
		data_files["val"] = args.validation_dir
	args.data_files = data_files if data_files else None

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


class MaskGenerator:

	def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6):
		self.input_size = input_size
		self.mask_patch_size = mask_patch_size
		self.model_patch_size = model_patch_size
		self.mask_ratio = mask_ratio

		if self.input_size % self.mask_patch_size != 0:
			raise ValueError("Input size must be divisible by mask patch size")
		if self.mask_patch_size % self.model_patch_size != 0:
			raise ValueError("Mask patch size must be divisible by model patch size")

		self.rand_size = self.input_size // self.mask_patch_size
		self.scale = self.mask_patch_size // self.model_patch_size

		self.token_count = self.rand_size**2
		self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))

	def __call__(self):
		mask_idx = np.random.permutation(self.token_count)[: self.mask_count]
		mask = np.zeros(self.token_count, dtype=int)
		mask[mask_idx] = 1

		mask = mask.reshape((self.rand_size, self.rand_size))
		mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1)

		return torch.tensor(mask.flatten())


def collate_fn(examples):
	pixel_values = torch.stack([example["pixel_values"] for example in examples])
	mask = torch.stack([example["mask"] for example in examples])
	return {"pixel_values": pixel_values, "bool_masked_pos": mask}


def main():
	args = parse_args()

	if args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		args.token = args.use_auth_token

	send_example_telemetry("run_mim_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(
		gradient_accumulation_steps=args.gradient_accumulation_steps,
		**accelerator_log_kwargs,
	)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	ds = load_dataset(
		args.dataset_name,
		args.dataset_config_name,
		data_files=args.data_files,
		cache_dir=args.cache_dir,
		token=args.token,
	)

	args.train_val_split = None if "validation" in ds.keys() else args.train_val_split
	if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:
		split = ds["train"].train_test_split(args.train_val_split)
		ds["train"] = split["train"]
		ds["validation"] = split["test"]

	config_kwargs = {
		"cache_dir": args.cache_dir,
		"revision": args.model_revision,
		"token": args.token,
		"trust_remote_code": args.trust_remote_code,
	}
	if args.config_name_or_path:
		config = AutoConfig.from_pretrained(args.config_name_or_path, **config_kwargs)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(args.model_name_or_path, **config_kwargs)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")
		if args.config_overrides is not None:
			logger.info(f"Overriding config: {args.config_overrides}")
			config.update_from_string(args.config_overrides)
			logger.info(f"New config: {config}")

	if hasattr(config, "decoder_type"):
		config.decoder_type = "simmim"

	args.image_size = args.image_size if args.image_size is not None else config.image_size
	args.patch_size = args.patch_size if args.patch_size is not None else config.patch_size
	args.encoder_stride = args.encoder_stride if args.encoder_stride is not None else config.encoder_stride

	config.update(
		{
			"image_size": args.image_size,
			"patch_size": args.patch_size,
			"encoder_stride": args.encoder_stride,
		}
	)

	if args.image_processor_name:
		image_processor = AutoImageProcessor.from_pretrained(args.image_processor_name, **config_kwargs)
	elif args.model_name_or_path:
		image_processor = AutoImageProcessor.from_pretrained(args.model_name_or_path, **config_kwargs)
	else:
		IMAGE_PROCESSOR_TYPES = {
			conf.model_type: image_processor_class for conf, image_processor_class in IMAGE_PROCESSOR_MAPPING.items()
		}
		image_processor = IMAGE_PROCESSOR_TYPES[args.model_type]()

	if args.model_name_or_path:
		model = AutoModelForMaskedImageModeling.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			cache_dir=args.cache_dir,
			revision=args.model_revision,
			token=args.token,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForMaskedImageModeling.from_config(
			config,
			token=args.token,
			trust_remote_code=args.trust_remote_code,
		)

	column_names = ds["train"].column_names

	if args.image_column_name is not None:
		image_column_name = args.image_column_name
	elif "image" in column_names:
		image_column_name = "image"
	elif "img" in column_names:
		image_column_name = "img"
	else:
		image_column_name = column_names[0]

	transforms = Compose(
		[
			Lambda(lambda img: img.convert("RGB")),
			RandomResizedCrop(args.image_size, scale=(0.67, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),
			RandomHorizontalFlip(),
			ToTensor(),
			Normalize(mean=image_processor.image_mean, std=image_processor.image_std),
		]
	)

	mask_generator = MaskGenerator(
		input_size=args.image_size,
		mask_patch_size=args.mask_patch_size,
		model_patch_size=args.patch_size,
		mask_ratio=args.mask_ratio,
	)

	def preprocess_images(examples):

		examples["pixel_values"] = [transforms(image) for image in examples[image_column_name]]
		examples["mask"] = [mask_generator() for i in range(len(examples[image_column_name]))]

		return examples

	if args.max_train_samples is not None:
		ds["train"] = ds["train"].shuffle(seed=args.seed).select(range(args.max_train_samples))
	ds["train"].set_transform(preprocess_images)

	if args.max_eval_samples is not None:
		ds["validation"] = ds["validation"].shuffle(seed=args.seed).select(range(args.max_eval_samples))
	ds["validation"].set_transform(preprocess_images)

	train_dataloader = DataLoader(
		ds["train"],
		shuffle=True,
		collate_fn=collate_fn,
		batch_size=args.per_device_train_batch_size,
	)
	eval_dataloader = DataLoader(
		ds["validation"],
		collate_fn=collate_fn,
		batch_size=args.per_device_eval_batch_size,
	)

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)


	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model,
		optimizer,
		train_dataloader,
		eval_dataloader,
		lr_scheduler,
	)

	if accelerator.distributed_type == DistributedType.TPU:
		model.tie_weights()

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("mim_no_trainer", experiment_config)

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(ds['train'])}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(int(args.max_train_steps)), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()
				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()
		losses = []
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				outputs = model(**batch)

			loss = outputs.loss
			losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

		losses = torch.cat(losses)
		eval_loss = torch.mean(losses)

		logger.info(f"epoch {epoch}: eval_loss: {eval_loss}")

		if args.with_tracking:
			accelerator.log(
				{
					"eval_loss": eval_loss,
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				image_processor.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			image_processor.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)


if __name__ == "__main__":
	main()

import logging
from typing import List, Optional, Tuple, Union, Sequence

import numpy as np

import torch
import torch.autograd as autograd

from allennlp.common import Lazy
from allennlp.common.tqdm import Tqdm
from allennlp.data import DatasetReader, DatasetReaderInput, Instance
from allennlp.data.data_loaders import DataLoader, SimpleDataLoader
from allennlp.interpret.influence_interpreters.influence_interpreter import (
	InfluenceInterpreter,
)
from allennlp.models.model import Model


logger = logging.getLogger(__name__)


@InfluenceInterpreter.register("simple-influence")
class SimpleInfluence(InfluenceInterpreter):

	def __init__(
		self,
		model: Model,
		train_data_path: DatasetReaderInput,
		train_dataset_reader: DatasetReader,
		*,
		test_dataset_reader: Optional[DatasetReader] = None,
		train_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		test_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		params_to_freeze: List[str] = None,
		cuda_device: int = -1,
		lissa_batch_size: int = 8,
		damping: float = 3e-3,
		num_samples: int = 1,
		recursion_depth: Union[float, int] = 0.25,
		scale: float = 1e4,
	) -> None:
		super().__init__(
			model=model,
			train_data_path=train_data_path,
			train_dataset_reader=train_dataset_reader,
			test_dataset_reader=test_dataset_reader,
			train_data_loader=train_data_loader,
			test_data_loader=test_data_loader,
			params_to_freeze=params_to_freeze,
			cuda_device=cuda_device,
		)

		self._lissa_dataloader = SimpleDataLoader(
			list(self._train_loader.iter_instances()),
			lissa_batch_size,
			shuffle=True,
			vocab=self.vocab,
		)
		self._lissa_dataloader.set_target_device(self.device)
		if isinstance(recursion_depth, float) and recursion_depth > 0.0:
			self._lissa_dataloader.batches_per_epoch = int(
				len(self._lissa_dataloader) * recursion_depth
			)
		elif isinstance(recursion_depth, int) and recursion_depth > 0:
			self._lissa_dataloader.batches_per_epoch = recursion_depth
		else:
			raise ValueError("'recursion_depth' should be a positive int or float")

		self._damping = damping
		self._num_samples = num_samples
		self._recursion_depth = recursion_depth
		self._scale = scale

	def _calculate_influence_scores(
		self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]
	) -> List[float]:
		inv_hvp = get_inverse_hvp_lissa(
			test_grads,
			self.model,
			self.used_params,
			self._lissa_dataloader,
			self._damping,
			self._num_samples,
			self._scale,
		)
		return [
			torch.dot(inv_hvp, _flatten_tensors(x.grads)).item()
			for x in Tqdm.tqdm(self.train_instances, desc="scoring train instances")
		]


def get_inverse_hvp_lissa(
	vs: Sequence[torch.Tensor],
	model: Model,
	used_params: Sequence[torch.Tensor],
	lissa_data_loader: DataLoader,
	damping: float,
	num_samples: int,
	scale: float,
) -> torch.Tensor:
	inverse_hvps = [torch.tensor(0) for _ in vs]
	for _ in Tqdm.tqdm(range(num_samples), desc="LiSSA samples", total=num_samples):
		cur_estimates = vs
		recursion_iter = Tqdm.tqdm(
			lissa_data_loader, desc="LiSSA depth", total=len(lissa_data_loader)
		)
		for j, training_batch in enumerate(recursion_iter):
			model.zero_grad()
			train_output_dict = model(**training_batch)
			hvps = get_hvp(train_output_dict["loss"], used_params, cur_estimates)

			cur_estimates = [
				v + (1 - damping) * cur_estimate - hvp / scale
				for v, cur_estimate, hvp in zip(vs, cur_estimates, hvps)
			]

			if (j % 50 == 0) or (j == len(lissa_data_loader) - 1):
				norm = np.linalg.norm(_flatten_tensors(cur_estimates).cpu().numpy())
				recursion_iter.set_description(desc=f"calculating inverse HVP, norm = {norm:.5f}")

		inverse_hvps = [
			inverse_hvp + cur_estimate / scale
			for inverse_hvp, cur_estimate in zip(inverse_hvps, cur_estimates)
		]
	return_ihvp = _flatten_tensors(inverse_hvps)
	return_ihvp /= num_samples
	return return_ihvp


def get_hvp(
	loss: torch.Tensor, params: Sequence[torch.Tensor], vectors: Sequence[torch.Tensor]
) -> Tuple[torch.Tensor, ...]:
	assert len(params) == len(vectors)
	assert all(p.size() == v.size() for p, v in zip(params, vectors))
	grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)
	hvp = autograd.grad(grads, params, grad_outputs=vectors)
	return hvp


def _flatten_tensors(tensors: Sequence[torch.Tensor]) -> torch.Tensor:
	views = []
	for p in tensors:
		if p.data.is_sparse:
			view = p.data.to_dense().view(-1)
		else:
			view = p.data.view(-1)
		views.append(view)
	return torch.cat(views, 0)

import os
from typing import Dict, Union, Optional


from tensorboardX import SummaryWriter
import torch

from allennlp.training.callbacks.callback import TrainerCallback
from allennlp.training.callbacks.log_writer import LogWriterCallback


@TrainerCallback.register("tensorboard")
class TensorBoardCallback(LogWriterCallback):

	def __init__(
		self,
		serialization_dir: str,
		summary_interval: int = 100,
		distribution_interval: Optional[int] = None,
		batch_size_interval: Optional[int] = None,
		should_log_parameter_statistics: bool = False,
		should_log_learning_rate: bool = False,
	) -> None:
		super().__init__(
			serialization_dir,
			summary_interval=summary_interval,
			distribution_interval=distribution_interval,
			batch_size_interval=batch_size_interval,
			should_log_parameter_statistics=should_log_parameter_statistics,
			should_log_learning_rate=should_log_learning_rate,
		)

		train_ser_dir = os.path.join(self.serialization_dir, "log", "train")
		os.makedirs(train_ser_dir, exist_ok=True)
		self._train_log = SummaryWriter(train_ser_dir)
		val_ser_dir = os.path.join(self.serialization_dir, "log", "validation")
		os.makedirs(val_ser_dir, exist_ok=True)
		self._validation_log = SummaryWriter(val_ser_dir)

	def log_scalars(
		self,
		scalars: Dict[str, Union[int, float]],
		log_prefix: str = "",
		epoch: Optional[int] = None,
	) -> None:
		assert self.trainer is not None
		timestep = epoch if epoch is not None else self.trainer._total_batches_completed
		log = self._train_log if not log_prefix.startswith("validation") else self._validation_log
		for key, value in scalars.items():
			name = f"{log_prefix}/{key}" if log_prefix else key
			log.add_scalar(name, value, timestep + 1)

	def log_tensors(
		self, tensors: Dict[str, torch.Tensor], log_prefix: str = "", epoch: Optional[int] = None
	) -> None:
		assert self.trainer is not None
		timestep = epoch if epoch is not None else self.trainer._total_batches_completed
		log = self._train_log if not log_prefix.startswith("validation") else self._validation_log
		for key, values in tensors.items():
			name = f"{log_prefix}/{key}" if log_prefix else key
			values_to_write = values.cpu().data.numpy().flatten()
			log.add_histogram(name, values_to_write, timestep + 1)

	def close(self) -> None:
		super().close()
		if self._train_log is not None:
			self._train_log.close()
		if self._validation_log is not None:
			self._validation_log.close()

from dataclasses import dataclass, field
from typing import ClassVar
from torch.distributed.fsdp import ShardingStrategy
from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType

@dataclass
class fsdp_config:
	mixed_precision: bool=True
	use_fp16: bool=False
	seed: int=42
	fsdp_activation_checkpointing: bool=True
	limit_all_gathers: bool=True
	sharding_strategy: ShardingStrategy = ShardingStrategy.FULL_SHARD #HYBRID_SHARD, SHARD_GRAD_OP
	checkpoint_type: StateDictType = StateDictType.FULL_STATE_DICT # alternatively can use SHARDED_STATE_DICT to avoid OOMs
	save_optimizer: bool=False
	
	
	
	

import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset
from PIL import Image
from torchvision.io import ImageReadMode, read_image
from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize
from torchvision.transforms.functional import InterpolationMode

import transformers
from transformers import (
	AutoImageProcessor,
	AutoModel,
	AutoTokenizer,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


logger = logging.getLogger(__name__)

check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/contrastive-image-text/requirements.txt")


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	image_processor_name: str = field(default=None, metadata={"help": "Name or path of preprocessor config."})
	cache_dir: Optional[str] = field(
		default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	freeze_vision_model: bool = field(
		default=False, metadata={"help": "Whether to freeze the vision model parameters or not."}
	)
	freeze_text_model: bool = field(
		default=False, metadata={"help": "Whether to freeze the text model parameters or not."}
	)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	data_dir: Optional[str] = field(default=None, metadata={"help": "The data directory containing input files."})
	image_column: Optional[str] = field(
		default="image_path",
		metadata={"help": "The name of the column in the datasets containing the full image file paths."},
	)
	caption_column: Optional[str] = field(
		default="caption",
		metadata={"help": "The name of the column in the datasets containing the image captions."},
	)
	train_file: Optional[str] = field(
		default=None, metadata={"help": "The input training data file (a jsonlines file)."}
	)
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file (a jsonlines file)."},
	)
	test_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input testing data file (a jsonlines file)."},
	)
	max_seq_length: Optional[int] = field(
		default=128,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)

	def __post_init__(self):
		if self.dataset_name is None and self.train_file is None and self.validation_file is None:
			raise ValueError("Need either a dataset name or a training/validation file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension == "json", "`validation_file` should be a json file."


dataset_name_mapping = {
	"image_caption_dataset.py": ("image_path", "caption"),
}


class Transform(torch.nn.Module):
	def __init__(self, image_size, mean, std):
		super().__init__()
		self.transforms = torch.nn.Sequential(
			Resize([image_size], interpolation=InterpolationMode.BICUBIC),
			CenterCrop(image_size),
			ConvertImageDtype(torch.float),
			Normalize(mean, std),
		)

	def forward(self, x) -> torch.Tensor:
		with torch.no_grad():
			x = self.transforms(x)
		return x


def collate_fn(examples):
	pixel_values = torch.stack([example["pixel_values"] for example in examples])
	input_ids = torch.tensor([example["input_ids"] for example in examples], dtype=torch.long)
	attention_mask = torch.tensor([example["attention_mask"] for example in examples], dtype=torch.long)
	return {
		"pixel_values": pixel_values,
		"input_ids": input_ids,
		"attention_mask": attention_mask,
		"return_loss": True,
	}


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_clip", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	if data_args.dataset_name is not None:
		dataset = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			keep_in_memory=False,
			data_dir=data_args.data_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
			extension = data_args.train_file.split(".")[-1]
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
			extension = data_args.validation_file.split(".")[-1]
		if data_args.test_file is not None:
			data_files["test"] = data_args.test_file
			extension = data_args.test_file.split(".")[-1]
		dataset = load_dataset(
			extension,
			data_files=data_files,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	if model_args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(
			model_args.tokenizer_name,
			cache_dir=model_args.cache_dir,
			use_fast=model_args.use_fast_tokenizer,
			token=model_args.token,
			trust_remote_code=model_args.trust_remote_code,
		)
	elif model_args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(
			model_args.model_name_or_path,
			cache_dir=model_args.cache_dir,
			use_fast=model_args.use_fast_tokenizer,
			token=model_args.token,
			trust_remote_code=model_args.trust_remote_code,
		)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	image_processor = AutoImageProcessor.from_pretrained(
		model_args.image_processor_name or model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	model = AutoModel.from_pretrained(
		model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	config = model.config

	def _freeze_params(module):
		for param in module.parameters():
			param.requires_grad = False

	if model_args.freeze_vision_model:
		_freeze_params(model.vision_model)

	if model_args.freeze_text_model:
		_freeze_params(model.text_model)

	set_seed(training_args.seed)

	if training_args.do_train:
		column_names = dataset["train"].column_names
	elif training_args.do_eval:
		column_names = dataset["validation"].column_names
	elif training_args.do_predict:
		column_names = dataset["test"].column_names
	else:
		logger.info("There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.")
		return

	dataset_columns = dataset_name_mapping.get(data_args.dataset_name, None)
	if data_args.image_column is None:
		image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
	else:
		image_column = data_args.image_column
		if image_column not in column_names:
			raise ValueError(
				f"--image_column' value '{data_args.image_column}' needs to be one of: {', '.join(column_names)}"
			)
	if data_args.caption_column is None:
		caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
	else:
		caption_column = data_args.caption_column
		if caption_column not in column_names:
			raise ValueError(
				f"--caption_column' value '{data_args.caption_column}' needs to be one of: {', '.join(column_names)}"
			)

	image_transformations = Transform(
		config.vision_config.image_size, image_processor.image_mean, image_processor.image_std
	)
	image_transformations = torch.jit.script(image_transformations)

	def tokenize_captions(examples):
		captions = list(examples[caption_column])
		text_inputs = tokenizer(captions, max_length=data_args.max_seq_length, padding="max_length", truncation=True)
		examples["input_ids"] = text_inputs.input_ids
		examples["attention_mask"] = text_inputs.attention_mask
		return examples

	def transform_images(examples):
		images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]
		examples["pixel_values"] = [image_transformations(image) for image in images]
		return examples

	def filter_corrupt_images(examples):
		valid_images = []
		for image_file in examples[image_column]:
			try:
				Image.open(image_file)
				valid_images.append(True)
			except Exception:
				valid_images.append(False)
		return valid_images

	if training_args.do_train:
		if "train" not in dataset:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = dataset["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

		train_dataset = train_dataset.filter(
			filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers
		)
		train_dataset = train_dataset.map(
			function=tokenize_captions,
			batched=True,
			remove_columns=[col for col in column_names if col != image_column],
			num_proc=data_args.preprocessing_num_workers,
			load_from_cache_file=not data_args.overwrite_cache,
			desc="Running tokenizer on train dataset",
		)

		train_dataset.set_transform(transform_images)

	if training_args.do_eval:
		if "validation" not in dataset:
			raise ValueError("--do_eval requires a train validation")
		eval_dataset = dataset["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

		eval_dataset = eval_dataset.filter(
			filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers
		)
		eval_dataset = eval_dataset.map(
			function=tokenize_captions,
			batched=True,
			num_proc=data_args.preprocessing_num_workers,
			remove_columns=[col for col in column_names if col != image_column],
			load_from_cache_file=not data_args.overwrite_cache,
			desc="Running tokenizer on validation dataset",
		)

		eval_dataset.set_transform(transform_images)

	if training_args.do_predict:
		if "test" not in dataset:
			raise ValueError("--do_predict requires a test dataset")
		test_dataset = dataset["test"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(test_dataset), data_args.max_eval_samples)
			test_dataset = test_dataset.select(range(max_eval_samples))

		test_dataset = test_dataset.filter(
			filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers
		)
		test_dataset = test_dataset.map(
			function=tokenize_captions,
			batched=True,
			num_proc=data_args.preprocessing_num_workers,
			remove_columns=[col for col in column_names if col != image_column],
			load_from_cache_file=not data_args.overwrite_cache,
			desc="Running tokenizer on test dataset",
		)

		test_dataset.set_transform(transform_images)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		data_collator=collate_fn,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()
		tokenizer.save_pretrained(training_args.output_dir)
		image_processor.save_pretrained(training_args.output_dir)
		trainer.log_metrics("train", train_result.metrics)
		trainer.save_metrics("train", train_result.metrics)
		trainer.save_state()

	if training_args.do_eval:
		metrics = trainer.evaluate()
		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	finetuned_from = model_args.model_name_or_path
	if os.path.isdir(finetuned_from):
		finetuned_from = None
	kwargs = {"finetuned_from": finetuned_from, "tasks": "contrastive-image-text-modeling"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


if __name__ == "__main__":
	main()

from typing import Optional, Tuple, Iterable, Callable, Union
import itertools
import numpy as np

from checklist.test_suite import TestSuite
from checklist.test_types import MFT, INV, DIR, Expect
from checklist.perturb import Perturb
from allennlp.confidence_checks.task_checklists.task_suite import TaskSuite
from allennlp.confidence_checks.task_checklists import utils
from allennlp.predictors import Predictor


def _wrap_apply_to_each(perturb_fn: Callable, both: bool = False, *args, **kwargs):

	def new_fn(pair, *args, **kwargs):
		premise, hypothesis = pair[0], pair[1]
		ret = []
		fn_premise = perturb_fn(premise, *args, **kwargs)
		fn_hypothesis = perturb_fn(hypothesis, *args, **kwargs)
		if type(fn_premise) != list:
			fn_premise = [fn_premise]
		if type(fn_hypothesis) != list:
			fn_hypothesis = [fn_hypothesis]
		ret.extend([(x, str(hypothesis)) for x in fn_premise])
		ret.extend([(str(premise), x) for x in fn_hypothesis])
		if both:
			ret.extend([(x, x2) for x, x2 in itertools.product(fn_premise, fn_hypothesis)])

		return [x for x in ret if x[0] and x[1]]

	return new_fn


@TaskSuite.register("textual-entailment")
class TextualEntailmentSuite(TaskSuite):
	def __init__(
		self,
		suite: Optional[TestSuite] = None,
		entails: int = 0,
		contradicts: int = 1,
		neutral: int = 2,
		premise: str = "premise",
		hypothesis: str = "hypothesis",
		probs_key: str = "probs",
		**kwargs,
	):

		self._entails = entails
		self._contradicts = contradicts
		self._neutral = neutral

		self._premise = premise
		self._hypothesis = hypothesis

		self._probs_key = probs_key

		super().__init__(suite, **kwargs)

	def _prediction_and_confidence_scores(self, predictor: Predictor):
		def preds_and_confs_fn(data):
			labels = []
			confs = []

			data = [{self._premise: pair[0], self._hypothesis: pair[1]} for pair in data]
			predictions = predictor.predict_batch_json(data)
			for pred in predictions:
				label = np.argmax(pred[self._probs_key])
				labels.append(label)
				confs.append(pred[self._probs_key])
			return np.array(labels), np.array(confs)

		return preds_and_confs_fn

	def _format_failing_examples(
		self,
		inputs: Tuple,
		pred: int,
		conf: Union[np.array, np.ndarray],
		label: Optional[int] = None,
		*args,
		**kwargs,
	):
		labels = {
			self._entails: "Entails",
			self._contradicts: "Contradicts",
			self._neutral: "Neutral",
		}
		ret = "Premise: %s\nHypothesis: %s" % (inputs[0], inputs[1])
		if label is not None:
			ret += "\nOriginal: %s" % labels[label]
		ret += "\nPrediction: Entails (%.1f), Contradicts (%.1f), Neutral (%.1f)" % (
			conf[self._entails],
			conf[self._contradicts],
			conf[self._neutral],
		)

		return ret

	@classmethod
	def contractions(cls):
		return _wrap_apply_to_each(Perturb.contractions, both=True)

	@classmethod
	def typos(cls):
		return _wrap_apply_to_each(Perturb.add_typos, both=False)

	@classmethod
	def punctuation(cls):
		return _wrap_apply_to_each(utils.toggle_punctuation, both=False)

	def _setup_editor(self):
		super()._setup_editor()

		antonyms = [
			("progressive", "conservative"),
			("positive", "negative"),
			("defensive", "offensive"),
			("rude", "polite"),
			("optimistic", "pessimistic"),
			("stupid", "smart"),
			("negative", "positive"),
			("unhappy", "happy"),
			("active", "passive"),
			("impatient", "patient"),
			("powerless", "powerful"),
			("visible", "invisible"),
			("fat", "thin"),
			("bad", "good"),
			("cautious", "brave"),
			("hopeful", "hopeless"),
			("insecure", "secure"),
			("humble", "proud"),
			("passive", "active"),
			("dependent", "independent"),
			("pessimistic", "optimistic"),
			("irresponsible", "responsible"),
			("courageous", "fearful"),
		]

		self.editor.add_lexicon("antonyms", antonyms, overwrite=True)

		synonyms = [
			("smart", "intelligent"),
			("optimistic", "hopeful"),
			("brave", "courageous"),
			("adorable", "cute"),
			("huge", "enormous"),
			("intelligent", "clever"),
			("lazy", "indolent"),
			("rude", "impolite"),
			("thin", "lean"),
			("sad", "unhappy"),
			("little", "small"),
		]

		self.editor.add_lexicon("synonyms", synonyms, overwrite=True)

		comp = [
			"smarter",
			"better",
			"worse",
			"brighter",
			"bigger",
			"louder",
			"longer",
			"larger",
			"smaller",
			"warmer",
			"colder",
			"thicker",
			"lighter",
			"heavier",
		]

		self.editor.add_lexicon("compare", comp, overwrite=True)

		nouns = [
			"humans",
			"cats",
			"dogs",
			"people",
			"mice",
			"pigs",
			"birds",
			"sheep",
			"cows",
			"rats",
			"chickens",
			"fish",
			"bears",
			"elephants",
			"rabbits",
			"lions",
			"monkeys",
			"snakes",
			"bees",
			"spiders",
			"bats",
			"puppies",
			"dolphins",
			"babies",
			"kittens",
			"children",
			"frogs",
			"ants",
			"butterflies",
			"insects",
			"turtles",
			"trees",
			"ducks",
			"whales",
			"robots",
			"animals",
			"bugs",
			"kids",
			"crabs",
			"carrots",
			"dragons",
			"mosquitoes",
			"cars",
			"sharks",
			"dinosaurs",
			"horses",
			"tigers",
		]
		self.editor.add_lexicon("nouns", nouns, overwrite=True)

		adjectives = [
			"good",
			"great",
			"excellent",
			"amazing",
			"extraordinary",
			"beautiful",
			"fantastic",
			"nice",
			"awful",
			"bad",
			"horrible",
			"weird",
			"rough",
		]
		self.editor.add_lexicon("adjectives", adjectives, overwrite=True)

		intens_adj = [
			"very",
			"really",
			"absolutely",
			"truly",
			"extremely",
			"quite",
			"incredibly",
			"amazingly",
			"especially",
			"exceptionally",
			"unbelievably",
			"utterly",
			"exceedingly",
			"rather",
			"totally",
			"particularly",
		]
		intens_verb = [
			"really",
			"absolutely",
			"truly",
			"extremely",
			"especially",
			"utterly",
			"totally",
			"particularly",
			"highly",
			"definitely",
			"certainly",
			"genuinely",
			"honestly",
			"strongly",
			"sure",
			"sincerely",
		]

		self.editor.add_lexicon("intens_adj", intens_adj, overwrite=True)
		self.editor.add_lexicon("intens_verb", intens_verb, overwrite=True)

		reducer_adj = [
			"somewhat",
			"kinda",
			"mostly",
			"probably",
			"generally",
			"reasonably",
			"a little",
			"a bit",
			"slightly",
		]

		self.editor.add_lexicon("reducer_adj", reducer_adj, overwrite=True)

		subclasses = [
			(
				"vehicles",
				[
					"cars",
					"trucks",
					"jeeps",
					"bikes",
					"motorcycles",
					"tractors",
					"vans",
					"SUVs",
					"minivans",
					"bicycles",
				],
			),
			(
				"animals",
				[
					"dogs",
					"cats",
					"turtles",
					"lizards",
					"snakes",
					"fish",
					"hamsters",
					"rabbits",
					"guinea pigs",
					"ducks",
				],
			),
			(
				"clothes",
				[
					"jackets",
					"pants",
					"shirts",
					"skirts",
					"t-shirts",
					"raincoats",
					"sweaters",
					"jeans",
					"sweatpants",
				],
			),
		]

		subclasses = [(a, b[i]) for a, b in subclasses for i in range(len(b))]
		self.editor.add_lexicon("subclasses", subclasses, overwrite=True)

	def _default_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):
		super()._default_tests(data, num_test_cases)
		self._setup_editor()
		self._default_vocabulary_tests(data, num_test_cases)
		self._default_ner_tests(data, num_test_cases)
		self._default_temporal_tests(data, num_test_cases)
		self._default_logic_tests(data, num_test_cases)
		self._default_negation_tests(data, num_test_cases)
		self._default_taxonomy_tests(data, num_test_cases)
		self._default_coreference_tests(data, num_test_cases)
		self._default_fairness_tests(data, num_test_cases)

	def _default_vocabulary_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):

		template = self.editor.template(
			(
				"{first_name1} is more {antonyms[0]} than {first_name2}",
				"{first_name2} is more {antonyms[1]} than {first_name1}",
			),
			remove_duplicates=True,
			nsamples=num_test_cases,
		)

		test = MFT(
			**template,
			labels=self._entails,
			name='"A is more COMP than B" entails "B is more antonym(COMP) than A"',
			capability="Vocabulary",
			description="Eg. A is more active than B implies that B is more passive than A",
		)

		self.add_test(test)

		template = self.editor.template(
			[
				("All {nouns} are {synonyms[0]}.", "Some {nouns} are {synonyms[0]}."),
				("All {nouns} are {synonyms[0]}.", "Some {nouns} are {synonyms[1]}."),
			],
			remove_duplicates=True,
			nsamples=num_test_cases,
		)

		_num_entails = len(template.data)

		template += self.editor.template(
			[
				("All {nouns} are {synonyms[0]}.", "Some {nouns} are not {synonyms[0]}."),
				("All {nouns} are {synonyms[0]}.", "Some {nouns} are not {synonyms[1]}."),
			],
			remove_duplicates=True,
			nsamples=num_test_cases,
		)

		_num_contradicts = len(template.data) - _num_entails

		test = INV(
			template.data,
			labels=[self._entails for i in range(_num_entails)]
			+ [self._contradicts for i in range(_num_contradicts)],
			name="Changing X to a synonym(X) should not change the label",
			capability="Vocabulary",
			description='"Eg. All tigers are huge -> All tigers are enormous" should not change the label',
		)

		self.add_test(test)

	def _default_taxonomy_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):

		template = self.editor.template(
			("{first_name1} owns {subclasses[1]}.", "{first_name1} owns {subclasses[0]}."),
			remove_duplicates=True,
			nsamples=num_test_cases,
		)

		test = MFT(
			**template,
			labels=self._entails,
			name='"A owns SUBTYPE" entails "A owns SUPERTYPE"',
			capability="Taxonomy",
			description="Eg. A owns rabbits implies that A owns animals.",
		)

		self.add_test(test)

	def _default_coreference_tests(
		self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100
	):

		_quarter = num_test_cases // 4

		template = self.editor.template(
			(
				"{first_name1} and {first_name2} are friends. The former is {a:profession}.",
				"{first_name1} is {a:profession}.",
			),
			remove_duplicates=True,
			nsamples=_quarter,
		)

		template += self.editor.template(
			(
				"{first_name1} and {first_name2} are friends. The latter is {a:profession}.",
				"{first_name2} is {a:profession}.",
			),
			remove_duplicates=True,
			nsamples=_quarter,
		)

		_num_entails = len(template.data)

		template += self.editor.template(
			(
				"{first_name1} and {first_name2} are friends. The former is {a:profession}.",
				"{first_name2} is {a:profession}.",
			),
			remove_duplicates=True,
			nsamples=_quarter,
		)

		template += self.editor.template(
			(
				"{first_name1} and {first_name2} are friends. The latter is {a:profession}.",
				"{first_name1} is {a:profession}.",
			),
			remove_duplicates=True,
			nsamples=_quarter,
		)

		_num_neutral = len(template.data) - _num_entails

		test = MFT(
			**template,
			labels=[self._entails for i in range(_num_entails)]
			+ [self._neutral for i in range(_num_neutral)],
			name="Former / Latter",
			capability="Coreference",
			description='Eg. "A and B are friends. The former is a teacher."'
			+ ' entails "A is a teacher." (while "B is a teacher" is neutral).',
		)

		self.add_test(test)

	def _default_robustness_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):

		template = self.editor.template(
			(
				"{nouns1} and {nouns2} are {adjectives}.",
				"{nouns2} and {nouns1} are {adjectives}.",
			),
			remove_duplicates=True,
			nsamples=num_test_cases,
		)

		test = MFT(
			**template,
			labels=self._entails,
			name='"A and B are X" entails "B and A are X"',
			capability="Vocabulary",
			description='Eg. "tigers and lions are huge" entails that "lions and tigers are huge"',
		)

		self.add_test(test)

		if data:

			template = Perturb.perturb(
				data, _wrap_apply_to_each(utils.add_random_strings), nsamples=num_test_cases
			)
			test = INV(
				template.data,
				name="Add random urls and handles",
				capability="Robustness",
				description="Add randomly generated urls and handles to the start or end of sentence",
			)

			self.add_test(test)

	def _default_logic_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):
		template = self.editor.template(
			("{nouns1} are {compare} than {nouns2}", "{nouns2} are {compare} than {nouns1}"),
			nsamples=num_test_cases,
			remove_duplicates=True,
		)

		test = MFT(
			**template,
			labels=self._contradicts,
			name='"A is COMP than B" contradicts "B is COMP than A"',
			capability="Logic",
			description='Eg. "A is better than B" contradicts "B is better than A"',
		)

		self.add_test(test)

		if data:
			template = Perturb.perturb(
				data, lambda x: (x[0], x[0]), nsamples=num_test_cases, keep_original=False
			)
			template += Perturb.perturb(
				data, lambda x: (x[1], x[1]), nsamples=num_test_cases, keep_original=False
			)

			test = MFT(
				**template,
				labels=self._entails,
				name="A entails A (premise == hypothesis)",
				capability="Logic",
				description="If premise and hypothesis are the same, then premise entails the hypothesis",
			)

			self.add_test(test)

	def _default_negation_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):

		template = self.editor.template(
			(
				"{first_name1} is {compare} than {first_name2}",
				"{first_name1} is not {compare} than {first_name2}",
			),
			nsamples=num_test_cases,
			remove_duplicates=True,
		)

		test = MFT(
			**template,
			labels=self._contradicts,
			name='"A is COMP than B" contradicts "A is not COMP than B"',
			capability="Negation",
			description="Eg. A is better than B contradicts A is not better than C",
		)

		self.add_test(test)

	def _default_ner_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):
		template = self.editor.template(
			(
				"{first_name1} is {compare} than {first_name2}",
				"{first_name1} is {compare} than {first_name3}",
			),
			nsamples=num_test_cases,
			remove_duplicates=True,
		)

		test = MFT(
			**template,
			labels=self._neutral,
			name='"A is COMP than B" gives no information about "A is COMP than C"',
			capability="NER",
			description='Eg. "A is better than B" gives no information about "A is better than C"',
		)

		self.add_test(test)

	def _default_temporal_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):
		template = self.editor.template(
			(
				"{first_name} works as {a:profession}",
				"{first_name} used to work as a {profession}",
			),
			nsamples=num_test_cases,
			remove_duplicates=True,
		)

		template += self.editor.template(
			(
				"{first_name} {last_name} is {a:profession}",
				"{first_name} {last_name} was {a:profession}",
			),
			nsamples=num_test_cases,
			remove_duplicates=True,
		)

		test = MFT(
			**template,
			labels=self._neutral,
			name='"A works as P" gives no information about "A used to work as P"',
			capability="Temporal",
			description='Eg. "A is a writer" gives no information about "A was a writer"',
		)

		self.add_test(test)

		template = self.editor.template(
			(
				"{first_name} was {a:profession1} before they were {a:profession2}",
				"{first_name} was {a:profession1} after they were {a:profession2}",
			),
			nsamples=num_test_cases,
			remove_duplicates=True,
		)

		test = MFT(
			**template,
			labels=self._contradicts,
			name="Before != After",
			capability="Temporal",
			description='Eg. "A was a writer before they were a journalist" '
			'contradicts "A was a writer after they were a journalist"',
		)

		self.add_test(test)

	def _default_fairness_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):



		male_stereotypes = [
			("maestro", "maestros"),
			("skipper", "skippers"),
			("philosopher", "philosophers"),
			("architect", "architects"),
			("financier", "financiers"),
			("warrior", "warriors"),
			("broadcaster", "broadcasters"),
			("doctor", "doctors"),
			("captain", "captains"),
			("protege", "proteges"),
			("boss", "bosses"),
			("magician", "magicians"),
			("fighter pilot", "fighter pilots"),
		]

		female_stereotypes = [
			("nurse", "nurses"),
			("homemaker", "homemakers"),
			("receptionist", "receptionists"),
			("stylist", "stylists"),
			("housekeeper", "housekeepers"),
			("socialite", "socialites"),
			("librarian", "librarians"),
			("hairdresser", "hairdressers"),
			("nanny", "nannies"),
			("interior designer", "interior designers"),
			("guidance counselor", "guidance counselors"),
		]

		template = self.editor.template(
			[
				(
					"All people in the park are {male_stereotypes[1]}. {male} is in the park.",
					"He is a {male_stereotypes[0]}.",
				),
				(
					"All people in the park are {male_stereotypes[1]}. {female} is in the park.",
					"She is a {male_stereotypes[0]}.",
				),
			],
			nsamples=num_test_cases // 2,
			remove_duplicates=True,
			male_stereotypes=male_stereotypes,
		)

		test = DIR(
			template.data,
			Expect.monotonic(label=self._entails, increasing=True, tolerance=0.1),
			name="Male stereotypes",
			capability="Fairness",
			description="Eg. 'All people are X' should entail 'He is X' and 'She is X' equally.",
		)

		self.add_test(test)

		template = self.editor.template(
			[
				(
					"All people in the park are {female_stereotypes[1]}. She is in the park.",
					"{female} is a {female_stereotypes[0]}.",
				),
				(
					"All people in the park are {female_stereotypes[1]}. He is in the park.",
					"{male} is a {female_stereotypes[0]}.",
				),
			],
			nsamples=num_test_cases // 2,
			remove_duplicates=True,
			female_stereotypes=female_stereotypes,
		)

		test = DIR(
			template.data,
			Expect.monotonic(label=self._entails, increasing=True, tolerance=0.1),
			name="Female stereotypes",
			capability="Fairness",
			description="Eg. 'All people are X' should entail 'He is X' and 'She is X' equally.",
		)

		self.add_test(test)

import torch
from torch.fx import symbolic_trace, replace_pattern



class M(torch.nn.Module):
	def __init__(self):
		super().__init__()

	def forward(self, x, w1, w2):
		val1 = torch.neg(w1)
		m1 = torch.cat([val1, w2]).sum()
		val2 = torch.neg(w1)
		m2 = torch.cat([val2, w2]).sum()
		return x + torch.max(m1) + torch.max(m2)

traced = symbolic_trace(M())

def pattern(a1, a2):
	val1 = torch.neg(a1)
	return torch.cat([val1, a2]).sum()

def replacement(w1, w2):
	return torch.stack([w1, w2])

replace_pattern(traced, pattern, replacement)


from typing import Dict, List, Set, Tuple, Optional
import logging
import textwrap


import torch

from allennlp.common.checks import ConfigurationError
from allennlp.data.fields.field import Field
from allennlp.data.fields.sequence_field import SequenceField
from allennlp.data.vocabulary import Vocabulary

logger = logging.getLogger(__name__)


class AdjacencyField(Field[torch.Tensor]):

	__slots__ = [
		"indices",
		"labels",
		"sequence_field",
		"_label_namespace",
		"_padding_value",
		"_indexed_labels",
	]

	_already_warned_namespaces: Set[str] = set()

	def __init__(
		self,
		indices: List[Tuple[int, int]],
		sequence_field: SequenceField,
		labels: List[str] = None,
		label_namespace: str = "labels",
		padding_value: int = -1,
	) -> None:
		self.indices = indices
		self.labels = labels
		self.sequence_field = sequence_field
		self._label_namespace = label_namespace
		self._padding_value = padding_value
		self._indexed_labels: Optional[List[int]] = None

		self._maybe_warn_for_namespace(label_namespace)
		field_length = sequence_field.sequence_length()

		if len(set(indices)) != len(indices):
			raise ConfigurationError(f"Indices must be unique, but found {indices}")

		if not all(
			0 <= index[1] < field_length and 0 <= index[0] < field_length for index in indices
		):
			raise ConfigurationError(
				f"Label indices and sequence length "
				f"are incompatible: {indices} and {field_length}"
			)

		if labels is not None and len(indices) != len(labels):
			raise ConfigurationError(
				f"Labelled indices were passed, but their lengths do not match: "
				f" {labels}, {indices}"
			)

	def _maybe_warn_for_namespace(self, label_namespace: str) -> None:
		if not (self._label_namespace.endswith("labels") or self._label_namespace.endswith("tags")):
			if label_namespace not in self._already_warned_namespaces:
				logger.warning(
					"Your label namespace was '%s'. We recommend you use a namespace "
					"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by "
					"default to your vocabulary.  See documentation for "
					"`non_padded_namespaces` parameter in Vocabulary.",
					self._label_namespace,
				)
				self._already_warned_namespaces.add(label_namespace)

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		if self._indexed_labels is None and self.labels is not None:
			for label in self.labels:
				counter[self._label_namespace][label] += 1  # type: ignore

	def index(self, vocab: Vocabulary):
		if self.labels is not None:
			self._indexed_labels = [
				vocab.get_token_index(label, self._label_namespace) for label in self.labels
			]

	def get_padding_lengths(self) -> Dict[str, int]:
		return {"num_tokens": self.sequence_field.sequence_length()}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:
		desired_num_tokens = padding_lengths["num_tokens"]
		tensor = torch.ones(desired_num_tokens, desired_num_tokens) * self._padding_value
		labels = self._indexed_labels or [1 for _ in range(len(self.indices))]

		for index, label in zip(self.indices, labels):
			tensor[index] = label
		return tensor

	def empty_field(self) -> "AdjacencyField":

		empty_list: List[Tuple[int, int]] = []
		adjacency_field = AdjacencyField(
			empty_list, self.sequence_field.empty_field(), padding_value=self._padding_value
		)
		return adjacency_field

	def __str__(self) -> str:
		length = self.sequence_field.sequence_length()
		formatted_labels = "".join(
			"\t\t" + labels + "\n" for labels in textwrap.wrap(repr(self.labels), 100)
		)
		formatted_indices = "".join(
			"\t\t" + index + "\n" for index in textwrap.wrap(repr(self.indices), 100)
		)
		return (
			f"AdjacencyField of length {length}\n"
			f"\t\twith indices:\n {formatted_indices}\n"
			f"\t\tand labels:\n {formatted_labels} \t\tin namespace: '{self._label_namespace}'."
		)

	def __len__(self):
		return len(self.sequence_field)

	def human_readable_repr(self):
		ret = {"indices": self.indices}
		if self.labels is not None:
			ret["labels"] = self.labels
		return ret

from allennlp.models.model import Model


class Head(Model):

	pass

from typing import Dict, Optional


import torch

from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("mean_absolute_error")
class MeanAbsoluteError(Metric):

	def __init__(self) -> None:
		self._absolute_error = 0.0
		self._total_count = 0.0

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	) -> None:
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		absolute_errors = torch.abs(predictions - gold_labels)

		if mask is not None:
			absolute_errors *= mask
			_total_count = torch.sum(mask)
		else:
			_total_count = gold_labels.numel()
		_absolute_error = torch.sum(absolute_errors)

		self._absolute_error += float(dist_reduce_sum(_absolute_error))
		self._total_count += int(dist_reduce_sum(_total_count))

	def get_metric(self, reset: bool = False) -> Dict[str, float]:
		mean_absolute_error = self._absolute_error / self._total_count
		if reset:
			self.reset()
		return {"mae": mean_absolute_error}

	def reset(self) -> None:
		self._absolute_error = 0.0
		self._total_count = 0.0

import torch
from allennlp.modules.token_embedders.token_embedder import TokenEmbedder


@TokenEmbedder.register("empty")
class EmptyEmbedder(TokenEmbedder):

	def __init__(self) -> None:
		super().__init__()

	def get_output_dim(self):
		return 0

	def forward(self, *inputs, **kwargs) -> torch.Tensor:
		return None


import argparse
import torch
import torch.nn as nn
from torchvision.datasets import MNIST
from torchvision.transforms import Compose, ToTensor, Normalize, Lambda
from torch.utils.data import DataLoader
from torch.optim import Adam


def get_y_neg(y):
	y_neg = y.clone()
	for idx, y_samp in enumerate(y):
		allowed_indices = list(range(10))
		allowed_indices.remove(y_samp.item())
		y_neg[idx] = torch.tensor(allowed_indices)[
			torch.randint(len(allowed_indices), size=(1,))
		].item()
	return y_neg.to(device)


def overlay_y_on_x(x, y, classes=10):
	x_ = x.clone()
	x_[:, :classes] *= 0.0
	x_[range(x.shape[0]), y] = x.max()
	return x_


class Net(torch.nn.Module):
	def __init__(self, dims):

		super().__init__()
		self.layers = []
		for d in range(len(dims) - 1):
			self.layers = self.layers + [Layer(dims[d], dims[d + 1]).to(device)]

	def predict(self, x):
		goodness_per_label = []
		for label in range(10):
			h = overlay_y_on_x(x, label)
			goodness = []
			for layer in self.layers:
				h = layer(h)
				goodness = goodness + [h.pow(2).mean(1)]
			goodness_per_label += [sum(goodness).unsqueeze(1)]
		goodness_per_label = torch.cat(goodness_per_label, 1)
		return goodness_per_label.argmax(1)

	def train(self, x_pos, x_neg):
		h_pos, h_neg = x_pos, x_neg
		for i, layer in enumerate(self.layers):
			print("training layer: ", i)
			h_pos, h_neg = layer.train(h_pos, h_neg)


class Layer(nn.Linear):
	def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):
		super().__init__(in_features, out_features, bias, device, dtype)
		self.relu = torch.nn.ReLU()
		self.opt = Adam(self.parameters(), lr=args.lr)
		self.threshold = args.threshold
		self.num_epochs = args.epochs

	def forward(self, x):
		x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)
		return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))

	def train(self, x_pos, x_neg):
		for i in range(self.num_epochs):
			g_pos = self.forward(x_pos).pow(2).mean(1)
			g_neg = self.forward(x_neg).pow(2).mean(1)
			loss = torch.log(
				1
				+ torch.exp(
					torch.cat([-g_pos + self.threshold, g_neg - self.threshold])
				)
			).mean()
			self.opt.zero_grad()
			loss.backward()
			self.opt.step()
			if i % args.log_interval == 0:
				print("Loss: ", loss.item())
		return self.forward(x_pos).detach(), self.forward(x_neg).detach()


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument(
		"--epochs",
		type=int,
		default=1000,
		metavar="N",
		help="number of epochs to train (default: 1000)",
	)
	parser.add_argument(
		"--lr",
		type=float,
		default=0.03,
		metavar="LR",
		help="learning rate (default: 0.03)",
	)
	parser.add_argument(
		"--no_cuda", action="store_true", default=False, help="disables CUDA training"
	)
	parser.add_argument(
		"--no_mps", action="store_true", default=False, help="disables MPS training"
	)
	parser.add_argument(
		"--seed", type=int, default=1, metavar="S", help="random seed (default: 1)"
	)
	parser.add_argument(
		"--save_model",
		action="store_true",
		default=False,
		help="For saving the current Model",
	)
	parser.add_argument(
		"--train_size", type=int, default=50000, help="size of training set"
	)
	parser.add_argument(
		"--threshold", type=float, default=2, help="threshold for training"
	)
	parser.add_argument("--test_size", type=int, default=10000, help="size of test set")
	parser.add_argument(
		"--save-model",
		action="store_true",
		default=False,
		help="For Saving the current Model",
	)
	parser.add_argument(
		"--log-interval",
		type=int,
		default=10,
		metavar="N",
		help="how many batches to wait before logging training status",
	)
	args = parser.parse_args()
	use_cuda = not args.no_cuda and torch.cuda.is_available()
	use_mps = not args.no_mps and torch.backends.mps.is_available()
	if use_cuda:
		device = torch.device("cuda")
	elif use_mps:
		device = torch.device("mps")
	else:
		device = torch.device("cpu")

	train_kwargs = {"batch_size": args.train_size}
	test_kwargs = {"batch_size": args.test_size}

	if use_cuda:
		cuda_kwargs = {"num_workers": 1, "pin_memory": True, "shuffle": True}
		train_kwargs.update(cuda_kwargs)
		test_kwargs.update(cuda_kwargs)

	transform = Compose(
		[
			ToTensor(),
			Normalize((0.1307,), (0.3081,)),
			Lambda(lambda x: torch.flatten(x)),
		]
	)
	train_loader = DataLoader(
		MNIST("./data/", train=True, download=True, transform=transform), **train_kwargs
	)
	test_loader = DataLoader(
		MNIST("./data/", train=False, download=True, transform=transform), **test_kwargs
	)
	net = Net([784, 500, 500])
	x, y = next(iter(train_loader))
	x, y = x.to(device), y.to(device)
	x_pos = overlay_y_on_x(x, y)
	y_neg = get_y_neg(y)
	x_neg = overlay_y_on_x(x, y_neg)
	net.train(x_pos, x_neg)
	print("train error:", 1.0 - net.predict(x).eq(y).float().mean().item())
	x_te, y_te = next(iter(test_loader))
	x_te, y_te = x_te.to(device), y_te.to(device)
	if args.save_model:
		torch.save(net.state_dict(), "mnist_ff.pt")
	print("test error:", 1.0 - net.predict(x_te).eq(y_te).float().mean().item())



import argparse
import json
import logging
import os
import shutil
import sys
import tempfile
import unittest
from unittest import mock

from accelerate.utils import write_basic_config

from transformers.testing_utils import (
	TestCasePlus,
	backend_device_count,
	run_command,
	slow,
	torch_device,
)


logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger()


def get_setup_file():
	parser = argparse.ArgumentParser()
	parser.add_argument("-f")
	args = parser.parse_args()
	return args.f


def get_results(output_dir):
	results = {}
	path = os.path.join(output_dir, "all_results.json")
	if os.path.exists(path):
		with open(path, "r") as f:
			results = json.load(f)
	else:
		raise ValueError(f"can't find {path}")
	return results


stream_handler = logging.StreamHandler(sys.stdout)
logger.addHandler(stream_handler)


class ExamplesTestsNoTrainer(TestCasePlus):
	@classmethod
	def setUpClass(cls):
		cls.tmpdir = tempfile.mkdtemp()
		cls.configPath = os.path.join(cls.tmpdir, "default_config.yml")
		write_basic_config(save_location=cls.configPath)
		cls._launch_args = ["accelerate", "launch", "--config_file", cls.configPath]

	@classmethod
	def tearDownClass(cls):
		shutil.rmtree(cls.tmpdir)

	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_glue_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_accuracy"], 0.75)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "epoch_0")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "glue_no_trainer")))

	@unittest.skip("Zach is working on this.")
	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_clm_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if backend_device_count(torch_device) > 1:
			return

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertLess(result["perplexity"], 100)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "epoch_0")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "clm_no_trainer")))

	@unittest.skip("Zach is working on this.")
	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_mlm_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertLess(result["perplexity"], 42)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "epoch_0")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "mlm_no_trainer")))

	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_ner_no_trainer(self):
		epochs = 7 if backend_device_count(torch_device) > 1 else 2

		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_accuracy"], 0.75)
		self.assertLess(result["train_loss"], 0.6)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "epoch_0")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "ner_no_trainer")))

	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_squad_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_f1"], 28)
		self.assertGreaterEqual(result["eval_exact"], 28)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "epoch_0")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "qa_no_trainer")))

	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_swag_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_accuracy"], 0.8)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "swag_no_trainer")))

	@slow
	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_summarization_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_rouge1"], 10)
		self.assertGreaterEqual(result["eval_rouge2"], 2)
		self.assertGreaterEqual(result["eval_rougeL"], 7)
		self.assertGreaterEqual(result["eval_rougeLsum"], 7)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "epoch_0")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "summarization_no_trainer")))

	@slow
	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_translation_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_bleu"], 30)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "epoch_0")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "translation_no_trainer")))

	@slow
	def test_run_semantic_segmentation_no_trainer(self):
		stream_handler = logging.StreamHandler(sys.stdout)
		logger.addHandler(stream_handler)

		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_overall_accuracy"], 0.10)

	@mock.patch.dict(os.environ, {"WANDB_MODE": "offline", "DVCLIVE_TEST": "true"})
	def test_run_image_classification_no_trainer(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		run_command(self._launch_args + testargs)
		result = get_results(tmp_dir)
		self.assertGreaterEqual(result["eval_accuracy"], 0.4)
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "step_1")))
		self.assertTrue(os.path.exists(os.path.join(tmp_dir, "image_classification_no_trainer")))

from typing import Optional


import torch

from allennlp.nn.util import dist_reduce_sum
from allennlp.common.checks import ConfigurationError
from allennlp.training.metrics.metric import Metric


@Metric.register("categorical_accuracy")
class CategoricalAccuracy(Metric):

	supports_distributed = True

	def __init__(self, top_k: int = 1, tie_break: bool = False) -> None:
		if top_k > 1 and tie_break:
			raise ConfigurationError(
				"Tie break in Categorical Accuracy can be done only for maximum (top_k = 1)"
			)
		if top_k <= 0:
			raise ConfigurationError("top_k passed to Categorical Accuracy must be > 0")
		self._top_k = top_k
		self._tie_break = tie_break
		self.correct_count = 0.0
		self.total_count = 0.0

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		num_classes = predictions.size(-1)
		if gold_labels.dim() != predictions.dim() - 1:
			raise ConfigurationError(
				"gold_labels must have dimension == predictions.size() - 1 but "
				"found tensor of shape: {}".format(predictions.size())
			)
		if (gold_labels >= num_classes).any():
			raise ConfigurationError(
				"A gold label passed to Categorical Accuracy contains an id >= {}, "
				"the number of classes.".format(num_classes)
			)

		predictions = predictions.view((-1, num_classes))
		gold_labels = gold_labels.view(-1).long()
		if not self._tie_break:
			if self._top_k == 1:
				top_k = predictions.max(-1)[1].unsqueeze(-1)
			else:
				_, sorted_indices = predictions.sort(dim=-1, descending=True)
				top_k = sorted_indices[..., : min(self._top_k, predictions.shape[-1])]

			correct = top_k.eq(gold_labels.unsqueeze(-1)).float()
		else:
			max_predictions = predictions.max(-1)[0]
			max_predictions_mask = predictions.eq(max_predictions.unsqueeze(-1))
			correct = max_predictions_mask[
				torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels
			].float()
			tie_counts = max_predictions_mask.sum(-1)
			correct /= tie_counts.float()
			correct.unsqueeze_(-1)

		if mask is not None:
			correct *= mask.view(-1, 1)
			_total_count = mask.sum()
		else:
			_total_count = torch.tensor(gold_labels.numel())
		_correct_count = correct.sum()

		self.correct_count += dist_reduce_sum(_correct_count).item()
		self.total_count += dist_reduce_sum(_total_count).item()

	def get_metric(self, reset: bool = False) -> float:
		if self.total_count > 1e-12:
			accuracy = float(self.correct_count) / float(self.total_count)
		else:
			accuracy = 0.0

		if reset:
			self.reset()

		return accuracy

	def reset(self):
		self.correct_count = 0.0
		self.total_count = 0.0

from allennlp.modules.span_extractors.span_extractor import SpanExtractor
from allennlp.modules.span_extractors.endpoint_span_extractor import EndpointSpanExtractor
from allennlp.modules.span_extractors.self_attentive_span_extractor import (
	SelfAttentiveSpanExtractor,
)
from allennlp.modules.span_extractors.bidirectional_endpoint_span_extractor import (
	BidirectionalEndpointSpanExtractor,
)
from allennlp.modules.span_extractors.max_pooling_span_extractor import MaxPoolingSpanExtractor

from typing import Union
import torch

from allennlp.common import FromParams

from allennlp.modules.transformer.transformer_module import TransformerModule

from transformers.models.bert.modeling_bert import ACT2FN


class ActivationLayer(TransformerModule, FromParams):
	def __init__(
		self,
		hidden_size: int,
		intermediate_size: int,
		activation: Union[str, torch.nn.Module],
		pool: bool = False,
	):
		super().__init__()
		self.dense = torch.nn.Linear(hidden_size, intermediate_size)
		if isinstance(activation, str):
			self.act_fn = ACT2FN[activation]
		else:
			self.act_fn = activation
		self.pool = pool

	def get_output_dim(self) -> int:
		return self.dense.out_features

	def forward(self, hidden_states):
		if self.pool:
			hidden_states = hidden_states[:, 0]
		hidden_states = self.dense(hidden_states)
		hidden_states = self.act_fn(hidden_states)
		return hidden_states


from allennlp.data.tokenizers.token_class import Token
from allennlp.data.tokenizers.tokenizer import Tokenizer
from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer
from allennlp.data.tokenizers.letters_digits_tokenizer import LettersDigitsTokenizer
from allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer
from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer
from allennlp.data.tokenizers.sentence_splitter import SentenceSplitter
from allennlp.data.tokenizers.whitespace_tokenizer import WhitespaceTokenizer

from allennlp.training.checkpointer import Checkpointer
from allennlp.training.no_op_trainer import NoOpTrainer
from allennlp.training.callbacks import TrainerCallback
from allennlp.training.trainer import Trainer
from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


from typing import Dict, List, Optional, Union
from dataclasses import dataclass

from allennlp.common.from_params import FromParams


@dataclass(frozen=True)
class ExampleCategory(FromParams):
	category: str
	examples: List[Dict[str, str]]


@dataclass(frozen=True)
class TaskCard(FromParams):

	id: str
	name: Optional[str] = None
	description: Optional[str] = None
	expected_inputs: Optional[str] = None
	expected_outputs: Optional[str] = None
	scope_and_limitations: Optional[str] = None
	examples: Optional[Union[List[Dict[str, str]], List[ExampleCategory]]] = None


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
import nltk  # Here to have a nice missing dependency error message early on
import numpy as np
from datasets import load_dataset
from filelock import FileLock

import transformers
from transformers import (
	AutoConfig,
	AutoModelForSeq2SeqLM,
	AutoTokenizer,
	DataCollatorForSeq2Seq,
	HfArgumentParser,
	MBart50Tokenizer,
	MBart50TokenizerFast,
	MBartTokenizer,
	MBartTokenizerFast,
	Seq2SeqTrainer,
	Seq2SeqTrainingArguments,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, is_offline_mode, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/summarization/requirements.txt")

logger = logging.getLogger(__name__)

try:
	nltk.data.find("tokenizers/punkt")
except (LookupError, OSError):
	if is_offline_mode():
		raise LookupError(
			"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files"
		)
	with FileLock(".lock") as lock:
		nltk.download("punkt", quiet=True)

MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	resize_position_embeddings: Optional[bool] = field(
		default=None,
		metadata={
			"help": (
				"Whether to automatically resize the position embeddings if `max_source_length` exceeds "
				"the model's position embeddings."
			)
		},
	)


@dataclass
class DataTrainingArguments:

	lang: Optional[str] = field(default=None, metadata={"help": "Language id for summarization."})

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	text_column: Optional[str] = field(
		default=None,
		metadata={"help": "The name of the column in the datasets containing the full texts (for summarization)."},
	)
	summary_column: Optional[str] = field(
		default=None,
		metadata={"help": "The name of the column in the datasets containing the summaries (for summarization)."},
	)
	train_file: Optional[str] = field(
		default=None, metadata={"help": "The input training data file (a jsonlines or csv file)."}
	)
	validation_file: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file)."
			)
		},
	)
	test_file: Optional[str] = field(
		default=None,
		metadata={
			"help": "An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file)."
		},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_source_length: Optional[int] = field(
		default=1024,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	max_target_length: Optional[int] = field(
		default=128,
		metadata={
			"help": (
				"The maximum total sequence length for target text after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	val_max_target_length: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"The maximum total sequence length for validation target text after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`. "
				"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used "
				"during ``evaluate`` and ``predict``."
			)
		},
	)
	pad_to_max_length: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to pad all samples to model maximum sentence length. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
				"efficient on GPU but very bad for TPU."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	num_beams: Optional[int] = field(
		default=1,
		metadata={
			"help": (
				"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, "
				"which is used during ``evaluate`` and ``predict``."
			)
		},
	)
	ignore_pad_token_for_loss: bool = field(
		default=True,
		metadata={
			"help": "Whether to ignore the tokens corresponding to padded labels in the loss computation or not."
		},
	)
	source_prefix: Optional[str] = field(
		default=None, metadata={"help": "A prefix to add before every source text (useful for T5 models)."}
	)

	forced_bos_token: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The token to force as the first generated token after the decoder_start_token_id. "
				"Useful for multilingual models like mBART where the first generated token"
				"needs to be the target language token (Usually it is the target language token)"
			)
		},
	)

	def __post_init__(self):
		if (
			self.dataset_name is None
			and self.train_file is None
			and self.validation_file is None
			and self.test_file is None
		):
			raise ValueError("Need either a dataset name or a training, validation, or test file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
			if self.test_file is not None:
				extension = self.test_file.split(".")[-1]
				assert extension in ["csv", "json"], "`test_file` should be a csv or a json file."
		if self.val_max_target_length is None:
			self.val_max_target_length = self.max_target_length


summarization_name_mapping = {
	"amazon_reviews_multi": ("review_body", "review_title"),
	"big_patent": ("description", "abstract"),
	"cnn_dailymail": ("article", "highlights"),
	"orange_sum": ("text", "summary"),
	"pn_summary": ("article", "summary"),
	"psc": ("extract_text", "summary_text"),
	"samsum": ("dialogue", "summary"),
	"thaisum": ("body", "summary"),
	"xglue": ("news_body", "news_title"),
	"xsum": ("document", "summary"),
	"wiki_summary": ("article", "highlights"),
	"multi_news": ("document", "summary"),
}


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_summarization", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	if data_args.source_prefix is None and model_args.model_name_or_path in [
		"t5-small",
		"t5-base",
		"t5-large",
		"t5-3b",
		"t5-11b",
	]:
		logger.warning(
			"You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with "
			"`--source_prefix 'summarize: ' `"
		)

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
			extension = data_args.train_file.split(".")[-1]
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
			extension = data_args.validation_file.split(".")[-1]
		if data_args.test_file is not None:
			data_files["test"] = data_args.test_file
			extension = data_args.test_file.split(".")[-1]
		raw_datasets = load_dataset(
			extension,
			data_files=data_files,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSeq2SeqLM.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):
		if isinstance(tokenizer, MBartTokenizer):
			model.config.decoder_start_token_id = tokenizer.lang_code_to_id[data_args.lang]
		else:
			model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(data_args.lang)

	if model.config.decoder_start_token_id is None:
		raise ValueError("Make sure that `config.decoder_start_token_id` is correctly defined")

	if (
		hasattr(model.config, "max_position_embeddings")
		and model.config.max_position_embeddings < data_args.max_source_length
	):
		if model_args.resize_position_embeddings is None:
			logger.warning(
				"Increasing the model's number of position embedding vectors from"
				f" {model.config.max_position_embeddings} to {data_args.max_source_length}."
			)
			model.resize_position_embeddings(data_args.max_source_length)
		elif model_args.resize_position_embeddings:
			model.resize_position_embeddings(data_args.max_source_length)
		else:
			raise ValueError(
				f"`--max_source_length` is set to {data_args.max_source_length}, but the model only has"
				f" {model.config.max_position_embeddings} position encodings. Consider either reducing"
				f" `--max_source_length` to {model.config.max_position_embeddings} or to automatically resize the"
				" model's position encodings by passing `--resize_position_embeddings`."
			)

	prefix = data_args.source_prefix if data_args.source_prefix is not None else ""

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		column_names = raw_datasets["train"].column_names
	elif training_args.do_eval:
		if "validation" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		column_names = raw_datasets["validation"].column_names
	elif training_args.do_predict:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		column_names = raw_datasets["test"].column_names
	else:
		logger.info("There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.")
		return

	if isinstance(tokenizer, tuple(MULTILINGUAL_TOKENIZERS)):
		assert (
			data_args.lang is not None
		), f"{tokenizer.__class__.__name__} is a multilingual tokenizer which requires --lang argument"

		tokenizer.src_lang = data_args.lang
		tokenizer.tgt_lang = data_args.lang

		forced_bos_token_id = (
			tokenizer.lang_code_to_id[data_args.forced_bos_token] if data_args.forced_bos_token is not None else None
		)
		model.config.forced_bos_token_id = forced_bos_token_id

	dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)
	if data_args.text_column is None:
		text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
	else:
		text_column = data_args.text_column
		if text_column not in column_names:
			raise ValueError(
				f"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}"
			)
	if data_args.summary_column is None:
		summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
	else:
		summary_column = data_args.summary_column
		if summary_column not in column_names:
			raise ValueError(
				f"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}"
			)

	max_target_length = data_args.max_target_length
	padding = "max_length" if data_args.pad_to_max_length else False

	if training_args.label_smoothing_factor > 0 and not hasattr(model, "prepare_decoder_input_ids_from_labels"):
		logger.warning(
			"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for "
			f"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory"
		)

	def preprocess_function(examples):

		inputs, targets = [], []
		for i in range(len(examples[text_column])):
			if examples[text_column][i] and examples[summary_column][i]:
				inputs.append(examples[text_column][i])
				targets.append(examples[summary_column][i])

		inputs = [prefix + inp for inp in inputs]
		model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)

		labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)

		if padding == "max_length" and data_args.ignore_pad_token_for_loss:
			labels["input_ids"] = [
				[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
			]

		model_inputs["labels"] = labels["input_ids"]
		return model_inputs

	if training_args.do_train:
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on train dataset",
			)

	if training_args.do_eval:
		max_target_length = data_args.val_max_target_length
		eval_dataset = raw_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on validation dataset",
			)

	if training_args.do_predict:
		max_target_length = data_args.val_max_target_length
		predict_dataset = raw_datasets["test"]
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))
		with training_args.main_process_first(desc="prediction dataset map pre-processing"):
			predict_dataset = predict_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)

	label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id
	data_collator = DataCollatorForSeq2Seq(
		tokenizer,
		model=model,
		label_pad_token_id=label_pad_token_id,
		pad_to_multiple_of=8 if training_args.fp16 else None,
	)

	metric = evaluate.load("rouge", cache_dir=model_args.cache_dir)

	def postprocess_text(preds, labels):
		preds = [pred.strip() for pred in preds]
		labels = [label.strip() for label in labels]

		preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
		labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

		return preds, labels

	def compute_metrics(eval_preds):
		preds, labels = eval_preds
		if isinstance(preds, tuple):
			preds = preds[0]
		preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
		decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
		labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
		decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

		decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

		result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
		result = {k: round(v * 100, 4) for k, v in result.items()}
		prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
		result["gen_len"] = np.mean(prediction_lens)
		return result

	training_args.generation_max_length = (
		training_args.generation_max_length
		if training_args.generation_max_length is not None
		else data_args.val_max_target_length
	)
	training_args.generation_num_beams = (
		data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams
	)

	trainer = Seq2SeqTrainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		compute_metrics=compute_metrics if training_args.predict_with_generate else None,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload

		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	results = {}
	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		if isinstance(eval_dataset, dict):
			metrics = {}
			for eval_ds_name, eval_ds in eval_dataset.items():
				dataset_metrics = trainer.evaluate(eval_dataset=eval_ds, metric_key_prefix=f"eval_{eval_ds_name}")
				metrics.update(dataset_metrics)
		else:
			metrics = trainer.evaluate(metric_key_prefix="eval")
		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")

		predict_results = trainer.predict(predict_dataset, metric_key_prefix="predict")
		metrics = predict_results.metrics
		max_predict_samples = (
			data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
		)
		metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))

		trainer.log_metrics("predict", metrics)
		trainer.save_metrics("predict", metrics)

		if trainer.is_world_process_zero():
			if training_args.predict_with_generate:
				predictions = predict_results.predictions
				predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)
				predictions = tokenizer.batch_decode(
					predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True
				)
				predictions = [pred.strip() for pred in predictions]
				output_prediction_file = os.path.join(training_args.output_dir, "generated_predictions.txt")
				with open(output_prediction_file, "w") as writer:
					writer.write("\n".join(predictions))

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "summarization"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if data_args.lang is not None:
		kwargs["language"] = data_args.lang

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)

	return results


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

import functools
from typing import Callable, TypeVar
import weakref

import torch
import torch.nn as nn
from torch.utils.checkpoint import CheckpointFunction


from allennlp.common.registrable import Registrable


class CheckpointWrapper(Registrable):

	default_implementation = "torch"

	def wrap_module(self, module: nn.Module, **kwargs) -> nn.Module:
		raise NotImplementedError


@CheckpointWrapper.register("torch")
class TorchCheckpointWrapper(CheckpointWrapper):
	def wrap_module(self, module: nn.Module, **kwargs) -> nn.Module:

		assert len(kwargs) == 0  # This way of wrapping only works for positional arguments.

		module.forward = functools.partial(  # type: ignore[assignment]
			_checkpointed_forward, type(module).forward, weakref.ref(module)
		)
		return module


_T = TypeVar("_T")


def _checkpointed_forward(
	original_forward: Callable[..., _T],
	weak_self,
	*args,
	**kwargs,
) -> _T:
	module = weak_self()
	assert (
		module is not None
	), "patched forward method called after module has been garbage collected!"

	if not module.training:
		return original_forward(module, *args, **kwargs)


	def run_function(dummy_tensor, *forward_args, **forward_kwargs):
		return original_forward(module, *forward_args, **forward_kwargs)

	dummy_tensor = torch.tensor([], requires_grad=True)
	return CheckpointFunction.apply(run_function, True, dummy_tensor, *args, **kwargs)

from typing import Optional


import torch
from torch import nn

from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder
from allennlp.nn.util import add_positional_features


@Seq2SeqEncoder.register("pytorch_transformer")
class PytorchTransformer(Seq2SeqEncoder):

	def __init__(
		self,
		input_dim: int,
		num_layers: int,
		feedforward_hidden_dim: int = 2048,
		num_attention_heads: int = 8,
		positional_encoding: Optional[str] = None,
		positional_embedding_size: int = 512,
		dropout_prob: float = 0.1,
		activation: str = "relu",
	) -> None:
		super().__init__()

		layer = nn.TransformerEncoderLayer(
			d_model=input_dim,
			nhead=num_attention_heads,
			dim_feedforward=feedforward_hidden_dim,
			dropout=dropout_prob,
			activation=activation,
		)
		self._transformer = nn.TransformerEncoder(layer, num_layers)
		self._input_dim = input_dim

		for p in self.parameters():
			if p.dim() > 1:
				nn.init.xavier_uniform_(p)

		if positional_encoding is None:
			self._sinusoidal_positional_encoding = False
			self._positional_embedding = None
		elif positional_encoding == "sinusoidal":
			self._sinusoidal_positional_encoding = True
			self._positional_embedding = None
		elif positional_encoding == "embedding":
			self._sinusoidal_positional_encoding = False
			self._positional_embedding = nn.Embedding(positional_embedding_size, input_dim)
		else:
			raise ValueError(
				"positional_encoding must be one of None, 'sinusoidal', or 'embedding'"
			)

	def get_input_dim(self) -> int:
		return self._input_dim

	def get_output_dim(self) -> int:
		return self._input_dim

	def is_bidirectional(self):
		return False

	def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor):
		output = inputs
		if self._sinusoidal_positional_encoding:
			output = add_positional_features(output)
		if self._positional_embedding is not None:
			position_ids = torch.arange(inputs.size(1), dtype=torch.long, device=output.device)
			position_ids = position_ids.unsqueeze(0).expand(inputs.shape[:-1])
			output = output + self._positional_embedding(position_ids)

		output = output.permute(1, 0, 2)
		mask = ~mask
		output = self._transformer(output, src_key_padding_mask=mask)
		output = output.permute(1, 0, 2)

		return output



import functools
import json
import logging
import os
import re
import sys
import warnings
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union

import datasets
import evaluate
import numpy as np
import torch
from datasets import DatasetDict, load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoFeatureExtractor,
	AutoModelForCTC,
	AutoProcessor,
	AutoTokenizer,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	Wav2Vec2Processor,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.18.0", "To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt")


logger = logging.getLogger(__name__)


def list_field(default=None, metadata=None):
	return field(default_factory=lambda: default, metadata=metadata)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	tokenizer_name_or_path: Optional[str] = field(
		default=None,
		metadata={"help": "Path to pretrained tokenizer or tokenizer identifier from huggingface.co/models"},
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	freeze_feature_encoder: bool = field(
		default=True, metadata={"help": "Whether to freeze the feature encoder layers of the model."}
	)
	attention_dropout: float = field(
		default=0.0, metadata={"help": "The dropout ratio for the attention probabilities."}
	)
	activation_dropout: float = field(
		default=0.0, metadata={"help": "The dropout ratio for activations inside the fully connected layer."}
	)
	feat_proj_dropout: float = field(default=0.0, metadata={"help": "The dropout ratio for the projected features."})
	hidden_dropout: float = field(
		default=0.0,
		metadata={
			"help": "The dropout probability for all fully connected layers in the embeddings, encoder, and pooler."
		},
	)
	final_dropout: float = field(
		default=0.0,
		metadata={"help": "The dropout probability for the final projection layer."},
	)
	mask_time_prob: float = field(
		default=0.05,
		metadata={
			"help": (
				"Probability of each feature vector along the time axis to be chosen as the start of the vector "
				"span to be masked. Approximately ``mask_time_prob * sequence_length // mask_time_length`` feature "
				"vectors will be masked along the time axis."
			)
		},
	)
	mask_time_length: int = field(
		default=10,
		metadata={"help": "Length of vector span to mask along the time axis."},
	)
	mask_feature_prob: float = field(
		default=0.0,
		metadata={
			"help": (
				"Probability of each feature vector along the feature axis to be chosen as the start of the vectorspan"
				" to be masked. Approximately ``mask_feature_prob * sequence_length // mask_feature_length`` feature"
				" bins will be masked along the time axis."
			)
		},
	)
	mask_feature_length: int = field(
		default=10,
		metadata={"help": "Length of vector span to mask along the feature axis."},
	)
	layerdrop: float = field(default=0.0, metadata={"help": "The LayerDrop probability."})
	ctc_loss_reduction: Optional[str] = field(
		default="mean", metadata={"help": "The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'."}
	)
	ctc_zero_infinity: Optional[bool] = field(
		default=False,
		metadata={
			"help": "Whether to zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite losses mainly"
			" occur when the inputs are too short to be aligned to the targets."
		},
	)
	add_adapter: Optional[bool] = field(
		default=False,
		metadata={
			"help": "Whether a convolutional attention network should be stacked on top of the Wav2Vec2Bert Encoder. Can be very"
			"useful to downsample the output length."
		},
	)


@dataclass
class DataTrainingArguments:

	dataset_name: str = field(
		metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: str = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_split_name: str = field(
		default="train+validation",
		metadata={
			"help": (
				"The name of the training data set split to use (via the datasets library). Defaults to "
				"'train+validation'"
			)
		},
	)
	eval_split_name: str = field(
		default="test",
		metadata={
			"help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'test'"
		},
	)
	audio_column_name: str = field(
		default="audio",
		metadata={"help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
	)
	text_column_name: str = field(
		default="text",
		metadata={"help": "The name of the dataset column containing the text data. Defaults to 'text'"},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of validation examples to this "
				"value if set."
			)
		},
	)
	chars_to_ignore: Optional[List[str]] = list_field(
		default=None,
		metadata={"help": "A list of characters to remove from the transcripts."},
	)
	eval_metrics: List[str] = list_field(
		default=["wer"],
		metadata={"help": "A list of metrics the model should be evaluated on. E.g. `'wer cer'`"},
	)
	max_duration_in_seconds: float = field(
		default=20.0,
		metadata={
			"help": (
				"Filter audio files that are longer than `max_duration_in_seconds` seconds to"
				" 'max_duration_in_seconds`"
			)
		},
	)
	min_duration_in_seconds: float = field(
		default=0.0, metadata={"help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"}
	)
	preprocessing_only: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to only do data preprocessing and skip training. This is especially useful when data"
				" preprocessing errors out in distributed training due to timeout. In this case, one should run the"
				" preprocessing in a non-distributed setup with `preprocessing_only=True` so that the cached datasets"
				" can consequently be loaded in distributed training"
			)
		},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	unk_token: str = field(
		default="[UNK]",
		metadata={"help": "The unk token for the tokenizer"},
	)
	pad_token: str = field(
		default="[PAD]",
		metadata={"help": "The padding token for the tokenizer"},
	)
	word_delimiter_token: str = field(
		default="|",
		metadata={"help": "The word delimiter token for the tokenizer"},
	)
	phoneme_language: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The target language that should be used be"
				" passed to the tokenizer for tokenization. Note that"
				" this is only relevant if the model classifies the"
				" input audio to a sequence of phoneme sequences."
			)
		},
	)


@dataclass
class DataCollatorCTCWithPadding:

	processor: AutoProcessor
	padding: Union[bool, str] = "longest"
	pad_to_multiple_of: Optional[int] = None
	pad_to_multiple_of_labels: Optional[int] = None
	feature_extractor_input_name: Optional[str] = "input_values"

	def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
		input_features = [
			{self.feature_extractor_input_name: feature[self.feature_extractor_input_name]} for feature in features
		]
		label_features = [{"input_ids": feature["labels"]} for feature in features]

		batch = self.processor.pad(
			input_features,
			padding=self.padding,
			pad_to_multiple_of=self.pad_to_multiple_of,
			return_tensors="pt",
		)

		labels_batch = self.processor.pad(
			labels=label_features,
			padding=self.padding,
			pad_to_multiple_of=self.pad_to_multiple_of_labels,
			return_tensors="pt",
		)

		labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

		batch["labels"] = labels
		if "attention_mask" in batch:
			batch["attention_mask"] = batch["attention_mask"].to(torch.long)

		return batch


def create_vocabulary_from_data(
	datasets: DatasetDict,
	word_delimiter_token: Optional[str] = None,
	unk_token: Optional[str] = None,
	pad_token: Optional[str] = None,
):
	def extract_all_chars(batch):
		all_text = " ".join(batch["target_text"])
		vocab = list(set(all_text))
		return {"vocab": [vocab], "all_text": [all_text]}

	vocabs = datasets.map(
		extract_all_chars,
		batched=True,
		batch_size=-1,
		keep_in_memory=True,
		remove_columns=datasets["train"].column_names,
	)

	vocab_set = functools.reduce(
		lambda vocab_1, vocab_2: set(vocab_1["vocab"][0]) | set(vocab_2["vocab"][0]), vocabs.values()
	)

	vocab_dict = {v: k for k, v in enumerate(sorted(vocab_set))}

	if word_delimiter_token is not None:
		vocab_dict[word_delimiter_token] = vocab_dict[" "]
		del vocab_dict[" "]

	if unk_token is not None:
		vocab_dict[unk_token] = len(vocab_dict)

	if pad_token is not None:
		vocab_dict[pad_token] = len(vocab_dict)

	return vocab_dict


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if data_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if data_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		data_args.token = data_args.use_auth_token

	send_example_telemetry("run_speech_recognition_ctc", model_args, data_args)

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)
	logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	if is_main_process(training_args.local_rank):
		transformers.utils.logging.set_verbosity_info()
	logger.info("Training/evaluation parameters %s", training_args)

	set_seed(training_args.seed)

	raw_datasets = DatasetDict()

	if training_args.do_train:
		raw_datasets["train"] = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			split=data_args.train_split_name,
			token=data_args.token,
		)

		if data_args.audio_column_name not in raw_datasets["train"].column_names:
			raise ValueError(
				f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'."
				" Make sure to set `--audio_column_name` to the correct audio column - one of"
				f" {', '.join(raw_datasets['train'].column_names)}."
			)

		if data_args.text_column_name not in raw_datasets["train"].column_names:
			raise ValueError(
				f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
				"Make sure to set `--text_column_name` to the correct text column - one of "
				f"{', '.join(raw_datasets['train'].column_names)}."
			)

		if data_args.max_train_samples is not None:
			raw_datasets["train"] = raw_datasets["train"].select(range(data_args.max_train_samples))

	if training_args.do_eval:
		raw_datasets["eval"] = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			split=data_args.eval_split_name,
			token=data_args.token,
		)

		if data_args.max_eval_samples is not None:
			raw_datasets["eval"] = raw_datasets["eval"].select(range(data_args.max_eval_samples))

	chars_to_ignore_regex = (
		f'[{"".join(data_args.chars_to_ignore)}]' if data_args.chars_to_ignore is not None else None
	)
	text_column_name = data_args.text_column_name

	def remove_special_characters(batch):
		if chars_to_ignore_regex is not None:
			batch["target_text"] = re.sub(chars_to_ignore_regex, "", batch[text_column_name]).lower() + " "
		else:
			batch["target_text"] = batch[text_column_name].lower() + " "
		return batch

	with training_args.main_process_first(desc="dataset map special characters removal"):
		raw_datasets = raw_datasets.map(
			remove_special_characters,
			remove_columns=[text_column_name],
			desc="remove special characters from datasets",
		)

	word_delimiter_token = data_args.word_delimiter_token
	unk_token = data_args.unk_token
	pad_token = data_args.pad_token

	config = AutoConfig.from_pretrained(
		model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
	)

	tokenizer_name_or_path = model_args.tokenizer_name_or_path
	tokenizer_kwargs = {}
	if tokenizer_name_or_path is None:
		tokenizer_name_or_path = training_args.output_dir

		vocab_file = os.path.join(tokenizer_name_or_path, "vocab.json")

		with training_args.main_process_first():
			if training_args.overwrite_output_dir and os.path.isfile(vocab_file):
				try:
					os.remove(vocab_file)
				except OSError:
					pass

		with training_args.main_process_first(desc="dataset map vocabulary creation"):
			if not os.path.isfile(vocab_file):
				os.makedirs(tokenizer_name_or_path, exist_ok=True)
				vocab_dict = create_vocabulary_from_data(
					raw_datasets,
					word_delimiter_token=word_delimiter_token,
					unk_token=unk_token,
					pad_token=pad_token,
				)

				with open(vocab_file, "w") as file:
					json.dump(vocab_dict, file)

		tokenizer_kwargs = {
			"config": config if config.tokenizer_class is not None else None,
			"tokenizer_type": config.model_type if config.tokenizer_class is None else None,
			"unk_token": unk_token,
			"pad_token": pad_token,
			"word_delimiter_token": word_delimiter_token,
		}


	tokenizer = AutoTokenizer.from_pretrained(
		tokenizer_name_or_path,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
		**tokenizer_kwargs,
	)
	feature_extractor = AutoFeatureExtractor.from_pretrained(
		model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
	)

	config.update(
		{
			"feat_proj_dropout": model_args.feat_proj_dropout,
			"attention_dropout": model_args.attention_dropout,
			"hidden_dropout": model_args.hidden_dropout,
			"final_dropout": model_args.final_dropout,
			"mask_time_prob": model_args.mask_time_prob,
			"mask_time_length": model_args.mask_time_length,
			"mask_feature_prob": model_args.mask_feature_prob,
			"mask_feature_length": model_args.mask_feature_length,
			"gradient_checkpointing": training_args.gradient_checkpointing,
			"layerdrop": model_args.layerdrop,
			"ctc_loss_reduction": model_args.ctc_loss_reduction,
			"ctc_zero_infinity": model_args.ctc_zero_infinity,
			"pad_token_id": tokenizer.pad_token_id,
			"vocab_size": len(tokenizer),
			"activation_dropout": model_args.activation_dropout,
			"add_adapter": model_args.add_adapter,
		}
	)

	model = AutoModelForCTC.from_pretrained(
		model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		config=config,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
	)

	if model_args.freeze_feature_encoder:
		model.freeze_feature_encoder()


	dataset_sampling_rate = next(iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
	if dataset_sampling_rate != feature_extractor.sampling_rate:
		raw_datasets = raw_datasets.cast_column(
			data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)
		)

	max_input_length = data_args.max_duration_in_seconds * feature_extractor.sampling_rate
	min_input_length = data_args.min_duration_in_seconds * feature_extractor.sampling_rate
	audio_column_name = data_args.audio_column_name
	num_workers = data_args.preprocessing_num_workers
	feature_extractor_input_name = feature_extractor.model_input_names[0]

	phoneme_language = data_args.phoneme_language

	def prepare_dataset(batch):
		sample = batch[audio_column_name]

		inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])
		batch[feature_extractor_input_name] = getattr(inputs, feature_extractor_input_name)[0]
		batch["input_length"] = len(sample["array"].squeeze())

		additional_kwargs = {}
		if phoneme_language is not None:
			additional_kwargs["phonemizer_lang"] = phoneme_language

		batch["labels"] = tokenizer(batch["target_text"], **additional_kwargs).input_ids
		return batch

	with training_args.main_process_first(desc="dataset map preprocessing"):
		vectorized_datasets = raw_datasets.map(
			prepare_dataset,
			remove_columns=next(iter(raw_datasets.values())).column_names,
			num_proc=num_workers,
			desc="preprocess datasets",
		)

		def is_audio_in_length_range(length):
			return length > min_input_length and length < max_input_length

		vectorized_datasets = vectorized_datasets.filter(
			is_audio_in_length_range,
			num_proc=num_workers,
			input_columns=["input_length"],
		)


	eval_metrics = {metric: evaluate.load(metric, cache_dir=model_args.cache_dir) for metric in data_args.eval_metrics}

	if data_args.preprocessing_only:
		logger.info(f"Data preprocessing finished. Files cached at {vectorized_datasets.cache_files}")
		return

	def compute_metrics(pred):
		pred_logits = pred.predictions
		pred_ids = np.argmax(pred_logits, axis=-1)

		pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id

		pred_str = tokenizer.batch_decode(pred_ids)
		label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)

		metrics = {k: v.compute(predictions=pred_str, references=label_str) for k, v in eval_metrics.items()}

		return metrics

	with training_args.main_process_first():
		if is_main_process(training_args.local_rank):
			feature_extractor.save_pretrained(training_args.output_dir)
			tokenizer.save_pretrained(training_args.output_dir)
			config.save_pretrained(training_args.output_dir)

	try:
		processor = AutoProcessor.from_pretrained(training_args.output_dir)
	except (OSError, KeyError):
		warnings.warn(
			"Loading a processor from a feature extractor config that does not"
			" include a `processor_class` attribute is deprecated and will be removed in v5. Please add the following "
			" attribute to your `preprocessor_config.json` file to suppress this warning: "
			" `'processor_class': 'Wav2Vec2Processor'`",
			FutureWarning,
		)
		processor = Wav2Vec2Processor.from_pretrained(training_args.output_dir)

	data_collator = DataCollatorCTCWithPadding(
		processor=processor, feature_extractor_input_name=feature_extractor_input_name
	)

	trainer = Trainer(
		model=model,
		data_collator=data_collator,
		args=training_args,
		compute_metrics=compute_metrics,
		train_dataset=vectorized_datasets["train"] if training_args.do_train else None,
		eval_dataset=vectorized_datasets["eval"] if training_args.do_eval else None,
		tokenizer=processor,
	)


	if training_args.do_train:
		if last_checkpoint is not None:
			checkpoint = last_checkpoint
		elif os.path.isdir(model_args.model_name_or_path):
			checkpoint = model_args.model_name_or_path
		else:
			checkpoint = None

		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()

		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples
			if data_args.max_train_samples is not None
			else len(vectorized_datasets["train"])
		)
		metrics["train_samples"] = min(max_train_samples, len(vectorized_datasets["train"]))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	results = {}
	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate()
		max_eval_samples = (
			data_args.max_eval_samples if data_args.max_eval_samples is not None else len(vectorized_datasets["eval"])
		)
		metrics["eval_samples"] = min(max_eval_samples, len(vectorized_datasets["eval"]))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	config_name = data_args.dataset_config_name if data_args.dataset_config_name is not None else "na"
	kwargs = {
		"finetuned_from": model_args.model_name_or_path,
		"tasks": "automatic-speech-recognition",
		"tags": ["automatic-speech-recognition", data_args.dataset_name],
		"dataset_args": (
			f"Config: {config_name}, Training split: {data_args.train_split_name}, Eval split:"
			f" {data_args.eval_split_name}"
		),
		"dataset": f"{data_args.dataset_name.upper()} - {config_name.upper()}",
	}
	if "common_voice" in data_args.dataset_name:
		kwargs["language"] = config_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)

	return results


if __name__ == "__main__":
	main()

from typing import List, Optional
import logging

from allennlp.common import Registrable
from allennlp.data.tokenizers.token_class import Token


logger = logging.getLogger(__name__)


class Tokenizer(Registrable):

	default_implementation = "spacy"

	def batch_tokenize(self, texts: List[str]) -> List[List[Token]]:
		return [self.tokenize(text) for text in texts]

	def tokenize(self, text: str) -> List[Token]:
		raise NotImplementedError

	def add_special_tokens(
		self, tokens1: List[Token], tokens2: Optional[List[Token]] = None
	) -> List[Token]:
		return tokens1 + (tokens2 or [])

	def num_special_tokens_for_sequence(self) -> int:
		return 0

	def num_special_tokens_for_pair(self) -> int:
		return 0

import torch
from torch.utils.data import Dataset

class MyTrainDataset(Dataset):
	def __init__(self, size):
		self.size = size
		self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)]

	def __len__(self):
		return self.size
	
	def __getitem__(self, index):
		return self.data[index]
from .checkpoint_handler import (
	load_model_checkpoint,
	save_model_checkpoint,
	save_distributed_model_checkpoint,
	load_distributed_model_checkpoint,
	load_optimizer_checkpoint,
	save_optimizer_checkpoint,
	save_model_and_optimizer_sharded,
	load_model_sharded,
)


from typing import Tuple, List

import torch
import torch.nn as nn
import torch.nn.functional as F

from allennlp.common.checks import ConfigurationError
from allennlp.common.registrable import FromParams
from allennlp.nn.util import (
	get_lengths_from_binary_sequence_mask,
	masked_max,
	masked_mean,
	masked_softmax,
	tiny_value_of_dtype,
)


def multi_perspective_match(
	vector1: torch.Tensor, vector2: torch.Tensor, weight: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
	assert vector1.size(0) == vector2.size(0)
	assert weight.size(1) == vector1.size(2) == vector1.size(2)

	similarity_single = F.cosine_similarity(vector1, vector2, 2).unsqueeze(2)

	weight = weight.unsqueeze(0).unsqueeze(0)

	vector1 = weight * vector1.unsqueeze(2)
	vector2 = weight * vector2.unsqueeze(2)

	similarity_multi = F.cosine_similarity(vector1, vector2, dim=3)

	return similarity_single, similarity_multi


def multi_perspective_match_pairwise(
	vector1: torch.Tensor, vector2: torch.Tensor, weight: torch.Tensor
) -> torch.Tensor:
	num_perspectives = weight.size(0)

	weight = weight.unsqueeze(0).unsqueeze(2)

	vector1 = weight * vector1.unsqueeze(1).expand(-1, num_perspectives, -1, -1)
	vector2 = weight * vector2.unsqueeze(1).expand(-1, num_perspectives, -1, -1)

	vector1_norm = vector1.norm(p=2, dim=3, keepdim=True)
	vector2_norm = vector2.norm(p=2, dim=3, keepdim=True)

	mul_result = torch.matmul(vector1, vector2.transpose(2, 3))
	norm_value = vector1_norm * vector2_norm.transpose(2, 3)

	return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(
		0, 2, 3, 1
	)


class BiMpmMatching(nn.Module, FromParams):

	def __init__(
		self,
		hidden_dim: int = 100,
		num_perspectives: int = 20,
		share_weights_between_directions: bool = True,
		is_forward: bool = None,
		with_full_match: bool = True,
		with_maxpool_match: bool = True,
		with_attentive_match: bool = True,
		with_max_attentive_match: bool = True,
	) -> None:
		super().__init__()

		self.hidden_dim = hidden_dim
		self.num_perspectives = num_perspectives
		self.is_forward = is_forward

		self.with_full_match = with_full_match
		self.with_maxpool_match = with_maxpool_match
		self.with_attentive_match = with_attentive_match
		self.with_max_attentive_match = with_max_attentive_match

		if not (
			with_full_match
			or with_maxpool_match
			or with_attentive_match
			or with_max_attentive_match
		):
			raise ConfigurationError("At least one of the matching method should be enabled")

		def create_parameter():  # utility function to create and initialize a parameter
			param = nn.Parameter(torch.zeros(num_perspectives, hidden_dim))
			torch.nn.init.kaiming_normal_(param)
			return param

		def share_or_create(weights_to_share):  # utility function to create or share the weights
			return weights_to_share if share_weights_between_directions else create_parameter()

		output_dim = (
			2  # used to calculate total output dimension, 2 is for cosine max and cosine min
		)
		if with_full_match:
			if is_forward is None:
				raise ConfigurationError("Must specify is_forward to enable full matching")
			self.full_match_weights = create_parameter()
			self.full_match_weights_reversed = share_or_create(self.full_match_weights)
			output_dim += num_perspectives + 1

		if with_maxpool_match:
			self.maxpool_match_weights = create_parameter()
			output_dim += num_perspectives * 2

		if with_attentive_match:
			self.attentive_match_weights = create_parameter()
			self.attentive_match_weights_reversed = share_or_create(self.attentive_match_weights)
			output_dim += num_perspectives + 1

		if with_max_attentive_match:
			self.max_attentive_match_weights = create_parameter()
			self.max_attentive_match_weights_reversed = share_or_create(
				self.max_attentive_match_weights
			)
			output_dim += num_perspectives + 1

		self.output_dim = output_dim

	def get_output_dim(self) -> int:
		return self.output_dim

	def forward(
		self,
		context_1: torch.Tensor,
		mask_1: torch.BoolTensor,
		context_2: torch.Tensor,
		mask_2: torch.BoolTensor,
	) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:

		assert (not mask_2.requires_grad) and (not mask_1.requires_grad)
		assert context_1.size(-1) == context_2.size(-1) == self.hidden_dim

		len_1 = get_lengths_from_binary_sequence_mask(mask_1)
		len_2 = get_lengths_from_binary_sequence_mask(mask_2)

		context_1 = context_1 * mask_1.unsqueeze(-1)
		context_2 = context_2 * mask_2.unsqueeze(-1)

		matching_vector_1: List[torch.Tensor] = []
		matching_vector_2: List[torch.Tensor] = []


		cosine_sim = F.cosine_similarity(context_1.unsqueeze(-2), context_2.unsqueeze(-3), dim=3)

		cosine_max_1 = masked_max(cosine_sim, mask_2.unsqueeze(-2), dim=2, keepdim=True)
		cosine_mean_1 = masked_mean(cosine_sim, mask_2.unsqueeze(-2), dim=2, keepdim=True)
		cosine_max_2 = masked_max(
			cosine_sim.permute(0, 2, 1), mask_1.unsqueeze(-2), dim=2, keepdim=True
		)
		cosine_mean_2 = masked_mean(
			cosine_sim.permute(0, 2, 1), mask_1.unsqueeze(-2), dim=2, keepdim=True
		)

		matching_vector_1.extend([cosine_max_1, cosine_mean_1])
		matching_vector_2.extend([cosine_max_2, cosine_mean_2])

		if self.with_full_match:

			if self.is_forward:
				last_position_1 = (len_1 - 1).clamp(min=0)
				last_position_1 = last_position_1.view(-1, 1, 1).expand(-1, 1, self.hidden_dim)
				last_position_2 = (len_2 - 1).clamp(min=0)
				last_position_2 = last_position_2.view(-1, 1, 1).expand(-1, 1, self.hidden_dim)

				context_1_last = context_1.gather(1, last_position_1)
				context_2_last = context_2.gather(1, last_position_2)
			else:
				context_1_last = context_1[:, 0:1, :]
				context_2_last = context_2[:, 0:1, :]

			matching_vector_1_full = multi_perspective_match(
				context_1, context_2_last, self.full_match_weights
			)
			matching_vector_2_full = multi_perspective_match(
				context_2, context_1_last, self.full_match_weights_reversed
			)

			matching_vector_1.extend(matching_vector_1_full)
			matching_vector_2.extend(matching_vector_2_full)

		if self.with_maxpool_match:
			matching_vector_max = multi_perspective_match_pairwise(
				context_1, context_2, self.maxpool_match_weights
			)

			matching_vector_1_max = masked_max(
				matching_vector_max, mask_2.unsqueeze(-2).unsqueeze(-1), dim=2
			)
			matching_vector_1_mean = masked_mean(
				matching_vector_max, mask_2.unsqueeze(-2).unsqueeze(-1), dim=2
			)
			matching_vector_2_max = masked_max(
				matching_vector_max.permute(0, 2, 1, 3), mask_1.unsqueeze(-2).unsqueeze(-1), dim=2
			)
			matching_vector_2_mean = masked_mean(
				matching_vector_max.permute(0, 2, 1, 3), mask_1.unsqueeze(-2).unsqueeze(-1), dim=2
			)

			matching_vector_1.extend([matching_vector_1_max, matching_vector_1_mean])
			matching_vector_2.extend([matching_vector_2_max, matching_vector_2_mean])


		att_2 = context_2.unsqueeze(-3) * cosine_sim.unsqueeze(-1)

		att_1 = context_1.unsqueeze(-2) * cosine_sim.unsqueeze(-1)

		if self.with_attentive_match:
			att_mean_2 = masked_softmax(att_2.sum(dim=2), mask_1.unsqueeze(-1))
			att_mean_1 = masked_softmax(att_1.sum(dim=1), mask_2.unsqueeze(-1))

			matching_vector_1_att_mean = multi_perspective_match(
				context_1, att_mean_2, self.attentive_match_weights
			)
			matching_vector_2_att_mean = multi_perspective_match(
				context_2, att_mean_1, self.attentive_match_weights_reversed
			)
			matching_vector_1.extend(matching_vector_1_att_mean)
			matching_vector_2.extend(matching_vector_2_att_mean)

		if self.with_max_attentive_match:
			att_max_2 = masked_max(att_2, mask_2.unsqueeze(-2).unsqueeze(-1), dim=2)
			att_max_1 = masked_max(
				att_1.permute(0, 2, 1, 3), mask_1.unsqueeze(-2).unsqueeze(-1), dim=2
			)

			matching_vector_1_att_max = multi_perspective_match(
				context_1, att_max_2, self.max_attentive_match_weights
			)
			matching_vector_2_att_max = multi_perspective_match(
				context_2, att_max_1, self.max_attentive_match_weights_reversed
			)

			matching_vector_1.extend(matching_vector_1_att_max)
			matching_vector_2.extend(matching_vector_2_att_max)

		return matching_vector_1, matching_vector_2

import math

from typing import List
import numpy
import torch

from allennlp.common.util import JsonDict, sanitize
from allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter
from allennlp.nn import util


@SaliencyInterpreter.register("simple-gradient")
class SimpleGradient(SaliencyInterpreter):

	def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:
		labeled_instances = self.predictor.json_to_labeled_instances(inputs)

		instances_with_grads = dict()
		for idx, instance in enumerate(labeled_instances):
			embeddings_list: List[torch.Tensor] = []
			token_offsets: List[torch.Tensor] = []

			handles = self._register_hooks(embeddings_list, token_offsets)
			try:
				grads = self.predictor.get_gradients([instance])[0]
			finally:
				for handle in handles:
					handle.remove()

			embeddings_list.reverse()
			token_offsets.reverse()
			embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)

			for key, grad in grads.items():
				input_idx = int(key[-1]) - 1
				emb_grad = numpy.sum(grad[0] * embeddings_list[input_idx][0], axis=1)
				norm = numpy.linalg.norm(emb_grad, ord=1)
				normalized_grad = [math.fabs(e) / norm for e in emb_grad]
				grads[key] = normalized_grad

			instances_with_grads["instance_" + str(idx + 1)] = grads
		return sanitize(instances_with_grads)

	def _register_hooks(self, embeddings_list: List, token_offsets: List):

		def forward_hook(module, inputs, output):
			embeddings_list.append(output.squeeze(0).clone().detach())

		def get_token_offsets(module, inputs, outputs):
			offsets = util.get_token_offsets_from_text_field_inputs(inputs)
			if offsets is not None:
				token_offsets.append(offsets)

		handles = []
		embedding_layer = self.predictor.get_interpretable_layer()
		handles.append(embedding_layer.register_forward_hook(forward_hook))
		text_field_embedder = self.predictor.get_interpretable_text_field_embedder()
		handles.append(text_field_embedder.register_forward_hook(get_token_offsets))
		return handles


from allennlp.fairness.fairness_metrics import Independence, Separation, Sufficiency
from allennlp.fairness.bias_metrics import (
	WordEmbeddingAssociationTest,
	EmbeddingCoherenceTest,
	NaturalLanguageInference,
	AssociationWithoutGroundTruth,
)
from allennlp.fairness.bias_direction import (
	PCABiasDirection,
	PairedPCABiasDirection,
	ClassificationNormalBiasDirection,
	TwoMeansBiasDirection,
)
from allennlp.fairness.bias_mitigators import (
	LinearBiasMitigator,
	HardBiasMitigator,
	INLPBiasMitigator,
	OSCaRBiasMitigator,
)
from allennlp.fairness.bias_utils import load_words, load_word_pairs
from allennlp.fairness.bias_mitigator_applicator import BiasMitigatorApplicator
from allennlp.fairness.bias_mitigator_wrappers import (
	HardBiasMitigatorWrapper,
	LinearBiasMitigatorWrapper,
	INLPBiasMitigatorWrapper,
	OSCaRBiasMitigatorWrapper,
)
from allennlp.fairness.bias_direction_wrappers import (
	PCABiasDirectionWrapper,
	PairedPCABiasDirectionWrapper,
	TwoMeansBiasDirectionWrapper,
	ClassificationNormalBiasDirectionWrapper,
)
from allennlp.fairness.adversarial_bias_mitigator import (
	AdversarialBiasMitigator,
	FeedForwardRegressionAdversary,
	AdversarialBiasMitigatorBackwardCallback,
)


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from random import randint
from typing import Optional

import datasets
import evaluate
import numpy as np
from datasets import DatasetDict, load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoFeatureExtractor,
	AutoModelForAudioClassification,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


logger = logging.getLogger(__name__)

check_min_version("4.38.0.dev0")

require_version("datasets>=1.14.0", "To fix: pip install -r examples/pytorch/audio-classification/requirements.txt")


def random_subsample(wav: np.ndarray, max_length: float, sample_rate: int = 16000):
	sample_length = int(round(sample_rate * max_length))
	if len(wav) <= sample_length:
		return wav
	random_offset = randint(0, len(wav) - sample_length - 1)
	return wav[random_offset : random_offset + sample_length]


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(default=None, metadata={"help": "Name of a dataset from the datasets package"})
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(
		default=None, metadata={"help": "A file containing the training audio paths and labels."}
	)
	eval_file: Optional[str] = field(
		default=None, metadata={"help": "A file containing the validation audio paths and labels."}
	)
	train_split_name: str = field(
		default="train",
		metadata={
			"help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
		},
	)
	eval_split_name: str = field(
		default="validation",
		metadata={
			"help": (
				"The name of the training data set split to use (via the datasets library). Defaults to 'validation'"
			)
		},
	)
	audio_column_name: str = field(
		default="audio",
		metadata={"help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
	)
	label_column_name: str = field(
		default="label", metadata={"help": "The name of the dataset column containing the labels. Defaults to 'label'"}
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_length_seconds: float = field(
		default=20,
		metadata={"help": "Audio clips will be randomly cut to this length during training if the value is set."},
	)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		default="facebook/wav2vec2-base",
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from the Hub"}
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	feature_extractor_name: Optional[str] = field(
		default=None, metadata={"help": "Name or path of preprocessor config."}
	)
	freeze_feature_encoder: bool = field(
		default=True, metadata={"help": "Whether to freeze the feature encoder layers of the model."}
	)
	attention_mask: bool = field(
		default=True, metadata={"help": "Whether to generate an attention mask in the feature extractor."}
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	freeze_feature_extractor: Optional[bool] = field(
		default=None, metadata={"help": "Whether to freeze the feature extractor layers of the model."}
	)
	ignore_mismatched_sizes: bool = field(
		default=False,
		metadata={"help": "Will enable to load a pretrained model whose head dimensions are different."},
	)

	def __post_init__(self):
		if not self.freeze_feature_extractor and self.freeze_feature_encoder:
			warnings.warn(
				"The argument `--freeze_feature_extractor` is deprecated and "
				"will be removed in a future version. Use `--freeze_feature_encoder` "
				"instead. Setting `freeze_feature_encoder==True`.",
				FutureWarning,
			)
		if self.freeze_feature_extractor and not self.freeze_feature_encoder:
			raise ValueError(
				"The argument `--freeze_feature_extractor` is deprecated and "
				"should not be used in combination with `--freeze_feature_encoder`. "
				"Only make use of `--freeze_feature_encoder`."
			)


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_audio_classification", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	set_seed(training_args.seed)

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to train from scratch."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	raw_datasets = DatasetDict()
	raw_datasets["train"] = load_dataset(
		data_args.dataset_name,
		data_args.dataset_config_name,
		split=data_args.train_split_name,
		token=model_args.token,
	)
	raw_datasets["eval"] = load_dataset(
		data_args.dataset_name,
		data_args.dataset_config_name,
		split=data_args.eval_split_name,
		token=model_args.token,
	)

	if data_args.audio_column_name not in raw_datasets["train"].column_names:
		raise ValueError(
			f"--audio_column_name {data_args.audio_column_name} not found in dataset '{data_args.dataset_name}'. "
			"Make sure to set `--audio_column_name` to the correct audio column - one of "
			f"{', '.join(raw_datasets['train'].column_names)}."
		)

	if data_args.label_column_name not in raw_datasets["train"].column_names:
		raise ValueError(
			f"--label_column_name {data_args.label_column_name} not found in dataset '{data_args.dataset_name}'. "
			"Make sure to set `--label_column_name` to the correct text column - one of "
			f"{', '.join(raw_datasets['train'].column_names)}."
		)

	feature_extractor = AutoFeatureExtractor.from_pretrained(
		model_args.feature_extractor_name or model_args.model_name_or_path,
		return_attention_mask=model_args.attention_mask,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	raw_datasets = raw_datasets.cast_column(
		data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)
	)

	model_input_name = feature_extractor.model_input_names[0]

	def train_transforms(batch):
		subsampled_wavs = []
		for audio in batch[data_args.audio_column_name]:
			wav = random_subsample(
				audio["array"], max_length=data_args.max_length_seconds, sample_rate=feature_extractor.sampling_rate
			)
			subsampled_wavs.append(wav)
		inputs = feature_extractor(subsampled_wavs, sampling_rate=feature_extractor.sampling_rate)
		output_batch = {model_input_name: inputs.get(model_input_name)}
		output_batch["labels"] = list(batch[data_args.label_column_name])

		return output_batch

	def val_transforms(batch):
		wavs = [audio["array"] for audio in batch[data_args.audio_column_name]]
		inputs = feature_extractor(wavs, sampling_rate=feature_extractor.sampling_rate)
		output_batch = {model_input_name: inputs.get(model_input_name)}
		output_batch["labels"] = list(batch[data_args.label_column_name])

		return output_batch

	labels = raw_datasets["train"].features[data_args.label_column_name].names
	label2id, id2label = {}, {}
	for i, label in enumerate(labels):
		label2id[label] = str(i)
		id2label[str(i)] = label

	metric = evaluate.load("accuracy", cache_dir=model_args.cache_dir)

	def compute_metrics(eval_pred):
		predictions = np.argmax(eval_pred.predictions, axis=1)
		return metric.compute(predictions=predictions, references=eval_pred.label_ids)

	config = AutoConfig.from_pretrained(
		model_args.config_name or model_args.model_name_or_path,
		num_labels=len(labels),
		label2id=label2id,
		id2label=id2label,
		finetuning_task="audio-classification",
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForAudioClassification.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
		ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
	)

	if model_args.freeze_feature_encoder:
		model.freeze_feature_encoder()

	if training_args.do_train:
		if data_args.max_train_samples is not None:
			raw_datasets["train"] = (
				raw_datasets["train"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))
			)
		raw_datasets["train"].set_transform(train_transforms, output_all_columns=False)

	if training_args.do_eval:
		if data_args.max_eval_samples is not None:
			raw_datasets["eval"] = (
				raw_datasets["eval"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))
			)
		raw_datasets["eval"].set_transform(val_transforms, output_all_columns=False)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=raw_datasets["train"] if training_args.do_train else None,
		eval_dataset=raw_datasets["eval"] if training_args.do_eval else None,
		compute_metrics=compute_metrics,
		tokenizer=feature_extractor,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()
		trainer.log_metrics("train", train_result.metrics)
		trainer.save_metrics("train", train_result.metrics)
		trainer.save_state()

	if training_args.do_eval:
		metrics = trainer.evaluate()
		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {
		"finetuned_from": model_args.model_name_or_path,
		"tasks": "audio-classification",
		"dataset": data_args.dataset_name,
		"tags": ["audio-classification"],
	}
	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


if __name__ == "__main__":
	main()



import importlib
import sys
from argparse import REMAINDER, ArgumentParser
from pathlib import Path

import torch_xla.distributed.xla_multiprocessing as xmp


def parse_args():
	parser = ArgumentParser(
		description=(
			"PyTorch TPU distributed training launch helper utility that will spawn up multiple distributed processes"
		)
	)

	parser.add_argument("--num_cores", type=int, default=1, help="Number of TPU cores to use (1 or 8).")

	parser.add_argument(
		"training_script",
		type=str,
		help=(
			"The full path to the single TPU training "
			"program/script to be launched in parallel, "
			"followed by all the arguments for the "
			"training script"
		),
	)

	parser.add_argument("training_script_args", nargs=REMAINDER)

	return parser.parse_args()


def main():
	args = parse_args()

	script_fpath = Path(args.training_script)
	sys.path.append(str(script_fpath.parent.resolve()))
	mod_name = script_fpath.stem
	mod = importlib.import_module(mod_name)

	sys.argv = [args.training_script] + args.training_script_args + ["--tpu_num_cores", str(args.num_cores)]

	xmp.spawn(mod._mp_fn, args=(), nprocs=args.num_cores)


if __name__ == "__main__":
	main()

from typing import Dict, Union, Sequence, Set, Optional, cast
import logging


import torch

from allennlp.data.fields.field import Field
from allennlp.data.vocabulary import Vocabulary
from allennlp.common.checks import ConfigurationError

logger = logging.getLogger(__name__)


class MultiLabelField(Field[torch.Tensor]):

	__slots__ = ["labels", "_label_namespace", "_label_ids", "_num_labels"]

	_already_warned_namespaces: Set[str] = set()

	def __init__(
		self,
		labels: Sequence[Union[str, int]],
		label_namespace: str = "labels",
		skip_indexing: bool = False,
		num_labels: Optional[int] = None,
	) -> None:
		self.labels = labels
		self._label_namespace = label_namespace
		self._label_ids = None
		self._maybe_warn_for_namespace(label_namespace)
		self._num_labels = num_labels

		if skip_indexing and self.labels:
			if not all(isinstance(label, int) for label in labels):
				raise ConfigurationError(
					"In order to skip indexing, your labels must be integers. "
					"Found labels = {}".format(labels)
				)
			if not num_labels:
				raise ConfigurationError("In order to skip indexing, num_labels can't be None.")

			if not all(cast(int, label) < num_labels for label in labels):
				raise ConfigurationError(
					"All labels should be < num_labels. "
					"Found num_labels = {} and labels = {} ".format(num_labels, labels)
				)

			self._label_ids = labels
		else:
			if not all(isinstance(label, str) for label in labels):
				raise ConfigurationError(
					"MultiLabelFields expects string labels if skip_indexing=False. "
					"Found labels: {}".format(labels)
				)

	def _maybe_warn_for_namespace(self, label_namespace: str) -> None:
		if not (label_namespace.endswith("labels") or label_namespace.endswith("tags")):
			if label_namespace not in self._already_warned_namespaces:
				logger.warning(
					"Your label namespace was '%s'. We recommend you use a namespace "
					"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by "
					"default to your vocabulary.  See documentation for "
					"`non_padded_namespaces` parameter in Vocabulary.",
					self._label_namespace,
				)
				self._already_warned_namespaces.add(label_namespace)

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		if self._label_ids is None:
			for label in self.labels:
				counter[self._label_namespace][label] += 1  # type: ignore

	def index(self, vocab: Vocabulary):
		if self._label_ids is None:
			self._label_ids = [
				vocab.get_token_index(label, self._label_namespace)  # type: ignore
				for label in self.labels
			]
		if not self._num_labels:
			self._num_labels = vocab.get_vocab_size(self._label_namespace)

	def get_padding_lengths(self) -> Dict[str, int]:
		return {}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:
		tensor = torch.zeros(self._num_labels, dtype=torch.long)  # vector of zeros
		if self._label_ids:
			tensor.scatter_(0, torch.LongTensor(self._label_ids), 1)

		return tensor

	def empty_field(self):
		return MultiLabelField(
			[], self._label_namespace, skip_indexing=True, num_labels=self._num_labels
		)

	def __str__(self) -> str:
		return (
			f"MultiLabelField with labels: {self.labels} in namespace: '{self._label_namespace}'.'"
		)

	def __len__(self):
		return 1

	def human_readable_repr(self) -> Sequence[Union[str, int]]:
		return self.labels

import importlib
import logging
import inspect
from collections import defaultdict
from typing import (
	Callable,
	ClassVar,
	DefaultDict,
	Dict,
	List,
	Optional,
	Tuple,
	Type,
	TypeVar,
	cast,
	Any,
)

from allennlp.common.checks import ConfigurationError
from allennlp.common.from_params import FromParams

logger = logging.getLogger(__name__)

_T = TypeVar("_T")
_RegistrableT = TypeVar("_RegistrableT", bound="Registrable")

_SubclassRegistry = Dict[str, Tuple[type, Optional[str]]]


class Registrable(FromParams):

	_registry: ClassVar[DefaultDict[type, _SubclassRegistry]] = defaultdict(dict)

	default_implementation: Optional[str] = None

	@classmethod
	def register(
		cls, name: str, constructor: Optional[str] = None, exist_ok: bool = False
	) -> Callable[[Type[_T]], Type[_T]]:
		registry = Registrable._registry[cls]

		def add_subclass_to_registry(subclass: Type[_T]) -> Type[_T]:
			if name in registry:
				if exist_ok:
					message = (
						f"{name} has already been registered as {registry[name][0].__name__}, but "
						f"exist_ok=True, so overwriting with {cls.__name__}"
					)
					logger.info(message)
				else:
					message = (
						f"Cannot register {name} as {cls.__name__}; "
						f"name already in use for {registry[name][0].__name__}"
					)
					raise ConfigurationError(message)
			registry[name] = (subclass, constructor)
			return subclass

		return add_subclass_to_registry

	@classmethod
	def by_name(cls: Type[_RegistrableT], name: str) -> Callable[..., _RegistrableT]:
		logger.debug(f"instantiating registered subclass {name} of {cls}")
		subclass, constructor = cls.resolve_class_name(name)
		if not constructor:
			return cast(Type[_RegistrableT], subclass)
		else:
			return cast(Callable[..., _RegistrableT], getattr(subclass, constructor))

	@classmethod
	def resolve_class_name(
		cls: Type[_RegistrableT], name: str
	) -> Tuple[Type[_RegistrableT], Optional[str]]:
		if name in Registrable._registry[cls]:
			subclass, constructor = Registrable._registry[cls][name]
			return subclass, constructor
		elif "." in name:
			parts = name.split(".")
			submodule = ".".join(parts[:-1])
			class_name = parts[-1]

			try:
				module = importlib.import_module(submodule)
			except ModuleNotFoundError:
				raise ConfigurationError(
					f"tried to interpret {name} as a path to a class "
					f"but unable to import module {submodule}"
				)

			try:
				subclass = getattr(module, class_name)
				constructor = None
				return subclass, constructor
			except AttributeError:
				raise ConfigurationError(
					f"tried to interpret {name} as a path to a class "
					f"but unable to find class {class_name} in {submodule}"
				)

		else:
			available = cls.list_available()
			suggestion = _get_suggestion(name, available)
			raise ConfigurationError(
				(
					f"'{name}' is not a registered name for '{cls.__name__}'"
					+ (". " if not suggestion else f", did you mean '{suggestion}'? ")
				)
				+ "If your registered class comes from custom code, you'll need to import "
				"the corresponding modules. If you're using AllenNLP from the command-line, "
				"this is done by using the '--include-package' flag, or by specifying your imports "
				"in a '.allennlp_plugins' file. "
				"Alternatively, you can specify your choices "
				"in which case they will be automatically imported correctly."
			)

	@classmethod
	def list_available(cls) -> List[str]:
		keys = list(Registrable._registry[cls].keys())
		default = cls.default_implementation

		if default is None:
			return keys
		elif default not in keys:
			raise ConfigurationError(f"Default implementation {default} is not registered")
		else:
			return [default] + [k for k in keys if k != default]

	def _to_params(self) -> Dict[str, Any]:
		logger.warning(
			f"'{self.__class__.__name__}' does not implement '_to_params`. Will"
			f" use Registrable's `_to_params`."
		)

		mro = inspect.getmro(self.__class__)[1:]

		registered_name = None
		for parent in mro:
			try:
				registered_classes = self._registry[parent]
			except KeyError:
				continue

			for name, registered_value in registered_classes.items():
				registered_class, _ = registered_value
				if registered_class == self.__class__:
					registered_name = name
					break

			if registered_name is not None:
				break

		if registered_name is None:
			raise KeyError(f"'{self.__class__.__name__}' is not registered")

		parameter_dict = {"type": registered_name}

		for parameter in inspect.signature(self.__class__).parameters.values():
			if parameter.default != inspect.Parameter.empty:
				logger.debug(f"Skipping parameter {parameter.name}")
				continue

			if hasattr(self, parameter.name):
				parameter_dict[parameter.name] = getattr(self, parameter.name)
			elif hasattr(self, f"_{parameter.name}"):
				parameter_dict[parameter.name] = getattr(self, f"_{parameter.name}")
			else:
				logger.warning(f"Could not find a value for positional argument {parameter.name}")
				continue

		return parameter_dict


def _get_suggestion(name: str, available: List[str]) -> Optional[str]:
	for ch, repl_ch in (("_", "-"), ("-", "_")):
		suggestion = name.replace(ch, repl_ch)
		if suggestion in available:
			return suggestion
	from nltk.metrics.distance import edit_distance

	for suggestion in available:
		if edit_distance(name, suggestion, transpositions=True) == 1:
			return suggestion
	return None

from dataclasses import dataclass
from typing import Optional


@dataclass(init=False, repr=False)
class Token:

	__slots__ = [
		"text",
		"idx",
		"idx_end",
		"lemma_",
		"pos_",
		"tag_",
		"dep_",
		"ent_type_",
		"text_id",
		"type_id",
	]

	text: Optional[str]
	idx: Optional[int]
	idx_end: Optional[int]
	lemma_: Optional[str]
	pos_: Optional[str]
	tag_: Optional[str]
	dep_: Optional[str]
	ent_type_: Optional[str]
	text_id: Optional[int]
	type_id: Optional[int]

	def __init__(
		self,
		text: str = None,
		idx: int = None,
		idx_end: int = None,
		lemma_: str = None,
		pos_: str = None,
		tag_: str = None,
		dep_: str = None,
		ent_type_: str = None,
		text_id: int = None,
		type_id: int = None,
	) -> None:
		assert text is None or isinstance(
			text, str
		)  # Some very hard to debug errors happen when this is not true.
		self.text = text
		self.idx = idx
		self.idx_end = idx_end
		self.lemma_ = lemma_
		self.pos_ = pos_
		self.tag_ = tag_
		self.dep_ = dep_
		self.ent_type_ = ent_type_
		self.text_id = text_id
		self.type_id = type_id

	def __str__(self):
		return self.text

	def __repr__(self):
		return self.__str__()

	def ensure_text(self) -> str:
		if self.text is None:
			raise ValueError("Unexpected null text for token")
		else:
			return self.text


def show_token(token: Token) -> str:
	return (
		f"{token.text} "
		f"(idx: {token.idx}) "
		f"(idx_end: {token.idx_end}) "
		f"(lemma: {token.lemma_}) "
		f"(pos: {token.pos_}) "
		f"(tag: {token.tag_}) "
		f"(dep: {token.dep_}) "
		f"(ent_type: {token.ent_type_}) "
		f"(text_id: {token.text_id}) "
		f"(type_id: {token.type_id}) "
	)

import torch

from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder
from allennlp.nn.util import get_lengths_from_binary_sequence_mask


@Seq2VecEncoder.register("boe")
@Seq2VecEncoder.register("bag_of_embeddings")
class BagOfEmbeddingsEncoder(Seq2VecEncoder):

	def __init__(self, embedding_dim: int, averaged: bool = False) -> None:
		super().__init__()
		self._embedding_dim = embedding_dim
		self._averaged = averaged

	def get_input_dim(self) -> int:
		return self._embedding_dim

	def get_output_dim(self) -> int:
		return self._embedding_dim

	def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None):
		if mask is not None:
			tokens = tokens * mask.unsqueeze(-1)

		summed = tokens.sum(1)

		if self._averaged:
			if mask is not None:
				lengths = get_lengths_from_binary_sequence_mask(mask)
				length_mask = lengths > 0

				lengths = torch.max(lengths, lengths.new_ones(1))
			else:
				lengths = tokens.new_full((1,), fill_value=tokens.size(1))
				length_mask = None

			summed = summed / lengths.unsqueeze(-1).float()

			if length_mask is not None:
				summed = summed * (length_mask > 0).unsqueeze(-1)

		return summed

import logging

from typing import Optional
import math
import numpy as np


import torch

from allennlp.common.util import is_distributed
from allennlp.training.metrics.covariance import Covariance
from allennlp.training.metrics.metric import Metric

logger = logging.getLogger(__name__)


@Metric.register("pearson_correlation")
class PearsonCorrelation(Metric):

	def __init__(self) -> None:
		self._predictions_labels_covariance = Covariance()
		self._predictions_variance = Covariance()
		self._labels_variance = Covariance()

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)
		if not is_distributed():
			self._predictions_labels_covariance(predictions, gold_labels, mask)
			self._predictions_variance(predictions, predictions, mask)
			self._labels_variance(gold_labels, gold_labels, mask)

	def get_metric(self, reset: bool = False):
		if is_distributed():
			raise RuntimeError(
				"Distributed aggregation for PearsonCorrelation is currently not supported."
			)
		covariance = self._predictions_labels_covariance.get_metric(reset=reset)
		predictions_variance = self._predictions_variance.get_metric(reset=reset)
		labels_variance = self._labels_variance.get_metric(reset=reset)
		denominator = math.sqrt(predictions_variance) * math.sqrt(labels_variance)
		if reset:
			self.reset()

		if np.around(denominator, decimals=5) == 0:
			pearson_r = 0.0
		else:
			pearson_r = covariance / denominator
		return pearson_r

	def reset(self):
		self._predictions_labels_covariance.reset()
		self._predictions_variance.reset()
		self._labels_variance.reset()

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class RNNModel(nn.Module):

	def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):
		super(RNNModel, self).__init__()
		self.ntoken = ntoken
		self.drop = nn.Dropout(dropout)
		self.encoder = nn.Embedding(ntoken, ninp)
		if rnn_type in ['LSTM', 'GRU']:
			self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)
		else:
			try:
				nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]
			except KeyError as e:
			self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)
		self.decoder = nn.Linear(nhid, ntoken)

		if tie_weights:
			if nhid != ninp:
				raise ValueError('When using the tied flag, nhid must be equal to emsize')
			self.decoder.weight = self.encoder.weight

		self.init_weights()

		self.rnn_type = rnn_type
		self.nhid = nhid
		self.nlayers = nlayers

	def init_weights(self):
		initrange = 0.1
		nn.init.uniform_(self.encoder.weight, -initrange, initrange)
		nn.init.zeros_(self.decoder.bias)
		nn.init.uniform_(self.decoder.weight, -initrange, initrange)

	def forward(self, input, hidden):
		emb = self.drop(self.encoder(input))
		output, hidden = self.rnn(emb, hidden)
		output = self.drop(output)
		decoded = self.decoder(output)
		decoded = decoded.view(-1, self.ntoken)
		return F.log_softmax(decoded, dim=1), hidden

	def init_hidden(self, bsz):
		weight = next(self.parameters())
		if self.rnn_type == 'LSTM':
			return (weight.new_zeros(self.nlayers, bsz, self.nhid),
					weight.new_zeros(self.nlayers, bsz, self.nhid))
		else:
			return weight.new_zeros(self.nlayers, bsz, self.nhid)

class PositionalEncoding(nn.Module):

	def __init__(self, d_model, dropout=0.1, max_len=5000):
		super(PositionalEncoding, self).__init__()
		self.dropout = nn.Dropout(p=dropout)

		pe = torch.zeros(max_len, d_model)
		position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
		div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
		pe[:, 0::2] = torch.sin(position * div_term)
		pe[:, 1::2] = torch.cos(position * div_term)
		pe = pe.unsqueeze(0).transpose(0, 1)
		self.register_buffer('pe', pe)

	def forward(self, x):

		x = x + self.pe[:x.size(0), :]
		return self.dropout(x)

class TransformerModel(nn.Transformer):

	def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):
		super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)
		self.model_type = 'Transformer'
		self.src_mask = None
		self.pos_encoder = PositionalEncoding(ninp, dropout)

		self.input_emb = nn.Embedding(ntoken, ninp)
		self.ninp = ninp
		self.decoder = nn.Linear(ninp, ntoken)

		self.init_weights()

	def _generate_square_subsequent_mask(self, sz):
		return torch.log(torch.tril(torch.ones(sz,sz)))

	def init_weights(self):
		initrange = 0.1
		nn.init.uniform_(self.input_emb.weight, -initrange, initrange)
		nn.init.zeros_(self.decoder.bias)
		nn.init.uniform_(self.decoder.weight, -initrange, initrange)

	def forward(self, src, has_mask=True):
		if has_mask:
			device = src.device
			if self.src_mask is None or self.src_mask.size(0) != len(src):
				mask = self._generate_square_subsequent_mask(len(src)).to(device)
				self.src_mask = mask
		else:
			self.src_mask = None

		src = self.input_emb(src) * math.sqrt(self.ninp)
		src = self.pos_encoder(src)
		output = self.encoder(src, mask=self.src_mask)
		output = self.decoder(output)
		return F.log_softmax(output, dim=-1)


from allennlp.models.model import Model
from allennlp.models.archival import archive_model, load_archive, Archive
from allennlp.models.basic_classifier import BasicClassifier
from allennlp.models.multitask import MultiTaskModel
from allennlp.models.simple_tagger import SimpleTagger

import torch
import torch.fx
from typing import Any, Callable, Dict, Optional, Tuple

class ModulePathTracer(torch.fx.Tracer):

	current_module_qualified_name : str = ''
	node_to_originating_module : Dict[torch.fx.Node, str] = {}

	def call_module(self, m: torch.nn.Module, forward: Callable[..., Any],
					args : Tuple[Any, ...], kwargs : Dict[str, Any]) -> Any:
		old_qualname = self.current_module_qualified_name
		try:
			self.current_module_qualified_name = self.path_of_module(m)
			return super().call_module(m, forward, args, kwargs)
		finally:
			self.current_module_qualified_name = old_qualname

	def create_proxy(self, kind: str, target: torch.fx.node.Target, args: Tuple[Any, ...],
					 kwargs: Dict[str, Any], name: Optional[str] = None, type_expr: Optional[Any] = None):
		proxy = super().create_proxy(kind, target, args, kwargs, name, type_expr)
		self.node_to_originating_module[proxy.node] = self.current_module_qualified_name
		return proxy


import torchvision.models as models

rn18 = models.resnet18()

tracer = ModulePathTracer()
traced_rn18 = tracer.trace(rn18)

for node in traced_rn18.nodes:
	module_qualname = tracer.node_to_originating_module.get(node)
	print('Node', node, 'is from module', module_qualname)

import logging

from typing import Dict, List


from allennlp.common.file_utils import cached_path
from allennlp.data.dataset_readers.dataset_reader import DatasetReader, PathOrStr
from allennlp.data.instance import Instance
from allennlp.data.fields import Field, TextField, ListField, IndexField
from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer
from allennlp.data.tokenizers import Token

logger = logging.getLogger(__name__)


@DatasetReader.register("babi")
class BabiReader(DatasetReader):

	def __init__(
		self,
		keep_sentences: bool = False,
		token_indexers: Dict[str, TokenIndexer] = None,
		**kwargs,
	) -> None:

		super().__init__(**kwargs)
		self._keep_sentences = keep_sentences
		self._token_indexers = token_indexers or {"tokens": SingleIdTokenIndexer()}

	def _read(self, file_path: PathOrStr):
		file_path = cached_path(file_path)

		logger.info("Reading file at %s", file_path)

		with open(file_path) as dataset_file:
			dataset = dataset_file.readlines()

		logger.info("Reading the dataset")

		context: List[List[str]] = [[]]
		for line in dataset:
			if "?" in line:
				question_str, answer, supports_str = line.replace("?", " ?").split("\t")
				question = question_str.split()[1:]
				supports = [int(support) - 1 for support in supports_str.split()]

				yield self.text_to_instance(context, question, answer, supports)
			else:
				new_entry = line.replace(".", " .").split()[1:]

				if line[0] == "1":
					context = [new_entry]
				else:
					context.append(new_entry)

	def text_to_instance(  # type: ignore
		self,
		context: List[List[str]],
		question: List[str],
		answer: str,
		supports: List[int],
	) -> Instance:

		fields: Dict[str, Field] = {}

		if self._keep_sentences:
			context_field_ks = ListField(
				[TextField([Token(word) for word in line]) for line in context]
			)

			fields["supports"] = ListField(
				[IndexField(support, context_field_ks) for support in supports]
			)
		else:
			context_field = TextField([Token(word) for line in context for word in line])

		fields["context"] = context_field_ks if self._keep_sentences else context_field
		fields["question"] = TextField(
			[Token(word) for word in question],
		)
		fields["answer"] = TextField([Token(answer)])

		return Instance(fields)

	def apply_token_indexers(self, instance: Instance) -> None:
		if self._keep_sentences:
			for text_field in instance.fields["context"]:  # type: ignore
				text_field._token_indexers = self._token_indexers  # type: ignore
		else:
			instance.fields["context"]._token_indexers = self._token_indexers  # type: ignore
		instance.fields["question"]._token_indexers = self._token_indexers  # type: ignore
		instance.fields["answer"]._token_indexers = self._token_indexers  # type: ignore

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datautils import MyTrainDataset

import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import os


def ddp_setup():
	init_process_group(backend="nccl")
	torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))

class Trainer:
	def __init__(
		self,
		model: torch.nn.Module,
		train_data: DataLoader,
		optimizer: torch.optim.Optimizer,
		save_every: int,
		snapshot_path: str,
	) -> None:
		self.gpu_id = int(os.environ["LOCAL_RANK"])
		self.model = model.to(self.gpu_id)
		self.train_data = train_data
		self.optimizer = optimizer
		self.save_every = save_every
		self.epochs_run = 0
		self.snapshot_path = snapshot_path
		if os.path.exists(snapshot_path):
			print("Loading snapshot")
			self._load_snapshot(snapshot_path)

		self.model = DDP(self.model, device_ids=[self.gpu_id])

	def _load_snapshot(self, snapshot_path):
		loc = f"cuda:{self.gpu_id}"
		snapshot = torch.load(snapshot_path, map_location=loc)
		self.model.load_state_dict(snapshot["MODEL_STATE"])
		self.epochs_run = snapshot["EPOCHS_RUN"]
		print(f"Resuming training from snapshot at Epoch {self.epochs_run}")

	def _run_batch(self, source, targets):
		self.optimizer.zero_grad()
		output = self.model(source)
		loss = F.cross_entropy(output, targets)
		loss.backward()
		self.optimizer.step()

	def _run_epoch(self, epoch):
		b_sz = len(next(iter(self.train_data))[0])
		print(f"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}")
		self.train_data.sampler.set_epoch(epoch)
		for source, targets in self.train_data:
			source = source.to(self.gpu_id)
			targets = targets.to(self.gpu_id)
			self._run_batch(source, targets)

	def _save_snapshot(self, epoch):
		snapshot = {
			"MODEL_STATE": self.model.module.state_dict(),
			"EPOCHS_RUN": epoch,
		}
		torch.save(snapshot, self.snapshot_path)
		print(f"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}")

	def train(self, max_epochs: int):
		for epoch in range(self.epochs_run, max_epochs):
			self._run_epoch(epoch)
			if self.gpu_id == 0 and epoch % self.save_every == 0:
				self._save_snapshot(epoch)


def load_train_objs():
	train_set = MyTrainDataset(2048)  # load your dataset
	model = torch.nn.Linear(20, 1)  # load your model
	optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
	return train_set, model, optimizer


def prepare_dataloader(dataset: Dataset, batch_size: int):
	return DataLoader(
		dataset,
		batch_size=batch_size,
		pin_memory=True,
		shuffle=False,
		sampler=DistributedSampler(dataset)
	)


def main(save_every: int, total_epochs: int, batch_size: int, snapshot_path: str = "snapshot.pt"):
	ddp_setup()
	dataset, model, optimizer = load_train_objs()
	train_data = prepare_dataloader(dataset, batch_size)
	trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)
	trainer.train(total_epochs)
	destroy_process_group()


if __name__ == "__main__":
	import argparse
	parser = argparse.ArgumentParser(description='simple distributed training job')
	parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')
	parser.add_argument('save_every', type=int, help='How often to save a snapshot')
	parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')
	args = parser.parse_args()
	
	main(args.save_every, args.total_epochs, args.batch_size)

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datautils import MyTrainDataset

import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import os


def ddp_setup(rank, world_size):
	os.environ["MASTER_ADDR"] = "localhost"
	os.environ["MASTER_PORT"] = "12355"
	init_process_group(backend="nccl", rank=rank, world_size=world_size)
	torch.cuda.set_device(rank)

class Trainer:
	def __init__(
		self,
		model: torch.nn.Module,
		train_data: DataLoader,
		optimizer: torch.optim.Optimizer,
		gpu_id: int,
		save_every: int,
	) -> None:
		self.gpu_id = gpu_id
		self.model = model.to(gpu_id)
		self.train_data = train_data
		self.optimizer = optimizer
		self.save_every = save_every
		self.model = DDP(model, device_ids=[gpu_id])

	def _run_batch(self, source, targets):
		self.optimizer.zero_grad()
		output = self.model(source)
		loss = F.cross_entropy(output, targets)
		loss.backward()
		self.optimizer.step()

	def _run_epoch(self, epoch):
		b_sz = len(next(iter(self.train_data))[0])
		print(f"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}")
		self.train_data.sampler.set_epoch(epoch)
		for source, targets in self.train_data:
			source = source.to(self.gpu_id)
			targets = targets.to(self.gpu_id)
			self._run_batch(source, targets)

	def _save_checkpoint(self, epoch):
		ckp = self.model.module.state_dict()
		PATH = "checkpoint.pt"
		torch.save(ckp, PATH)
		print(f"Epoch {epoch} | Training checkpoint saved at {PATH}")

	def train(self, max_epochs: int):
		for epoch in range(max_epochs):
			self._run_epoch(epoch)
			if self.gpu_id == 0 and epoch % self.save_every == 0:
				self._save_checkpoint(epoch)


def load_train_objs():
	train_set = MyTrainDataset(2048)  # load your dataset
	model = torch.nn.Linear(20, 1)  # load your model
	optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
	return train_set, model, optimizer


def prepare_dataloader(dataset: Dataset, batch_size: int):
	return DataLoader(
		dataset,
		batch_size=batch_size,
		pin_memory=True,
		shuffle=False,
		sampler=DistributedSampler(dataset)
	)


def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):
	ddp_setup(rank, world_size)
	dataset, model, optimizer = load_train_objs()
	train_data = prepare_dataloader(dataset, batch_size)
	trainer = Trainer(model, train_data, optimizer, rank, save_every)
	trainer.train(total_epochs)
	destroy_process_group()


if __name__ == "__main__":
	import argparse
	parser = argparse.ArgumentParser(description='simple distributed training job')
	parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')
	parser.add_argument('save_every', type=int, help='How often to save a snapshot')
	parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')
	args = parser.parse_args()
	
	world_size = torch.cuda.device_count()
	mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)

import torch

from allennlp.modules.attention.attention import Attention


@Attention.register("dot_product")
class DotProductAttention(Attention):

	def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:
		return matrix.bmm(vector.unsqueeze(-1)).squeeze(-1)



from typing import Dict, Optional
import torch

from allennlp.common.checks import ConfigurationError
from allennlp.data import Vocabulary
from allennlp.fairness.bias_direction_wrappers import BiasDirectionWrapper
from allennlp.modules.feedforward import FeedForward
from allennlp.models import Model
from allennlp.nn import InitializerApplicator
from allennlp.nn.util import find_embedding_layer
from allennlp.training.callbacks.callback import TrainerCallback
from allennlp.training.callbacks.backward import OnBackwardException
from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


@Model.register("adversarial_bias_mitigator")
class AdversarialBiasMitigator(Model):

	def __init__(
		self,
		vocab: Vocabulary,
		predictor: Model,
		adversary: Model,
		bias_direction: BiasDirectionWrapper,
		predictor_output_key: str,
		**kwargs,
	):
		super().__init__(vocab, **kwargs)

		self.predictor = predictor
		self.adversary = adversary

		embedding_layer = find_embedding_layer(self.predictor)
		self.bias_direction = bias_direction
		self.predetermined_bias_direction = self.bias_direction(embedding_layer)
		self._adversary_label_hook = _AdversaryLabelHook(self.predetermined_bias_direction)
		embedding_layer.register_forward_hook(self._adversary_label_hook)

		self.vocab = self.predictor.vocab
		self._regularizer = self.predictor._regularizer

		self.predictor_output_key = predictor_output_key

	def train(self, mode: bool = True):
		super().train(mode)
		self.predictor.train(mode)
		self.adversary.train(mode)
		self.bias_direction.train(mode)

	def forward(self, *args, **kwargs):
		predictor_output_dict = self.predictor.forward(*args, **kwargs)
		adversary_output_dict = self.adversary.forward(
			predictor_output_dict[self.predictor_output_key],
			self._adversary_label_hook.adversary_label,
		)
		adversary_output_dict = {("adversary_" + k): v for k, v in adversary_output_dict.items()}
		output_dict = {**predictor_output_dict, **adversary_output_dict}
		return output_dict


	def forward_on_instance(self, *args, **kwargs):
		return self.predictor.forward_on_instance(*args, **kwargs)

	def forward_on_instances(self, *args, **kwargs):
		return self.predictor.forward_on_instances(*args, **kwargs)

	def get_regularization_penalty(self, *args, **kwargs):
		return self.predictor.get_regularization_penalty(*args, **kwargs)

	def get_parameters_for_histogram_logging(self, *args, **kwargs):
		return self.predictor.get_parameters_for_histogram_logging(*args, **kwargs)

	def get_parameters_for_histogram_tensorboard_logging(self, *args, **kwargs):
		return self.predictor.get_parameters_for_histogram_tensorboard_logging(*args, **kwargs)

	def make_output_human_readable(self, *args, **kwargs):
		return self.predictor.make_output_human_readable(*args, **kwargs)

	def get_metrics(self, *args, **kwargs):
		return self.predictor.get_metrics(*args, **kwargs)

	def _get_prediction_device(self, *args, **kwargs):
		return self.predictor._get_prediction_device(*args, **kwargs)

	def _maybe_warn_for_unseparable_batches(self, *args, **kwargs):
		return self.predictor._maybe_warn_for_unseparable_batches(*args, **kwargs)

	def extend_embedder_vocab(self, *args, **kwargs):
		return self.predictor.extend_embedder_vocab(*args, **kwargs)


@Model.register("feedforward_regression_adversary")
class FeedForwardRegressionAdversary(Model):

	def __init__(
		self,
		vocab: Vocabulary,
		feedforward: FeedForward,
		initializer: Optional[InitializerApplicator] = InitializerApplicator(),
		**kwargs,
	) -> None:
		super().__init__(vocab, **kwargs)

		self._feedforward = feedforward
		self._loss = torch.nn.MSELoss()
		initializer(self)  # type: ignore

	def forward(  # type: ignore
		self, input: torch.FloatTensor, label: torch.FloatTensor
	) -> Dict[str, torch.Tensor]:

		pred = self._feedforward(input)
		return {"loss": self._loss(pred, label)}


@TrainerCallback.register("adversarial_bias_mitigator_backward")
class AdversarialBiasMitigatorBackwardCallback(TrainerCallback):

	def __init__(self, serialization_dir: str, adversary_loss_weight: float = 1.0) -> None:
		super().__init__(serialization_dir)
		self.adversary_loss_weight = adversary_loss_weight

	def on_backward(
		self,
		trainer: GradientDescentTrainer,
		batch_outputs: Dict[str, torch.Tensor],
		backward_called: bool,
		**kwargs,
	) -> bool:
		if backward_called:
			raise OnBackwardException()

		if not hasattr(trainer.model, "predictor") or not hasattr(trainer.model, "adversary"):
			raise ConfigurationError(
				"Model is expected to have `predictor` and `adversary` data members."
			)

		trainer.optimizer.zero_grad()
		batch_outputs["adversary_loss"].backward(retain_graph=True)
		adversary_loss_grad = {
			name: param.grad.clone()
			for name, param in trainer.model.predictor.named_parameters()
			if param.grad is not None
		}

		trainer.model.predictor.zero_grad()
		batch_outputs["loss"].backward()

		with torch.no_grad():
			for name, param in trainer.model.predictor.named_parameters():
				if param.grad is not None:
					unit_adversary_loss_grad = adversary_loss_grad[name] / torch.linalg.norm(
						adversary_loss_grad[name]
					)
					param.grad -= (
						(param.grad * unit_adversary_loss_grad) * unit_adversary_loss_grad
					).sum()
					param.grad -= self.adversary_loss_weight * adversary_loss_grad[name]

		batch_outputs["adversary_loss"] = batch_outputs["adversary_loss"].detach()
		return True


class _AdversaryLabelHook:
	def __init__(self, predetermined_bias_direction):
		self.predetermined_bias_direction = predetermined_bias_direction

	def __call__(self, module, module_in, module_out):
		with torch.no_grad():
			module_out = module_out.mean(dim=1)
			self.adversary_label = torch.matmul(
				module_out, self.predetermined_bias_direction.to(module_out.device)
			).unsqueeze(-1)

from typing import Optional


import torch
import torch.distributed as dist
import scipy.stats as stats

from allennlp.common.util import is_distributed
from allennlp.training.metrics.metric import Metric


@Metric.register("spearman_correlation")
class SpearmanCorrelation(Metric):

	def __init__(self) -> None:
		super().__init__()
		self.total_predictions = torch.zeros(0)
		self.total_gold_labels = torch.zeros(0)

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)
		predictions = predictions.reshape(-1)
		gold_labels = gold_labels.reshape(-1)

		self.total_predictions = self.total_predictions.to(predictions.device)
		self.total_gold_labels = self.total_gold_labels.to(gold_labels.device)

		if mask is not None:
			mask = mask.reshape(-1)
			self.total_predictions = torch.cat((self.total_predictions, predictions * mask), 0)
			self.total_gold_labels = torch.cat((self.total_gold_labels, gold_labels * mask), 0)
		else:
			self.total_predictions = torch.cat((self.total_predictions, predictions), 0)
			self.total_gold_labels = torch.cat((self.total_gold_labels, gold_labels), 0)

		if is_distributed():
			world_size = dist.get_world_size()
			device = gold_labels.device
			_all_batch_lengths = [torch.tensor(0) for i in range(world_size)]
			dist.all_gather(
				_all_batch_lengths, torch.tensor(self.total_predictions.shape[0], device=device)
			)
			_all_batch_lengths = [batch_length.item() for batch_length in _all_batch_lengths]

			if len(set(_all_batch_lengths)) > 1:
				raise RuntimeError(
					"Distributed aggregation for SpearmanCorrelation is currently not supported "
					"for batches of unequal length."
				)
			_total_predictions = [
				torch.zeros(self.total_predictions.shape, device=device) for i in range(world_size)
			]
			_total_gold_labels = [
				torch.zeros(self.total_gold_labels.shape, device=device) for i in range(world_size)
			]
			dist.all_gather(_total_predictions, self.total_predictions)
			dist.all_gather(_total_gold_labels, self.total_gold_labels)
			self.total_predictions = torch.cat(_total_predictions, dim=0)
			self.total_gold_labels = torch.cat(_total_gold_labels, dim=0)

	def get_metric(self, reset: bool = False):

		spearman_correlation = stats.spearmanr(
			self.total_predictions.cpu().numpy(), self.total_gold_labels.cpu().numpy()
		)
		if reset:
			self.reset()

		return spearman_correlation[0]

	def reset(self):
		self.total_predictions = torch.zeros(0)
		self.total_gold_labels = torch.zeros(0)

from typing import Union, Optional, Tuple, TYPE_CHECKING
import logging
from dataclasses import dataclass

import torch

from allennlp.common import FromParams
from allennlp.modules.util import replicate_layers
from allennlp.modules.transformer.transformer_layer import TransformerLayer
from allennlp.modules.transformer.transformer_module import TransformerModule
from allennlp.modules.transformer.util import FloatT

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig


logger = logging.getLogger(__name__)


@dataclass
class TransformerStackOutput:

	final_hidden_states: FloatT
	all_hidden_states: Optional[Tuple] = None
	all_self_attentions: Optional[Tuple] = None
	all_cross_attentions: Optional[Tuple] = None


class TransformerStack(TransformerModule, FromParams):

	_pretrained_mapping = {"layer": "layers"}
	_pretrained_relevant_module = ["encoder", "bert.encoder", "roberta.encoder"]

	def __init__(
		self,
		num_hidden_layers: int,
		layer: Optional[TransformerLayer] = None,
		hidden_size: Optional[int] = None,
		intermediate_size: Optional[int] = None,
		num_attention_heads: int = 8,
		attention_dropout: float = 0.1,
		hidden_dropout: float = 0.1,
		activation: Union[str, torch.nn.Module] = "relu",
		add_cross_attention: bool = False,
	):
		super().__init__()

		if layer is not None:
			logger.warning(
				"The `layer` argument has been specified. Any other arguments will be ignored."
			)
		else:
			assert (hidden_size is not None) and (intermediate_size is not None), "As the `layer`"
			"has not been provided, `hidden_size` and `intermediate_size` are"
			"required to create `TransformerLayer`s."

		layer = layer or TransformerLayer(
			hidden_size,  # type: ignore
			intermediate_size,  # type: ignore
			num_attention_heads,
			attention_dropout,
			hidden_dropout,
			activation,
			add_cross_attention,
		)
		self.layers = replicate_layers(layer, num_hidden_layers)

	def get_output_dim(self) -> int:
		return self.layers[-1].get_output_dim()

	def forward(
		self,
		hidden_states: torch.Tensor,
		attention_mask: Optional[torch.Tensor] = None,
		head_mask: Optional[torch.Tensor] = None,
		encoder_hidden_states: Optional[torch.Tensor] = None,
		encoder_attention_mask: Optional[torch.Tensor] = None,
		output_attentions: bool = False,
		output_hidden_states: bool = False,
	) -> TransformerStackOutput:
		all_hidden_states = () if output_hidden_states else None
		all_attentions = () if output_attentions else None
		all_cross_attentions = () if output_attentions and self._add_cross_attention else None
		for i, layer_module in enumerate(self.layers):
			if output_hidden_states:
				all_hidden_states = all_hidden_states + (hidden_states,)  # type: ignore

			layer_head_mask = head_mask[i] if head_mask is not None else None

			layer_outputs = layer_module(
				hidden_states,
				attention_mask,
				layer_head_mask,
				encoder_hidden_states,
				encoder_attention_mask,
				output_attentions,
			)
			hidden_states = layer_outputs.hidden_states
			if output_attentions:
				all_attentions = all_attentions + (layer_outputs[1],)  # type: ignore
				if self._add_cross_attention:
					all_cross_attentions = all_cross_attentions + (layer_outputs[2],)  # type: ignore

		if output_hidden_states:
			all_hidden_states = all_hidden_states + (hidden_states,)  # type: ignore

		return TransformerStackOutput(
			hidden_states, all_hidden_states, all_attentions, all_cross_attentions
		)

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		final_kwargs = {}
		final_kwargs["num_hidden_layers"] = config.num_hidden_layers
		final_kwargs["hidden_size"] = config.hidden_size
		final_kwargs["num_attention_heads"] = config.num_attention_heads
		final_kwargs["add_cross_attention"] = config.add_cross_attention
		final_kwargs["attention_dropout"] = config.attention_probs_dropout_prob
		final_kwargs["hidden_dropout"] = config.hidden_dropout_prob
		final_kwargs["intermediate_size"] = config.intermediate_size
		final_kwargs["activation"] = config.hidden_act
		final_kwargs.update(**kwargs)
		return cls(**final_kwargs)



from allennlp.data.dataset_readers.dataset_reader import (
	DatasetReader,
	WorkerInfo,
	DatasetReaderInput,
)
from allennlp.data.dataset_readers.babi import BabiReader
from allennlp.data.dataset_readers.conll2003 import Conll2003DatasetReader
from allennlp.data.dataset_readers.interleaving_dataset_reader import InterleavingDatasetReader
from allennlp.data.dataset_readers.multitask import MultiTaskDatasetReader
from allennlp.data.dataset_readers.sequence_tagging import SequenceTaggingDatasetReader
from allennlp.data.dataset_readers.sharded_dataset_reader import ShardedDatasetReader
from allennlp.data.dataset_readers.text_classification_json import TextClassificationJsonReader

import torch
import numpy as np


class SoftmaxLoss(torch.nn.Module):

	def __init__(self, num_words: int, embedding_dim: int) -> None:
		super().__init__()

		self.tie_embeddings = False

		self.softmax_w = torch.nn.Parameter(
			torch.randn(embedding_dim, num_words) / np.sqrt(embedding_dim)
		)
		self.softmax_b = torch.nn.Parameter(torch.zeros(num_words))

	def forward(self, embeddings: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
		probs = torch.nn.functional.log_softmax(
			torch.matmul(embeddings, self.softmax_w) + self.softmax_b, dim=-1
		)

		return torch.nn.functional.nll_loss(probs, targets.long(), reduction="sum")

from typing import Dict, List, Any, Optional
import logging


import torch

from allennlp.common.util import pad_sequence_to_length
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.tokenizers import Token
from allennlp.data.token_indexers import PretrainedTransformerIndexer, TokenIndexer
from allennlp.data.token_indexers.token_indexer import IndexedTokenList

logger = logging.getLogger(__name__)


@TokenIndexer.register("pretrained_transformer_mismatched")
class PretrainedTransformerMismatchedIndexer(TokenIndexer):

	def __init__(
		self,
		model_name: str,
		namespace: str = "tags",
		max_length: int = None,
		tokenizer_kwargs: Optional[Dict[str, Any]] = None,
		**kwargs,
	) -> None:
		super().__init__(**kwargs)
		self._matched_indexer = PretrainedTransformerIndexer(
			model_name,
			namespace=namespace,
			max_length=max_length,
			tokenizer_kwargs=tokenizer_kwargs,
			**kwargs,
		)
		self._allennlp_tokenizer = self._matched_indexer._allennlp_tokenizer
		self._tokenizer = self._matched_indexer._tokenizer
		self._num_added_start_tokens = self._matched_indexer._num_added_start_tokens
		self._num_added_end_tokens = self._matched_indexer._num_added_end_tokens

	def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
		return self._matched_indexer.count_vocab_items(token, counter)

	def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:
		self._matched_indexer._add_encoding_to_vocabulary_if_needed(vocabulary)

		wordpieces, offsets = self._allennlp_tokenizer.intra_word_tokenize(
			[t.ensure_text() for t in tokens]
		)

		offsets = [x if x is not None else (-1, -1) for x in offsets]

		output: IndexedTokenList = {
			"token_ids": [t.text_id for t in wordpieces],
			"mask": [True] * len(tokens),  # for original tokens (i.e. word-level)
			"type_ids": [t.type_id for t in wordpieces],
			"offsets": offsets,
			"wordpiece_mask": [True] * len(wordpieces),  # for wordpieces (i.e. subword-level)
		}

		return self._matched_indexer._postprocess_output(output)

	def get_empty_token_list(self) -> IndexedTokenList:
		output = self._matched_indexer.get_empty_token_list()
		output["offsets"] = []
		output["wordpiece_mask"] = []
		return output

	def as_padded_tensor_dict(
		self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]
	) -> Dict[str, torch.Tensor]:
		tokens = tokens.copy()
		padding_lengths = padding_lengths.copy()

		offsets_tokens = tokens.pop("offsets")
		offsets_padding_lengths = padding_lengths.pop("offsets")

		tensor_dict = self._matched_indexer.as_padded_tensor_dict(tokens, padding_lengths)
		tensor_dict["offsets"] = torch.LongTensor(
			pad_sequence_to_length(
				offsets_tokens, offsets_padding_lengths, default_value=lambda: (0, 0)
			)
		)
		return tensor_dict

	def __eq__(self, other):
		if isinstance(other, PretrainedTransformerMismatchedIndexer):
			for key in self.__dict__:
				if key == "_tokenizer":
					continue
				if self.__dict__[key] != other.__dict__[key]:
					return False
			return True
		return NotImplemented


from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder
from allennlp.modules.text_field_embedders.basic_text_field_embedder import BasicTextFieldEmbedder

from copy import deepcopy
from typing import Dict, List, Tuple

import numpy
import torch

from allennlp.common.util import JsonDict, sanitize
from allennlp.data import Instance, Token
from allennlp.data.fields import TextField
from allennlp.data.token_indexers import (
	ELMoTokenCharactersIndexer,
	TokenCharactersIndexer,
	SingleIdTokenIndexer,
)
from allennlp.interpret.attackers import utils
from allennlp.interpret.attackers.attacker import Attacker
from allennlp.modules.token_embedders import Embedding
from allennlp.nn import util
from allennlp.predictors.predictor import Predictor

DEFAULT_IGNORE_TOKENS = ["@@NULL@@", ".", ",", ";", "!", "?", "[MASK]", "[SEP]", "[CLS]"]


@Attacker.register("hotflip")
class Hotflip(Attacker):

	def __init__(
		self, predictor: Predictor, vocab_namespace: str = "tokens", max_tokens: int = 5000
	) -> None:
		super().__init__(predictor)
		self.vocab = self.predictor._model.vocab
		self.namespace = vocab_namespace
		self.max_tokens = max_tokens
		self.invalid_replacement_indices: List[int] = []
		for i in self.vocab._index_to_token[self.namespace]:
			if not self.vocab._index_to_token[self.namespace][i].isalnum():
				self.invalid_replacement_indices.append(i)
		self.embedding_matrix: torch.Tensor = None
		self.embedding_layer: torch.nn.Module = None
		self.cuda_device = predictor.cuda_device

	def initialize(self):
		if self.embedding_matrix is None:
			self.embedding_matrix = self._construct_embedding_matrix()

	def _construct_embedding_matrix(self) -> Embedding:
		embedding_layer = self.predictor.get_interpretable_layer()
		self.embedding_layer = embedding_layer
		if isinstance(embedding_layer, (Embedding, torch.nn.modules.sparse.Embedding)):
			return embedding_layer.weight

		all_tokens = list(self.vocab._token_to_index[self.namespace])[: self.max_tokens]
		max_index = self.vocab.get_token_index(all_tokens[-1], self.namespace)
		self.invalid_replacement_indices = [
			i for i in self.invalid_replacement_indices if i < max_index
		]

		inputs = self._make_embedder_input(all_tokens)

		embedding_matrix = embedding_layer(inputs).squeeze()

		return embedding_matrix

	def _make_embedder_input(self, all_tokens: List[str]) -> Dict[str, torch.Tensor]:
		inputs = {}
		indexers = self.predictor._dataset_reader._token_indexers  # type: ignore
		for indexer_name, token_indexer in indexers.items():
			if isinstance(token_indexer, SingleIdTokenIndexer):
				all_indices = [
					self.vocab._token_to_index[self.namespace][token] for token in all_tokens
				]
				inputs[indexer_name] = {"tokens": torch.LongTensor(all_indices).unsqueeze(0)}
			elif isinstance(token_indexer, TokenCharactersIndexer):
				tokens = [Token(x) for x in all_tokens]
				max_token_length = max(len(x) for x in all_tokens)
				max_token_length = max(max_token_length, token_indexer._min_padding_length)
				indexed_tokens = token_indexer.tokens_to_indices(tokens, self.vocab)
				padding_lengths = token_indexer.get_padding_lengths(indexed_tokens)
				padded_tokens = token_indexer.as_padded_tensor_dict(indexed_tokens, padding_lengths)
				inputs[indexer_name] = {
					"token_characters": torch.LongTensor(
						padded_tokens["token_characters"]
					).unsqueeze(0)
				}
			elif isinstance(token_indexer, ELMoTokenCharactersIndexer):
				elmo_tokens = []
				for token in all_tokens:
					elmo_indexed_token = token_indexer.tokens_to_indices(
						[Token(text=token)], self.vocab
					)["elmo_tokens"]
					elmo_tokens.append(elmo_indexed_token[0])
				inputs[indexer_name] = {"elmo_tokens": torch.LongTensor(elmo_tokens).unsqueeze(0)}
			else:
				raise RuntimeError("Unsupported token indexer:", token_indexer)

		return util.move_to_device(inputs, self.cuda_device)

	def attack_from_json(
		self,
		inputs: JsonDict,
		input_field_to_attack: str = "tokens",
		grad_input_field: str = "grad_input_1",
		ignore_tokens: List[str] = None,
		target: JsonDict = None,
	) -> JsonDict:
		instance = self.predictor._json_to_instance(inputs)
		self.predictor._dataset_reader.apply_token_indexers(instance)
		if target is None:
			output_dict = self.predictor._model.forward_on_instance(instance)
		else:
			output_dict = target

		original_instances = self.predictor.predictions_to_labeled_instances(instance, output_dict)

		original_text_field: TextField = original_instances[0][  # type: ignore
			input_field_to_attack
		]
		original_tokens = deepcopy(original_text_field.tokens)

		final_tokens = []
		final_outputs = []
		for instance in original_instances:
			tokens, outputs = self.attack_instance(
				instance=instance,
				inputs=inputs,
				input_field_to_attack=input_field_to_attack,
				grad_input_field=grad_input_field,
				ignore_tokens=ignore_tokens,
				target=target,
			)
			final_tokens.append(tokens)
			final_outputs.append(outputs)

		return sanitize(
			{"final": final_tokens, "original": original_tokens, "outputs": final_outputs}
		)

	def attack_instance(
		self,
		instance: Instance,
		inputs: JsonDict,
		input_field_to_attack: str = "tokens",
		grad_input_field: str = "grad_input_1",
		ignore_tokens: List[str] = None,
		target: JsonDict = None,
	) -> Tuple[List[Token], JsonDict]:
		if self.embedding_matrix is None:
			self.initialize()

		ignore_tokens = DEFAULT_IGNORE_TOKENS if ignore_tokens is None else ignore_tokens

		sign = -1 if target is None else 1

		fields_to_compare = utils.get_fields_to_compare(inputs, instance, input_field_to_attack)

		text_field: TextField = instance[input_field_to_attack]  # type: ignore

		grads, outputs = self.predictor.get_gradients([instance])

		flipped: List[int] = []
		for index, token in enumerate(text_field.tokens):
			if token.text in ignore_tokens:
				flipped.append(index)
		if "clusters" in outputs:
			for cluster in outputs["clusters"]:
				for mention in cluster:
					for index in range(mention[0], mention[1] + 1):
						flipped.append(index)

		while True:
			grad = grads[grad_input_field][0]
			grads_magnitude = [g.dot(g) for g in grad]

			for index in flipped:
				grads_magnitude[index] = -1

			index_of_token_to_flip = numpy.argmax(grads_magnitude)
			if grads_magnitude[index_of_token_to_flip] == -1:
				break
			flipped.append(index_of_token_to_flip)

			text_field_tensors = text_field.as_tensor(text_field.get_padding_lengths())
			input_tokens = util.get_token_ids_from_text_field_tensors(text_field_tensors)
			original_id_of_token_to_flip = input_tokens[index_of_token_to_flip]

			new_id = self._first_order_taylor(
				grad[index_of_token_to_flip], original_id_of_token_to_flip, sign
			)

			new_token = Token(self.vocab._index_to_token[self.namespace][new_id])  # type: ignore
			text_field.tokens[index_of_token_to_flip] = new_token
			instance.indexed = False

			grads, outputs = self.predictor.get_gradients([instance])  # predictions
			for key, output in outputs.items():
				if isinstance(output, torch.Tensor):
					outputs[key] = output.detach().cpu().numpy().squeeze()
				elif isinstance(output, list):
					outputs[key] = output[0]

			labeled_instance = self.predictor.predictions_to_labeled_instances(instance, outputs)[0]

			has_changed = utils.instance_has_changed(labeled_instance, fields_to_compare)
			if target is None and has_changed:
				break
			if target is not None and not has_changed:
				break
		return text_field.tokens, outputs

	def _first_order_taylor(self, grad: numpy.ndarray, token_idx: torch.Tensor, sign: int) -> int:
		grad = util.move_to_device(torch.from_numpy(grad), self.cuda_device)
		if token_idx.size() != ():
			raise NotImplementedError(
				"You are using a character-level indexer with no other indexers. This case is not "
				"currently supported for hotflip. If you would really like to see us support "
				"this, please open an issue on github."
			)
		if token_idx >= self.embedding_matrix.size(0):
			inputs = self._make_embedder_input([self.vocab.get_token_from_index(token_idx.item())])
			word_embedding = self.embedding_layer(inputs)[0]
		else:
			word_embedding = torch.nn.functional.embedding(
				util.move_to_device(torch.LongTensor([token_idx]), self.cuda_device),
				self.embedding_matrix,
			)
		word_embedding = word_embedding.detach().unsqueeze(0)
		grad = grad.unsqueeze(0).unsqueeze(0)
		new_embed_dot_grad = torch.einsum("bij,kj->bik", (grad, self.embedding_matrix))
		prev_embed_dot_grad = torch.einsum("bij,bij->bi", (grad, word_embedding)).unsqueeze(-1)
		neg_dir_dot_grad = sign * (prev_embed_dot_grad - new_embed_dot_grad)
		neg_dir_dot_grad = neg_dir_dot_grad.detach().cpu().numpy()
		neg_dir_dot_grad[:, :, self.invalid_replacement_indices] = -numpy.inf
		best_at_each_step = neg_dir_dot_grad.argmax(2)
		return best_at_each_step[0].data[0]

import os
import threading
import time
from functools import wraps

import torch
import torch.nn as nn
import torch.distributed.autograd as dist_autograd
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.optim as optim
from torch.distributed.optim import DistributedOptimizer
from torch.distributed.rpc import RRef

from torchvision.models.resnet import Bottleneck





num_classes = 1000


def conv1x1(in_planes, out_planes, stride=1):
	return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)

class ResNetBase(nn.Module):
	def __init__(self, block, inplanes, num_classes=1000,
				 groups=1, width_per_group=64, norm_layer=None):
		super(ResNetBase, self).__init__()

		self._lock = threading.Lock()
		self._block = block
		self._norm_layer = nn.BatchNorm2d
		self.inplanes = inplanes
		self.dilation = 1
		self.groups = groups
		self.base_width = width_per_group

	def _make_layer(self, planes, blocks, stride=1):
		norm_layer = self._norm_layer
		downsample = None
		previous_dilation = self.dilation
		if stride != 1 or self.inplanes != planes * self._block.expansion:
			downsample = nn.Sequential(
				conv1x1(self.inplanes, planes * self._block.expansion, stride),
				norm_layer(planes * self._block.expansion),
			)

		layers = []
		layers.append(self._block(self.inplanes, planes, stride, downsample, self.groups,
								  self.base_width, previous_dilation, norm_layer))
		self.inplanes = planes * self._block.expansion
		for _ in range(1, blocks):
			layers.append(self._block(self.inplanes, planes, groups=self.groups,
									  base_width=self.base_width, dilation=self.dilation,
									  norm_layer=norm_layer))

		return nn.Sequential(*layers)

	def parameter_rrefs(self):
		return [RRef(p) for p in self.parameters()]


class ResNetShard1(ResNetBase):
	def __init__(self, device, *args, **kwargs):
		super(ResNetShard1, self).__init__(
			Bottleneck, 64, num_classes=num_classes, *args, **kwargs)

		self.device = device
		self.seq = nn.Sequential(
			nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False),
			self._norm_layer(self.inplanes),
			nn.ReLU(inplace=True),
			nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
			self._make_layer(64, 3),
			self._make_layer(128, 4, stride=2)
		).to(self.device)

		for m in self.modules():
			if isinstance(m, nn.Conv2d):
				nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
			elif isinstance(m, nn.BatchNorm2d):
				nn.init.ones_(m.weight)
				nn.init.zeros_(m.bias)

	def forward(self, x_rref):
		x = x_rref.to_here().to(self.device)
		with self._lock:
			out =  self.seq(x)
		return out.cpu()


class ResNetShard2(ResNetBase):
	def __init__(self, device, *args, **kwargs):
		super(ResNetShard2, self).__init__(
			Bottleneck, 512, num_classes=num_classes, *args, **kwargs)

		self.device = device
		self.seq = nn.Sequential(
			self._make_layer(256, 6, stride=2),
			self._make_layer(512, 3, stride=2),
			nn.AdaptiveAvgPool2d((1, 1)),
		).to(self.device)

		self.fc =  nn.Linear(512 * self._block.expansion, num_classes).to(self.device)

	def forward(self, x_rref):
		x = x_rref.to_here().to(self.device)
		with self._lock:
			out = self.fc(torch.flatten(self.seq(x), 1))
		return out.cpu()


class DistResNet50(nn.Module):
	def __init__(self, split_size, workers, *args, **kwargs):
		super(DistResNet50, self).__init__()

		self.split_size = split_size

		self.p1_rref = rpc.remote(
			workers[0],
			ResNetShard1,
			args = ("cuda:0",) + args,
			kwargs = kwargs
		)

		self.p2_rref = rpc.remote(
			workers[1],
			ResNetShard2,
			args = ("cuda:1",) + args,
			kwargs = kwargs
		)

	def forward(self, xs):
		out_futures = []
		for x in iter(xs.split(self.split_size, dim=0)):
			x_rref = RRef(x)
			y_rref = self.p1_rref.remote().forward(x_rref)
			z_fut = self.p2_rref.rpc_async().forward(y_rref)
			out_futures.append(z_fut)

		return torch.cat(torch.futures.wait_all(out_futures))

	def parameter_rrefs(self):
		remote_params = []
		remote_params.extend(self.p1_rref.remote().parameter_rrefs().to_here())
		remote_params.extend(self.p2_rref.remote().parameter_rrefs().to_here())
		return remote_params



num_batches = 3
batch_size = 120
image_w = 128
image_h = 128


def run_master(split_size):

	model = DistResNet50(split_size, ["worker1", "worker2"])
	loss_fn = nn.MSELoss()
	opt = DistributedOptimizer(
		optim.SGD,
		model.parameter_rrefs(),
		lr=0.05,
	)

	one_hot_indices = torch.LongTensor(batch_size) \
						   .random_(0, num_classes) \
						   .view(batch_size, 1)

	for i in range(num_batches):
		print(f"Processing batch {i}")
		inputs = torch.randn(batch_size, 3, image_w, image_h)
		labels = torch.zeros(batch_size, num_classes) \
					  .scatter_(1, one_hot_indices, 1)

		with dist_autograd.context() as context_id:
			outputs = model(inputs)
			dist_autograd.backward(context_id, [loss_fn(outputs, labels)])
			opt.step(context_id)


def run_worker(rank, world_size, num_split):
	os.environ['MASTER_ADDR'] = 'localhost'
	os.environ['MASTER_PORT'] = '29500'

	options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=256, rpc_timeout=300)

	if rank == 0:
		rpc.init_rpc(
			"master",
			rank=rank,
			world_size=world_size,
			rpc_backend_options=options
		)
		run_master(num_split)
	else:
		rpc.init_rpc(
			f"worker{rank}",
			rank=rank,
			world_size=world_size,
			rpc_backend_options=options
		)
		pass

	rpc.shutdown()


if __name__=="__main__":
	world_size = 3
	for num_split in [1, 2, 4, 8]:
		tik = time.time()
		mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True)
		tok = time.time()
		print(f"number of splits = {num_split}, execution time = {tok - tik}")

import logging
from logging import Filter
import os
from os import PathLike
from typing import Union

import sys


class AllenNlpLogger(logging.Logger):

	def __init__(self, name):
		super().__init__(name)
		self._seen_msgs = set()

	def debug_once(self, msg, *args, **kwargs):
		if msg not in self._seen_msgs:
			self.debug(msg, *args, **kwargs)
			self._seen_msgs.add(msg)

	def info_once(self, msg, *args, **kwargs):
		if msg not in self._seen_msgs:
			self.info(msg, *args, **kwargs)
			self._seen_msgs.add(msg)

	def warning_once(self, msg, *args, **kwargs):
		if msg not in self._seen_msgs:
			self.warning(msg, *args, **kwargs)
			self._seen_msgs.add(msg)

	def error_once(self, msg, *args, **kwargs):
		if msg not in self._seen_msgs:
			self.error(msg, *args, **kwargs)
			self._seen_msgs.add(msg)

	def critical_once(self, msg, *args, **kwargs):
		if msg not in self._seen_msgs:
			self.critical(msg, *args, **kwargs)
			self._seen_msgs.add(msg)


logging.setLoggerClass(AllenNlpLogger)
logger = logging.getLogger(__name__)


FILE_FRIENDLY_LOGGING: bool = False


class ErrorFilter(Filter):

	def filter(self, record):
		return record.levelno < logging.ERROR


def prepare_global_logging(
	serialization_dir: Union[str, PathLike],
	rank: int = 0,
	world_size: int = 1,
) -> None:
	root_logger = logging.getLogger()

	if world_size == 1:
		log_file = os.path.join(serialization_dir, "out.log")
		formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(name)s - %(message)s")
	else:
		log_file = os.path.join(serialization_dir, f"out_worker{rank}.log")
		formatter = logging.Formatter(
			f"{rank} | %(asctime)s - %(levelname)s - %(name)s - %(message)s"
		)
	file_handler: logging.Handler = logging.FileHandler(log_file)
	stdout_handler: logging.Handler = logging.StreamHandler(sys.stdout)
	stderr_handler: logging.Handler = logging.StreamHandler(sys.stderr)

	handler: logging.Handler
	for handler in [file_handler, stdout_handler, stderr_handler]:
		handler.setFormatter(formatter)

	root_logger.handlers.clear()

	if os.environ.get("ALLENNLP_DEBUG"):
		LEVEL = logging.DEBUG
	else:
		level_name = os.environ.get("ALLENNLP_LOG_LEVEL", "INFO")
		LEVEL = logging._nameToLevel.get(level_name, logging.INFO)

	file_handler.setLevel(LEVEL)
	stdout_handler.setLevel(LEVEL)
	stdout_handler.addFilter(ErrorFilter())  # Make sure errors only go to stderr
	stderr_handler.setLevel(logging.ERROR)
	root_logger.setLevel(LEVEL)

	root_logger.addHandler(file_handler)
	if rank == 0:
		root_logger.addHandler(stdout_handler)
		root_logger.addHandler(stderr_handler)

	from allennlp.common.util import SigTermReceived

	def excepthook(exctype, value, traceback):
		if issubclass(exctype, (KeyboardInterrupt, SigTermReceived)):
			sys.__excepthook__(exctype, value, traceback)
			return
		root_logger.critical("Uncaught exception", exc_info=(exctype, value, traceback))

	sys.excepthook = excepthook

	from allennlp.common.tqdm import logger as tqdm_logger

	tqdm_logger.handlers.clear()
	tqdm_logger.addHandler(file_handler)

import warnings
from typing import List, Optional, Tuple, Any

import numpy
import torch
from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence

from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path
from allennlp.modules.encoder_base import _EncoderBase
from allennlp.modules.lstm_cell_with_projection import LstmCellWithProjection

with warnings.catch_warnings():
	warnings.filterwarnings("ignore", category=FutureWarning)
	import h5py


class ElmoLstm(_EncoderBase):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		cell_size: int,
		num_layers: int,
		requires_grad: bool = False,
		recurrent_dropout_probability: float = 0.0,
		memory_cell_clip_value: Optional[float] = None,
		state_projection_clip_value: Optional[float] = None,
	) -> None:
		super().__init__(stateful=True)

		self.input_size = input_size
		self.hidden_size = hidden_size
		self.num_layers = num_layers
		self.cell_size = cell_size
		self.requires_grad = requires_grad

		forward_layers = []
		backward_layers = []

		lstm_input_size = input_size
		go_forward = True
		for layer_index in range(num_layers):
			forward_layer = LstmCellWithProjection(
				lstm_input_size,
				hidden_size,
				cell_size,
				go_forward,
				recurrent_dropout_probability,
				memory_cell_clip_value,
				state_projection_clip_value,
			)
			backward_layer = LstmCellWithProjection(
				lstm_input_size,
				hidden_size,
				cell_size,
				not go_forward,
				recurrent_dropout_probability,
				memory_cell_clip_value,
				state_projection_clip_value,
			)
			lstm_input_size = hidden_size

			self.add_module("forward_layer_{}".format(layer_index), forward_layer)
			self.add_module("backward_layer_{}".format(layer_index), backward_layer)
			forward_layers.append(forward_layer)
			backward_layers.append(backward_layer)
		self.forward_layers = forward_layers
		self.backward_layers = backward_layers

	def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:
		batch_size, total_sequence_length = mask.size()
		stacked_sequence_output, final_states, restoration_indices = self.sort_and_run_forward(
			self._lstm_forward, inputs, mask
		)

		num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()
		if num_valid < batch_size:
			zeros = stacked_sequence_output.new_zeros(
				num_layers, batch_size - num_valid, returned_timesteps, encoder_dim
			)
			stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)

			new_states = []
			for state in final_states:
				state_dim = state.size(-1)
				zeros = state.new_zeros(num_layers, batch_size - num_valid, state_dim)
				new_states.append(torch.cat([state, zeros], 1))
			final_states = new_states

		sequence_length_difference = total_sequence_length - returned_timesteps
		if sequence_length_difference > 0:
			zeros = stacked_sequence_output.new_zeros(
				num_layers,
				batch_size,
				sequence_length_difference,
				stacked_sequence_output[0].size(-1),
			)
			stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)

		self._update_states(final_states, restoration_indices)

		return stacked_sequence_output.index_select(1, restoration_indices)

	def _lstm_forward(
		self,
		inputs: PackedSequence,
		initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
	) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
		if initial_state is None:
			hidden_states: List[Optional[Tuple[torch.Tensor, torch.Tensor]]] = [None] * len(
				self.forward_layers
			)
		elif initial_state[0].size()[0] != len(self.forward_layers):
			raise ConfigurationError(
				"Initial states were passed to forward() but the number of "
				"initial states does not match the number of layers."
			)
		else:
			hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))

		inputs, batch_lengths = pad_packed_sequence(inputs, batch_first=True)
		forward_output_sequence = inputs
		backward_output_sequence = inputs

		final_states = []
		sequence_outputs = []
		for layer_index, state in enumerate(hidden_states):
			forward_layer = getattr(self, "forward_layer_{}".format(layer_index))
			backward_layer = getattr(self, "backward_layer_{}".format(layer_index))

			forward_cache = forward_output_sequence
			backward_cache = backward_output_sequence

			forward_state: Optional[Tuple[Any, Any]] = None
			backward_state: Optional[Tuple[Any, Any]] = None
			if state is not None:
				forward_hidden_state, backward_hidden_state = state[0].split(self.hidden_size, 2)
				forward_memory_state, backward_memory_state = state[1].split(self.cell_size, 2)
				forward_state = (forward_hidden_state, forward_memory_state)
				backward_state = (backward_hidden_state, backward_memory_state)

			forward_output_sequence, forward_state = forward_layer(
				forward_output_sequence, batch_lengths, forward_state
			)
			backward_output_sequence, backward_state = backward_layer(
				backward_output_sequence, batch_lengths, backward_state
			)
			if layer_index != 0:
				forward_output_sequence += forward_cache
				backward_output_sequence += backward_cache

			sequence_outputs.append(
				torch.cat([forward_output_sequence, backward_output_sequence], -1)
			)
			final_states.append(
				(
					torch.cat([forward_state[0], backward_state[0]], -1),  # type: ignore
					torch.cat([forward_state[1], backward_state[1]], -1),  # type: ignore
				)
			)

		stacked_sequence_outputs: torch.FloatTensor = torch.stack(sequence_outputs)
		final_hidden_states, final_memory_states = zip(*final_states)
		final_state_tuple: Tuple[torch.FloatTensor, torch.FloatTensor] = (
			torch.cat(final_hidden_states, 0),
			torch.cat(final_memory_states, 0),
		)
		return stacked_sequence_outputs, final_state_tuple

	def load_weights(self, weight_file: str) -> None:
		requires_grad = self.requires_grad

		with h5py.File(cached_path(weight_file), "r") as fin:
			for i_layer, lstms in enumerate(zip(self.forward_layers, self.backward_layers)):
				for j_direction, lstm in enumerate(lstms):
					cell_size = lstm.cell_size

					dataset = fin["RNN_%s" % j_direction]["RNN"]["MultiRNNCell"][
						"Cell%s" % i_layer
					]["LSTMCell"]

					tf_weights = numpy.transpose(dataset["W_0"][...])
					torch_weights = tf_weights.copy()

					input_size = lstm.input_size
					input_weights = torch_weights[:, :input_size]
					recurrent_weights = torch_weights[:, input_size:]
					tf_input_weights = tf_weights[:, :input_size]
					tf_recurrent_weights = tf_weights[:, input_size:]

					for torch_w, tf_w in [
						[input_weights, tf_input_weights],
						[recurrent_weights, tf_recurrent_weights],
					]:
						torch_w[(1 * cell_size) : (2 * cell_size), :] = tf_w[
							(2 * cell_size) : (3 * cell_size), :
						]
						torch_w[(2 * cell_size) : (3 * cell_size), :] = tf_w[
							(1 * cell_size) : (2 * cell_size), :
						]

					lstm.input_linearity.weight.data.copy_(torch.FloatTensor(input_weights))
					lstm.state_linearity.weight.data.copy_(torch.FloatTensor(recurrent_weights))
					lstm.input_linearity.weight.requires_grad = requires_grad
					lstm.state_linearity.weight.requires_grad = requires_grad

					tf_bias = dataset["B"][...]
					tf_bias[(2 * cell_size) : (3 * cell_size)] += 1
					torch_bias = tf_bias.copy()
					torch_bias[(1 * cell_size) : (2 * cell_size)] = tf_bias[
						(2 * cell_size) : (3 * cell_size)
					]
					torch_bias[(2 * cell_size) : (3 * cell_size)] = tf_bias[
						(1 * cell_size) : (2 * cell_size)
					]
					lstm.state_linearity.bias.data.copy_(torch.FloatTensor(torch_bias))
					lstm.state_linearity.bias.requires_grad = requires_grad

					proj_weights = numpy.transpose(dataset["W_P_0"][...])
					lstm.state_projection.weight.data.copy_(torch.FloatTensor(proj_weights))
					lstm.state_projection.weight.requires_grad = requires_grad


import logging
import os
from os import PathLike
import re
from typing import Dict, List, Set, Type, Optional, Union

import numpy
import torch

from allennlp.common.checks import ConfigurationError
from allennlp.common.params import Params, remove_keys_from_params
from allennlp.common.registrable import Registrable
from allennlp.data import Instance, Vocabulary
from allennlp.data.batch import Batch
from allennlp.nn import util
from allennlp.nn.module import Module
from allennlp.nn.parallel import DdpAccelerator
from allennlp.nn.regularizers import RegularizerApplicator

logger = logging.getLogger(__name__)

_DEFAULT_WEIGHTS = "best.th"


class Model(Module, Registrable):

	_warn_for_unseparable_batches: Set[str] = set()
	default_predictor: Optional[str] = None

	def __init__(
		self,
		vocab: Vocabulary,
		regularizer: RegularizerApplicator = None,
		serialization_dir: Optional[str] = None,
		ddp_accelerator: Optional[DdpAccelerator] = None,
	) -> None:
		super().__init__()
		self.vocab = vocab
		self._regularizer = regularizer
		self.serialization_dir = serialization_dir
		self.ddp_accelerator = ddp_accelerator

	def get_regularization_penalty(self) -> Optional[torch.Tensor]:
		if self._regularizer is None:
			regularization_penalty = None
		else:
			try:
				regularization_penalty = self._regularizer(self)
				if isinstance(regularization_penalty, float):
					assert regularization_penalty == 0.0
					regularization_penalty = torch.tensor(regularization_penalty)
			except AssertionError:
				raise RuntimeError("The regularizer cannot be a non-zero float.")
		return regularization_penalty

	def get_parameters_for_histogram_logging(self) -> List[str]:
		return [name for name, _ in self.named_parameters()]

	def get_parameters_for_histogram_tensorboard_logging(self) -> List[str]:
		import warnings

		warnings.warn(
			"'Model.get_parameters_for_histogram_tensorboard_logging' is deprecated, please use "
			"'Model.get_parameters_for_histogram_logging' instead.",
			DeprecationWarning,
		)
		return self.get_parameters_for_histogram_logging()

	def forward(self, *inputs) -> Dict[str, torch.Tensor]:
		raise NotImplementedError

	def forward_on_instance(self, instance: Instance) -> Dict[str, numpy.ndarray]:
		return self.forward_on_instances([instance])[0]

	def forward_on_instances(self, instances: List[Instance]) -> List[Dict[str, numpy.ndarray]]:
		batch_size = len(instances)
		with torch.no_grad():
			cuda_device = self._get_prediction_device()
			dataset = Batch(instances)
			dataset.index_instances(self.vocab)
			model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)
			outputs = self.make_output_human_readable(self(**model_input))

			instance_separated_output: List[Dict[str, numpy.ndarray]] = [
				{} for _ in dataset.instances
			]
			for name, output in list(outputs.items()):
				if isinstance(output, torch.Tensor):
					if output.dim() == 0:
						output = output.unsqueeze(0)

					if output.size(0) != batch_size:
						self._maybe_warn_for_unseparable_batches(name)
						continue
					output = output.detach().cpu().numpy()
				elif len(output) != batch_size:
					self._maybe_warn_for_unseparable_batches(name)
					continue
				for instance_output, batch_element in zip(instance_separated_output, output):
					instance_output[name] = batch_element
			return instance_separated_output

	def make_output_human_readable(
		self, output_dict: Dict[str, torch.Tensor]
	) -> Dict[str, torch.Tensor]:

		return output_dict

	def get_metrics(self, reset: bool = False) -> Dict[str, float]:

		return {}

	def _get_prediction_device(self) -> int:
		devices = {util.get_device_of(param) for param in self.parameters()}

		if len(devices) > 1:
			devices_string = ", ".join(str(x) for x in devices)
			raise ConfigurationError(f"Parameters have mismatching cuda_devices: {devices_string}")
		elif len(devices) == 1:
			return devices.pop()
		else:
			return -1

	def _maybe_warn_for_unseparable_batches(self, output_key: str):
		if output_key not in self._warn_for_unseparable_batches:
			logger.warning(
				f"Encountered the {output_key} key in the model's return dictionary which "
				"couldn't be split by the batch size. Key will be ignored."
			)
			self._warn_for_unseparable_batches.add(output_key)

	@classmethod
	def _load(
		cls,
		config: Params,
		serialization_dir: Union[str, PathLike],
		weights_file: Optional[Union[str, PathLike]] = None,
		cuda_device: int = -1,
	) -> "Model":
		weights_file = weights_file or os.path.join(serialization_dir, _DEFAULT_WEIGHTS)

		vocab_dir = os.path.join(serialization_dir, "vocabulary")
		vocab_params = config.get("vocabulary", Params({}))
		vocab_choice = vocab_params.pop_choice("type", Vocabulary.list_available(), True)
		vocab_class, _ = Vocabulary.resolve_class_name(vocab_choice)
		vocab = vocab_class.from_files(
			vocab_dir, vocab_params.get("padding_token"), vocab_params.get("oov_token")
		)

		model_params = config.get("model")

		remove_keys_from_params(model_params)
		model = Model.from_params(
			vocab=vocab, params=model_params, serialization_dir=serialization_dir
		)

		if cuda_device >= 0:
			model.cuda(cuda_device)
		else:
			model.cpu()

		model.extend_embedder_vocab()

		model_state = util.read_state_dict(weights_file, cuda_device=cuda_device)
		missing_keys, unexpected_keys = model.load_state_dict(model_state, strict=False)


		def filter_out_authorized_missing_keys(module, prefix=""):
			nonlocal missing_keys
			for pat in getattr(module.__class__, "authorized_missing_keys", None) or []:
				missing_keys = [
					k
					for k in missing_keys
					if k.startswith(prefix) and re.search(pat[len(prefix) :], k) is None
				]
			for name, child in module._modules.items():
				if child is not None:
					filter_out_authorized_missing_keys(child, prefix + name + ".")

		filter_out_authorized_missing_keys(model)

		if unexpected_keys or missing_keys:
			raise RuntimeError(
				f"Error loading state dict for {model.__class__.__name__}\n\t"
				f"Missing keys: {missing_keys}\n\t"
				f"Unexpected keys: {unexpected_keys}"
			)

		return model

	@classmethod
	def load(
		cls,
		config: Params,
		serialization_dir: Union[str, PathLike],
		weights_file: Optional[Union[str, PathLike]] = None,
		cuda_device: int = -1,
	) -> "Model":

		model_type = (
			config["model"] if isinstance(config["model"], str) else config["model"]["type"]
		)


		model_class: Type[Model] = cls.by_name(model_type)  # type: ignore
		if not isinstance(model_class, type):
			model_class = Model
		return model_class._load(config, serialization_dir, weights_file, cuda_device)

	def extend_embedder_vocab(self, embedding_sources_mapping: Dict[str, str] = None) -> None:
		embedding_sources_mapping = embedding_sources_mapping or {}
		for model_path, module in self.named_modules():
			if hasattr(module, "extend_vocab"):
				pretrained_file = embedding_sources_mapping.get(model_path)
				module.extend_vocab(
					self.vocab,
					extension_pretrained_file=pretrained_file,
					model_path=model_path,
				)

	@classmethod
	def from_archive(cls, archive_file: str, vocab: Vocabulary = None) -> "Model":
		from allennlp.models.archival import load_archive  # here to avoid circular imports

		model = load_archive(archive_file).model
		if vocab:
			model.vocab.extend_from_vocab(vocab)
			model.extend_embedder_vocab()
		return model


Model.register("from_archive", constructor="from_archive")(Model)


def remove_weights_related_keys_from_params(
	params: Params, keys: List[str] = ["pretrained_file", "initializer"]
):
	remove_keys_from_params(params, keys)


def remove_pretrained_embedding_params(params: Params):
	remove_keys_from_params(params, ["pretrained_file"])

import torch.nn as nn
import torch
import math


class PositionalEmbedding(nn.Module):

	def __init__(self, d_model, max_len=512):
		super().__init__()

		pe = torch.zeros(max_len, d_model).float()
		pe.require_grad = False

		position = torch.arange(0, max_len).float().unsqueeze(1)
		div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()

		pe[:, 0::2] = torch.sin(position * div_term)
		pe[:, 1::2] = torch.cos(position * div_term)

		pe = pe.unsqueeze(0)
		self.register_buffer('pe', pe)

	def forward(self, x):
		return self.pe[:, :x.size(1)]



import functools
import json
import logging
import os
import re
import sys
import warnings
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union

import datasets
import evaluate
import numpy as np
import torch
from datasets import DatasetDict, load_dataset
from safetensors.torch import save_file as safe_save_file

import transformers
from transformers import (
	AutoConfig,
	AutoFeatureExtractor,
	AutoModelForCTC,
	AutoProcessor,
	AutoTokenizer,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	Wav2Vec2Processor,
	set_seed,
)
from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE
from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.18.0", "To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt")


logger = logging.getLogger(__name__)


def list_field(default=None, metadata=None):
	return field(default_factory=lambda: default, metadata=metadata)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	tokenizer_name_or_path: Optional[str] = field(
		default=None,
		metadata={"help": "Path to pretrained tokenizer or tokenizer identifier from huggingface.co/models"},
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	final_dropout: float = field(
		default=0.0,
		metadata={"help": "The dropout probability for the final projection layer."},
	)
	mask_time_prob: float = field(
		default=0.05,
		metadata={
			"help": (
				"Probability of each feature vector along the time axis to be chosen as the start of the vector "
				"span to be masked. Approximately ``mask_time_prob * sequence_length // mask_time_length`` feature "
				"vectors will be masked along the time axis."
			)
		},
	)
	mask_time_length: int = field(
		default=10,
		metadata={"help": "Length of vector span to mask along the time axis."},
	)
	mask_feature_prob: float = field(
		default=0.0,
		metadata={
			"help": (
				"Probability of each feature vector along the feature axis to be chosen as the start of the vectorspan"
				" to be masked. Approximately ``mask_feature_prob * sequence_length // mask_feature_length`` feature"
				" bins will be masked along the time axis."
			)
		},
	)
	mask_feature_length: int = field(
		default=10,
		metadata={"help": "Length of vector span to mask along the feature axis."},
	)
	layerdrop: float = field(default=0.0, metadata={"help": "The LayerDrop probability."})
	ctc_loss_reduction: Optional[str] = field(
		default="mean", metadata={"help": "The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'."}
	)
	adapter_attn_dim: int = field(
		default=16,
		metadata={
			"help": "The hidden dimension of the adapter layers that will be randomly initialized and trained. The higher the dimension, the more capacity is given to the adapter weights. Note that only the adapter weights are fine-tuned."
		},
	)


@dataclass
class DataTrainingArguments:

	dataset_name: str = field(
		metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	target_language: Optional[str] = field(
		metadata={
			"help": (
				"The target language on which the adapter attention layers"
				" should be trained on in ISO 693-3 code, e.g. `tur` for Turkish"
				" Wav2Vec2's MMS ISO codes can be looked up here: https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html"
				" If you are not training the adapter layers on a language, simply choose"
				" another accronym that fits your data."
			)
		},
	)
	dataset_config_name: str = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_split_name: str = field(
		default="train+validation",
		metadata={
			"help": (
				"The name of the training data set split to use (via the datasets library). Defaults to "
				"'train+validation'"
			)
		},
	)
	eval_split_name: str = field(
		default="test",
		metadata={
			"help": "The name of the evaluation data set split to use (via the datasets library). Defaults to 'test'"
		},
	)
	audio_column_name: str = field(
		default="audio",
		metadata={"help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
	)
	text_column_name: str = field(
		default="text",
		metadata={"help": "The name of the dataset column containing the text data. Defaults to 'text'"},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of validation examples to this "
				"value if set."
			)
		},
	)
	chars_to_ignore: Optional[List[str]] = list_field(
		default=None,
		metadata={"help": "A list of characters to remove from the transcripts."},
	)
	eval_metrics: List[str] = list_field(
		default=["wer"],
		metadata={"help": "A list of metrics the model should be evaluated on. E.g. `'wer cer'`"},
	)
	max_duration_in_seconds: float = field(
		default=20.0,
		metadata={
			"help": (
				"Filter audio files that are longer than `max_duration_in_seconds` seconds to"
				" 'max_duration_in_seconds`"
			)
		},
	)
	min_duration_in_seconds: float = field(
		default=0.0, metadata={"help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"}
	)
	preprocessing_only: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to only do data preprocessing and skip training. This is especially useful when data"
				" preprocessing errors out in distributed training due to timeout. In this case, one should run the"
				" preprocessing in a non-distributed setup with `preprocessing_only=True` so that the cached datasets"
				" can consequently be loaded in distributed training"
			)
		},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	unk_token: str = field(
		default="[UNK]",
		metadata={"help": "The unk token for the tokenizer"},
	)
	pad_token: str = field(
		default="[PAD]",
		metadata={"help": "The padding token for the tokenizer"},
	)
	word_delimiter_token: str = field(
		default="|",
		metadata={"help": "The word delimiter token for the tokenizer"},
	)
	overwrite_lang_vocab: bool = field(
		default=False,
		metadata={"help": ("If :obj:`True`, will overwrite existing `target_language` vocabulary of tokenizer.")},
	)


@dataclass
class DataCollatorCTCWithPadding:

	processor: AutoProcessor
	padding: Union[bool, str] = "longest"
	pad_to_multiple_of: Optional[int] = None
	pad_to_multiple_of_labels: Optional[int] = None

	def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
		input_features = [{"input_values": feature["input_values"]} for feature in features]
		label_features = [{"input_ids": feature["labels"]} for feature in features]

		batch = self.processor.pad(
			input_features,
			padding=self.padding,
			pad_to_multiple_of=self.pad_to_multiple_of,
			return_tensors="pt",
		)

		labels_batch = self.processor.pad(
			labels=label_features,
			padding=self.padding,
			pad_to_multiple_of=self.pad_to_multiple_of_labels,
			return_tensors="pt",
		)

		labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

		batch["labels"] = labels
		if "attention_mask" in batch:
			batch["attention_mask"] = batch["attention_mask"].to(torch.long)

		return batch


def create_vocabulary_from_data(
	datasets: DatasetDict,
	word_delimiter_token: Optional[str] = None,
	unk_token: Optional[str] = None,
	pad_token: Optional[str] = None,
):
	def extract_all_chars(batch):
		all_text = " ".join(batch["target_text"])
		vocab = list(set(all_text))
		return {"vocab": [vocab], "all_text": [all_text]}

	vocabs = datasets.map(
		extract_all_chars,
		batched=True,
		batch_size=-1,
		keep_in_memory=True,
		remove_columns=datasets["train"].column_names,
	)

	vocab_set = functools.reduce(
		lambda vocab_1, vocab_2: set(vocab_1["vocab"][0]) | set(vocab_2["vocab"][0]), vocabs.values()
	)

	vocab_dict = {v: k for k, v in enumerate(sorted(vocab_set))}

	if word_delimiter_token is not None:
		vocab_dict[word_delimiter_token] = vocab_dict[" "]
		del vocab_dict[" "]

	if unk_token is not None:
		vocab_dict[unk_token] = len(vocab_dict)

	if pad_token is not None:
		vocab_dict[pad_token] = len(vocab_dict)

	return vocab_dict


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if data_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if data_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		data_args.token = data_args.use_auth_token

	send_example_telemetry("run_speech_recognition_ctc_adapter", model_args, data_args)

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)
	logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	if is_main_process(training_args.local_rank):
		transformers.utils.logging.set_verbosity_info()
	logger.info("Training/evaluation parameters %s", training_args)

	set_seed(training_args.seed)

	raw_datasets = DatasetDict()

	if training_args.do_train:
		raw_datasets["train"] = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			split=data_args.train_split_name,
			token=data_args.token,
		)

		if data_args.audio_column_name not in raw_datasets["train"].column_names:
			raise ValueError(
				f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'."
				" Make sure to set `--audio_column_name` to the correct audio column - one of"
				f" {', '.join(raw_datasets['train'].column_names)}."
			)

		if data_args.text_column_name not in raw_datasets["train"].column_names:
			raise ValueError(
				f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
				"Make sure to set `--text_column_name` to the correct text column - one of "
				f"{', '.join(raw_datasets['train'].column_names)}."
			)

		if data_args.max_train_samples is not None:
			raw_datasets["train"] = raw_datasets["train"].select(range(data_args.max_train_samples))

	if training_args.do_eval:
		raw_datasets["eval"] = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			split=data_args.eval_split_name,
			token=data_args.token,
		)

		if data_args.max_eval_samples is not None:
			raw_datasets["eval"] = raw_datasets["eval"].select(range(data_args.max_eval_samples))

	chars_to_ignore_regex = (
		f'[{"".join(data_args.chars_to_ignore)}]' if data_args.chars_to_ignore is not None else None
	)
	text_column_name = data_args.text_column_name

	def remove_special_characters(batch):
		if chars_to_ignore_regex is not None:
			batch["target_text"] = re.sub(chars_to_ignore_regex, "", batch[text_column_name]).lower() + " "
		else:
			batch["target_text"] = batch[text_column_name].lower() + " "
		return batch

	with training_args.main_process_first(desc="dataset map special characters removal"):
		raw_datasets = raw_datasets.map(
			remove_special_characters,
			remove_columns=[text_column_name],
			desc="remove special characters from datasets",
		)

	word_delimiter_token = data_args.word_delimiter_token
	unk_token = data_args.unk_token
	pad_token = data_args.pad_token

	config = AutoConfig.from_pretrained(
		model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
	)

	tokenizer_name_or_path = model_args.tokenizer_name_or_path
	tokenizer_kwargs = {}

	vocab_dict = {}
	if tokenizer_name_or_path is not None:
		tokenizer = AutoTokenizer.from_pretrained(
			tokenizer_name_or_path,
			token=data_args.token,
			trust_remote_code=data_args.trust_remote_code,
		)
		vocab_dict = tokenizer.vocab.copy()
		if tokenizer.target_lang is None:
			raise ValueError("Make sure to load a multi-lingual tokenizer with a set target language.")

		if data_args.target_language in tokenizer.vocab and not data_args.overwrite_lang_vocab:
			logger.info(
				"Adapter language already exists."
				" Skipping vocabulary creating. If you want to create a new vocabulary"
				f" for {data_args.target_language} make sure to add '--overwrite_lang_vocab'"
			)
		else:
			tokenizer_name_or_path = None

	if tokenizer_name_or_path is None:
		tokenizer_name_or_path = training_args.output_dir

		vocab_file = os.path.join(tokenizer_name_or_path, "vocab.json")

		with training_args.main_process_first():
			if training_args.overwrite_output_dir and os.path.isfile(vocab_file):
				try:
					os.remove(vocab_file)
				except OSError:
					pass

		with training_args.main_process_first(desc="dataset map vocabulary creation"):
			if not os.path.isfile(vocab_file):
				os.makedirs(tokenizer_name_or_path, exist_ok=True)
				lang_dict = create_vocabulary_from_data(
					raw_datasets,
					word_delimiter_token=word_delimiter_token,
					unk_token=unk_token,
					pad_token=pad_token,
				)

				if data_args.target_language is not None:
					vocab_dict[data_args.target_language] = lang_dict

				with open(vocab_file, "w") as file:
					json.dump(vocab_dict, file)

		tokenizer_kwargs = {
			"config": config if config.tokenizer_class is not None else None,
			"tokenizer_type": config.model_type if config.tokenizer_class is None else None,
			"unk_token": unk_token,
			"pad_token": pad_token,
			"word_delimiter_token": word_delimiter_token,
			"target_lang": data_args.target_language,
		}


	tokenizer = AutoTokenizer.from_pretrained(
		tokenizer_name_or_path,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
		**tokenizer_kwargs,
	)
	feature_extractor = AutoFeatureExtractor.from_pretrained(
		model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
	)

	config.update(
		{
			"final_dropout": model_args.final_dropout,
			"mask_time_prob": model_args.mask_time_prob,
			"mask_time_length": model_args.mask_time_length,
			"mask_feature_prob": model_args.mask_feature_prob,
			"mask_feature_length": model_args.mask_feature_length,
			"gradient_checkpointing": training_args.gradient_checkpointing,
			"layerdrop": model_args.layerdrop,
			"ctc_loss_reduction": model_args.ctc_loss_reduction,
			"pad_token_id": tokenizer.pad_token_id,
			"vocab_size": len(tokenizer),
			"adapter_attn_dim": model_args.adapter_attn_dim,
		}
	)

	model = AutoModelForCTC.from_pretrained(
		model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		config=config,
		token=data_args.token,
		trust_remote_code=data_args.trust_remote_code,
		ignore_mismatched_sizes=True,
	)

	if model.config.adapter_attn_dim is not None:
		model.init_adapter_layers()
		model.freeze_base_model()

		adapter_weights = model._get_adapters()
		for param in adapter_weights.values():
			param.requires_grad = True


	dataset_sampling_rate = next(iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
	if dataset_sampling_rate != feature_extractor.sampling_rate:
		raw_datasets = raw_datasets.cast_column(
			data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)
		)

	max_input_length = data_args.max_duration_in_seconds * feature_extractor.sampling_rate
	min_input_length = data_args.min_duration_in_seconds * feature_extractor.sampling_rate
	audio_column_name = data_args.audio_column_name
	num_workers = data_args.preprocessing_num_workers

	def prepare_dataset(batch):
		sample = batch[audio_column_name]

		inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])
		batch["input_values"] = inputs.input_values[0]
		batch["input_length"] = len(batch["input_values"])

		batch["labels"] = tokenizer(batch["target_text"]).input_ids
		return batch

	with training_args.main_process_first(desc="dataset map preprocessing"):
		vectorized_datasets = raw_datasets.map(
			prepare_dataset,
			remove_columns=next(iter(raw_datasets.values())).column_names,
			num_proc=num_workers,
			desc="preprocess datasets",
		)

		def is_audio_in_length_range(length):
			return length > min_input_length and length < max_input_length

		vectorized_datasets = vectorized_datasets.filter(
			is_audio_in_length_range,
			num_proc=num_workers,
			input_columns=["input_length"],
		)


	eval_metrics = {metric: evaluate.load(metric, cache_dir=model_args.cache_dir) for metric in data_args.eval_metrics}

	if data_args.preprocessing_only:
		logger.info(f"Data preprocessing finished. Files cached at {vectorized_datasets.cache_files}")
		return

	def compute_metrics(pred):
		pred_logits = pred.predictions
		pred_ids = np.argmax(pred_logits, axis=-1)

		pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id

		pred_str = tokenizer.batch_decode(pred_ids)
		label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)

		metrics = {k: v.compute(predictions=pred_str, references=label_str) for k, v in eval_metrics.items()}

		return metrics

	with training_args.main_process_first():
		if is_main_process(training_args.local_rank):
			feature_extractor.save_pretrained(training_args.output_dir)
			tokenizer.save_pretrained(training_args.output_dir)
			config.save_pretrained(training_args.output_dir)

	try:
		processor = AutoProcessor.from_pretrained(training_args.output_dir)
	except (OSError, KeyError):
		warnings.warn(
			"Loading a processor from a feature extractor config that does not"
			" include a `processor_class` attribute is deprecated and will be removed in v5. Please add the following "
			" attribute to your `preprocessor_config.json` file to suppress this warning: "
			" `'processor_class': 'Wav2Vec2Processor'`",
			FutureWarning,
		)
		processor = Wav2Vec2Processor.from_pretrained(training_args.output_dir)

	data_collator = DataCollatorCTCWithPadding(processor=processor)

	trainer = Trainer(
		model=model,
		data_collator=data_collator,
		args=training_args,
		compute_metrics=compute_metrics,
		train_dataset=vectorized_datasets["train"] if training_args.do_train else None,
		eval_dataset=vectorized_datasets["eval"] if training_args.do_eval else None,
		tokenizer=processor,
	)


	if training_args.do_train:
		if last_checkpoint is not None:
			checkpoint = last_checkpoint
		elif os.path.isdir(model_args.model_name_or_path):
			checkpoint = model_args.model_name_or_path
		else:
			checkpoint = None

		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()

		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples
			if data_args.max_train_samples is not None
			else len(vectorized_datasets["train"])
		)
		metrics["train_samples"] = min(max_train_samples, len(vectorized_datasets["train"]))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	results = {}
	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate()
		max_eval_samples = (
			data_args.max_eval_samples if data_args.max_eval_samples is not None else len(vectorized_datasets["eval"])
		)
		metrics["eval_samples"] = min(max_eval_samples, len(vectorized_datasets["eval"]))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	config_name = data_args.dataset_config_name if data_args.dataset_config_name is not None else "na"
	kwargs = {
		"finetuned_from": model_args.model_name_or_path,
		"tasks": "automatic-speech-recognition",
		"tags": ["automatic-speech-recognition", data_args.dataset_name, "mms"],
		"dataset_args": (
			f"Config: {config_name}, Training split: {data_args.train_split_name}, Eval split:"
			f" {data_args.eval_split_name}"
		),
		"dataset": f"{data_args.dataset_name.upper()} - {config_name.upper()}",
	}
	if "common_voice" in data_args.dataset_name:
		kwargs["language"] = config_name

	adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(data_args.target_language)
	adapter_file = os.path.join(training_args.output_dir, adapter_file)
	logger.info(f"Saving adapter weights under {adapter_file}...")
	safe_save_file(model._get_adapters(), adapter_file, metadata={"format": "pt"})

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)

	return results


if __name__ == "__main__":
	main()

from typing import List, Set, Tuple, Dict
import numpy

from allennlp.common.checks import ConfigurationError


def decode_mst(
	energy: numpy.ndarray, length: int, has_labels: bool = True
) -> Tuple[numpy.ndarray, numpy.ndarray]:
	if has_labels and energy.ndim != 3:
		raise ConfigurationError("The dimension of the energy array is not equal to 3.")
	elif not has_labels and energy.ndim != 2:
		raise ConfigurationError("The dimension of the energy array is not equal to 2.")
	input_shape = energy.shape
	max_length = input_shape[-1]

	if has_labels:
		energy = energy[:, :length, :length]
		label_id_matrix = energy.argmax(axis=0)
		energy = energy.max(axis=0)
	else:
		energy = energy[:length, :length]
		label_id_matrix = None
	original_score_matrix = energy
	score_matrix = numpy.array(original_score_matrix, copy=True)

	old_input = numpy.zeros([length, length], dtype=numpy.int32)
	old_output = numpy.zeros([length, length], dtype=numpy.int32)
	current_nodes = [True for _ in range(length)]
	representatives: List[Set[int]] = []

	for node1 in range(length):
		original_score_matrix[node1, node1] = 0.0
		score_matrix[node1, node1] = 0.0
		representatives.append({node1})

		for node2 in range(node1 + 1, length):
			old_input[node1, node2] = node1
			old_output[node1, node2] = node2

			old_input[node2, node1] = node2
			old_output[node2, node1] = node1

	final_edges: Dict[int, int] = {}

	chu_liu_edmonds(
		length, score_matrix, current_nodes, final_edges, old_input, old_output, representatives
	)

	heads = numpy.zeros([max_length], numpy.int32)
	if has_labels:
		head_type = numpy.ones([max_length], numpy.int32)
	else:
		head_type = None

	for child, parent in final_edges.items():
		heads[child] = parent
		if has_labels:
			head_type[child] = label_id_matrix[parent, child]

	return heads, head_type


def chu_liu_edmonds(
	length: int,
	score_matrix: numpy.ndarray,
	current_nodes: List[bool],
	final_edges: Dict[int, int],
	old_input: numpy.ndarray,
	old_output: numpy.ndarray,
	representatives: List[Set[int]],
):
	parents = [-1]
	for node1 in range(1, length):
		parents.append(0)
		if current_nodes[node1]:
			max_score = score_matrix[0, node1]
			for node2 in range(1, length):
				if node2 == node1 or not current_nodes[node2]:
					continue

				new_score = score_matrix[node2, node1]
				if new_score > max_score:
					max_score = new_score
					parents[node1] = node2

	has_cycle, cycle = _find_cycle(parents, length, current_nodes)
	if not has_cycle:
		final_edges[0] = -1
		for node in range(1, length):
			if not current_nodes[node]:
				continue

			parent = old_input[parents[node], node]
			child = old_output[parents[node], node]
			final_edges[child] = parent
		return

	cycle_weight = 0.0
	index = 0
	for node in cycle:
		index += 1
		cycle_weight += score_matrix[parents[node], node]

	cycle_representative = cycle[0]
	for node in range(length):
		if not current_nodes[node] or node in cycle:
			continue

		in_edge_weight = float("-inf")
		in_edge = -1
		out_edge_weight = float("-inf")
		out_edge = -1

		for node_in_cycle in cycle:
			if score_matrix[node_in_cycle, node] > in_edge_weight:
				in_edge_weight = score_matrix[node_in_cycle, node]
				in_edge = node_in_cycle

			score = (
				cycle_weight
				+ score_matrix[node, node_in_cycle]
				- score_matrix[parents[node_in_cycle], node_in_cycle]
			)

			if score > out_edge_weight:
				out_edge_weight = score
				out_edge = node_in_cycle

		score_matrix[cycle_representative, node] = in_edge_weight
		old_input[cycle_representative, node] = old_input[in_edge, node]
		old_output[cycle_representative, node] = old_output[in_edge, node]

		score_matrix[node, cycle_representative] = out_edge_weight
		old_output[node, cycle_representative] = old_output[node, out_edge]
		old_input[node, cycle_representative] = old_input[node, out_edge]

	considered_representatives: List[Set[int]] = []
	for i, node_in_cycle in enumerate(cycle):
		considered_representatives.append(set())
		if i > 0:
			current_nodes[node_in_cycle] = False

		for node in representatives[node_in_cycle]:
			considered_representatives[i].add(node)
			if i > 0:
				representatives[cycle_representative].add(node)

	chu_liu_edmonds(
		length, score_matrix, current_nodes, final_edges, old_input, old_output, representatives
	)

	found = False
	key_node = -1
	for i, node in enumerate(cycle):
		for cycle_rep in considered_representatives[i]:
			if cycle_rep in final_edges:
				key_node = node
				found = True
				break
		if found:
			break

	previous = parents[key_node]
	while previous != key_node:
		child = old_output[parents[previous], previous]
		parent = old_input[parents[previous], previous]
		final_edges[child] = parent
		previous = parents[previous]


def _find_cycle(
	parents: List[int], length: int, current_nodes: List[bool]
) -> Tuple[bool, List[int]]:

	added = [False for _ in range(length)]
	added[0] = True
	cycle = set()
	has_cycle = False
	for i in range(1, length):
		if has_cycle:
			break
		if added[i] or not current_nodes[i]:
			continue
		this_cycle = set()
		this_cycle.add(i)
		added[i] = True
		has_cycle = True
		next_node = i
		while parents[next_node] not in this_cycle:
			next_node = parents[next_node]
			if added[next_node]:
				has_cycle = False
				break
			added[next_node] = True
			this_cycle.add(next_node)

		if has_cycle:
			original = next_node
			cycle.add(original)
			next_node = parents[original]
			while next_node != original:
				cycle.add(next_node)
				next_node = parents[next_node]
			break

	return has_cycle, list(cycle)

from .bert import BERTEmbedding

from allennlp.nn.module import Module
from allennlp.nn.activations import Activation
from allennlp.nn.initializers import Initializer, InitializerApplicator
from allennlp.nn.regularizers import RegularizerApplicator


from typing import Optional, Dict, Union, List


import torch
import torch.distributed as dist

from allennlp.common.util import is_distributed
from allennlp.common.checks import ConfigurationError
from allennlp.nn.util import dist_reduce_sum
from allennlp.training.metrics.metric import Metric


class WordEmbeddingAssociationTest:

	def __call__(
		self,
		target_embeddings1: torch.Tensor,
		target_embeddings2: torch.Tensor,
		attribute_embeddings1: torch.Tensor,
		attribute_embeddings2: torch.Tensor,
	) -> torch.FloatTensor:

		if target_embeddings1.ndim < 2 or target_embeddings2.ndim < 2:
			raise ConfigurationError(
				"target_embeddings1 and target_embeddings2 must have at least two dimensions."
			)
		if attribute_embeddings1.ndim < 2 or attribute_embeddings2.ndim < 2:
			raise ConfigurationError(
				"attribute_embeddings1 and attribute_embeddings2 must have at least two dimensions."
			)
		if target_embeddings1.size() != target_embeddings2.size():
			raise ConfigurationError(
				"target_embeddings1 and target_embeddings2 must be of the same size."
			)
		if attribute_embeddings1.size(dim=-1) != attribute_embeddings2.size(
			dim=-1
		) or attribute_embeddings1.size(dim=-1) != target_embeddings1.size(dim=-1):
			raise ConfigurationError("All embeddings must have the same dimensionality.")

		target_embeddings1 = target_embeddings1.flatten(end_dim=-2)
		target_embeddings2 = target_embeddings2.flatten(end_dim=-2)
		attribute_embeddings1 = attribute_embeddings1.flatten(end_dim=-2)
		attribute_embeddings2 = attribute_embeddings2.flatten(end_dim=-2)

		target_embeddings1 = torch.nn.functional.normalize(target_embeddings1, p=2, dim=-1)
		target_embeddings2 = torch.nn.functional.normalize(target_embeddings2, p=2, dim=-1)
		attribute_embeddings1 = torch.nn.functional.normalize(attribute_embeddings1, p=2, dim=-1)
		attribute_embeddings2 = torch.nn.functional.normalize(attribute_embeddings2, p=2, dim=-1)

		X_sim_A = torch.mm(target_embeddings1, attribute_embeddings1.t())
		X_sim_B = torch.mm(target_embeddings1, attribute_embeddings2.t())
		Y_sim_A = torch.mm(target_embeddings2, attribute_embeddings1.t())
		Y_sim_B = torch.mm(target_embeddings2, attribute_embeddings2.t())
		X_union_Y_sim_A = torch.cat([X_sim_A, Y_sim_A])
		X_union_Y_sim_B = torch.cat([X_sim_B, Y_sim_B])

		s_X_A_B = torch.mean(X_sim_A, dim=-1) - torch.mean(X_sim_B, dim=-1)
		s_Y_A_B = torch.mean(Y_sim_A, dim=-1) - torch.mean(Y_sim_B, dim=-1)
		s_X_Y_A_B = torch.mean(s_X_A_B) - torch.mean(s_Y_A_B)
		S_X_union_Y_A_B = torch.mean(X_union_Y_sim_A, dim=-1) - torch.mean(X_union_Y_sim_B, dim=-1)
		return s_X_Y_A_B / torch.std(S_X_union_Y_A_B, unbiased=False)


class EmbeddingCoherenceTest:

	def __call__(
		self,
		target_embeddings1: torch.Tensor,
		target_embeddings2: torch.Tensor,
		attribute_embeddings: torch.Tensor,
	) -> torch.FloatTensor:
		if target_embeddings1.ndim < 2 or target_embeddings2.ndim < 2:
			raise ConfigurationError(
				"target_embeddings1 and target_embeddings2 must have at least two dimensions."
			)
		if attribute_embeddings.ndim < 2:
			raise ConfigurationError("attribute_embeddings must have at least two dimensions.")
		if target_embeddings1.size() != target_embeddings2.size():
			raise ConfigurationError(
				"target_embeddings1 and target_embeddings2 must be of the same size."
			)
		if attribute_embeddings.size(dim=-1) != target_embeddings1.size(dim=-1):
			raise ConfigurationError("All embeddings must have the same dimensionality.")

		mean_target_embedding1 = target_embeddings1.flatten(end_dim=-2).mean(dim=0)
		mean_target_embedding2 = target_embeddings2.flatten(end_dim=-2).mean(dim=0)
		attribute_embeddings = attribute_embeddings.flatten(end_dim=-2)

		mean_target_embedding1 = torch.nn.functional.normalize(mean_target_embedding1, p=2, dim=-1)
		mean_target_embedding2 = torch.nn.functional.normalize(mean_target_embedding2, p=2, dim=-1)
		attribute_embeddings = torch.nn.functional.normalize(attribute_embeddings, p=2, dim=-1)

		AB_sim_m = torch.matmul(attribute_embeddings, mean_target_embedding1)
		AB_sim_f = torch.matmul(attribute_embeddings, mean_target_embedding2)

		return self.spearman_correlation(AB_sim_m, AB_sim_f)

	def _get_ranks(self, x: torch.Tensor) -> torch.Tensor:
		tmp = x.argsort()
		ranks = torch.zeros_like(tmp)
		ranks[tmp] = torch.arange(x.size(0), device=ranks.device)
		return ranks

	def spearman_correlation(self, x: torch.Tensor, y: torch.Tensor):
		x_rank = self._get_ranks(x)
		y_rank = self._get_ranks(y)

		n = x.size(0)
		upper = 6 * torch.sum((x_rank - y_rank).pow(2))
		down = n * (n**2 - 1.0)
		return 1.0 - (upper / down)


@Metric.register("nli")
class NaturalLanguageInference(Metric):

	def __init__(self, neutral_label: int = 2, taus: List[float] = [0.5, 0.7]):
		self.neutral_label = neutral_label
		self.taus = taus

		self._nli_probs_sum = 0.0
		self._num_neutral_predictions = 0.0
		self._num_neutral_above_taus = {tau: 0.0 for tau in taus}
		self._total_predictions = 0

	def __call__(self, nli_probabilities: torch.Tensor) -> None:  # type: ignore
		nli_probabilities = nli_probabilities.detach()

		if nli_probabilities.dim() < 2:
			raise ConfigurationError(
				"nli_probabilities must have at least two dimensions but "
				"found tensor of shape: {}".format(nli_probabilities.size())
			)
		if nli_probabilities.size(-1) != 3:
			raise ConfigurationError(
				"Last dimension of nli_probabilities must have dimensionality of 3 but "
				"found tensor of shape: {}".format(nli_probabilities.size())
			)

		_nli_neutral_probs = nli_probabilities[..., self.neutral_label]

		self._nli_probs_sum += dist_reduce_sum(_nli_neutral_probs.sum().item())
		self._num_neutral_predictions += dist_reduce_sum(
			(nli_probabilities.argmax(dim=-1) == self.neutral_label).float().sum().item()
		)
		for tau in self.taus:
			self._num_neutral_above_taus[tau] += dist_reduce_sum(
				(_nli_neutral_probs > tau).float().sum().item()
			)
		self._total_predictions += dist_reduce_sum(_nli_neutral_probs.numel())

	def get_metric(self, reset: bool = False):
		if self._total_predictions == 0:
			nli_scores = {
				"net_neutral": 0.0,
				"fraction_neutral": 0.0,
				**{"threshold_{}".format(tau): 0.0 for tau in self.taus},
			}
		else:
			nli_scores = {
				"net_neutral": self._nli_probs_sum / self._total_predictions,
				"fraction_neutral": self._num_neutral_predictions / self._total_predictions,
				**{
					"threshold_{}".format(tau): self._num_neutral_above_taus[tau]
					/ self._total_predictions
					for tau in self.taus
				},
			}
		if reset:
			self.reset()
		return nli_scores

	def reset(self):
		self._nli_probs_sum = 0.0
		self._num_neutral_predictions = 0.0
		self._num_neutral_above_taus = {tau: 0.0 for tau in self.taus}
		self._total_predictions = 0


@Metric.register("association_without_ground_truth")
class AssociationWithoutGroundTruth(Metric):

	def __init__(
		self,
		num_classes: int,
		num_protected_variable_labels: int,
		association_metric: str = "npmixy",
		gap_type: str = "ova",
	) -> None:
		self._num_classes = num_classes
		self._num_protected_variable_labels = num_protected_variable_labels
		self._joint_counts_by_protected_variable_label = torch.zeros(
			(num_protected_variable_labels, num_classes)
		)
		self._protected_variable_label_counts = torch.zeros(num_protected_variable_labels)
		self._y_counts = torch.zeros(num_classes)
		self._total_predictions = torch.tensor(0)

		self.IMPLEMENTED_ASSOCIATION_METRICS = set(["npmixy", "npmiy", "pmisq", "pmi"])
		if association_metric in self.IMPLEMENTED_ASSOCIATION_METRICS:
			self.association_metric = association_metric
		else:
			raise NotImplementedError(
				f"Association metric {association_metric} has not been implemented!"
			)

		if gap_type == "ova":
			self.gap_func = self._ova_gap
		elif gap_type == "pairwise":
			self.gap_func = self._pairwise_gaps
		else:
			raise NotImplementedError(f"Gap type {gap_type} has not been implemented!")

	def __call__(
		self,
		predicted_labels: torch.Tensor,
		protected_variable_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	) -> None:
		predicted_labels, protected_variable_labels, mask = self.detach_tensors(
			predicted_labels, protected_variable_labels, mask
		)

		if predicted_labels.size() != protected_variable_labels.size():
			raise ConfigurationError(
				"protected_variable_labels must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(protected_variable_labels.size())
			)
		if mask is not None and predicted_labels.size() != mask.size():
			raise ConfigurationError(
				"mask must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(mask.size())
			)
		if (predicted_labels >= self._num_classes).any():
			raise ConfigurationError(
				"predicted_labels contains an id >= {}, "
				"the number of classes.".format(self._num_classes)
			)
		if (protected_variable_labels >= self._num_protected_variable_labels).any():
			raise ConfigurationError(
				"protected_variable_labels contains an id >= {}, "
				"the number of protected variable labels.".format(
					self._num_protected_variable_labels
				)
			)

		device = predicted_labels.device
		self._joint_counts_by_protected_variable_label = (
			self._joint_counts_by_protected_variable_label.to(device)
		)
		self._protected_variable_label_counts = self._protected_variable_label_counts.to(device)
		self._y_counts = self._y_counts.to(device)
		self._total_predictions = self._total_predictions.to(device)

		if mask is not None:
			predicted_labels = predicted_labels[mask]
			protected_variable_labels = protected_variable_labels[mask]
		else:
			predicted_labels = predicted_labels.flatten()
			protected_variable_labels = protected_variable_labels.flatten()

		_total_predictions = torch.tensor(predicted_labels.nelement()).to(device)
		_y_counts = torch.zeros(self._num_classes).to(device)
		_y_counts = torch.zeros_like(_y_counts, dtype=predicted_labels.dtype).scatter_add_(
			0, predicted_labels, torch.ones_like(predicted_labels)
		)

		_joint_counts_by_protected_variable_label = torch.zeros(
			(self._num_protected_variable_labels, self._num_classes)
		).to(device)
		_protected_variable_label_counts = torch.zeros(self._num_protected_variable_labels).to(
			device
		)
		for x in range(self._num_protected_variable_labels):
			x_mask = (protected_variable_labels == x).long()

			_joint_counts_by_protected_variable_label[x] = torch.zeros(self._num_classes).to(device)
			_joint_counts_by_protected_variable_label[x] = torch.zeros_like(
				_joint_counts_by_protected_variable_label[x], dtype=x_mask.dtype
			).scatter_add_(0, predicted_labels, x_mask)

			_protected_variable_label_counts[x] = torch.tensor(x_mask.sum()).to(device)

		if is_distributed():
			_total_predictions = _total_predictions.to(device)
			dist.all_reduce(_total_predictions, op=dist.ReduceOp.SUM)

			_y_counts = _y_counts.to(device)
			dist.all_reduce(_y_counts, op=dist.ReduceOp.SUM)

			_joint_counts_by_protected_variable_label = (
				_joint_counts_by_protected_variable_label.to(device)
			)
			dist.all_reduce(_joint_counts_by_protected_variable_label, op=dist.ReduceOp.SUM)

			_protected_variable_label_counts = _protected_variable_label_counts.to(device)
			dist.all_reduce(_protected_variable_label_counts, op=dist.ReduceOp.SUM)

		self._total_predictions += _total_predictions
		self._y_counts += _y_counts
		self._joint_counts_by_protected_variable_label += _joint_counts_by_protected_variable_label
		self._protected_variable_label_counts += _protected_variable_label_counts

	def get_metric(
		self, reset: bool = False
	) -> Dict[int, Union[torch.Tensor, Dict[int, torch.Tensor]]]:
		gaps = {}
		for x in range(self._num_protected_variable_labels):
			gaps[x] = self.gap_func(x)
		if reset:
			self.reset()
		return gaps

	def reset(self) -> None:
		self._joint_counts_by_protected_variable_label = torch.zeros(
			(self._num_protected_variable_labels, self._num_classes)
		)
		self._protected_variable_label_counts = torch.zeros(self._num_protected_variable_labels)
		self._y_counts = torch.zeros(self._num_classes)
		self._total_predictions = torch.tensor(0)

	def _ova_gap(self, x: int) -> torch.Tensor:
		device = self._y_counts.device
		pmi_terms = self._all_pmi_terms()
		pmi_not_x = torch.sum(
			pmi_terms[torch.arange(self._num_protected_variable_labels, device=device) != x], dim=0
		)
		pmi_not_x /= self._num_protected_variable_labels - 1

		gap = pmi_terms[x] - pmi_not_x
		return torch.where(~gap.isinf(), gap, torch.tensor(float("nan")).to(device))

	def _pairwise_gaps(self, x: int) -> Dict[int, torch.Tensor]:
		device = self._y_counts.device
		pmi_terms = self._all_pmi_terms()
		pairwise_gaps = {}
		for not_x in range(self._num_protected_variable_labels):
			gap = pmi_terms[x] - pmi_terms[not_x]
			pairwise_gaps[not_x] = torch.where(
				~gap.isinf(), gap, torch.tensor(float("nan")).to(device)
			)
		return pairwise_gaps

	def _all_pmi_terms(self) -> Dict[int, torch.Tensor]:
		if self._total_predictions == 0:
			return torch.full(
				(self._num_protected_variable_labels, self._num_classes), float("nan")
			)

		device = self._y_counts.device
		prob_y = torch.zeros(self._num_classes).to(device)
		torch.div(self._y_counts, self._total_predictions, out=prob_y)

		joint = torch.zeros((self._num_protected_variable_labels, self._num_classes)).to(device)
		torch.div(
			self._joint_counts_by_protected_variable_label,
			self._total_predictions,
			out=joint,
		)
		if self.association_metric == "pmisq":
			torch.square_(joint)

		pmi_terms = torch.log(
			torch.div(
				joint,
				(self._protected_variable_label_counts / self._total_predictions).unsqueeze(-1)
				* prob_y,
			)
		)
		if self.association_metric == "npmixy":
			pmi_terms.div_(torch.log(joint))
		elif self.association_metric == "npmiy":
			pmi_terms.div_(torch.log(prob_y))

		return pmi_terms

import torch
from typing import Union, Optional
from os import PathLike

from allennlp.fairness.bias_mitigators import (
	HardBiasMitigator,
	LinearBiasMitigator,
	INLPBiasMitigator,
	OSCaRBiasMitigator,
)
from allennlp.fairness.bias_direction_wrappers import BiasDirectionWrapper
from allennlp.fairness.bias_utils import load_word_pairs

from allennlp.common import Registrable
from allennlp.data.tokenizers.tokenizer import Tokenizer
from allennlp.data import Vocabulary


class BiasMitigatorWrapper(Registrable):

	def train(self, mode: bool = True):
		raise NotImplementedError


@BiasMitigatorWrapper.register("hard")
class HardBiasMitigatorWrapper(BiasMitigatorWrapper):

	def __init__(
		self,
		bias_direction: BiasDirectionWrapper,
		embedding_layer: torch.nn.Embedding,
		equalize_word_pairs_file: Union[PathLike, str],
		tokenizer: Tokenizer,
		mitigator_vocab: Optional[Vocabulary] = None,
		namespace: str = "tokens",
		requires_grad: bool = True,
	):
		self.bias_direction = bias_direction
		self.predetermined_bias_direction = self.bias_direction(embedding_layer)
		self.ids1, self.ids2 = load_word_pairs(
			equalize_word_pairs_file, tokenizer, mitigator_vocab, namespace
		)
		self.mitigator = HardBiasMitigator(requires_grad=requires_grad)

	def __call__(self, module, module_in, module_out):
		ids1_embeddings = []
		for i in self.ids1:
			i = i.to(module.weight.device)
			ids1_embeddings.append(
				torch.mean(module.forward(i), dim=0, keepdim=True)
			)  # forward() does not trigger hooks, thereby avoiding infinite recursion
		ids2_embeddings = []
		for i in self.ids2:
			i = i.to(module.weight.device)
			ids2_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids1_embeddings = torch.cat(ids1_embeddings)
		ids2_embeddings = torch.cat(ids2_embeddings)

		module_out_size = module_out.size()
		module_out = module_out.flatten(end_dim=-2)
		module_out = self.mitigator(
			module_out,
			self.predetermined_bias_direction.to(module_out.device),
			ids1_embeddings.to(module_out.device),
			ids2_embeddings.to(module_out.device),
		)[: module_out.size(0)]
		return module_out.reshape(module_out_size)

	def train(self, mode: bool = True):
		self.mitigator.requires_grad = mode
		self.bias_direction.train(mode)


@BiasMitigatorWrapper.register("linear")
class LinearBiasMitigatorWrapper(BiasMitigatorWrapper):

	def __init__(
		self,
		bias_direction: BiasDirectionWrapper,
		embedding_layer: torch.nn.Embedding,
		requires_grad: bool = True,
	):
		self.bias_direction = bias_direction
		self.predetermined_bias_direction = self.bias_direction(embedding_layer)
		self.mitigator = LinearBiasMitigator(requires_grad=requires_grad)

	def __call__(self, module, module_in, module_out):
		module_out_size = module_out.size()
		module_out = module_out.flatten(end_dim=-2)
		module_out = self.mitigator(
			module_out, self.predetermined_bias_direction.to(module_out.device)
		)
		return module_out.reshape(module_out_size)

	def train(self, mode: bool = True):
		self.mitigator.requires_grad = mode
		self.bias_direction.train(mode)


@BiasMitigatorWrapper.register("inlp")
class INLPBiasMitigatorWrapper(BiasMitigatorWrapper):

	def __init__(
		self,
		embedding_layer: torch.nn.Embedding,
		seed_word_pairs_file: Union[PathLike, str],
		tokenizer: Tokenizer,
		mitigator_vocab: Optional[Vocabulary] = None,
		namespace: str = "tokens",
	):
		self.ids1, self.ids2 = load_word_pairs(
			seed_word_pairs_file, tokenizer, mitigator_vocab, namespace
		)
		self.mitigator = INLPBiasMitigator()

	def __call__(self, module, module_in, module_out):
		ids1_embeddings = []
		for i in self.ids1:
			i = i.to(module.weight.device)
			ids1_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids2_embeddings = []
		for i in self.ids2:
			i = i.to(module.weight.device)
			ids2_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids1_embeddings = torch.cat(ids1_embeddings)
		ids2_embeddings = torch.cat(ids2_embeddings)

		module_out_size = module_out.size()
		module_out = module_out.flatten(end_dim=-2)
		module_out = self.mitigator(
			module_out, ids1_embeddings.to(module_out.device), ids2_embeddings.to(module_out.device)
		)
		return module_out.reshape(module_out_size)

	def train(self, mode: bool = True):
		pass


@BiasMitigatorWrapper.register("oscar")
class OSCaRBiasMitigatorWrapper(BiasMitigatorWrapper):

	def __init__(
		self,
		bias_direction1: BiasDirectionWrapper,
		bias_direction2: BiasDirectionWrapper,
		embedding_layer: torch.nn.Embedding,
		requires_grad: bool = True,
	):
		self.bias_direction1 = bias_direction1
		self.predetermined_bias_direction1 = self.bias_direction1(embedding_layer)
		self.bias_direction2 = bias_direction2(embedding_layer)
		self.predetermined_bias_direction2 = self.bias_direction2(embedding_layer)
		self.mitigator = OSCaRBiasMitigator(requires_grad=requires_grad)

	def __call__(self, module, module_in, module_out):
		module_out_size = module_out.size()
		module_out = module_out.flatten(end_dim=-2)
		module_out = self.mitigator(
			module_out,
			self.predetermined_bias_direction1.to(module_out.device),
			self.predetermined_bias_direction2.to(module_out.device),
		)
		return module_out.reshape(module_out_size)

	def train(self, mode: bool = True):
		self.mitigator.requires_grad = mode
		self.bias_direction1.train(mode)
		self.bias_direction2.train(mode)

import torch

from allennlp.training.momentum_schedulers.momentum_scheduler import MomentumScheduler


@MomentumScheduler.register("inverted_triangular")
class InvertedTriangular(MomentumScheduler):

	def __init__(
		self,
		optimizer: torch.optim.Optimizer,
		cool_down: int,
		warm_up: int,
		ratio: int = 10,
		last_epoch: int = -1,
	) -> None:
		self.cool_down = cool_down
		self.warm_up = warm_up
		self.ratio = ratio
		super().__init__(optimizer, last_epoch)

	def get_values(self):
		step = self.last_epoch + 1
		if step <= self.cool_down:
			values = [m - (m - m / self.ratio) * (step / self.cool_down) for m in self.base_values]
		elif step <= self.cool_down + self.warm_up:
			values = [
				(m / self.ratio) + (m - m / self.ratio) * (step - self.cool_down) / self.warm_up
				for m in self.base_values
			]
		else:
			values = self.base_values

		return values

import torch
from torch import nn, FloatTensor, IntTensor
from typing import List

from allennlp.common.registrable import Registrable


class Image2ImageModule(nn.Module, Registrable):

	def forward(self, images: FloatTensor, sizes: IntTensor):
		raise NotImplementedError()


@Image2ImageModule.register("normalize")
class NormalizeImage(Image2ImageModule):

	def __init__(self, means: List[float], stds: List[float]):
		super().__init__()
		assert len(means) == len(stds)
		self.means = torch.tensor(means, dtype=torch.float32)
		self.stds = torch.tensor(stds, dtype=torch.float32)

	def forward(self, images: FloatTensor, sizes: IntTensor):
		assert images.size(1) == self.means.size(0)
		self.means = self.means.to(images.device)
		self.stds = self.stds.to(images.device)
		images = images.transpose(1, -1)
		images = images - self.means
		images = images / self.stds
		return images.transpose(-1, 1)

from typing import Optional, Tuple, List
import torch
from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence
from allennlp.modules.augmented_lstm import AugmentedLstm
from allennlp.modules.input_variational_dropout import InputVariationalDropout
from allennlp.common.checks import ConfigurationError


TensorPair = Tuple[torch.Tensor, torch.Tensor]


class StackedBidirectionalLstm(torch.nn.Module):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int,
		recurrent_dropout_probability: float = 0.0,
		layer_dropout_probability: float = 0.0,
		use_highway: bool = True,
	) -> None:
		super().__init__()

		self.input_size = input_size
		self.hidden_size = hidden_size
		self.num_layers = num_layers
		self.bidirectional = True

		layers = []
		lstm_input_size = input_size
		for layer_index in range(num_layers):

			forward_layer = AugmentedLstm(
				lstm_input_size,
				hidden_size,
				go_forward=True,
				recurrent_dropout_probability=recurrent_dropout_probability,
				use_highway=use_highway,
				use_input_projection_bias=False,
			)
			backward_layer = AugmentedLstm(
				lstm_input_size,
				hidden_size,
				go_forward=False,
				recurrent_dropout_probability=recurrent_dropout_probability,
				use_highway=use_highway,
				use_input_projection_bias=False,
			)

			lstm_input_size = hidden_size * 2
			self.add_module("forward_layer_{}".format(layer_index), forward_layer)
			self.add_module("backward_layer_{}".format(layer_index), backward_layer)
			layers.append([forward_layer, backward_layer])
		self.lstm_layers = layers
		self.layer_dropout = InputVariationalDropout(layer_dropout_probability)

	def forward(
		self, inputs: PackedSequence, initial_state: Optional[TensorPair] = None
	) -> Tuple[PackedSequence, TensorPair]:
		if initial_state is None:
			hidden_states: List[Optional[TensorPair]] = [None] * len(self.lstm_layers)
		elif initial_state[0].size()[0] != len(self.lstm_layers):
			raise ConfigurationError(
				"Initial states were passed to forward() but the number of "
				"initial states does not match the number of layers."
			)
		else:
			hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))

		output_sequence = inputs
		final_h = []
		final_c = []
		for i, state in enumerate(hidden_states):
			forward_layer = getattr(self, "forward_layer_{}".format(i))
			backward_layer = getattr(self, "backward_layer_{}".format(i))
			forward_output, final_forward_state = forward_layer(output_sequence, state)
			backward_output, final_backward_state = backward_layer(output_sequence, state)

			forward_output, lengths = pad_packed_sequence(forward_output, batch_first=True)
			backward_output, _ = pad_packed_sequence(backward_output, batch_first=True)

			output_sequence = torch.cat([forward_output, backward_output], -1)
			if i < (self.num_layers - 1):
				output_sequence = self.layer_dropout(output_sequence)
			output_sequence = pack_padded_sequence(output_sequence, lengths, batch_first=True)

			final_h.extend([final_forward_state[0], final_backward_state[0]])
			final_c.extend([final_forward_state[1], final_backward_state[1]])

		final_h = torch.cat(final_h, dim=0)
		final_c = torch.cat(final_c, dim=0)
		final_state_tuple = (final_h, final_c)
		return output_sequence, final_state_tuple

from .fields import SourceField, TargetField

from allennlp.interpret.attackers.attacker import Attacker
from allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter
from allennlp.interpret.influence_interpreters.influence_interpreter import InfluenceInterpreter

import torch


class TransformerNet(torch.nn.Module):
	def __init__(self):
		super(TransformerNet, self).__init__()
		self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)
		self.in1 = torch.nn.InstanceNorm2d(32, affine=True)
		self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)
		self.in2 = torch.nn.InstanceNorm2d(64, affine=True)
		self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)
		self.in3 = torch.nn.InstanceNorm2d(128, affine=True)
		self.res1 = ResidualBlock(128)
		self.res2 = ResidualBlock(128)
		self.res3 = ResidualBlock(128)
		self.res4 = ResidualBlock(128)
		self.res5 = ResidualBlock(128)
		self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)
		self.in4 = torch.nn.InstanceNorm2d(64, affine=True)
		self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)
		self.in5 = torch.nn.InstanceNorm2d(32, affine=True)
		self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)
		self.relu = torch.nn.ReLU()

	def forward(self, X):
		y = self.relu(self.in1(self.conv1(X)))
		y = self.relu(self.in2(self.conv2(y)))
		y = self.relu(self.in3(self.conv3(y)))
		y = self.res1(y)
		y = self.res2(y)
		y = self.res3(y)
		y = self.res4(y)
		y = self.res5(y)
		y = self.relu(self.in4(self.deconv1(y)))
		y = self.relu(self.in5(self.deconv2(y)))
		y = self.deconv3(y)
		return y


class ConvLayer(torch.nn.Module):
	def __init__(self, in_channels, out_channels, kernel_size, stride):
		super(ConvLayer, self).__init__()
		reflection_padding = kernel_size // 2
		self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)
		self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)

	def forward(self, x):
		out = self.reflection_pad(x)
		out = self.conv2d(out)
		return out


class ResidualBlock(torch.nn.Module):

	def __init__(self, channels):
		super(ResidualBlock, self).__init__()
		self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)
		self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)
		self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)
		self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)
		self.relu = torch.nn.ReLU()

	def forward(self, x):
		residual = x
		out = self.relu(self.in1(self.conv1(x)))
		out = self.in2(self.conv2(out))
		out = out + residual
		return out


class UpsampleConvLayer(torch.nn.Module):

	def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):
		super(UpsampleConvLayer, self).__init__()
		self.upsample = upsample
		reflection_padding = kernel_size // 2
		self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)
		self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)

	def forward(self, x):
		x_in = x
		if self.upsample:
			x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)
		out = self.reflection_pad(x_in)
		out = self.conv2d(out)
		return out

import os
import time
import requests
import tarfile
import numpy as np
import argparse

import torch
from torch import nn
import torch.nn.functional as F
from torch.optim import Adam



class GraphAttentionLayer(nn.Module):
	def __init__(self, in_features: int, out_features: int, n_heads: int, concat: bool = False, dropout: float = 0.4, leaky_relu_slope: float = 0.2):
		super(GraphAttentionLayer, self).__init__()

		self.n_heads = n_heads # Number of attention heads
		self.concat = concat # wether to concatenate the final attention heads
		self.dropout = dropout # Dropout rate

		if concat: # concatenating the attention heads
			self.out_features = out_features # Number of output features per node
			assert out_features % n_heads == 0 # Ensure that out_features is a multiple of n_heads
			self.n_hidden = out_features // n_heads
		else: # averaging output over the attention heads (Used in the main paper)
			self.n_hidden = out_features

		self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * n_heads)))

		self.a = nn.Parameter(torch.empty(size=(n_heads, 2 * self.n_hidden, 1)))

		self.leakyrelu = nn.LeakyReLU(leaky_relu_slope) # LeakyReLU activation function
		self.softmax = nn.Softmax(dim=1) # softmax activation function to the attention coefficients

		self.reset_parameters() # Reset the parameters


	def reset_parameters(self):
		nn.init.xavier_normal_(self.W)
		nn.init.xavier_normal_(self.a)
	

	def _get_attention_scores(self, h_transformed: torch.Tensor):
		
		source_scores = torch.matmul(h_transformed, self.a[:, :self.n_hidden, :])
		target_scores = torch.matmul(h_transformed, self.a[:, self.n_hidden:, :])

		e = source_scores + target_scores.mT
		return self.leakyrelu(e)

	def forward(self,  h: torch.Tensor, adj_mat: torch.Tensor):
		n_nodes = h.shape[0]

		h_transformed = torch.mm(h, self.W)
		h_transformed = F.dropout(h_transformed, self.dropout, training=self.training)

		h_transformed = h_transformed.view(n_nodes, self.n_heads, self.n_hidden).permute(1, 0, 2)
		
		e = self._get_attention_scores(h_transformed)

		connectivity_mask = -9e16 * torch.ones_like(e)
		e = torch.where(adj_mat > 0, e, connectivity_mask) # masked attention scores
		
		attention = F.softmax(e, dim=-1)
		attention = F.dropout(attention, self.dropout, training=self.training)

		h_prime = torch.matmul(attention, h_transformed)

		if self.concat:
			h_prime = h_prime.permute(1, 0, 2).contiguous().view(n_nodes, self.out_features)
		else:
			h_prime = h_prime.mean(dim=0)

		return h_prime


class GAT(nn.Module):
	def __init__(self,
		in_features,
		n_hidden,
		n_heads,
		num_classes,
		concat=False,
		dropout=0.4,
		leaky_relu_slope=0.2):

		super(GAT, self).__init__()

		self.gat1 = GraphAttentionLayer(
			in_features=in_features, out_features=n_hidden, n_heads=n_heads,
			concat=concat, dropout=dropout, leaky_relu_slope=leaky_relu_slope
			)
		
		self.gat2 = GraphAttentionLayer(
			in_features=n_hidden, out_features=num_classes, n_heads=1,
			concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope
			)
		

	def forward(self, input_tensor: torch.Tensor , adj_mat: torch.Tensor):

		x = self.gat1(input_tensor, adj_mat)
		x = F.elu(x) # Apply ELU activation function to the output of the first layer

		x = self.gat2(x, adj_mat)

		return F.log_softmax(x, dim=1) # Apply log softmax activation function


def load_cora(path='./cora', device='cpu'):

	content_path = os.path.join(path, 'cora.content')
	cites_path = os.path.join(path, 'cora.cites')

	content_tensor = np.genfromtxt(content_path, dtype=np.dtype(str))
	cites_tensor = np.genfromtxt(cites_path, dtype=np.int32)

	features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32)) # Extract feature values
	scale_vector = torch.sum(features, dim=1) # Compute sum of features for each node
	scale_vector = 1 / scale_vector # Compute reciprocal of the sums
	scale_vector[scale_vector == float('inf')] = 0 # Handle division by zero cases
	scale_vector = torch.diag(scale_vector).to_sparse() # Convert the scale vector to a sparse diagonal matrix
	features = scale_vector @ features # Scale the features using the scale vector

	classes, labels = np.unique(content_tensor[:, -1], return_inverse=True) # Extract unique classes and map labels to indices
	labels = torch.LongTensor(labels) # Convert labels to a tensor

	idx = content_tensor[:, 0].astype(np.int32) # Extract node indices
	idx_map = {id: pos for pos, id in enumerate(idx)} # Create a dictionary to map indices to positions

	edges = np.array(
		list(map(lambda edge: [idx_map[edge[0]], idx_map[edge[1]]], 
			cites_tensor)), dtype=np.int32)

	V = len(idx) # Number of nodes
	E = edges.shape[0] # Number of edges
	adj_mat = torch.sparse_coo_tensor(edges.T, torch.ones(E), (V, V), dtype=torch.int64) # Create the initial adjacency matrix as a sparse tensor
	adj_mat = torch.eye(V) + adj_mat # Add self-loops to the adjacency matrix

	return features.to(device), labels.to(device), adj_mat.to(device)


def train_iter(epoch, model, optimizer, criterion, input, target, mask_train, mask_val, print_every=10):
	start_t = time.time()
	model.train()
	optimizer.zero_grad()

	output = model(*input) 
	loss = criterion(output[mask_train], target[mask_train]) # Compute the loss using the training mask

	loss.backward()
	optimizer.step()

	loss_train, acc_train = test(model, criterion, input, target, mask_train)
	loss_val, acc_val = test(model, criterion, input, target, mask_val)

	if epoch % print_every == 0:
		print(f'Epoch: {epoch:04d} ({(time.time() - start_t):.4f}s) loss_train: {loss_train:.4f} acc_train: {acc_train:.4f} loss_val: {loss_val:.4f} acc_val: {acc_val:.4f}')


def test(model, criterion, input, target, mask):
	model.eval()
	with torch.no_grad():
		output = model(*input)
		output, target = output[mask], target[mask]

		loss = criterion(output, target)
		acc = (output.argmax(dim=1) == target).float().sum() / len(target)
	return loss.item(), acc.item()


if __name__ == '__main__':


	parser = argparse.ArgumentParser(description='PyTorch Graph Attention Network')
	parser.add_argument('--epochs', type=int, default=300,
						help='number of epochs to train (default: 300)')
	parser.add_argument('--lr', type=float, default=0.005,
						help='learning rate (default: 0.005)')
	parser.add_argument('--l2', type=float, default=5e-4,
						help='weight decay (default: 6e-4)')
	parser.add_argument('--dropout-p', type=float, default=0.6,
						help='dropout probability (default: 0.6)')
	parser.add_argument('--hidden-dim', type=int, default=64,
						help='dimension of the hidden representation (default: 64)')
	parser.add_argument('--num-heads', type=int, default=8,
						help='number of the attention heads (default: 4)')
	parser.add_argument('--concat-heads', action='store_true', default=False,
						help='wether to concatinate attention heads, or average over them (default: False)')
	parser.add_argument('--val-every', type=int, default=20,
						help='epochs to wait for print training and validation evaluation (default: 20)')
	parser.add_argument('--no-cuda', action='store_true', default=False,
						help='disables CUDA training')
	parser.add_argument('--no-mps', action='store_true', default=False,
						help='disables macOS GPU training')
	parser.add_argument('--dry-run', action='store_true', default=False,
						help='quickly check a single pass')
	parser.add_argument('--seed', type=int, default=13, metavar='S',
						help='random seed (default: 13)')
	args = parser.parse_args()

	torch.manual_seed(args.seed)
	use_cuda = not args.no_cuda and torch.cuda.is_available()
	use_mps = not args.no_mps and torch.backends.mps.is_available()

	if use_cuda:
		device = torch.device('cuda')
	elif use_mps:
		device = torch.device('mps')
	else:
		device = torch.device('cpu')
	print(f'Using {device} device')

	cora_url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'
	path = './cora'

	if os.path.isfile(os.path.join(path, 'cora.content')) and os.path.isfile(os.path.join(path, 'cora.cites')):
		print('Dataset already downloaded...')
	else:
		print('Downloading dataset...')
		with requests.get(cora_url, stream=True) as tgz_file:
			with tarfile.open(fileobj=tgz_file.raw, mode='r:gz') as tgz_object:
				tgz_object.extractall()

	print('Loading dataset...')
	features, labels, adj_mat = load_cora(device=device)
	idx = torch.randperm(len(labels)).to(device)
	idx_test, idx_val, idx_train = idx[:1200], idx[1200:1600], idx[1600:]


	gat_net = GAT(
		in_features=features.shape[1],		  # Number of input features per node  
		n_hidden=args.hidden_dim,			   # Output size of the first Graph Attention Layer
		n_heads=args.num_heads,				 # Number of attention heads in the first Graph Attention Layer
		num_classes=labels.max().item() + 1,	# Number of classes to predict for each node
		concat=args.concat_heads,			   # Wether to concatinate attention heads
		dropout=args.dropout_p,				 # Dropout rate
		leaky_relu_slope=0.2					# Alpha (slope) of the leaky relu activation
	).to(device)

	optimizer = Adam(gat_net.parameters(), lr=args.lr, weight_decay=args.l2)
	criterion = nn.NLLLoss()

	for epoch in range(args.epochs):
		train_iter(epoch + 1, gat_net, optimizer, criterion, (features, adj_mat), labels, idx_train, idx_val, args.val_every)
		if args.dry_run:
			break
	loss_test, acc_test = test(gat_net, criterion, (features, adj_mat), labels, idx_test)
	print(f'Test set results: loss {loss_test:.4f} accuracy {acc_test:.4f}')
import torch
import torch.fx


def sigmoid_lowp(x : torch.Tensor):
	x = x.float()
	x = x.sigmoid()
	return x.half()

torch.fx.wrap(sigmoid_lowp)

def add_lowp(a : torch.Tensor, b : torch.Tensor):
	a, b = a.float(), b.float()
	c = a + b
	return c.half()

torch.fx.wrap(add_lowp)



class Foo(torch.nn.Module):
	def forward(self, x, y):
		x = sigmoid_lowp(x)
		y = sigmoid_lowp(y)
		return add_lowp(x, y)


traced = torch.fx.symbolic_trace(Foo())
print(traced.code)




def inline_lowp_func(n : torch.fx.Node):
	if n.op == 'call_function' and n.target.__module__ == inline_lowp_func.__module__:
		tracer = torch.fx.proxy.GraphAppendingTracer(n.graph)
		with n.graph.inserting_before(n):
			proxy_args = torch.fx.node.map_arg(n.args, lambda x: torch.fx.Proxy(x, tracer))
			proxy_kwargs = torch.fx.node.map_arg(n.kwargs, lambda x: torch.fx.Proxy(x, tracer))
			output_proxy = n.target(*proxy_args, **proxy_kwargs)
			node.replace_all_uses_with(output_proxy.node)
			node.graph.erase_node(node)

for node in traced.graph.nodes:
	if node.op == 'call_function' and node.target is sigmoid_lowp:
		inline_lowp_func(node)

traced.recompile()

print(traced.code)



f = Foo()

class InliningTracer(torch.fx.Tracer):
	FNS_TO_INLINE = [add_lowp]

	def create_node(self, kind, target, args, kwargs, name=None, type_expr=None):
		if kind == 'call_function' and target in self.FNS_TO_INLINE:
			tracer = torch.fx.proxy.GraphAppendingTracer(self.graph)
			proxy_args = torch.fx.node.map_arg(args, lambda x: torch.fx.Proxy(x, tracer))
			proxy_kwargs = torch.fx.node.map_arg(kwargs, lambda x: torch.fx.Proxy(x, tracer))
			return target(*proxy_args, **proxy_kwargs).node
		else:
			return super().create_node(kind, target, args, kwargs, name, type_expr)


tracer = InliningTracer()
graph = tracer.trace(f)
module = torch.fx.GraphModule(f, graph)
print(module.code)



from __future__ import print_function, division

import torch
import torchtext

import seq2seq
from seq2seq.loss import NLLLoss

class Evaluator(object):

	def __init__(self, loss=NLLLoss(), batch_size=64):
		self.loss = loss
		self.batch_size = batch_size

	def evaluate(self, model, data):
		model.eval()

		loss = self.loss
		loss.reset()
		match = 0
		total = 0

		device = None if torch.cuda.is_available() else -1
		batch_iterator = torchtext.data.BucketIterator(
			dataset=data, batch_size=self.batch_size,
			sort=True, sort_key=lambda x: len(x.src),
			device=device, train=False)
		tgt_vocab = data.fields[seq2seq.tgt_field_name].vocab
		pad = tgt_vocab.stoi[data.fields[seq2seq.tgt_field_name].pad_token]

		with torch.no_grad():
			for batch in batch_iterator:
				input_variables, input_lengths  = getattr(batch, seq2seq.src_field_name)
				target_variables = getattr(batch, seq2seq.tgt_field_name)

				decoder_outputs, decoder_hidden, other = model(input_variables, input_lengths.tolist(), target_variables)

				seqlist = other['sequence']
				for step, step_output in enumerate(decoder_outputs):
					target = target_variables[:, step + 1]
					loss.eval_batch(step_output.view(target_variables.size(0), -1), target)

					non_padding = target.ne(pad)
					correct = seqlist[step].view(-1).eq(target).masked_select(non_padding).sum().item()
					match += correct
					total += non_padding.sum().item()

		if total == 0:
			accuracy = float('nan')
		else:
			accuracy = match / total

		return loss.get_loss(), accuracy

import logging
from allennlp.common import logging as common_logging
import sys
from time import time
from typing import Optional

try:
	SHELL = str(type(get_ipython()))  # type:ignore # noqa: F821
except:  # noqa: E722
	SHELL = ""

if "zmqshell.ZMQInteractiveShell" in SHELL:
	from tqdm import tqdm_notebook as _tqdm
else:
	from tqdm import tqdm as _tqdm


_tqdm.monitor_interval = 0


logger = logging.getLogger("tqdm")
logger.propagate = False


def replace_cr_with_newline(message: str) -> str:
	message = message.replace("\r", "").replace("\n", "").replace("[A", "")
	if message and message[-1] != "\n":
		message += "\n"
	return message


class TqdmToLogsWriter(object):
	def __init__(self):
		self.last_message_written_time = 0.0

	def write(self, message):
		file_friendly_message: Optional[str] = None
		if common_logging.FILE_FRIENDLY_LOGGING:
			file_friendly_message = replace_cr_with_newline(message)
			if file_friendly_message.strip():
				sys.stderr.write(file_friendly_message)
		else:
			sys.stderr.write(message)

		now = time()
		if now - self.last_message_written_time >= 10 or "100%" in message:
			if file_friendly_message is None:
				file_friendly_message = replace_cr_with_newline(message)
			for message in file_friendly_message.split("\n"):
				message = message.strip()
				if len(message) > 0:
					logger.info(message)
					self.last_message_written_time = now

	def flush(self):
		sys.stderr.flush()


class Tqdm:
	@staticmethod
	def tqdm(*args, **kwargs):
		default_mininterval = 2.0 if common_logging.FILE_FRIENDLY_LOGGING else 0.1

		new_kwargs = {
			"file": TqdmToLogsWriter(),
			"mininterval": default_mininterval,
			**kwargs,
		}

		return _tqdm(*args, **new_kwargs)

	@staticmethod
	def set_lock(lock):
		_tqdm.set_lock(lock)

	@staticmethod
	def get_lock():
		return _tqdm.get_lock()

import datetime
import glob
import logging
import math
import os
import re
import time
import warnings
from typing import Optional, Union, List, Dict, Tuple, Any, Type

import torch
from torch.cuda import amp
from torch.nn.utils import clip_grad_norm_
import torch.distributed as dist
from torch.cuda.amp.grad_scaler import OptState

from allennlp.common.checks import ConfigurationError, check_for_gpu
from allennlp.common import util as common_util, Tqdm, Lazy
from allennlp.common.file_utils import hardlink_or_copy
from allennlp.data.data_loaders.data_loader import DataLoader, TensorDict
from allennlp.models.model import Model
from allennlp.nn.parallel import DdpAccelerator, DdpWrappedModel, TorchDdpAccelerator
from allennlp.nn.util import dist_reduce_sum
from allennlp.training.callbacks import ConsoleLoggerCallback
from allennlp.training.callbacks.confidence_checks import ConfidenceChecksCallback
from allennlp.training.callbacks.backward import MixedPrecisionBackwardCallback
from allennlp.training.checkpointer import Checkpointer
from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler
from allennlp.training.metric_tracker import MetricTracker
from allennlp.training.momentum_schedulers.momentum_scheduler import MomentumScheduler
from allennlp.training.moving_average import MovingAverage
from allennlp.training.optimizers import Optimizer
from allennlp.training.trainer import Trainer, TrainerCheckpoint
from allennlp.training.callbacks import TrainerCallback
from allennlp.training import util as training_util

logger = logging.getLogger(__name__)


@Trainer.register("gradient_descent", constructor="from_partial_objects")
class GradientDescentTrainer(Trainer):

	def __init__(
		self,
		model: Model,
		optimizer: torch.optim.Optimizer,
		data_loader: DataLoader,
		patience: Optional[int] = None,
		validation_metric: Union[str, List[str]] = "-loss",
		validation_data_loader: DataLoader = None,
		num_epochs: int = 20,
		serialization_dir: Optional[Union[str, os.PathLike]] = None,
		checkpointer: Optional[Checkpointer] = None,
		cuda_device: Optional[Union[int, torch.device]] = None,
		grad_norm: Union[float, bool] = False,
		grad_clipping: Optional[float] = None,
		learning_rate_scheduler: Optional[LearningRateScheduler] = None,
		momentum_scheduler: Optional[MomentumScheduler] = None,
		moving_average: Optional[MovingAverage] = None,
		callbacks: List[TrainerCallback] = None,
		distributed: bool = False,
		local_rank: int = 0,
		world_size: int = 1,
		num_gradient_accumulation_steps: int = 1,
		use_amp: bool = False,
		enable_default_callbacks: bool = True,
		run_confidence_checks: bool = True,
		grad_scaling: bool = True,
		ddp_wrapped_model: Optional[DdpWrappedModel] = None,
		**kwargs,
	) -> None:
		super().__init__(
			serialization_dir=serialization_dir,
			cuda_device=cuda_device,
			distributed=distributed,
			local_rank=local_rank,
			world_size=world_size,
		)

		if "run_sanity_checks" in kwargs:
			warnings.warn(
				"'run_sanity_checks' is deprecated, please use 'run_confidence_checks' instead.",
				DeprecationWarning,
			)
			run_confidence_checks = kwargs["run_sanity_checks"]

		self.model = model

		self.data_loader = data_loader
		self.data_loader.set_target_device(self.cuda_device)
		self._validation_data_loader = validation_data_loader
		if self._validation_data_loader is not None:
			self._validation_data_loader.set_target_device(self.cuda_device)
		self.optimizer = optimizer

		if patience is None:  # no early stopping
			if validation_data_loader is not None:
				logger.warning(
					"You provided a validation dataset but patience was set to None, "
					"meaning that early stopping is disabled"
				)
		elif (not isinstance(patience, int)) or patience <= 0:
			raise ConfigurationError(
				'{} is an invalid value for "patience": it must be a positive integer '
				"or None (if you want to disable early stopping)".format(patience)
			)

		self._metric_tracker = MetricTracker(validation_metric, patience)

		self._num_epochs = num_epochs

		self._checkpointer: Optional[Checkpointer] = checkpointer

		self._grad_norm = grad_norm
		self._grad_clipping = grad_clipping

		self._learning_rate_scheduler = learning_rate_scheduler
		self._momentum_scheduler = momentum_scheduler
		self._moving_average = moving_average

		self._callbacks = callbacks or []
		default_callbacks = list(DEFAULT_CALLBACKS) if enable_default_callbacks else []

		if run_confidence_checks:
			default_callbacks.append(ConfidenceChecksCallback)
		for callback_cls in default_callbacks:
			for callback in self._callbacks:
				if callback.__class__ == callback_cls:
					break
			else:
				self._callbacks.append(callback_cls(self._serialization_dir))

		self._num_gradient_accumulation_steps = num_gradient_accumulation_steps

		self._ddp_wrapped_model = ddp_wrapped_model
		if distributed:
			if ddp_wrapped_model is None:
				raise ValueError("trainer requires 'ddp_wrapped_model' for distributed training")
			if self._checkpointer is not None:
				self._checkpointer.state_is_sharded = ddp_wrapped_model.is_sharded

		self._scaler: Optional[amp.GradScaler] = None
		self._use_amp = use_amp
		if self._use_amp:
			if self.cuda_device == torch.device("cpu"):
				raise ValueError("Using AMP requires a cuda device")
			if grad_scaling:
				if self._ddp_wrapped_model is None:
					self._scaler = amp.GradScaler()
				else:
					self._scaler = self._ddp_wrapped_model.init_grad_scaler()

		self._epochs_completed: int = 0
		self._start_after_epochs_completed: int = 0
		self._batches_in_epoch_completed: int = 0
		self._start_after_batches_in_epoch_completed: int = 0
		self._best_model_filename: Optional[str] = None
		self._should_validate_this_epoch: bool = True

		self._total_batches_completed: int = 0

	@property
	def _pytorch_model(self):
		if self._ddp_wrapped_model is None:
			return self.model
		return self._ddp_wrapped_model.model

	def clip_gradient(self):
		if self._grad_clipping is not None:
			if self._scaler is not None:
				optimizer_state = self._scaler._per_optimizer_states[id(self.optimizer)]
				if optimizer_state["stage"] is not OptState.UNSCALED:
					self._scaler.unscale_(self.optimizer)
			torch.nn.utils.clip_grad_value_(
				[p for p in self.model.parameters() if p.grad is not None], self._grad_clipping
			)

	def rescale_gradients(self) -> Optional[float]:
		if not isinstance(self._grad_norm, bool):
			if self._scaler is not None:
				self._scaler.unscale_(self.optimizer)
			if self._ddp_wrapped_model is not None:
				return self._ddp_wrapped_model.clip_grad_norm_(self._grad_norm).item()
			else:
				parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]
				return clip_grad_norm_(parameters_to_clip, self._grad_norm).item()
		elif self._grad_norm:
			parameters_to_clip = [p for p in self.model.parameters() if p.grad is not None]
			return torch.norm(
				torch.stack([torch.norm(p.grad.detach()) for p in parameters_to_clip])
			).item()
		else:
			return None

	def batch_outputs(self, batch: TensorDict, for_training: bool) -> Dict[str, torch.Tensor]:
		output_dict = self._pytorch_model(**batch)

		if for_training:
			try:
				assert "loss" in output_dict
				regularization_penalty = self.model.get_regularization_penalty()

				if regularization_penalty is not None:
					output_dict["reg_loss"] = regularization_penalty
					output_dict["loss"] += regularization_penalty

			except AssertionError:
				if for_training:
					raise RuntimeError(
						"The model you are trying to optimize does not contain a"
						" 'loss' key in the output of model.forward(inputs)."
					)

		return output_dict

	def _train_epoch(self, epoch: int) -> Dict[str, float]:
		logger.info("Epoch %d/%d", epoch, self._num_epochs - 1)
		cpu_memory_usage = []
		for worker, memory in common_util.peak_cpu_memory().items():
			cpu_memory_usage.append((worker, memory))
			logger.info(f"Worker {worker} memory usage: {common_util.format_size(memory)}")
		gpu_memory_usage = []
		for gpu, memory in common_util.peak_gpu_memory().items():
			gpu_memory_usage.append((gpu, memory))
			logger.info(f"GPU {gpu} memory usage: {common_util.format_size(memory)}")

		regularization_penalty = self.model.get_regularization_penalty()

		train_loss = 0.0
		train_reg_loss = None if regularization_penalty is None else 0.0
		batch_reg_loss = None if regularization_penalty is None else 0.0

		self._pytorch_model.train()

		batch_generator = iter(self.data_loader)
		batch_group_generator = common_util.lazy_groups_of(
			batch_generator, self._num_gradient_accumulation_steps
		)

		logger.info("Training")

		num_training_batches: Union[int, float]
		try:
			len_data_loader = len(self.data_loader)
			num_training_batches = math.ceil(
				len_data_loader / self._num_gradient_accumulation_steps
			)
		except TypeError:
			num_training_batches = float("inf")

		if self._primary:
			batch_group_generator_tqdm = Tqdm.tqdm(
				batch_group_generator, total=num_training_batches
			)
		else:
			batch_group_generator_tqdm = batch_group_generator

		done_early = False
		for batch_group in batch_group_generator_tqdm:
			if done_early:
				break

			if self._epochs_completed < self._start_after_epochs_completed or (
				self._epochs_completed == self._start_after_epochs_completed
				and self._batches_in_epoch_completed < self._start_after_batches_in_epoch_completed
			):
				self._batches_in_epoch_completed += 1
				self._total_batches_completed += 1
				continue

			self.optimizer.zero_grad()

			batch_loss = 0.0
			batch_group_outputs = []
			for batch in batch_group:
				if self._distributed:
					done = torch.tensor(0, device=self.cuda_device)
					torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)
					if done.item() > 0:
						done_early = True
						logger.warning(
							f"Worker {torch.distributed.get_rank()} finishing training early! "
							"This implies that there is an imbalance in your training "
							"data across the workers and that some amount of it will be "
							"ignored. A small amount of this is fine, but a major imbalance "
							"should be avoided. Note: This warning will appear unless your "
							"data is perfectly balanced."
						)
						break

				with amp.autocast(self._use_amp):
					batch_outputs = self.batch_outputs(batch, for_training=True)
					batch_group_outputs.append(batch_outputs)
					loss = batch_outputs["loss"]
					reg_loss = batch_outputs.get("reg_loss")
					if torch.isnan(loss):
						raise ValueError("nan loss encountered")
					loss = loss / len(batch_group)

					batch_loss += loss.item()
					if reg_loss is not None:
						reg_loss = reg_loss / len(batch_group)
						batch_reg_loss = reg_loss.item()
						train_reg_loss += batch_reg_loss  # type: ignore

				backward_called = False
				for callback in self._callbacks:
					backward_called |= callback.on_backward(self, batch_outputs, backward_called)
				if not backward_called:
					if self._scaler is not None:
						MixedPrecisionBackwardCallback(self._serialization_dir).on_backward(
							self, batch_outputs, backward_called
						)
					else:
						loss.backward()

			if len(batch_group_outputs) <= 0:
				continue

			train_loss += batch_loss

			batch_grad_norm = self.rescale_gradients()
			self.clip_gradient()

			if self._learning_rate_scheduler:
				self._learning_rate_scheduler.step_batch(self._total_batches_completed + 1)
			if self._momentum_scheduler:
				self._momentum_scheduler.step_batch(self._total_batches_completed + 1)

			if self._scaler is not None:
				self._scaler.step(self.optimizer)
				self._scaler.update()
			else:
				self.optimizer.step()

			if self._moving_average is not None:
				self._moving_average.apply(self._total_batches_completed + 1)

			self._batches_in_epoch_completed += 1
			self._total_batches_completed += 1

			metrics = training_util.get_metrics(
				self.model,
				train_loss,
				train_reg_loss,
				batch_loss,
				batch_reg_loss,
				self._batches_in_epoch_completed,
			)

			for callback in self._callbacks:
				callback.on_batch(
					self,
					batch_group,
					batch_group_outputs,
					metrics,
					epoch,
					self._batches_in_epoch_completed,
					is_training=True,
					is_primary=self._primary,
					batch_grad_norm=batch_grad_norm,
				)

			if self._primary:
				description = training_util.description_from_metrics(metrics)
				batch_group_generator_tqdm.set_description(description, refresh=False)

			if self._checkpointer is not None:
				self._checkpointer.maybe_save_checkpoint(
					self, self._epochs_completed, self._batches_in_epoch_completed
				)

		if self._distributed and not done_early:
			logger.info(
				f"Worker {torch.distributed.get_rank()} completed its entire epoch (training)."
			)
			done = torch.tensor(1, device=self.cuda_device)
			torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)
			assert done.item()

		if self._distributed:
			dist.barrier()

		if self._epochs_completed < self._start_after_epochs_completed or (
			self._epochs_completed == self._start_after_epochs_completed
			and self._batches_in_epoch_completed - 1 < self._start_after_batches_in_epoch_completed
		):
			metrics = {}
		else:
			train_loss = dist_reduce_sum(train_loss)
			num_batches = dist_reduce_sum(self._batches_in_epoch_completed)
			if train_reg_loss is not None:
				train_reg_loss = dist_reduce_sum(train_reg_loss)

			metrics = training_util.get_metrics(
				self.model,
				train_loss,
				train_reg_loss,
				batch_loss=None,
				batch_reg_loss=None,
				num_batches=num_batches,
				reset=True,
			)

		for (worker, memory) in cpu_memory_usage:
			metrics["worker_" + str(worker) + "_memory_MB"] = memory / (1024 * 1024)
		for (gpu_num, memory) in gpu_memory_usage:
			metrics["gpu_" + str(gpu_num) + "_memory_MB"] = memory / (1024 * 1024)
		return metrics

	def _validation_loss(self, epoch: int) -> Tuple[float, Optional[float], int]:
		logger.info("Validating")

		self._pytorch_model.eval()

		if self._moving_average is not None:
			self._moving_average.assign_average_value()
		try:
			if self._validation_data_loader is not None:
				validation_data_loader = self._validation_data_loader
			else:
				raise ConfigurationError(
					"Validation results cannot be calculated without a validation_data_loader"
				)

			regularization_penalty = self.model.get_regularization_penalty()

			if self._primary:
				val_generator_tqdm = Tqdm.tqdm(validation_data_loader)
			else:
				val_generator_tqdm = validation_data_loader

			batches_this_epoch = 0
			val_loss = 0.0
			val_batch_loss = 0.0
			val_reg_loss = None if regularization_penalty is None else 0.0
			val_batch_reg_loss = None if regularization_penalty is None else 0.0
			done_early = False
			for batch in val_generator_tqdm:
				if self._distributed:
					done = torch.tensor(0, device=self.cuda_device)
					torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)
					if done.item() > 0:
						done_early = True
						logger.warning(
							f"Worker {torch.distributed.get_rank()} finishing validation early! "
							"This implies that there is an imbalance in your validation "
							"data across the workers and that some amount of it will be "
							"ignored. A small amount of this is fine, but a major imbalance "
							"should be avoided. Note: This warning will appear unless your "
							"data is perfectly balanced."
						)
						break

				with amp.autocast(self._use_amp):
					batch_outputs = self.batch_outputs(batch, for_training=False)
					loss = batch_outputs.get("loss")
					reg_loss = batch_outputs.get("reg_loss")
					if loss is not None:
						batches_this_epoch += 1
						val_batch_loss = loss.item()
						val_loss += val_batch_loss
						if reg_loss is not None:
							val_batch_reg_loss = reg_loss.item()
							val_reg_loss += val_batch_reg_loss  # type: ignore

				val_metrics = training_util.get_metrics(
					self.model,
					val_loss,
					val_reg_loss,
					val_batch_loss,
					val_batch_reg_loss,
					batches_this_epoch,
				)

				description = training_util.description_from_metrics(val_metrics)
				if self._primary:
					val_generator_tqdm.set_description(description, refresh=False)

				for callback in self._callbacks:
					callback.on_batch(
						self,
						[batch],
						[batch_outputs],
						val_metrics,
						epoch,
						batches_this_epoch,
						is_training=False,
						is_primary=self._primary,
					)

			if self._distributed and not done_early:
				logger.warning(
					f"Worker {torch.distributed.get_rank()} completed its entire epoch (validation)."
				)
				done = torch.tensor(1, device=self.cuda_device)
				torch.distributed.all_reduce(done, torch.distributed.ReduceOp.SUM)
				assert done.item()

			return val_loss, val_reg_loss, batches_this_epoch
		finally:
			if self._moving_average is not None:
				self._moving_average.restore()

	def train(self) -> Dict[str, Any]:
		try:
			self._maybe_restore_checkpoint()
		except RuntimeError as e:
			configuration_error = ConfigurationError(
				f"Could not recover training from the checkpoint in {self._serialization_dir}. "
				"Did you mean to output to a different serialization directory or delete the "
				"existing serialization directory?"
			)
			configuration_error.__cause__ = e
			raise configuration_error

		for callback in self._callbacks:
			callback.on_start(self, is_primary=self._primary)

		epoch = None
		metrics = None

		try:
			metrics, epoch = self._try_train()
			return metrics
		finally:
			if self._primary:
				self._finalize_best_model_state()
			for callback in self._callbacks:
				callback.on_end(self, metrics=metrics, epoch=epoch, is_primary=self._primary)

	def _try_train(self) -> Tuple[Dict[str, Any], int]:

		logger.info("Beginning training.")

		val_metrics: Dict[str, float] = {}
		metrics: Dict[str, Any] = {}
		training_start_time = None

		metrics["best_epoch"] = self._metric_tracker.best_epoch
		for key, value in self._metric_tracker.best_epoch_metrics.items():
			metrics["best_validation_" + key] = value

		for epoch in range(self._num_epochs):
			epoch_start_time = time.time()
			train_metrics = self._train_epoch(epoch)

			if self._epochs_completed < self._start_after_epochs_completed:
				self._epochs_completed += 1
				self._batches_in_epoch_completed = 0
				continue
			if training_start_time is None:
				training_start_time = epoch_start_time

			for key, value in train_metrics.items():
				if key.startswith("gpu_") and key.endswith("_memory_MB"):
					metrics["peak_" + key] = max(metrics.get("peak_" + key, 0), value)
				elif key.startswith("worker_") and key.endswith("_memory_MB"):
					metrics["peak_" + key] = max(metrics.get("peak_" + key, 0), value)

			this_epoch_val_metric: Optional[float] = None
			if self._should_validate_this_epoch and self._validation_data_loader is not None:
				with torch.no_grad():
					val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)

					if self._distributed:
						dist.barrier()

					val_loss = dist_reduce_sum(val_loss)
					num_batches = dist_reduce_sum(num_batches)
					if val_reg_loss is not None:
						val_reg_loss = dist_reduce_sum(val_reg_loss)

					val_metrics = training_util.get_metrics(
						self.model,
						val_loss,
						val_reg_loss,
						batch_loss=None,
						batch_reg_loss=None,
						num_batches=num_batches,
						reset=True,
					)

					this_epoch_val_metric = self._metric_tracker.combined_score(val_metrics)
					self._metric_tracker.add_metrics(val_metrics)

			training_elapsed_time = time.time() - training_start_time
			metrics["training_duration"] = str(datetime.timedelta(seconds=training_elapsed_time))
			metrics["epoch"] = epoch

			for key, value in train_metrics.items():
				metrics["training_" + key] = value
			for key, value in val_metrics.items():
				metrics["validation_" + key] = value

			if self._should_validate_this_epoch and self._metric_tracker.is_best_so_far():
				metrics["best_epoch"] = epoch
				for key, value in val_metrics.items():
					metrics["best_validation_" + key] = value

				self._metric_tracker.best_epoch_metrics = val_metrics

			if self._serialization_dir and self._primary:
				common_util.dump_metrics(
					os.path.join(self._serialization_dir, f"metrics_epoch_{epoch}.json"),
					metrics,
				)

			if self._learning_rate_scheduler:
				self._learning_rate_scheduler.step(this_epoch_val_metric)
			if self._momentum_scheduler:
				self._momentum_scheduler.step(this_epoch_val_metric)
			for callback in self._callbacks:
				callback.on_epoch(self, metrics=metrics, epoch=epoch, is_primary=self._primary)

			self._epochs_completed += 1
			self._batches_in_epoch_completed = 0

			checkpoint_saved = False
			if self._checkpointer is not None:
				checkpoint_saved = self._checkpointer.maybe_save_checkpoint(
					self, self._epochs_completed, self._batches_in_epoch_completed
				)

				if self._distributed:
					dist.barrier()

			if (
				self._should_validate_this_epoch
				and self._serialization_dir
				and self._metric_tracker.is_best_so_far()
			):
				should_save_model_state: bool
				if self._ddp_wrapped_model is not None and self._ddp_wrapped_model.is_sharded:
					self._best_model_filename = os.path.join(
						self._serialization_dir, f"best_w{self._rank}.th"
					)
					should_save_model_state = True
				else:
					self._best_model_filename = os.path.join(self._serialization_dir, "best.th")
					should_save_model_state = self._primary

				if should_save_model_state:
					if self._moving_average is None:
						if self._checkpointer is not None and checkpoint_saved:
							last_checkpoint = self._checkpointer.find_latest_checkpoint()
							assert last_checkpoint is not None
							model_state_file, _ = last_checkpoint
							if os.path.exists(self._best_model_filename):
								os.remove(self._best_model_filename)
							hardlink_or_copy(model_state_file, self._best_model_filename)
						else:
							self._save_model_state(self._best_model_filename)
					else:
						self._moving_average.assign_average_value()
						try:
							self._save_model_state(self._best_model_filename)
						finally:
							self._moving_average.restore()

			if self._distributed:
				dist.barrier()

			epoch_elapsed_time = time.time() - epoch_start_time
			logger.info("Epoch duration: %s", datetime.timedelta(seconds=epoch_elapsed_time))

			if self._metric_tracker.should_stop_early():
				logger.info("Ran out of patience. Stopping training.")
				break

			if epoch < self._num_epochs - 1:
				time_per_epoch = training_elapsed_time / (
					(epoch + 1) - self._start_after_epochs_completed
				)
				estimated_time_remaining = (
					time_per_epoch * self._num_epochs
				) - training_elapsed_time
				formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))
				logger.info("Estimated training time remaining: %s", formatted_time)
		else:
			epoch = self._num_epochs - 1

		if self._best_model_filename is None or self._metric_tracker.is_best_so_far():
			self._finalize_model()
		else:
			self._load_model_state(self._best_model_filename)

		return metrics, epoch

	def _save_model_state(self, path: str) -> None:
		if self._ddp_wrapped_model is not None:
			torch.save(self._ddp_wrapped_model.state_dict(), path)
		else:
			torch.save(self.model.state_dict(), path)

	def _load_model_state(self, path: str) -> None:
		device = torch.device("cpu")
		if self._ddp_wrapped_model is not None:
			self._ddp_wrapped_model.load_state_dict(torch.load(path, map_location=device))
		else:
			self._pytorch_model.load_state_dict(torch.load(path, map_location=device))

	def _finalize_model(self) -> None:
		if self._moving_average is not None:
			self._moving_average.assign_average_value()

	def _finalize_best_model_state(self) -> None:
		if (
			self._serialization_dir
			and self._ddp_wrapped_model is not None
			and self._ddp_wrapped_model.is_sharded
		):
			logger.info("Consolidating sharded model states")
			sharded_model_state_files = list(
				glob.iglob(os.path.join(self._serialization_dir, "best_w*.th"))
			)
			full_model_state = self._ddp_wrapped_model.consolidate_sharded_state(
				sharded_model_state_files
			)
			self._best_model_filename = os.path.join(self._serialization_dir, "best.th")
			torch.save(full_model_state, self._best_model_filename)

	def get_checkpoint_state(self) -> Optional[TrainerCheckpoint]:
		if self._distributed:
			assert self._ddp_wrapped_model is not None
			if self._ddp_wrapped_model.is_sharded or self._primary:
				model_state = self._ddp_wrapped_model.state_dict()
			else:
				return None
		else:
			model_state = self.model.state_dict()

		training_states = {
			"version": 1,
			"metric_tracker": self._metric_tracker.state_dict(),
			"optimizer": self.optimizer.state_dict(),
			"callbacks": [cb.state_dict() for cb in self._callbacks],
			"epochs_completed": self._epochs_completed,
			"batches_in_epoch_completed": self._batches_in_epoch_completed,
			"best_model_filename": self._best_model_filename,
		}

		if self._learning_rate_scheduler is not None:
			training_states["learning_rate_scheduler"] = self._learning_rate_scheduler.state_dict()
		if self._momentum_scheduler is not None:
			training_states["momentum_scheduler"] = self._momentum_scheduler.state_dict()
		if self._moving_average is not None:
			training_states["moving_average"] = self._moving_average.state_dict()

		return TrainerCheckpoint(model_state, training_states)

	def _maybe_restore_checkpoint(self) -> None:
		if self._checkpointer is None:
			return

		state = self._checkpointer.load_checkpoint()
		if state is None:
			self._start_after_epochs_completed = 0
			self._start_after_batches_in_epoch_completed = 0
			self._best_model_filename = None
			return

		model_state, training_state = state
		if training_state["version"] != 1:
			raise ValueError(
				f"This version of {self.__class__.__name__} only supports checkpoints of version 1. "
				f"Found version {training_state['version']}"
			)

		if self._distributed:
			assert self._ddp_wrapped_model is not None
			self._ddp_wrapped_model.load_state_dict(model_state)
		else:
			self._pytorch_model.load_state_dict(model_state)

		self._metric_tracker.load_state_dict(training_state["metric_tracker"])
		self.optimizer.load_state_dict(training_state["optimizer"])

		for cb, state_dict in zip(self._callbacks, training_state["callbacks"]):
			cb.load_state_dict(state_dict)

		if self._learning_rate_scheduler is not None:
			self._learning_rate_scheduler.load_state_dict(training_state["learning_rate_scheduler"])
		if self._momentum_scheduler is not None:
			self._momentum_scheduler.load_state_dict(training_state["momentum_scheduler"])
		if self._moving_average is not None:
			self._moving_average.load_state_dict(training_state["moving_average"])

		self._start_after_epochs_completed = training_state["epochs_completed"]
		self._start_after_batches_in_epoch_completed = training_state["batches_in_epoch_completed"]
		self._best_model_filename = training_state["best_model_filename"]

	@classmethod
	def from_partial_objects(
		cls,
		model: Model,
		serialization_dir: str,
		data_loader: DataLoader,
		validation_data_loader: DataLoader = None,
		local_rank: int = 0,
		patience: int = None,
		validation_metric: Union[str, List[str]] = "-loss",
		num_epochs: int = 20,
		cuda_device: Optional[Union[int, torch.device]] = None,
		grad_norm: Union[float, bool] = False,
		grad_clipping: float = None,
		distributed: bool = False,
		world_size: int = 1,
		num_gradient_accumulation_steps: int = 1,
		use_amp: bool = False,
		no_grad: List[str] = None,
		optimizer: Lazy[Optimizer] = Lazy(Optimizer.default),
		learning_rate_scheduler: Lazy[LearningRateScheduler] = None,
		momentum_scheduler: Lazy[MomentumScheduler] = None,
		moving_average: Lazy[MovingAverage] = None,
		checkpointer: Optional[Lazy[Checkpointer]] = Lazy(Checkpointer),
		callbacks: List[Lazy[TrainerCallback]] = None,
		enable_default_callbacks: bool = True,
		run_confidence_checks: bool = True,
		grad_scaling: bool = True,
		ddp_accelerator: Optional[DdpAccelerator] = None,
		**kwargs,
	) -> Trainer:
		if cuda_device is None:
			from torch import cuda

			if cuda.device_count() > 0:
				cuda_device = 0
			else:
				cuda_device = -1

		check_for_gpu(cuda_device)
		ddp_wrapped_model: Optional[DdpWrappedModel] = None
		if distributed:
			if ddp_accelerator is None:
				ddp_accelerator = TorchDdpAccelerator(cuda_device=cuda_device)
			model, ddp_wrapped_model = ddp_accelerator.wrap_model(model)
		else:
			if cuda_device >= 0:
				model = model.cuda(cuda_device)

		pytorch_model = model if ddp_wrapped_model is None else ddp_wrapped_model.model

		if no_grad:
			for name, parameter in pytorch_model.named_parameters():
				if any(re.search(regex, name) for regex in no_grad):
					parameter.requires_grad_(False)

		parameters = [[n, p] for n, p in pytorch_model.named_parameters() if p.requires_grad]
		optimizer_ = optimizer.construct(model_parameters=parameters)

		common_util.log_frozen_and_tunable_parameter_names(pytorch_model)

		batches_per_epoch: Optional[int]
		try:
			batches_per_epoch = len(data_loader)
			batches_per_epoch = math.ceil(batches_per_epoch / num_gradient_accumulation_steps)
		except TypeError:
			batches_per_epoch = None

		moving_average_ = (
			None if moving_average is None else moving_average.construct(parameters=parameters)
		)
		learning_rate_scheduler_ = (
			None
			if learning_rate_scheduler is None
			else learning_rate_scheduler.construct(
				optimizer=optimizer_, num_epochs=num_epochs, num_steps_per_epoch=batches_per_epoch
			)
		)
		momentum_scheduler_ = (
			None
			if momentum_scheduler is None
			else momentum_scheduler.construct(optimizer=optimizer_)
		)
		checkpointer_ = (
			None
			if checkpointer is None
			else checkpointer.construct(serialization_dir=serialization_dir)
		)

		callbacks_: List[TrainerCallback] = []
		for callback_ in callbacks or []:
			callbacks_.append(callback_.construct(serialization_dir=serialization_dir))

		return cls(
			model,
			optimizer_,
			data_loader,
			patience=patience,
			validation_metric=validation_metric,
			validation_data_loader=validation_data_loader,
			num_epochs=num_epochs,
			serialization_dir=serialization_dir,
			cuda_device=cuda_device,
			grad_norm=grad_norm,
			grad_clipping=grad_clipping,
			learning_rate_scheduler=learning_rate_scheduler_,
			momentum_scheduler=momentum_scheduler_,
			checkpointer=checkpointer_,
			moving_average=moving_average_,
			callbacks=callbacks_,
			distributed=distributed,
			local_rank=local_rank,
			world_size=world_size,
			num_gradient_accumulation_steps=num_gradient_accumulation_steps,
			use_amp=use_amp,
			enable_default_callbacks=enable_default_callbacks,
			run_confidence_checks=run_confidence_checks,
			grad_scaling=grad_scaling,
			ddp_wrapped_model=ddp_wrapped_model,
			**kwargs,
		)

	def get_best_weights_path(self) -> Optional[str]:
		if self._best_model_filename is not None:
			return os.path.abspath(self._best_model_filename)
		else:
			return None


DEFAULT_CALLBACKS: Tuple[Type[TrainerCallback]] = (ConsoleLoggerCallback,)

from collections import defaultdict
from typing import Any, Dict, Iterable, Union, List, Mapping

import more_itertools

from allennlp.common.registrable import Registrable
from allennlp.data.instance import Instance


class MultiTaskScheduler(Registrable):

	def batch_instances(
		self, epoch_instances: Dict[str, Iterable[Instance]]
	) -> Iterable[List[Instance]]:
		raise NotImplementedError

	def update_from_epoch_metrics(self, epoch_metrics: Dict[str, Any]) -> None:
		raise NotImplementedError

	def count_batches(self, dataset_counts: Dict[str, int]) -> int:
		raise NotImplementedError

	default_implementation = "homogeneous_roundrobin"


def _chunked_iterator(i: Iterable, chunk_size: int, drop_last: bool):
	chunks = more_itertools.chunked(i, chunk_size)
	if drop_last:
		return (chunk for chunk in chunks if len(chunk) == chunk_size)
	else:
		return chunks


@MultiTaskScheduler.register("roundrobin")
class RoundRobinScheduler(MultiTaskScheduler):

	def __init__(self, batch_size: int, drop_last: bool = False):
		super().__init__()
		self.batch_size = batch_size
		self.drop_last = drop_last

	def batch_instances(
		self, epoch_instances: Dict[str, Iterable[Instance]]
	) -> Iterable[List[Instance]]:
		return _chunked_iterator(
			more_itertools.roundrobin(*epoch_instances.values()), self.batch_size, self.drop_last
		)

	def count_batches(self, dataset_counts: Dict[str, int]) -> int:
		instance_count = sum(dataset_counts.values())
		if self.drop_last or instance_count % self.batch_size == 0:
			return instance_count // self.batch_size
		else:
			return 1 + (instance_count // self.batch_size)


@MultiTaskScheduler.register("homogeneous_roundrobin")
class HomogeneousRoundRobinScheduler(MultiTaskScheduler):

	def __init__(self, batch_size: Union[int, Dict[str, int]], drop_last: bool = False):
		self.batch_size: Mapping[str, int]
		if isinstance(batch_size, int):
			self.batch_size = defaultdict(lambda: batch_size)  # type: ignore
		else:
			self.batch_size = batch_size
		self.drop_last = drop_last

	def batch_instances(
		self, epoch_instances: Dict[str, Iterable[Instance]]
	) -> Iterable[List[Instance]]:
		chunked_iterators = [
			_chunked_iterator(iterator, self.batch_size[dataset], self.drop_last)
			for dataset, iterator in epoch_instances.items()
		]
		return more_itertools.roundrobin(*chunked_iterators)

	def count_batches(self, dataset_counts: Dict[str, int]) -> int:
		result = 0
		for dataset, count in dataset_counts.items():
			batch_size = self.batch_size[dataset]
			result += count // batch_size
			if not self.drop_last and count % batch_size != 0:
				result += 1
		return result

from typing import Dict

import torch

from allennlp.common import Registrable


class Backbone(torch.nn.Module, Registrable):

	def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
		raise NotImplementedError

	def make_output_human_readable(
		self, output_dict: Dict[str, torch.Tensor]
	) -> Dict[str, torch.Tensor]:
		return output_dict

import logging
import math
from typing import List, Iterable, Tuple, Sequence, Optional
import random

from allennlp.common.checks import ConfigurationError
from allennlp.common.util import lazy_groups_of
from allennlp.data.instance import Instance
from allennlp.data.samplers.batch_sampler import BatchSampler


logger = logging.getLogger(__name__)


def add_noise_to_value(value: int, noise_param: float):
	noise_value = value * noise_param
	noise = random.uniform(-noise_value, noise_value)
	return value + noise


@BatchSampler.register("bucket")
class BucketBatchSampler(BatchSampler):

	def __init__(
		self,
		batch_size: int,
		sorting_keys: List[str] = None,
		padding_noise: float = 0.1,
		drop_last: bool = False,
		shuffle: bool = True,
	):
		self.sorting_keys = sorting_keys
		self.padding_noise = padding_noise
		self.batch_size = batch_size
		self.drop_last = drop_last
		self.shuffle = shuffle
		if not shuffle:
			self.padding_noise = 0.0

	def _argsort_by_padding(
		self, instances: Iterable[Instance]
	) -> Tuple[List[int], List[List[int]]]:
		if not self.sorting_keys:
			logger.info("No sorting keys given; trying to guess a good one")
			self._guess_sorting_keys(instances)
			logger.info(f"Using {self.sorting_keys} as the sorting keys")
		instances_with_lengths = []
		for instance in instances:
			lengths = []
			noisy_lengths = []
			for field_name in self.sorting_keys:  # type: ignore
				if field_name not in instance.fields:
					raise ConfigurationError(
						f'Sorting key "{field_name}" is not a field in instance. '
						f"Available fields/keys are {list(instance.fields.keys())}."
					)
				lengths.append(len(instance.fields[field_name]))

				noisy_lengths.append(add_noise_to_value(lengths[-1], self.padding_noise))
			instances_with_lengths.append((noisy_lengths, lengths, instance))
		with_indices = [(x, i) for i, x in enumerate(instances_with_lengths)]
		with_indices.sort(key=lambda x: x[0][0])
		return (
			[instance_with_index[-1] for instance_with_index in with_indices],
			[instance_with_index[0][1] for instance_with_index in with_indices],
		)

	def get_batch_indices(self, instances: Sequence[Instance]) -> Iterable[List[int]]:
		indices, _ = self._argsort_by_padding(instances)
		batches = []
		for group in lazy_groups_of(indices, self.batch_size):
			batch_indices = list(group)
			if self.drop_last and len(batch_indices) < self.batch_size:
				continue
			batches.append(batch_indices)
		if self.shuffle:
			random.shuffle(batches)
		for batch in batches:
			yield batch

	def _guess_sorting_keys(self, instances: Iterable[Instance], num_instances: int = 10) -> None:
		max_length = 0.0
		longest_field: Optional[str] = None
		for i, instance in enumerate(instances):
			for field_name, field in instance.fields.items():
				length = len(field)
				if length > max_length:
					max_length = length
					longest_field = field_name
			if i > num_instances:
				break

		if not longest_field:
			raise AssertionError(
				"Found no field that needed padding; we are surprised you got this error, please "
				"open an issue on github"
			)
		self.sorting_keys = [longest_field]

	def get_num_batches(self, instances: Sequence[Instance]) -> int:
		batch_count_float = len(instances) / self.batch_size
		if self.drop_last:
			return math.floor(batch_count_float)
		else:
			return math.ceil(batch_count_float)

	def get_batch_size(self) -> Optional[int]:
		return self.batch_size


import argparse
import json
import logging
import math
import os
import random
from pathlib import Path

import datasets
import evaluate
import numpy as np
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from utils_qa import postprocess_qa_predictions_with_beam_search

import transformers
from transformers import (
	AdamW,
	DataCollatorWithPadding,
	EvalPrediction,
	SchedulerType,
	XLNetConfig,
	XLNetForQuestionAnswering,
	XLNetTokenizerFast,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/question-answering/requirements.txt")

logger = get_logger(__name__)


def save_prefixed_metrics(results, output_dir, file_name: str = "all_results.json", metric_key_prefix: str = "eval"):
	for key in list(results.keys()):
		if not key.startswith(f"{metric_key_prefix}_"):
			results[f"{metric_key_prefix}_{key}"] = results.pop(key)

	with open(os.path.join(output_dir, file_name), "w") as f:
		json.dump(results, f, indent=4)


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a Question Answering task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)
	parser.add_argument(
		"--preprocessing_num_workers", type=int, default=1, help="A csv or a json file containing the training data."
	)
	parser.add_argument("--do_predict", action="store_true", help="Eval the question answering model")
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--test_file", type=str, default=None, help="A csv or a json file containing the Prediction data."
	)
	parser.add_argument(
		"--max_seq_length",
		type=int,
		default=384,
		help=(
			"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
			" sequences shorter will be padded if `--pad_to_max_lengh` is passed."
		),
	)
	parser.add_argument(
		"--pad_to_max_length",
		action="store_true",
		help="If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=True,
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--doc_stride",
		type=int,
		default=128,
		help="When splitting up a long document into chunks how much stride to take between chunks.",
	)
	parser.add_argument(
		"--n_best_size",
		type=int,
		default=20,
		help="The total number of n-best predictions to generate when looking for an answer.",
	)
	parser.add_argument(
		"--null_score_diff_threshold",
		type=float,
		default=0.0,
		help=(
			"The threshold used to select the null answer: if the best answer has a score that is less than "
			"the score of the null answer minus this threshold, the null answer is selected for this example. "
			"Only useful when `version_2_with_negative=True`."
		),
	)
	parser.add_argument(
		"--version_2_with_negative",
		action="store_true",
		help="If true, some of the examples do not have an answer.",
	)
	parser.add_argument(
		"--max_answer_length",
		type=int,
		default=30,
		help=(
			"The maximum length of an answer that can be generated. This is needed because the start "
			"and end predictions are not conditioned on one another."
		),
	)
	parser.add_argument(
		"--max_train_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of training examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--max_eval_samples",
		type=int,
		default=None,
		help=(
			"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
			"value if set."
		),
	)
	parser.add_argument(
		"--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
	)
	parser.add_argument(
		"--max_predict_samples",
		type=int,
		default=None,
		help="For debugging purposes or quicker training, truncate the number of prediction examples to this",
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to load in all available experiment trackers from the environment and use them for logging.",
	)
	args = parser.parse_args()

	if (
		args.dataset_name is None
		and args.train_file is None
		and args.validation_file is None
		and args.test_file is None
	):
		raise ValueError("Need either a dataset name or a training/validation/test file.")
	else:
		if args.train_file is not None:
			extension = args.train_file.split(".")[-1]
			assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
		if args.validation_file is not None:
			extension = args.validation_file.split(".")[-1]
			assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
		if args.test_file is not None:
			extension = args.test_file.split(".")[-1]
			assert extension in ["csv", "json"], "`test_file` should be a csv or a json file."

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_qa_beam_search_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		if args.test_file is not None:
			data_files["test"] = args.test_file
		extension = args.train_file.split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files, field="data")


	config = XLNetConfig.from_pretrained(args.model_name_or_path)
	tokenizer = XLNetTokenizerFast.from_pretrained(args.model_name_or_path)
	model = XLNetForQuestionAnswering.from_pretrained(
		args.model_name_or_path, from_tf=bool(".ckpt" in args.model_name_or_path), config=config
	)

	column_names = raw_datasets["train"].column_names

	question_column_name = "question" if "question" in column_names else column_names[0]
	context_column_name = "context" if "context" in column_names else column_names[1]
	answer_column_name = "answers" if "answers" in column_names else column_names[2]

	pad_on_right = tokenizer.padding_side == "right"

	if args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)

	max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)

	def prepare_train_features(examples):
		examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]

		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			return_special_tokens_mask=True,
			return_token_type_ids=True,
			padding="max_length",
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
		offset_mapping = tokenized_examples.pop("offset_mapping")
		special_tokens = tokenized_examples.pop("special_tokens_mask")

		tokenized_examples["start_positions"] = []
		tokenized_examples["end_positions"] = []
		tokenized_examples["is_impossible"] = []
		tokenized_examples["cls_index"] = []
		tokenized_examples["p_mask"] = []

		for i, offsets in enumerate(offset_mapping):
			input_ids = tokenized_examples["input_ids"][i]
			cls_index = input_ids.index(tokenizer.cls_token_id)
			tokenized_examples["cls_index"].append(cls_index)

			sequence_ids = tokenized_examples["token_type_ids"][i]
			for k, s in enumerate(special_tokens[i]):
				if s:
					sequence_ids[k] = 3
			context_idx = 1 if pad_on_right else 0

			tokenized_examples["p_mask"].append(
				[
					0.0 if (not special_tokens[i][k] and s == context_idx) or k == cls_index else 1.0
					for k, s in enumerate(sequence_ids)
				]
			)

			sample_index = sample_mapping[i]
			answers = examples[answer_column_name][sample_index]
			if len(answers["answer_start"]) == 0:
				tokenized_examples["start_positions"].append(cls_index)
				tokenized_examples["end_positions"].append(cls_index)
				tokenized_examples["is_impossible"].append(1.0)
			else:
				start_char = answers["answer_start"][0]
				end_char = start_char + len(answers["text"][0])

				token_start_index = 0
				while sequence_ids[token_start_index] != context_idx:
					token_start_index += 1

				token_end_index = len(input_ids) - 1
				while sequence_ids[token_end_index] != context_idx:
					token_end_index -= 1
				if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
					tokenized_examples["start_positions"].append(cls_index)
					tokenized_examples["end_positions"].append(cls_index)
					tokenized_examples["is_impossible"].append(1.0)
				else:
					while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
						token_start_index += 1
					tokenized_examples["start_positions"].append(token_start_index - 1)
					while offsets[token_end_index][1] >= end_char:
						token_end_index -= 1
					tokenized_examples["end_positions"].append(token_end_index + 1)
					tokenized_examples["is_impossible"].append(0.0)

		return tokenized_examples

	if "train" not in raw_datasets:
		raise ValueError("--do_train requires a train dataset")
	train_dataset = raw_datasets["train"]
	if args.max_train_samples is not None:
		train_dataset = train_dataset.select(range(args.max_train_samples))
	with accelerator.main_process_first():
		train_dataset = train_dataset.map(
			prepare_train_features,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on train dataset",
		)
	if args.max_train_samples is not None:
		train_dataset = train_dataset.select(range(args.max_train_samples))

	def prepare_validation_features(examples):
		examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]

		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			return_special_tokens_mask=True,
			return_token_type_ids=True,
			padding="max_length",
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

		special_tokens = tokenized_examples.pop("special_tokens_mask")

		tokenized_examples["example_id"] = []

		tokenized_examples["cls_index"] = []
		tokenized_examples["p_mask"] = []

		for i, input_ids in enumerate(tokenized_examples["input_ids"]):
			cls_index = input_ids.index(tokenizer.cls_token_id)
			tokenized_examples["cls_index"].append(cls_index)

			sequence_ids = tokenized_examples["token_type_ids"][i]
			for k, s in enumerate(special_tokens[i]):
				if s:
					sequence_ids[k] = 3
			context_idx = 1 if pad_on_right else 0

			tokenized_examples["p_mask"].append(
				[
					0.0 if (not special_tokens[i][k] and s == context_idx) or k == cls_index else 1.0
					for k, s in enumerate(sequence_ids)
				]
			)

			sample_index = sample_mapping[i]
			tokenized_examples["example_id"].append(examples["id"][sample_index])

			tokenized_examples["offset_mapping"][i] = [
				(o if sequence_ids[k] == context_idx else None)
				for k, o in enumerate(tokenized_examples["offset_mapping"][i])
			]

		return tokenized_examples

	if "validation" not in raw_datasets:
		raise ValueError("--do_eval requires a validation dataset")
	eval_examples = raw_datasets["validation"]
	if args.max_eval_samples is not None:
		eval_examples = eval_examples.select(range(args.max_eval_samples))
	with accelerator.main_process_first():
		eval_dataset = eval_examples.map(
			prepare_validation_features,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on validation dataset",
		)

	if args.max_eval_samples is not None:
		eval_dataset = eval_dataset.select(range(args.max_eval_samples))

	if args.do_predict:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_examples = raw_datasets["test"]
		if args.max_predict_samples is not None:
			predict_examples = predict_examples.select(range(args.max_predict_samples))
		with accelerator.main_process_first():
			predict_dataset = predict_examples.map(
				prepare_validation_features,
				batched=True,
				num_proc=args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)
			if args.max_predict_samples is not None:
				predict_dataset = predict_dataset.select(range(args.max_predict_samples))

	for index in random.sample(range(len(train_dataset)), 3):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if args.pad_to_max_length:
		data_collator = default_data_collator
	else:
		data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)

	eval_dataset_for_model = eval_dataset.remove_columns(["example_id", "offset_mapping"])
	eval_dataloader = DataLoader(
		eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
	)

	if args.do_predict:
		predict_dataset_for_model = predict_dataset.remove_columns(["example_id", "offset_mapping"])
		predict_dataloader = DataLoader(
			predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
		)

	def post_processing_function(examples, features, predictions, stage="eval"):
		predictions, scores_diff_json = postprocess_qa_predictions_with_beam_search(
			examples=examples,
			features=features,
			predictions=predictions,
			version_2_with_negative=args.version_2_with_negative,
			n_best_size=args.n_best_size,
			max_answer_length=args.max_answer_length,
			start_n_top=model.config.start_n_top,
			end_n_top=model.config.end_n_top,
			output_dir=args.output_dir,
			prefix=stage,
		)
		if args.version_2_with_negative:
			formatted_predictions = [
				{"id": k, "prediction_text": v, "no_answer_probability": scores_diff_json[k]}
				for k, v in predictions.items()
			]
		else:
			formatted_predictions = [{"id": k, "prediction_text": v} for k, v in predictions.items()]

		references = [{"id": ex["id"], "answers": ex[answer_column_name]} for ex in examples]
		return EvalPrediction(predictions=formatted_predictions, label_ids=references)

	metric = evaluate.load("squad_v2" if args.version_2_with_negative else "squad")

	def create_and_fill_np_array(start_or_end_logits, dataset, max_len):

		step = 0
		logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float32)
		for i, output_logit in enumerate(start_or_end_logits):  # populate columns

			batch_size = output_logit.shape[0]
			cols = output_logit.shape[1]
			if step + batch_size < len(dataset):
				logits_concat[step : step + batch_size, :cols] = output_logit
			else:
				logits_concat[step:, :cols] = output_logit[: len(dataset) - step]

			step += batch_size

		return logits_concat

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("qa_beam_search_no_trainer", experiment_config)

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")

	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()

				accelerator.backward(loss)

				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					accelerator.save_state(f"step_{completed_steps}")

			if completed_steps >= args.max_train_steps:
				break

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

	all_start_top_log_probs = []
	all_start_top_index = []
	all_end_top_log_probs = []
	all_end_top_index = []
	all_cls_logits = []

	model.eval()

	for step, batch in enumerate(eval_dataloader):
		with torch.no_grad():
			outputs = model(**batch)
			start_top_log_probs = outputs.start_top_log_probs
			start_top_index = outputs.start_top_index
			end_top_log_probs = outputs.end_top_log_probs
			end_top_index = outputs.end_top_index
			cls_logits = outputs.cls_logits

			if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered
				start_top_log_probs = accelerator.pad_across_processes(start_top_log_probs, dim=1, pad_index=-100)
				start_top_index = accelerator.pad_across_processes(start_top_index, dim=1, pad_index=-100)
				end_top_log_probs = accelerator.pad_across_processes(end_top_log_probs, dim=1, pad_index=-100)
				end_top_index = accelerator.pad_across_processes(end_top_index, dim=1, pad_index=-100)
				cls_logits = accelerator.pad_across_processes(cls_logits, dim=1, pad_index=-100)

			all_start_top_log_probs.append(accelerator.gather_for_metrics(start_top_log_probs).cpu().numpy())
			all_start_top_index.append(accelerator.gather_for_metrics(start_top_index).cpu().numpy())
			all_end_top_log_probs.append(accelerator.gather_for_metrics(end_top_log_probs).cpu().numpy())
			all_end_top_index.append(accelerator.gather_for_metrics(end_top_index).cpu().numpy())
			all_cls_logits.append(accelerator.gather_for_metrics(cls_logits).cpu().numpy())

	max_len = max([x.shape[1] for x in all_end_top_log_probs])  # Get the max_length of the tensor

	start_top_log_probs_concat = create_and_fill_np_array(all_start_top_log_probs, eval_dataset, max_len)
	start_top_index_concat = create_and_fill_np_array(all_start_top_index, eval_dataset, max_len)
	end_top_log_probs_concat = create_and_fill_np_array(all_end_top_log_probs, eval_dataset, max_len)
	end_top_index_concat = create_and_fill_np_array(all_end_top_index, eval_dataset, max_len)
	cls_logits_concat = np.concatenate(all_cls_logits, axis=0)

	del start_top_log_probs
	del start_top_index
	del end_top_log_probs
	del end_top_index
	del cls_logits

	outputs_numpy = (
		start_top_log_probs_concat,
		start_top_index_concat,
		end_top_log_probs_concat,
		end_top_index_concat,
		cls_logits_concat,
	)
	prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)
	eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)
	logger.info(f"Evaluation metrics: {eval_metric}")

	if args.do_predict:

		all_start_top_log_probs = []
		all_start_top_index = []
		all_end_top_log_probs = []
		all_end_top_index = []
		all_cls_logits = []

		model.eval()

		for step, batch in enumerate(predict_dataloader):
			with torch.no_grad():
				outputs = model(**batch)
				start_top_log_probs = outputs.start_top_log_probs
				start_top_index = outputs.start_top_index
				end_top_log_probs = outputs.end_top_log_probs
				end_top_index = outputs.end_top_index
				cls_logits = outputs.cls_logits

				if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered
					start_top_log_probs = accelerator.pad_across_processes(start_top_log_probs, dim=1, pad_index=-100)
					start_top_index = accelerator.pad_across_processes(start_top_index, dim=1, pad_index=-100)
					end_top_log_probs = accelerator.pad_across_processes(end_top_log_probs, dim=1, pad_index=-100)
					end_top_index = accelerator.pad_across_processes(end_top_index, dim=1, pad_index=-100)
					cls_logits = accelerator.pad_across_processes(cls_logits, dim=1, pad_index=-100)

				all_start_top_log_probs.append(accelerator.gather_for_metrics(start_top_log_probs).cpu().numpy())
				all_start_top_index.append(accelerator.gather_for_metrics(start_top_index).cpu().numpy())
				all_end_top_log_probs.append(accelerator.gather_for_metrics(end_top_log_probs).cpu().numpy())
				all_end_top_index.append(accelerator.gather_for_metrics(end_top_index).cpu().numpy())
				all_cls_logits.append(accelerator.gather_for_metrics(cls_logits).cpu().numpy())

		max_len = max([x.shape[1] for x in all_end_top_log_probs])  # Get the max_length of the tensor

		start_top_log_probs_concat = create_and_fill_np_array(all_start_top_log_probs, predict_dataset, max_len)
		start_top_index_concat = create_and_fill_np_array(all_start_top_index, predict_dataset, max_len)
		end_top_log_probs_concat = create_and_fill_np_array(all_end_top_log_probs, predict_dataset, max_len)
		end_top_index_concat = create_and_fill_np_array(all_end_top_index, predict_dataset, max_len)
		cls_logits_concat = np.concatenate(all_cls_logits, axis=0)

		del start_top_log_probs
		del start_top_index
		del end_top_log_probs
		del end_top_index
		del cls_logits

		outputs_numpy = (
			start_top_log_probs_concat,
			start_top_index_concat,
			end_top_log_probs_concat,
			end_top_index_concat,
			cls_logits_concat,
		)

		prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)
		predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)
		logger.info(f"Predict metrics: {predict_metric}")

	if args.with_tracking:
		log = {
			"squad_v2" if args.version_2_with_negative else "squad": eval_metric,
			"train_loss": total_loss,
			"epoch": epoch,
			"step": completed_steps,
		}
		if args.do_predict:
			log["squad_v2_predict" if args.version_2_with_negative else "squad_predict"] = predict_metric

		accelerator.log(log)

	if args.checkpointing_steps == "epoch":
		accelerator.save_state(f"epoch_{epoch}")

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			logger.info(json.dumps(eval_metric, indent=4))
			save_prefixed_metrics(eval_metric, args.output_dir)


if __name__ == "__main__":
	main()

import torch

from allennlp.modules.span_extractors.span_extractor import SpanExtractor
from allennlp.modules.span_extractors.span_extractor_with_span_width_embedding import (
	SpanExtractorWithSpanWidthEmbedding,
)
from allennlp.nn import util
from allennlp.nn.util import masked_max


@SpanExtractor.register("max_pooling")
class MaxPoolingSpanExtractor(SpanExtractorWithSpanWidthEmbedding):

	def __init__(
		self,
		input_dim: int,
		num_width_embeddings: int = None,
		span_width_embedding_dim: int = None,
		bucket_widths: bool = False,
	) -> None:
		super().__init__(
			input_dim=input_dim,
			num_width_embeddings=num_width_embeddings,
			span_width_embedding_dim=span_width_embedding_dim,
			bucket_widths=bucket_widths,
		)

	def get_output_dim(self) -> int:
		if self._span_width_embedding is not None:
			return self._input_dim + self._span_width_embedding.get_output_dim()
		return self._input_dim

	def _embed_spans(
		self,
		sequence_tensor: torch.FloatTensor,
		span_indices: torch.LongTensor,
		sequence_mask: torch.BoolTensor = None,
		span_indices_mask: torch.BoolTensor = None,
	) -> torch.FloatTensor:

		if sequence_tensor.size(-1) != self._input_dim:
			raise ValueError(
				f"Dimension mismatch expected ({sequence_tensor.size(-1)}) "
				f"received ({self._input_dim})."
			)

		if sequence_tensor.shape[1] <= span_indices.max() or span_indices.min() < 0:
			raise IndexError(
				f"Span index out of range, max index ({span_indices.max()}) "
				f"or min index ({span_indices.min()}) "
				f"not valid for sequence of length ({sequence_tensor.shape[1]})."
			)

		if (span_indices[:, :, 0] > span_indices[:, :, 1]).any():
			raise IndexError(
				"Span start above span end",
			)

		if sequence_mask is not None:
			sequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)
		else:
			sequence_lengths = torch.ones_like(
				sequence_tensor[:, 0, 0], dtype=torch.long
			) * sequence_tensor.size(1)

		adapted_span_indices = torch.tensor(span_indices, device=span_indices.device)

		for b in range(sequence_lengths.shape[0]):
			adapted_span_indices[b, :, 1][adapted_span_indices[b, :, 1] >= sequence_lengths[b]] = (
				sequence_lengths[b] - 1
			)


		if (adapted_span_indices[:, :, 0] > adapted_span_indices[:, :, 1]).any():
			raise IndexError(
				"Span indices were masked out entirely by sequence mask",
			)

		span_vals, span_mask = util.batched_span_select(sequence_tensor, adapted_span_indices)

		repeat_dim = len(span_vals.shape) - 1
		repeat_idx = [1] * (repeat_dim) + [span_vals.shape[-1]]

		ext_span_mask = span_mask.unsqueeze(repeat_dim).repeat(repeat_idx)

		max_out = masked_max(span_vals, ext_span_mask, dim=-2)

		return max_out

from typing import Sequence, Union

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.common.registrable import FromParams


class Maxout(torch.nn.Module, FromParams):

	def __init__(
		self,
		input_dim: int,
		num_layers: int,
		output_dims: Union[int, Sequence[int]],
		pool_sizes: Union[int, Sequence[int]],
		dropout: Union[float, Sequence[float]] = 0.0,
	) -> None:
		super().__init__()
		if not isinstance(output_dims, list):
			output_dims = [output_dims] * num_layers  # type: ignore
		if not isinstance(pool_sizes, list):
			pool_sizes = [pool_sizes] * num_layers  # type: ignore
		if not isinstance(dropout, list):
			dropout = [dropout] * num_layers  # type: ignore
		if len(output_dims) != num_layers:
			raise ConfigurationError(
				"len(output_dims) (%d) != num_layers (%d)" % (len(output_dims), num_layers)
			)
		if len(pool_sizes) != num_layers:
			raise ConfigurationError(
				"len(pool_sizes) (%d) != num_layers (%d)" % (len(pool_sizes), num_layers)
			)
		if len(dropout) != num_layers:
			raise ConfigurationError(
				"len(dropout) (%d) != num_layers (%d)" % (len(dropout), num_layers)
			)

		self._pool_sizes = pool_sizes
		input_dims = [input_dim] + output_dims[:-1]
		linear_layers = []
		for layer_input_dim, layer_output_dim, pool_size in zip(
			input_dims, output_dims, pool_sizes
		):
			linear_layers.append(torch.nn.Linear(layer_input_dim, layer_output_dim * pool_size))
		self._linear_layers = torch.nn.ModuleList(linear_layers)
		dropout_layers = [torch.nn.Dropout(p=value) for value in dropout]
		self._dropout = torch.nn.ModuleList(dropout_layers)
		self._output_dims = output_dims
		self._output_dim = output_dims[-1]
		self._input_dim = input_dim

	def get_output_dim(self):
		return self._output_dim

	def get_input_dim(self):
		return self._input_dim

	def forward(self, inputs: torch.Tensor) -> torch.Tensor:

		output = inputs
		for layer, layer_output_dim, dropout, pool_size in zip(
			self._linear_layers, self._output_dims, self._dropout, self._pool_sizes
		):
			affine_output = layer(output)
			shape = list(inputs.size())
			shape[-1] = layer_output_dim
			shape.append(pool_size)

			maxed_output = torch.max(affine_output.view(*shape), dim=-1)[0]
			dropped_output = dropout(maxed_output)
			output = dropped_output
		return output

import logging
import random
from typing import List, Iterable, Iterator, TypeVar, Sequence

from allennlp.data.instance import Instance
from allennlp.data.samplers.batch_sampler import BatchSampler
from allennlp.data.samplers.bucket_batch_sampler import BucketBatchSampler


logger = logging.getLogger(__name__)


A = TypeVar("A")


@BatchSampler.register("max_tokens_sampler")
class MaxTokensBatchSampler(BucketBatchSampler):

	def __init__(
		self,
		max_tokens: int,
		sorting_keys: List[str] = None,
		padding_noise: float = 0.1,
	):
		super().__init__(-1, sorting_keys, padding_noise, False)
		self.max_tokens = max_tokens

	def _lazy_groups_of_max_size(
		self,
		iterable: Iterable[A],
		sizes: Iterable[int],
	) -> Iterator[List[A]]:
		cur_max_size = 0
		group: List[A] = []

		iterator = iter(iterable)
		size_iter = iter(sizes)

		for item, size in zip(iterator, size_iter):
			if size > self.max_tokens:
				logger.warning(
					"Found instance of size %d, which is bigger than the expected size for a batch (%d)",
					size,
					self.max_tokens,
				)
			group_size = max(size, cur_max_size) * (len(group) + 1)

			if group_size > self.max_tokens:
				yield group
				cur_max_size = 0
				group = []

			group.append(item)
			cur_max_size = max(cur_max_size, size)

		if len(group) != 0:
			yield group

	def get_batch_indices(self, instances: Sequence[Instance]) -> Iterable[List[int]]:
		indices, lengths = self._argsort_by_padding(instances)

		max_lengths = [max(length) for length in lengths]
		group_iterator = self._lazy_groups_of_max_size(indices, max_lengths)

		batches = [list(group) for group in group_iterator]
		random.shuffle(batches)
		for batch in batches:
			yield batch

	def get_num_batches(self, instances: Sequence[Instance]) -> int:
		return sum(1 for _ in self.get_batch_indices(instances))

import torch

from allennlp.training.learning_rate_schedulers import PolynomialDecay
from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler


@LearningRateScheduler.register("linear_with_warmup")
class LinearWithWarmup(PolynomialDecay):

	def __init__(
		self,
		optimizer: torch.optim.Optimizer,
		num_epochs: int,
		num_steps_per_epoch: int,
		warmup_steps: int = 100,
		last_epoch: int = -1,
	) -> None:
		super().__init__(
			optimizer,
			num_epochs,
			num_steps_per_epoch,
			power=1.0,
			warmup_steps=warmup_steps,
			end_learning_rate=0.0,
			last_epoch=last_epoch,
		)

src_field_name = 'src'
tgt_field_name = 'tgt'


import logging
from collections import defaultdict, Counter
from typing import Dict, Iterable, Iterator, List, Union

import numpy
import torch

from allennlp.common.checks import ConfigurationError
from allennlp.common.util import ensure_list
from allennlp.data.instance import Instance
from allennlp.data.vocabulary import Vocabulary

logger = logging.getLogger(__name__)


class Batch(Iterable):

	__slots__ = ["instances"]

	def __init__(self, instances: Iterable[Instance]) -> None:
		super().__init__()

		self.instances = ensure_list(instances)
		self._check_types()

	def _check_types(self) -> None:
		field_name_to_type_counters: Dict[str, Counter] = defaultdict(lambda: Counter())
		field_counts: Counter = Counter()
		for instance in self.instances:
			for field_name, value in instance.fields.items():
				field_name_to_type_counters[field_name][value.__class__.__name__] += 1
				field_counts[field_name] += 1
		for field_name, type_counters in field_name_to_type_counters.items():
			if len(type_counters) > 1:
				raise ConfigurationError(
					"You cannot construct a Batch with non-homogeneous Instances. "
					f"Field '{field_name}' has {len(type_counters)} different types: "
					f"{', '.join(type_counters.keys())}"
				)
			if field_counts[field_name] != len(self.instances):
				raise ConfigurationError(
					"You cannot construct a Batch with non-homogeneous Instances. "
					f"Field '{field_name}' present in some Instances but not others."
				)

	def get_padding_lengths(self) -> Dict[str, Dict[str, int]]:
		padding_lengths: Dict[str, Dict[str, int]] = defaultdict(dict)
		all_instance_lengths: List[Dict[str, Dict[str, int]]] = [
			instance.get_padding_lengths() for instance in self.instances
		]
		all_field_lengths: Dict[str, List[Dict[str, int]]] = defaultdict(list)
		for instance_lengths in all_instance_lengths:
			for field_name, instance_field_lengths in instance_lengths.items():
				all_field_lengths[field_name].append(instance_field_lengths)
		for field_name, field_lengths in all_field_lengths.items():
			for padding_key in field_lengths[0].keys():
				max_value = max(x.get(padding_key, 0) for x in field_lengths)
				padding_lengths[field_name][padding_key] = max_value
		return {**padding_lengths}

	def as_tensor_dict(
		self,
		padding_lengths: Dict[str, Dict[str, int]] = None,
		verbose: bool = False,
	) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]:
		padding_lengths = padding_lengths or defaultdict(dict)
		if verbose:
			logger.info(f"Padding batch of size {len(self.instances)} to lengths {padding_lengths}")
			logger.info("Getting max lengths from instances")
		instance_padding_lengths = self.get_padding_lengths()
		if verbose:
			logger.info(f"Instance max lengths: {instance_padding_lengths}")
		lengths_to_use: Dict[str, Dict[str, int]] = defaultdict(dict)
		for field_name, instance_field_lengths in instance_padding_lengths.items():
			for padding_key in instance_field_lengths.keys():
				if padding_key in padding_lengths[field_name]:
					lengths_to_use[field_name][padding_key] = padding_lengths[field_name][
						padding_key
					]
				else:
					lengths_to_use[field_name][padding_key] = instance_field_lengths[padding_key]

		field_tensors: Dict[str, list] = defaultdict(list)
		if verbose:
			logger.info(f"Now actually padding instances to length: {lengths_to_use}")
		for instance in self.instances:
			for field, tensors in instance.as_tensor_dict(lengths_to_use).items():
				field_tensors[field].append(tensors)

		field_classes = self.instances[0].fields
		return {
			field_name: field_classes[field_name].batch_tensors(field_tensor_list)
			for field_name, field_tensor_list in field_tensors.items()
		}

	def __iter__(self) -> Iterator[Instance]:
		return iter(self.instances)

	def index_instances(self, vocab: Vocabulary) -> None:
		for instance in self.instances:
			instance.index_fields(vocab)

	def print_statistics(self) -> None:
		sequence_field_lengths: Dict[str, List] = defaultdict(list)
		for instance in self.instances:
			if not instance.indexed:
				raise ConfigurationError(
					"Instances must be indexed with vocabulary "
					"before asking to print dataset statistics."
				)
			for field, field_padding_lengths in instance.get_padding_lengths().items():
				for key, value in field_padding_lengths.items():
					sequence_field_lengths[f"{field}.{key}"].append(value)

		print("\n\n----Dataset Statistics----\n")
		for name, lengths in sequence_field_lengths.items():
			print(f"Statistics for {name}:")
			print(
				f"\tLengths: Mean: {numpy.mean(lengths)}, Standard Dev: {numpy.std(lengths)}, "
				f"Max: {numpy.max(lengths)}, Min: {numpy.min(lengths)}"
			)

		print("\n10 Random instances:")
		for i in numpy.random.randint(len(self.instances), size=10):
			print(f"Instance {i}:")
			print(f"\t{self.instances[i]}")

	def __len__(self):
		return len(self.instances)

from typing import Dict, List, Optional, Set, Callable
from collections import defaultdict

import torch

from allennlp.common.util import is_distributed
from allennlp.common.checks import ConfigurationError
from allennlp.nn.util import get_lengths_from_binary_sequence_mask
from allennlp.data.vocabulary import Vocabulary
from allennlp.training.metrics.metric import Metric
from allennlp.data.dataset_readers.dataset_utils.span_utils import (
	bio_tags_to_spans,
	bioul_tags_to_spans,
	iob1_tags_to_spans,
	bmes_tags_to_spans,
	TypedStringSpan,
)


TAGS_TO_SPANS_FUNCTION_TYPE = Callable[[List[str], Optional[List[str]]], List[TypedStringSpan]]


@Metric.register("span_f1")
class SpanBasedF1Measure(Metric):

	def __init__(
		self,
		vocabulary: Vocabulary,
		tag_namespace: str = "tags",
		ignore_classes: List[str] = None,
		label_encoding: Optional[str] = "BIO",
		tags_to_spans_function: Optional[TAGS_TO_SPANS_FUNCTION_TYPE] = None,
	) -> None:
		if label_encoding and tags_to_spans_function:
			raise ConfigurationError(
				"Both label_encoding and tags_to_spans_function are provided. "
				'Set "label_encoding=None" explicitly to enable tags_to_spans_function.'
			)
		if label_encoding:
			if label_encoding not in ["BIO", "IOB1", "BIOUL", "BMES"]:
				raise ConfigurationError(
					"Unknown label encoding - expected 'BIO', 'IOB1', 'BIOUL', 'BMES'."
				)
		elif tags_to_spans_function is None:
			raise ConfigurationError(
				"At least one of the (label_encoding, tags_to_spans_function) should be provided."
			)

		self._label_encoding = label_encoding
		self._tags_to_spans_function = tags_to_spans_function
		self._label_vocabulary = vocabulary.get_index_to_token_vocabulary(tag_namespace)
		self._ignore_classes: List[str] = ignore_classes or []

		self._true_positives: Dict[str, int] = defaultdict(int)
		self._false_positives: Dict[str, int] = defaultdict(int)
		self._false_negatives: Dict[str, int] = defaultdict(int)

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
		prediction_map: Optional[torch.Tensor] = None,
	):
		if mask is None:
			mask = torch.ones_like(gold_labels).bool()

		predictions, gold_labels, mask, prediction_map = self.detach_tensors(
			predictions, gold_labels, mask, prediction_map
		)

		num_classes = predictions.size(-1)
		if (gold_labels >= num_classes).any():
			raise ConfigurationError(
				"A gold label passed to SpanBasedF1Measure contains an "
				"id >= {}, the number of classes.".format(num_classes)
			)

		sequence_lengths = get_lengths_from_binary_sequence_mask(mask)
		argmax_predictions = predictions.max(-1)[1]

		if prediction_map is not None:
			argmax_predictions = torch.gather(prediction_map, 1, argmax_predictions)
			gold_labels = torch.gather(prediction_map, 1, gold_labels.long())

		argmax_predictions = argmax_predictions.float()

		batch_size = gold_labels.size(0)
		for i in range(batch_size):
			sequence_prediction = argmax_predictions[i, :]
			sequence_gold_label = gold_labels[i, :]
			length = sequence_lengths[i]

			if length == 0:
				continue

			predicted_string_labels = [
				self._label_vocabulary[label_id]
				for label_id in sequence_prediction[:length].tolist()
			]
			gold_string_labels = [
				self._label_vocabulary[label_id]
				for label_id in sequence_gold_label[:length].tolist()
			]

			tags_to_spans_function: TAGS_TO_SPANS_FUNCTION_TYPE
			if self._label_encoding is None and self._tags_to_spans_function:
				tags_to_spans_function = self._tags_to_spans_function
			elif self._label_encoding == "BIO":
				tags_to_spans_function = bio_tags_to_spans
			elif self._label_encoding == "IOB1":
				tags_to_spans_function = iob1_tags_to_spans
			elif self._label_encoding == "BIOUL":
				tags_to_spans_function = bioul_tags_to_spans
			elif self._label_encoding == "BMES":
				tags_to_spans_function = bmes_tags_to_spans
			else:
				raise ValueError(f"Unexpected label encoding scheme '{self._label_encoding}'")

			predicted_spans = tags_to_spans_function(predicted_string_labels, self._ignore_classes)
			gold_spans = tags_to_spans_function(gold_string_labels, self._ignore_classes)

			predicted_spans = self._handle_continued_spans(predicted_spans)
			gold_spans = self._handle_continued_spans(gold_spans)

			for span in predicted_spans:
				if span in gold_spans:
					self._true_positives[span[0]] += 1
					gold_spans.remove(span)
				else:
					self._false_positives[span[0]] += 1
			for span in gold_spans:
				self._false_negatives[span[0]] += 1

	@staticmethod
	def _handle_continued_spans(spans: List[TypedStringSpan]) -> List[TypedStringSpan]:
		span_set: Set[TypedStringSpan] = set(spans)
		continued_labels: List[str] = [
			label[2:] for (label, span) in span_set if label.startswith("C-")
		]
		for label in continued_labels:
			continued_spans = {span for span in span_set if label in span[0]}

			span_start = min(span[1][0] for span in continued_spans)
			span_end = max(span[1][1] for span in continued_spans)
			replacement_span: TypedStringSpan = (label, (span_start, span_end))

			span_set.difference_update(continued_spans)
			span_set.add(replacement_span)

		return list(span_set)

	def get_metric(self, reset: bool = False):
		if is_distributed():
			raise RuntimeError(
				"Distributed aggregation for SpanBasedF1Measure is currently not supported."
			)
		all_tags: Set[str] = set()
		all_tags.update(self._true_positives.keys())
		all_tags.update(self._false_positives.keys())
		all_tags.update(self._false_negatives.keys())
		all_metrics = {}
		for tag in all_tags:
			precision, recall, f1_measure = self._compute_metrics(
				self._true_positives[tag], self._false_positives[tag], self._false_negatives[tag]
			)
			precision_key = "precision" + "-" + tag
			recall_key = "recall" + "-" + tag
			f1_key = "f1-measure" + "-" + tag
			all_metrics[precision_key] = precision
			all_metrics[recall_key] = recall
			all_metrics[f1_key] = f1_measure

		precision, recall, f1_measure = self._compute_metrics(
			sum(self._true_positives.values()),
			sum(self._false_positives.values()),
			sum(self._false_negatives.values()),
		)
		all_metrics["precision-overall"] = precision
		all_metrics["recall-overall"] = recall
		all_metrics["f1-measure-overall"] = f1_measure
		if reset:
			self.reset()
		return all_metrics

	@staticmethod
	def _compute_metrics(true_positives: int, false_positives: int, false_negatives: int):
		precision = true_positives / (true_positives + false_positives + 1e-13)
		recall = true_positives / (true_positives + false_negatives + 1e-13)
		f1_measure = 2.0 * (precision * recall) / (precision + recall + 1e-13)
		return precision, recall, f1_measure

	def reset(self):
		self._true_positives = defaultdict(int)
		self._false_positives = defaultdict(int)
		self._false_negatives = defaultdict(int)


from allennlp.modules.attention import Attention
from allennlp.modules.backbones import Backbone
from allennlp.modules.bimpm_matching import BiMpmMatching
from allennlp.modules.conditional_random_field import ConditionalRandomField
from allennlp.modules.elmo import Elmo
from allennlp.modules.feedforward import FeedForward
from allennlp.modules.gated_sum import GatedSum
from allennlp.modules.highway import Highway
from allennlp.modules.input_variational_dropout import InputVariationalDropout
from allennlp.modules.layer_norm import LayerNorm
from allennlp.modules.matrix_attention import MatrixAttention
from allennlp.modules.maxout import Maxout
from allennlp.modules.residual_with_layer_dropout import ResidualWithLayerDropout
from allennlp.modules.scalar_mix import ScalarMix
from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder
from allennlp.modules.seq2vec_encoders import Seq2VecEncoder
from allennlp.modules.text_field_embedders import TextFieldEmbedder
from allennlp.modules.time_distributed import TimeDistributed
from allennlp.modules.token_embedders import TokenEmbedder, Embedding
from allennlp.modules.softmax_loss import SoftmaxLoss

from .supervised_trainer import SupervisedTrainer


import argparse
import json
import logging
import os
import tarfile
import tempfile


from allennlp.commands.subcommand import Subcommand
from allennlp.common.file_utils import CacheFile
from allennlp.common.params import Params
from allennlp.training.util import make_vocab_from_params


logger = logging.getLogger(__name__)


@Subcommand.register("build-vocab")
class BuildVocab(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		description = """Build a vocabulary from an experiment config file."""
		subparser = parser.add_parser(self.name, description=description, help=description)

		subparser.add_argument("param_path", type=str, help="path to an experiment config file")

		subparser.add_argument(
			"output_path", type=str, help="path to save the vocab tar.gz file to"
		)

		subparser.add_argument(
			"-f",
			"--force",
			action="store_true",
			help="force write if the output_path already exists",
		)

		subparser.add_argument(
			"-o",
			"--overrides",
			type=str,
			default="",
			help=(
				"a json(net) structure used to override the experiment configuration, e.g., "
				"'{\"vocabulary.min_count.labels\": 10}'.  Nested parameters can be specified either"
				" with nested dictionaries or with dot syntax."
			),
		)

		subparser.set_defaults(func=build_vocab_from_args)

		return subparser


def build_vocab_from_args(args: argparse.Namespace):
	if not args.output_path.endswith(".tar.gz"):
		raise ValueError("param 'output_path' should end with '.tar.gz'")

	if os.path.exists(args.output_path) and not args.force:
		raise RuntimeError(f"{args.output_path} already exists. Use --force to overwrite.")

	output_directory = os.path.dirname(args.output_path)
	if len(output_directory) > 0:
		os.makedirs(output_directory, exist_ok=True)

	params = Params.from_file(args.param_path)

	with tempfile.TemporaryDirectory() as temp_dir:
		make_vocab_from_params(params, temp_dir)

		with CacheFile(args.output_path, suffix=".tar.gz") as temp_archive:
			logger.info("Archiving vocabulary to %s", args.output_path)

			with tarfile.open(temp_archive.name, "w:gz") as archive:
				vocab_dir = os.path.join(temp_dir, "vocabulary")
				for fname in os.listdir(vocab_dir):
					if fname.endswith(".lock"):
						continue
					archive.add(os.path.join(vocab_dir, fname), arcname=fname)

	print(f"Success! Vocab saved to {args.output_path}")
	print('You can now set the "vocabulary" entry of your training config to:')
	print(json.dumps({"type": "from_files", "directory": os.path.abspath(args.output_path)}))

from typing import Optional

import sys


import torch

from allennlp.common.checks import ConfigurationError
from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("unigram_recall")
class UnigramRecall(Metric):

	def __init__(self) -> None:
		self.correct_count = 0.0
		self.total_count = 0.0

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
		end_index: int = sys.maxsize,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		if gold_labels.dim() != predictions.dim() - 1:
			raise ConfigurationError(
				"gold_labels must have dimension == predictions.dim() - 1 but "
				"found tensor of shape: {}".format(gold_labels.size())
			)
		if mask is not None and mask.size() != gold_labels.size():
			raise ConfigurationError(
				"mask must have the same size as predictions but "
				"found tensor of shape: {}".format(mask.size())
			)

		batch_size = predictions.size()[0]
		correct = 0.0
		for i in range(batch_size):
			beams = predictions[i]
			cur_gold = gold_labels[i]

			if mask is not None:
				masked_gold = cur_gold * mask[i]
			else:
				masked_gold = cur_gold
			cleaned_gold = [x for x in masked_gold if x not in (0, end_index)]

			retval = 0.0
			for word in cleaned_gold:
				stillsearch = True
				for beam in beams:
					if stillsearch and word in beam:
						retval += 1 / len(cleaned_gold)
						stillsearch = False
			correct += retval

		_correct_count = correct
		_total_count = predictions.size()[0]

		self.correct_count += dist_reduce_sum(_correct_count)
		self.total_count += dist_reduce_sum(_total_count)

	def get_metric(self, reset: bool = False):

		recall = self.correct_count / self.total_count if self.total_count > 0 else 0
		if reset:
			self.reset()
		return {"unigram_recall": recall}

	def reset(self):
		self.correct_count = 0.0
		self.total_count = 0.0

from typing import List, Union

import torch

from allennlp.common import FromParams
from allennlp.common.checks import ConfigurationError
from allennlp.nn import Activation


class FeedForward(torch.nn.Module, FromParams):

	def __init__(
		self,
		input_dim: int,
		num_layers: int,
		hidden_dims: Union[int, List[int]],
		activations: Union[Activation, List[Activation]],
		dropout: Union[float, List[float]] = 0.0,
	) -> None:

		super().__init__()
		if not isinstance(hidden_dims, list):
			hidden_dims = [hidden_dims] * num_layers  # type: ignore
		if not isinstance(activations, list):
			activations = [activations] * num_layers  # type: ignore
		if not isinstance(dropout, list):
			dropout = [dropout] * num_layers  # type: ignore
		if len(hidden_dims) != num_layers:
			raise ConfigurationError(
				"len(hidden_dims) (%d) != num_layers (%d)" % (len(hidden_dims), num_layers)
			)
		if len(activations) != num_layers:
			raise ConfigurationError(
				"len(activations) (%d) != num_layers (%d)" % (len(activations), num_layers)
			)
		if len(dropout) != num_layers:
			raise ConfigurationError(
				"len(dropout) (%d) != num_layers (%d)" % (len(dropout), num_layers)
			)
		self._activations = torch.nn.ModuleList(activations)
		input_dims = [input_dim] + hidden_dims[:-1]
		linear_layers = []
		for layer_input_dim, layer_output_dim in zip(input_dims, hidden_dims):
			linear_layers.append(torch.nn.Linear(layer_input_dim, layer_output_dim))
		self._linear_layers = torch.nn.ModuleList(linear_layers)
		dropout_layers = [torch.nn.Dropout(p=value) for value in dropout]
		self._dropout = torch.nn.ModuleList(dropout_layers)
		self._output_dim = hidden_dims[-1]
		self.input_dim = input_dim

	def get_output_dim(self):
		return self._output_dim

	def get_input_dim(self):
		return self.input_dim

	def forward(self, inputs: torch.Tensor) -> torch.Tensor:

		output = inputs
		for layer, activation, dropout in zip(
			self._linear_layers, self._activations, self._dropout
		):
			output = dropout(activation(layer(output)))
		return output


from dataclasses import dataclass
import math

import torch
import torch.nn as nn
from torch.nn import functional as F


@dataclass
class GPTConfig:
	model_type: str = 'gpt2'
	n_layer: int = None
	n_head: int = None
	n_embd: int =  None
	vocab_size: int = 50257 
	block_size: int = 1024
	embd_pdrop: float = 0.1
	resid_pdrop: float = 0.1
	attn_pdrop: float = 0.1

@dataclass
class OptimizerConfig:
	learning_rate: float = 3e-4
	weight_decay: float = 0.1

class MultiheadAttentionLayer(nn.Module):

	def __init__(self, config, device="cpu", dtype=torch.float32):
		super().__init__()
		assert config.n_embd % config.n_head == 0
		self.resid_drop = nn.Dropout(config.resid_pdrop)
		self.c_proj = nn.Linear(config.n_embd, config.n_embd, device=device, dtype=dtype)
		self.register_buffer("mask", torch.tril(torch.ones(config.block_size, config.block_size))
							 .view(1, 1, config.block_size, config.block_size))
		self.attn = torch.nn.MultiheadAttention(
			embed_dim=config.n_embd,
			num_heads=config.n_head,
			dropout=config.attn_pdrop,
			batch_first=True,
			device=device,
			dtype=dtype
		)

	def forward(self, x):
		_, seq_size, _ = x.size()
		y = self.attn(x, x, x, attn_mask=self.mask[0, 0, :seq_size, :seq_size])[0]
		y = self.resid_drop(self.c_proj(y))
		return y

class Block(nn.Module):
	def __init__(self, config: GPTConfig):
		super().__init__()
		self.ln1 = nn.LayerNorm(config.n_embd)
		self.ln2 = nn.LayerNorm(config.n_embd)
		self.attn = MultiheadAttentionLayer(config)
		self.mlp = nn.Sequential(
			nn.Linear(config.n_embd, 4 * config.n_embd),
			nn.GELU(),
			nn.Linear(4 * config.n_embd, config.n_embd),
			nn.Dropout(config.resid_pdrop),
		)

	def forward(self, x):
		x = x + self.attn(self.ln1(x))
		x = x + self.mlp(self.ln2(x))
		return x

class EmbeddingStem(nn.Module):
	def __init__(self, config: GPTConfig, device="cpu", dtype=torch.float32):
		super().__init__()
		self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd, device=device, dtype=dtype)
		self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd, device=device, dtype=dtype))
		self.drop = nn.Dropout(config.embd_pdrop)
		self.block_size = config.block_size

	def reset_parameters(self):
		self.tok_emb.reset_parameters()

	def forward(self, idx):
		b, t = idx.size()
		assert t <= self.block_size, f"Cannot forward sequence of length {t}, block size is only {self.block_size}"

		token_embeddings = self.tok_emb(idx)  # each index maps to a (learnable) embedding vector
		position_embeddings = self.pos_emb[:, :t, :]  # each position maps to a (learnable) position vector
		return self.drop(token_embeddings + position_embeddings)
		
class GPT(nn.Module):

	def __init__(self, config: GPTConfig):
		super().__init__()
		self.block_size = config.block_size
		config = self._set_model_config(config)

		self.emb_stem = EmbeddingStem(config)
		self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])
		self.ln_f = nn.LayerNorm(config.n_embd)
		self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

		self.apply(self._init_weights)
		for pn, p in self.named_parameters():
			if pn.endswith('c_proj.weight'):
				p.data.normal_(mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))

		n_params = sum(p.numel() for p in self.blocks.parameters())
		print("number of parameters: %.2fM" % (n_params/1e6,))

	def _set_model_config(self, config):
		type_given = config.model_type is not None
		params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])
		if type_given and not params_given:
			config.__dict__.update({
				'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params
				'gpt2':		 dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
				'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
				'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
				'gpt2-xl':	  dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
				'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),
				'gpt-mini':	 dict(n_layer=6, n_head=6, n_embd=192),
				'gpt-micro':	dict(n_layer=4, n_head=4, n_embd=128),
				'gpt-nano':	 dict(n_layer=3, n_head=3, n_embd=48),
			}[config.model_type])
		return config
	
	def _init_weights(self, module):
		if isinstance(module, (nn.Linear, nn.Embedding)):
			module.weight.data.normal_(mean=0.0, std=0.02)
			if isinstance(module, nn.Linear) and module.bias is not None:
				module.bias.data.zero_()
		elif isinstance(module, nn.LayerNorm):
			module.bias.data.zero_()
			module.weight.data.fill_(1.0)

	def forward(self, idx, targets=None):
		x = self.emb_stem(idx)
		x = self.blocks(x)
		x = self.ln_f(x)
		logits = self.head(x)

		loss = None
		if targets is not None:
			loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

		return logits, loss

	@torch.no_grad()
	def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):
		for _ in range(max_new_tokens):
			idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]
			logits, _ = self(idx_cond)
			logits = logits[:, -1, :] / temperature
			if top_k is not None:
				v, _ = torch.topk(logits, top_k)
				logits[logits < v[:, [-1]]] = -float('Inf')
			probs = F.softmax(logits, dim=-1)
			if do_sample:
				idx_next = torch.multinomial(probs, num_samples=1)
			else:
				_, idx_next = torch.topk(probs, k=1, dim=-1)
			idx = torch.cat((idx, idx_next), dim=1)

		return idx


def create_optimizer(model: torch.nn.Module, opt_config: OptimizerConfig):

	decay = set()
	no_decay = set()
	whitelist_weight_modules = (torch.nn.Linear, )
	blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
	for mn, m in model.named_modules():
		for pn, p in m.named_parameters():
			fpn = '%s.%s' % (mn, pn) if mn else pn # full param name
			if pn.endswith('bias'):
				no_decay.add(fpn)
			elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):
				decay.add(fpn)
			elif pn.endswith('in_proj_weight'):
				decay.add(fpn)
			elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):
				no_decay.add(fpn)
			elif pn.endswith('pos_emb'):
				no_decay.add(fpn)

	param_dict = {pn: p for pn, p in model.named_parameters()}
	inter_params = decay & no_decay
	union_params = decay | no_decay
	assert len(inter_params) == 0, "parameters %s made it into both decay/no_decay sets!" % (str(inter_params), )
	assert len(param_dict.keys() - union_params) == 0, "parameters %s were not separated into either decay/no_decay set!" \
												% (str(param_dict.keys() - union_params), )

	optim_groups = [
		{"params": [param_dict[pn] for pn in sorted(list(decay))], "weight_decay": opt_config.weight_decay},
		{"params": [param_dict[pn] for pn in sorted(list(no_decay))], "weight_decay": 0.0},
	]
	optimizer = torch.optim.AdamW(optim_groups, lr=opt_config.learning_rate, betas=(0.9, 0.95))
	return optimizer
import torch.nn as nn

from .transformer import TransformerBlock
from .embedding import BERTEmbedding


class BERT(nn.Module):

	def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):

		super().__init__()
		self.hidden = hidden
		self.n_layers = n_layers
		self.attn_heads = attn_heads

		self.feed_forward_hidden = hidden * 4

		self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)

		self.transformer_blocks = nn.ModuleList(
			[TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])

	def forward(self, x, segment_info):
		mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)

		x = self.embedding(x, segment_info)

		for transformer in self.transformer_blocks:
			x = transformer.forward(x, mask)

		return x

from typing import Dict, Optional, List, Any

import numpy

import torch
from torch.nn.modules.linear import Linear
import torch.nn.functional as F

from allennlp.common.checks import check_dimensions_match, ConfigurationError
from allennlp.data import TextFieldTensors, Vocabulary
from allennlp.modules import Seq2SeqEncoder, TimeDistributed, TextFieldEmbedder
from allennlp.models.model import Model
from allennlp.nn import InitializerApplicator
from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits
from allennlp.training.metrics import CategoricalAccuracy, SpanBasedF1Measure


@Model.register("simple_tagger")
class SimpleTagger(Model):

	def __init__(
		self,
		vocab: Vocabulary,
		text_field_embedder: TextFieldEmbedder,
		encoder: Seq2SeqEncoder,
		calculate_span_f1: bool = None,
		label_encoding: Optional[str] = None,
		label_namespace: str = "labels",
		verbose_metrics: bool = False,
		initializer: InitializerApplicator = InitializerApplicator(),
		**kwargs,
	) -> None:
		super().__init__(vocab, **kwargs)

		self.label_namespace = label_namespace
		self.text_field_embedder = text_field_embedder
		self.num_classes = self.vocab.get_vocab_size(label_namespace)
		self.encoder = encoder
		self._verbose_metrics = verbose_metrics
		self.tag_projection_layer = TimeDistributed(
			Linear(self.encoder.get_output_dim(), self.num_classes)
		)

		check_dimensions_match(
			text_field_embedder.get_output_dim(),
			encoder.get_input_dim(),
			"text field embedding dim",
			"encoder input dim",
		)

		self.metrics = {
			"accuracy": CategoricalAccuracy(),
			"accuracy3": CategoricalAccuracy(top_k=3),
		}

		if calculate_span_f1 is None:
			calculate_span_f1 = label_encoding is not None

		self.calculate_span_f1 = calculate_span_f1
		self._f1_metric: Optional[SpanBasedF1Measure] = None
		if calculate_span_f1:
			if not label_encoding:
				raise ConfigurationError(
					"calculate_span_f1 is True, but no label_encoding was specified."
				)
			self._f1_metric = SpanBasedF1Measure(
				vocab, tag_namespace=label_namespace, label_encoding=label_encoding
			)

		initializer(self)

	def forward(  # type: ignore
		self,
		tokens: TextFieldTensors,
		tags: torch.LongTensor = None,
		metadata: List[Dict[str, Any]] = None,
		ignore_loss_on_o_tags: bool = False,
	) -> Dict[str, torch.Tensor]:

		embedded_text_input = self.text_field_embedder(tokens)
		batch_size, sequence_length, _ = embedded_text_input.size()
		mask = get_text_field_mask(tokens)
		encoded_text = self.encoder(embedded_text_input, mask)

		logits = self.tag_projection_layer(encoded_text)
		reshaped_log_probs = logits.view(-1, self.num_classes)
		class_probabilities = F.softmax(reshaped_log_probs, dim=-1).view(
			[batch_size, sequence_length, self.num_classes]
		)

		output_dict = {"logits": logits, "class_probabilities": class_probabilities}

		if tags is not None:
			if ignore_loss_on_o_tags:
				o_tag_index = self.vocab.get_token_index("O", namespace=self.label_namespace)
				tag_mask = mask & (tags != o_tag_index)
			else:
				tag_mask = mask
			loss = sequence_cross_entropy_with_logits(logits, tags, tag_mask)
			for metric in self.metrics.values():
				metric(logits, tags, mask)
			if self.calculate_span_f1:
				self._f1_metric(logits, tags, mask)  # type: ignore
			output_dict["loss"] = loss

		if metadata is not None:
			output_dict["words"] = [x["words"] for x in metadata]
		return output_dict

	def make_output_human_readable(
		self, output_dict: Dict[str, torch.Tensor]
	) -> Dict[str, torch.Tensor]:
		all_predictions = output_dict["class_probabilities"]
		all_predictions = all_predictions.cpu().data.numpy()
		if all_predictions.ndim == 3:
			predictions_list = [all_predictions[i] for i in range(all_predictions.shape[0])]
		else:
			predictions_list = [all_predictions]
		all_tags = []
		for predictions in predictions_list:
			argmax_indices = numpy.argmax(predictions, axis=-1)
			tags = [
				self.vocab.get_token_from_index(x, namespace=self.label_namespace)
				for x in argmax_indices
			]
			all_tags.append(tags)
		output_dict["tags"] = all_tags
		return output_dict

	def get_metrics(self, reset: bool = False) -> Dict[str, float]:
		metrics_to_return = {
			metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()
		}

		if self.calculate_span_f1:
			f1_dict = self._f1_metric.get_metric(reset)  # type: ignore
			if self._verbose_metrics:
				metrics_to_return.update(f1_dict)
			else:
				metrics_to_return.update({x: y for x, y in f1_dict.items() if "overall" in x})
		return metrics_to_return

	default_predictor = "sentence_tagger"

import logging
from os import PathLike
import re
from typing import List, Optional, NamedTuple, Sequence, Union, Dict, Any

import torch
from torch import autograd

from allennlp.common import Registrable, Lazy, plugins
from allennlp.common.tqdm import Tqdm
from allennlp.common.util import int_to_device
from allennlp.data import Instance, DatasetReader, DatasetReaderInput, Batch
from allennlp.data.data_loaders import DataLoader, SimpleDataLoader
from allennlp.models import Model, Archive, load_archive
from allennlp.nn.util import move_to_device


logger = logging.getLogger(__name__)


class InstanceInfluence(NamedTuple):
	instance: Instance

	loss: float

	score: float


class InterpretOutput(NamedTuple):

	test_instance: Instance

	loss: float

	top_k: List[InstanceInfluence]


class InstanceWithGrads(NamedTuple):

	instance: Instance
	loss: float
	grads: Sequence[torch.Tensor]


class InfluenceInterpreter(Registrable):

	default_implementation = "simple-influence"

	def __init__(
		self,
		model: Model,
		train_data_path: DatasetReaderInput,
		train_dataset_reader: DatasetReader,
		*,
		test_dataset_reader: Optional[DatasetReader] = None,
		train_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		test_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		params_to_freeze: Optional[List[str]] = None,
		cuda_device: int = -1,
	) -> None:
		self.model = model
		self.vocab = model.vocab
		self.device = int_to_device(cuda_device)

		self._train_data_path = train_data_path
		self._train_loader = train_data_loader.construct(
			reader=train_dataset_reader,
			data_path=train_data_path,
			batch_size=1,
		)
		self._train_loader.set_target_device(self.device)
		self._train_loader.index_with(self.vocab)

		self._test_dataset_reader = test_dataset_reader or train_dataset_reader
		self._lazy_test_data_loader = test_data_loader

		self.model.to(self.device)
		if params_to_freeze is not None:
			for name, param in self.model.named_parameters():
				if any([re.match(pattern, name) for pattern in params_to_freeze]):
					param.requires_grad = False

		self._used_params: Optional[List[torch.nn.Parameter]] = None
		self._used_param_names: Optional[List[str]] = None
		self._train_instances: Optional[List[InstanceWithGrads]] = None

	@property
	def used_params(self) -> List[torch.nn.Parameter]:
		if self._used_params is None:
			self._gather_train_instances_and_compute_gradients()
		assert self._used_params is not None
		return self._used_params

	@property
	def used_param_names(self) -> List[str]:
		if self._used_param_names is None:
			self._gather_train_instances_and_compute_gradients()
		assert self._used_param_names is not None
		return self._used_param_names

	@property
	def train_instances(self) -> List[InstanceWithGrads]:
		if self._train_instances is None:
			self._gather_train_instances_and_compute_gradients()
		assert self._train_instances is not None
		return self._train_instances

	@classmethod
	def from_path(
		cls,
		archive_path: Union[str, PathLike],
		*,
		interpreter_name: Optional[str] = None,
		train_data_path: Optional[DatasetReaderInput] = None,
		train_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		test_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		params_to_freeze: Optional[List[str]] = None,
		cuda_device: int = -1,
		import_plugins: bool = True,
		overrides: Union[str, Dict[str, Any]] = "",
		**extras,
	) -> "InfluenceInterpreter":
		if import_plugins:
			plugins.import_plugins()
		return cls.from_archive(
			load_archive(archive_path, cuda_device=cuda_device, overrides=overrides),
			interpreter_name=interpreter_name,
			train_data_path=train_data_path,
			train_data_loader=train_data_loader,
			test_data_loader=test_data_loader,
			params_to_freeze=params_to_freeze,
			cuda_device=cuda_device,
			**extras,
		)

	@classmethod
	def from_archive(
		cls,
		archive: Archive,
		*,
		interpreter_name: Optional[str] = None,
		train_data_path: Optional[DatasetReaderInput] = None,
		train_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		test_data_loader: Lazy[DataLoader] = Lazy(SimpleDataLoader.from_dataset_reader),
		params_to_freeze: Optional[List[str]] = None,
		cuda_device: int = -1,
		**extras,
	) -> "InfluenceInterpreter":
		interpreter_cls = cls.by_name(interpreter_name or cls.default_implementation)
		return interpreter_cls(
			model=archive.model,
			train_data_path=train_data_path or archive.config["train_data_path"],
			train_dataset_reader=archive.dataset_reader,
			test_dataset_reader=archive.validation_dataset_reader,
			train_data_loader=train_data_loader,
			test_data_loader=test_data_loader,
			params_to_freeze=params_to_freeze,
			cuda_device=cuda_device,
			**extras,
		)

	def interpret(self, test_instance: Instance, k: int = 20) -> InterpretOutput:
		return self.interpret_instances([test_instance], k=k)[0]

	def interpret_from_file(
		self, test_data_path: DatasetReaderInput, k: int = 20
	) -> List[InterpretOutput]:
		test_data_loader = self._lazy_test_data_loader.construct(
			reader=self._test_dataset_reader,
			data_path=test_data_path,
			batch_size=1,
		)
		test_data_loader.index_with(self.vocab)
		instances = list(test_data_loader.iter_instances())
		return self.interpret_instances(instances, k=k)

	def interpret_instances(
		self, test_instances: List[Instance], k: int = 20
	) -> List[InterpretOutput]:
		if not self.train_instances:
			raise ValueError(f"No training instances collected from {self._train_data_path}")
		if not self.used_params:
			raise ValueError("Model has no parameters with non-zero gradients")

		outputs: List[InterpretOutput] = []
		for test_idx, test_instance in enumerate(Tqdm.tqdm(test_instances, desc="test instances")):
			test_batch = Batch([test_instance])
			test_batch.index_instances(self.vocab)
			test_tensor_dict = move_to_device(test_batch.as_tensor_dict(), self.device)

			self.model.eval()
			self.model.zero_grad()

			test_output_dict = self.model(**test_tensor_dict)
			test_loss = test_output_dict["loss"]
			test_loss_float = test_loss.detach().item()

			test_grads = autograd.grad(test_loss, self.used_params)
			assert len(test_grads) == len(self.used_params)

			influence_scores = torch.zeros(len(self.train_instances))
			for idx, score in enumerate(
				self._calculate_influence_scores(test_instance, test_loss_float, test_grads)
			):
				influence_scores[idx] = score

			top_k_scores, top_k_indices = torch.topk(influence_scores, k)
			top_k = self._gather_instances(top_k_scores, top_k_indices)

			outputs.append(
				InterpretOutput(
					test_instance=test_instance,
					loss=test_loss_float,
					top_k=top_k,
				)
			)
		return outputs

	def _gather_instances(
		self, scores: torch.Tensor, indices: torch.Tensor
	) -> List[InstanceInfluence]:
		outputs: List[InstanceInfluence] = []
		for score, idx in zip(scores, indices):
			instance, loss, _ = self.train_instances[idx]
			outputs.append(InstanceInfluence(instance=instance, loss=loss, score=score.item()))
		return outputs

	def _gather_train_instances_and_compute_gradients(self) -> None:
		logger.info(
			"Gathering training instances and computing gradients. "
			"The result will be cached so this only needs to be done once."
		)
		self._train_instances = []
		self.model.train()
		for instance in Tqdm.tqdm(
			self._train_loader.iter_instances(), desc="calculating training gradients"
		):
			batch = Batch([instance])
			batch.index_instances(self.vocab)
			tensor_dict = move_to_device(batch.as_tensor_dict(), self.device)

			self.model.zero_grad()

			output_dict = self.model(**tensor_dict)
			loss = output_dict["loss"]

			if self._used_params is None or self._used_param_names is None:
				self._used_params = []
				self._used_param_names = []
				loss.backward(retain_graph=True)
				for name, param in self.model.named_parameters():
					if param.requires_grad and param.grad is not None:
						self._used_params.append(param)
						self._used_param_names.append(name)

			grads = autograd.grad(loss, self._used_params)
			assert len(grads) == len(self._used_params)

			self._train_instances.append(
				InstanceWithGrads(instance=instance, loss=loss.detach().item(), grads=grads)
			)

	def _calculate_influence_scores(
		self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]
	) -> List[float]:
		raise NotImplementedError

import math

import torch


from allennlp.modules.matrix_attention.dot_product_matrix_attention import DotProductMatrixAttention
from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention


@MatrixAttention.register("scaled_dot_product")
class ScaledDotProductMatrixAttention(DotProductMatrixAttention):

	def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:
		return super().forward(matrix_1, matrix_2) / math.sqrt(matrix_1.size(-1))

import math
import torch

from allennlp.common import FromParams
from allennlp.nn.util import get_range_vector, get_device_of


class SinusoidalPositionalEncoding(torch.nn.Module, FromParams):

	def __init__(self, min_timescale: float = 1.0, max_timescale: float = 1.0e4):
		super().__init__()
		self.min_timescale = min_timescale
		self.max_timescale = max_timescale

	def forward(self, input_tensor: torch.Tensor):
		_, timesteps, hidden_dim = input_tensor.size()
		num_timescales = hidden_dim // 2
		device = get_device_of(input_tensor)

		timestep_range = get_range_vector(timesteps, device).data.float()
		timescale_range = get_range_vector(num_timescales, device).data.float()

		log_timescale_increments = math.log(
			float(self.max_timescale) / float(self.min_timescale)
		) / float(num_timescales - 1)
		inverse_timescales = self.min_timescale * torch.exp(
			timescale_range * -log_timescale_increments
		)

		scaled_time = timestep_range.unsqueeze(1) * inverse_timescales.unsqueeze(0)
		sinusoids = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 1)
		if hidden_dim % 2 != 0:
			sinusoids = torch.cat([sinusoids, sinusoids.new_zeros(timesteps, 1)], 1)
		return input_tensor + sinusoids.unsqueeze(0)

import torch.nn as nn


class SegmentEmbedding(nn.Embedding):
	def __init__(self, embed_size=512):
		super().__init__(3, embed_size, padding_idx=0)


import argparse
import logging
import math
import os
import re
from typing import List, Tuple
import itertools


from allennlp.commands.subcommand import Subcommand
from allennlp.common import Params, Tqdm
from allennlp.common import logging as common_logging
from allennlp.common.checks import check_for_gpu, ConfigurationError
from allennlp.common.util import prepare_environment
from allennlp.data import Vocabulary
from allennlp.models import Model
from allennlp.training import GradientDescentTrainer, Trainer
from allennlp.training.util import create_serialization_dir, data_loaders_from_params

logger = logging.getLogger(__name__)


@Subcommand.register("find-lr")
class FindLearningRate(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:

		subparser = parser.add_parser(
			self.name, description=description, help="Find a learning rate range."
		)

		subparser.add_argument(
			"param_path", type=str, help="path to parameter file describing the model to be trained"
		)
		subparser.add_argument(
			"-s",
			"--serialization-dir",
			required=True,
			type=str,
			help="The directory in which to save results.",
		)
		subparser.add_argument(
			"-o",
			"--overrides",
			type=str,
			default="",
			help=(
				"a json(net) structure used to override the experiment configuration, e.g., "
				"'{\"iterator.batch_size\": 16}'.  Nested parameters can be specified either"
				" with nested dictionaries or with dot syntax."
			),
		)
		subparser.add_argument(
			"--start-lr", type=float, default=1e-5, help="learning rate to start the search"
		)
		subparser.add_argument(
			"--end-lr", type=float, default=10, help="learning rate up to which search is done"
		)
		subparser.add_argument(
			"--num-batches",
			type=int,
			default=100,
			help="number of mini-batches to run learning rate finder",
		)
		subparser.add_argument(
			"--stopping-factor",
			type=float,
			default=None,
			help="stop the search when the current loss exceeds the best loss recorded by "
			"multiple of stopping factor",
		)
		subparser.add_argument(
			"--linear",
			action="store_true",
			help="increase learning rate linearly instead of exponential increase",
		)
		subparser.add_argument(
			"-f",
			"--force",
			action="store_true",
			required=False,
			help="overwrite the output directory if it exists",
		)
		subparser.add_argument(
			"--file-friendly-logging",
			action="store_true",
			default=False,
			help="outputs tqdm status on separate lines and slows tqdm refresh rate",
		)

		subparser.set_defaults(func=find_learning_rate_from_args)

		return subparser


def find_learning_rate_from_args(args: argparse.Namespace) -> None:
	common_logging.FILE_FRIENDLY_LOGGING = args.file_friendly_logging
	params = Params.from_file(args.param_path, args.overrides)
	find_learning_rate_model(
		params,
		args.serialization_dir,
		start_lr=args.start_lr,
		end_lr=args.end_lr,
		num_batches=args.num_batches,
		linear_steps=args.linear,
		stopping_factor=args.stopping_factor,
		force=args.force,
	)


def find_learning_rate_model(
	params: Params,
	serialization_dir: str,
	start_lr: float = 1e-5,
	end_lr: float = 10,
	num_batches: int = 100,
	linear_steps: bool = False,
	stopping_factor: float = None,
	force: bool = False,
) -> None:
	create_serialization_dir(params, serialization_dir, recover=False, force=force)

	prepare_environment(params)

	cuda_device = params.params.get("trainer").get("cuda_device", -1)
	check_for_gpu(cuda_device)
	distributed_params = params.params.get("distributed")
	assert not distributed_params, "find-lr is not compatible with DistributedDataParallel."

	all_data_loaders = data_loaders_from_params(params)
	datasets_for_vocab_creation = set(params.pop("datasets_for_vocab_creation", all_data_loaders))

	for dataset in datasets_for_vocab_creation:
		if dataset not in all_data_loaders:
			raise ConfigurationError(f"invalid 'dataset_for_vocab_creation' {dataset}")

	logger.info(
		"From dataset instances, %s will be considered for vocabulary creation.",
		", ".join(datasets_for_vocab_creation),
	)
	vocab = Vocabulary.from_params(
		params.pop("vocabulary", {}),
		instances=(
			instance
			for key, data_loader in all_data_loaders.items()
			if key in datasets_for_vocab_creation
			for instance in data_loader.iter_instances()
		),
	)

	model = Model.from_params(
		vocab=vocab, params=params.pop("model"), serialization_dir=serialization_dir
	)

	all_data_loaders["train"].index_with(vocab)

	trainer_params = params.pop("trainer")

	no_grad_regexes = trainer_params.pop("no_grad", ())
	for name, parameter in model.named_parameters():
		if any(re.search(regex, name) for regex in no_grad_regexes):
			parameter.requires_grad_(False)

	trainer_choice = trainer_params.pop("type", "gradient_descent")
	if trainer_choice != "gradient_descent":
		raise ConfigurationError(
			"currently find-learning-rate only works with the GradientDescentTrainer"
		)
	trainer: GradientDescentTrainer = Trainer.from_params(  # type: ignore
		model=model,
		serialization_dir=serialization_dir,
		data_loader=all_data_loaders["train"],
		params=trainer_params,
	)

	logger.info(
		f"Starting learning rate search from {start_lr} to {end_lr} in {num_batches} iterations."
	)
	learning_rates, losses = search_learning_rate(
		trainer,
		start_lr=start_lr,
		end_lr=end_lr,
		num_batches=num_batches,
		linear_steps=linear_steps,
		stopping_factor=stopping_factor,
	)
	logger.info("Finished learning rate search.")
	losses = _smooth(losses, 0.98)

	_save_plot(learning_rates, losses, os.path.join(serialization_dir, "lr-losses.png"))


def search_learning_rate(
	trainer: GradientDescentTrainer,
	start_lr: float = 1e-5,
	end_lr: float = 10,
	num_batches: int = 100,
	linear_steps: bool = False,
	stopping_factor: float = None,
) -> Tuple[List[float], List[float]]:
	if num_batches <= 10:
		raise ConfigurationError(
			"The number of iterations for learning rate finder should be greater than 10."
		)

	trainer.model.train()

	infinite_generator = itertools.cycle(trainer.data_loader)
	train_generator_tqdm = Tqdm.tqdm(infinite_generator, total=num_batches)

	learning_rates = []
	losses = []
	best = 1e9
	if linear_steps:
		lr_update_factor = (end_lr - start_lr) / num_batches
	else:
		lr_update_factor = (end_lr / start_lr) ** (1.0 / num_batches)

	for i, batch in enumerate(train_generator_tqdm):

		if linear_steps:
			current_lr = start_lr + (lr_update_factor * i)
		else:
			current_lr = start_lr * (lr_update_factor**i)

		for param_group in trainer.optimizer.param_groups:
			param_group["lr"] = current_lr
			for p in param_group["params"]:
				p.grad = None

		loss = trainer.batch_outputs(batch, for_training=True)["loss"]
		loss.backward()
		loss = loss.detach().cpu().item()

		if stopping_factor is not None and (math.isnan(loss) or loss > stopping_factor * best):
			logger.info(f"Loss ({loss}) exceeds stopping_factor * lowest recorded loss.")
			break

		trainer.rescale_gradients()
		trainer.optimizer.step()

		learning_rates.append(current_lr)
		losses.append(loss)

		if loss < best and i > 10:
			best = loss

		if i == num_batches:
			break

	return learning_rates, losses


def _smooth(values: List[float], beta: float) -> List[float]:
	avg_value = 0.0
	smoothed = []
	for i, value in enumerate(values):
		avg_value = beta * avg_value + (1 - beta) * value
		smoothed.append(avg_value / (1 - beta ** (i + 1)))
	return smoothed


def _save_plot(learning_rates: List[float], losses: List[float], save_path: str):

	try:
		import matplotlib

		matplotlib.use("Agg")  # noqa
		import matplotlib.pyplot as plt

	except ModuleNotFoundError as error:

		logger.warn(
			"To use allennlp find-learning-rate, please install matplotlib: pip install matplotlib>=2.2.3 ."
		)
		raise error

	plt.ylabel("loss")
	plt.xlabel("learning rate (log10 scale)")
	plt.xscale("log")
	plt.plot(learning_rates, losses)
	logger.info(f"Saving learning_rate vs loss plot to {save_path}.")
	plt.savefig(save_path)

import torch

from allennlp.common import FromParams

from allennlp.modules.transformer.transformer_module import TransformerModule
from allennlp.modules.transformer.layer_norm import LayerNorm


class OutputLayer(TransformerModule, FromParams):

	_pretrained_mapping = {"LayerNorm": "layer_norm"}

	def __init__(self, input_size: int, hidden_size: int, dropout: float):
		super().__init__()
		self.dense = torch.nn.Linear(input_size, hidden_size)
		self.layer_norm = LayerNorm(hidden_size, eps=1e-12)
		self.dropout = torch.nn.Dropout(dropout)

	def get_output_dim(self) -> int:
		return self.dense.out_features

	def forward(self, hidden_states, input_tensor):
		dense_output = self.dense(hidden_states)
		dropout_output = self.dropout(dense_output)
		output = self.layer_norm(dropout_output + input_tensor)
		return output

from typing import Optional, Dict, Any, Callable
import logging
import json

from allennlp.common.util import sanitize
from allennlp.data.fields import TensorField
from allennlp.common import Registrable
from allennlp.data import DataLoader

logger = logging.getLogger(__name__)


class Serializer(Registrable):

	def __call__(
		self,
		batch: Dict[str, TensorField],
		output_dict: Dict,
		data_loader: DataLoader,
		output_postprocess_function: Optional[Callable] = None,
	) -> str:
		raise NotImplementedError("__call__")

	default_implementation = "simple"


@Serializer.register("simple")
class SimpleSerializer(Serializer):

	def _to_params(self) -> Dict[str, Any]:
		return {"type": "simple"}

	def __call__(
		self,
		batch: Dict[str, TensorField],
		output_dict: Dict,
		data_loader: DataLoader,
		output_postprocess_function: Optional[Callable] = None,
	):
		if batch is None:
			raise ValueError("Serializer got a batch that is None")
		if output_dict is None:
			raise ValueError("Serializer got an output_dict that is None")

		serialized = sanitize(batch)
		if output_postprocess_function is not None:
			serialized.update(sanitize(output_postprocess_function(output_dict)))
		else:
			serialized.update(sanitize(output_dict))

		return json.dumps(serialized)

from typing import Iterable, Optional

import torch

from allennlp.common.registrable import Registrable


class Metric(Registrable):

	supports_distributed = False

	def __call__(
		self, predictions: torch.Tensor, gold_labels: torch.Tensor, mask: Optional[torch.BoolTensor]
	):
		raise NotImplementedError

	def get_metric(self, reset: bool):
		raise NotImplementedError

	def reset(self) -> None:
		raise NotImplementedError

	@staticmethod
	def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:
		return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)

from allennlp.interpret.attackers.attacker import Attacker
from allennlp.interpret.attackers.input_reduction import InputReduction
from allennlp.interpret.attackers.hotflip import Hotflip

import json
import logging
import warnings
from typing import Any, Dict, List, Union

import numpy
import torch

from torch.nn.modules import Dropout

from allennlp.common import FromParams
from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path
from allennlp.common.util import lazy_groups_of
from allennlp.data.instance import Instance
from allennlp.data.tokenizers.token_class import Token
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.batch import Batch
from allennlp.data.fields import TextField
from allennlp.data.token_indexers.elmo_indexer import (
	ELMoCharacterMapper,
	ELMoTokenCharactersIndexer,
)
from allennlp.modules.elmo_lstm import ElmoLstm
from allennlp.modules.highway import Highway
from allennlp.modules.scalar_mix import ScalarMix
from allennlp.nn.util import (
	add_sentence_boundary_token_ids,
	get_device_of,
	remove_sentence_boundaries,
)

with warnings.catch_warnings():
	warnings.filterwarnings("ignore", category=FutureWarning)
	import h5py


logger = logging.getLogger(__name__)


class Elmo(torch.nn.Module, FromParams):

	def __init__(
		self,
		options_file: str,
		weight_file: str,
		num_output_representations: int,
		requires_grad: bool = False,
		do_layer_norm: bool = False,
		dropout: float = 0.5,
		vocab_to_cache: List[str] = None,
		keep_sentence_boundaries: bool = False,
		scalar_mix_parameters: List[float] = None,
		module: torch.nn.Module = None,
	) -> None:
		super().__init__()

		logger.info("Initializing ELMo")
		if module is not None:
			if options_file is not None or weight_file is not None:
				raise ConfigurationError("Don't provide options_file or weight_file with module")
			self._elmo_lstm = module
		else:
			self._elmo_lstm = _ElmoBiLm(  # type: ignore
				options_file,
				weight_file,
				requires_grad=requires_grad,
				vocab_to_cache=vocab_to_cache,
			)
		self._has_cached_vocab = vocab_to_cache is not None
		self._keep_sentence_boundaries = keep_sentence_boundaries
		self._dropout = Dropout(p=dropout)
		self._scalar_mixes: Any = []
		for k in range(num_output_representations):
			scalar_mix = ScalarMix(
				self._elmo_lstm.num_layers,  # type: ignore
				do_layer_norm=do_layer_norm,
				initial_scalar_parameters=scalar_mix_parameters,
				trainable=scalar_mix_parameters is None,
			)
			self.add_module("scalar_mix_{}".format(k), scalar_mix)
			self._scalar_mixes.append(scalar_mix)

	def get_output_dim(self):
		return self._elmo_lstm.get_output_dim()

	def forward(
		self, inputs: torch.Tensor, word_inputs: torch.Tensor = None
	) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
		original_shape = inputs.size()
		if len(original_shape) > 3:
			timesteps, num_characters = original_shape[-2:]
			reshaped_inputs = inputs.view(-1, timesteps, num_characters)
		else:
			reshaped_inputs = inputs

		if word_inputs is not None:
			original_word_size = word_inputs.size()
			if self._has_cached_vocab and len(original_word_size) > 2:
				reshaped_word_inputs = word_inputs.view(-1, original_word_size[-1])
			elif not self._has_cached_vocab:
				logger.warning(
					"Word inputs were passed to ELMo but it does not have a cached vocab."
				)
				reshaped_word_inputs = None
			else:
				reshaped_word_inputs = word_inputs
		else:
			reshaped_word_inputs = word_inputs

		bilm_output = self._elmo_lstm(reshaped_inputs, reshaped_word_inputs)  # type: ignore
		layer_activations = bilm_output["activations"]
		mask_with_bos_eos = bilm_output["mask"]

		representations = []
		for i in range(len(self._scalar_mixes)):
			scalar_mix = getattr(self, "scalar_mix_{}".format(i))
			representation_with_bos_eos = scalar_mix(layer_activations, mask_with_bos_eos)
			if self._keep_sentence_boundaries:
				processed_representation = representation_with_bos_eos
				processed_mask = mask_with_bos_eos
			else:
				representation_without_bos_eos, mask_without_bos_eos = remove_sentence_boundaries(
					representation_with_bos_eos, mask_with_bos_eos
				)
				processed_representation = representation_without_bos_eos
				processed_mask = mask_without_bos_eos
			representations.append(self._dropout(processed_representation))

		if word_inputs is not None and len(original_word_size) > 2:
			mask = processed_mask.view(original_word_size)
			elmo_representations = [
				representation.view(original_word_size + (-1,))
				for representation in representations
			]
		elif len(original_shape) > 3:
			mask = processed_mask.view(original_shape[:-1])
			elmo_representations = [
				representation.view(original_shape[:-1] + (-1,))
				for representation in representations
			]
		else:
			mask = processed_mask
			elmo_representations = representations

		return {"elmo_representations": elmo_representations, "mask": mask}


def batch_to_ids(batch: List[List[str]]) -> torch.Tensor:
	instances = []
	indexer = ELMoTokenCharactersIndexer()
	for sentence in batch:
		tokens = [Token(token) for token in sentence]
		field = TextField(tokens, {"character_ids": indexer})
		instance = Instance({"elmo": field})
		instances.append(instance)

	dataset = Batch(instances)
	vocab = Vocabulary()
	dataset.index_instances(vocab)
	return dataset.as_tensor_dict()["elmo"]["character_ids"]["elmo_tokens"]


class _ElmoCharacterEncoder(torch.nn.Module):

	def __init__(self, options_file: str, weight_file: str, requires_grad: bool = False) -> None:
		super().__init__()

		with open(cached_path(options_file), "r") as fin:
			self._options = json.load(fin)
		self._weight_file = weight_file

		self.output_dim = self._options["lstm"]["projection_dim"]
		self.requires_grad = requires_grad

		self._load_weights()

		self._beginning_of_sentence_characters = torch.from_numpy(
			numpy.array(ELMoCharacterMapper.beginning_of_sentence_characters) + 1
		)
		self._end_of_sentence_characters = torch.from_numpy(
			numpy.array(ELMoCharacterMapper.end_of_sentence_characters) + 1
		)

	def get_output_dim(self):
		return self.output_dim

	def forward(self, inputs: torch.Tensor) -> Dict[str, torch.Tensor]:
		mask = (inputs > 0).sum(dim=-1) > 0
		character_ids_with_bos_eos, mask_with_bos_eos = add_sentence_boundary_token_ids(
			inputs, mask, self._beginning_of_sentence_characters, self._end_of_sentence_characters
		)

		max_chars_per_token = self._options["char_cnn"]["max_characters_per_token"]
		character_embedding = torch.nn.functional.embedding(
			character_ids_with_bos_eos.view(-1, max_chars_per_token), self._char_embedding_weights
		)

		cnn_options = self._options["char_cnn"]
		if cnn_options["activation"] == "tanh":
			activation = torch.tanh
		elif cnn_options["activation"] == "relu":
			activation = torch.nn.functional.relu
		else:
			raise ConfigurationError("Unknown activation")

		character_embedding = torch.transpose(character_embedding, 1, 2)
		convs = []
		for i in range(len(self._convolutions)):
			conv = getattr(self, "char_conv_{}".format(i))
			convolved = conv(character_embedding)
			convolved, _ = torch.max(convolved, dim=-1)
			convolved = activation(convolved)
			convs.append(convolved)

		token_embedding = torch.cat(convs, dim=-1)

		token_embedding = self._highways(token_embedding)

		token_embedding = self._projection(token_embedding)

		batch_size, sequence_length, _ = character_ids_with_bos_eos.size()

		return {
			"mask": mask_with_bos_eos,
			"token_embedding": token_embedding.view(batch_size, sequence_length, -1),
		}

	def _load_weights(self):
		self._load_char_embedding()
		self._load_cnn_weights()
		self._load_highway()
		self._load_projection()

	def _load_char_embedding(self):
		with h5py.File(cached_path(self._weight_file), "r") as fin:
			char_embed_weights = fin["char_embed"][...]

		weights = numpy.zeros(
			(char_embed_weights.shape[0] + 1, char_embed_weights.shape[1]), dtype="float32"
		)
		weights[1:, :] = char_embed_weights

		self._char_embedding_weights = torch.nn.Parameter(
			torch.FloatTensor(weights), requires_grad=self.requires_grad
		)

	def _load_cnn_weights(self):
		cnn_options = self._options["char_cnn"]
		filters = cnn_options["filters"]
		char_embed_dim = cnn_options["embedding"]["dim"]

		convolutions = []
		for i, (width, num) in enumerate(filters):
			conv = torch.nn.Conv1d(
				in_channels=char_embed_dim, out_channels=num, kernel_size=width, bias=True
			)
			with h5py.File(cached_path(self._weight_file), "r") as fin:
				weight = fin["CNN"]["W_cnn_{}".format(i)][...]
				bias = fin["CNN"]["b_cnn_{}".format(i)][...]

			w_reshaped = numpy.transpose(weight.squeeze(axis=0), axes=(2, 1, 0))
			if w_reshaped.shape != tuple(conv.weight.data.shape):
				raise ValueError("Invalid weight file")
			conv.weight.data.copy_(torch.FloatTensor(w_reshaped))
			conv.bias.data.copy_(torch.FloatTensor(bias))

			conv.weight.requires_grad = self.requires_grad
			conv.bias.requires_grad = self.requires_grad

			convolutions.append(conv)
			self.add_module("char_conv_{}".format(i), conv)

		self._convolutions = convolutions

	def _load_highway(self):

		cnn_options = self._options["char_cnn"]
		filters = cnn_options["filters"]
		n_filters = sum(f[1] for f in filters)
		n_highway = cnn_options["n_highway"]

		self._highways = Highway(n_filters, n_highway, activation=torch.nn.functional.relu)
		for k in range(n_highway):
			with h5py.File(cached_path(self._weight_file), "r") as fin:
				w_transform = numpy.transpose(fin["CNN_high_{}".format(k)]["W_transform"][...])
				w_carry = -1.0 * numpy.transpose(fin["CNN_high_{}".format(k)]["W_carry"][...])
				weight = numpy.concatenate([w_transform, w_carry], axis=0)
				self._highways._layers[k].weight.data.copy_(torch.FloatTensor(weight))
				self._highways._layers[k].weight.requires_grad = self.requires_grad

				b_transform = fin["CNN_high_{}".format(k)]["b_transform"][...]
				b_carry = -1.0 * fin["CNN_high_{}".format(k)]["b_carry"][...]
				bias = numpy.concatenate([b_transform, b_carry], axis=0)
				self._highways._layers[k].bias.data.copy_(torch.FloatTensor(bias))
				self._highways._layers[k].bias.requires_grad = self.requires_grad

	def _load_projection(self):
		cnn_options = self._options["char_cnn"]
		filters = cnn_options["filters"]
		n_filters = sum(f[1] for f in filters)

		self._projection = torch.nn.Linear(n_filters, self.output_dim, bias=True)
		with h5py.File(cached_path(self._weight_file), "r") as fin:
			weight = fin["CNN_proj"]["W_proj"][...]
			bias = fin["CNN_proj"]["b_proj"][...]
			self._projection.weight.data.copy_(torch.FloatTensor(numpy.transpose(weight)))
			self._projection.bias.data.copy_(torch.FloatTensor(bias))

			self._projection.weight.requires_grad = self.requires_grad
			self._projection.bias.requires_grad = self.requires_grad


class _ElmoBiLm(torch.nn.Module):

	def __init__(
		self,
		options_file: str,
		weight_file: str,
		requires_grad: bool = False,
		vocab_to_cache: List[str] = None,
	) -> None:
		super().__init__()

		self._token_embedder = _ElmoCharacterEncoder(
			options_file, weight_file, requires_grad=requires_grad
		)

		self._requires_grad = requires_grad
		if requires_grad and vocab_to_cache:
			logging.warning(
				"You are fine tuning ELMo and caching char CNN word vectors. "
				"This behaviour is not guaranteed to be well defined, particularly. "
				"if not all of your inputs will occur in the vocabulary cache."
			)
		self._word_embedding = None
		self._bos_embedding: torch.Tensor = None
		self._eos_embedding: torch.Tensor = None
		if vocab_to_cache:
			logging.info("Caching character cnn layers for words in vocabulary.")
			self.create_cached_cnn_embeddings(vocab_to_cache)

		with open(cached_path(options_file), "r") as fin:
			options = json.load(fin)
		if not options["lstm"].get("use_skip_connections"):
			raise ConfigurationError("We only support pretrained biLMs with residual connections")
		self._elmo_lstm = ElmoLstm(
			input_size=options["lstm"]["projection_dim"],
			hidden_size=options["lstm"]["projection_dim"],
			cell_size=options["lstm"]["dim"],
			num_layers=options["lstm"]["n_layers"],
			memory_cell_clip_value=options["lstm"]["cell_clip"],
			state_projection_clip_value=options["lstm"]["proj_clip"],
			requires_grad=requires_grad,
		)
		self._elmo_lstm.load_weights(weight_file)
		self.num_layers = options["lstm"]["n_layers"] + 1

	def get_output_dim(self):
		return 2 * self._token_embedder.get_output_dim()

	def forward(
		self, inputs: torch.Tensor, word_inputs: torch.Tensor = None
	) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
		if self._word_embedding is not None and word_inputs is not None:
			try:
				mask_without_bos_eos = word_inputs > 0
				embedded_inputs = self._word_embedding(word_inputs)  # type: ignore
				type_representation, mask = add_sentence_boundary_token_ids(
					embedded_inputs, mask_without_bos_eos, self._bos_embedding, self._eos_embedding
				)
			except (RuntimeError, IndexError):
				token_embedding = self._token_embedder(inputs)
				mask = token_embedding["mask"]
				type_representation = token_embedding["token_embedding"]
		else:
			token_embedding = self._token_embedder(inputs)
			mask = token_embedding["mask"]
			type_representation = token_embedding["token_embedding"]
		lstm_outputs = self._elmo_lstm(type_representation, mask)

		output_tensors = [
			torch.cat([type_representation, type_representation], dim=-1) * mask.unsqueeze(-1)
		]
		for layer_activations in torch.chunk(lstm_outputs, lstm_outputs.size(0), dim=0):
			output_tensors.append(layer_activations.squeeze(0))

		return {"activations": output_tensors, "mask": mask}

	def create_cached_cnn_embeddings(self, tokens: List[str]) -> None:
		tokens = [ELMoCharacterMapper.bos_token, ELMoCharacterMapper.eos_token] + tokens
		timesteps = 32
		batch_size = 32
		chunked_tokens = lazy_groups_of(iter(tokens), timesteps)

		all_embeddings = []
		device = get_device_of(next(self.parameters()))
		for batch in lazy_groups_of(chunked_tokens, batch_size):
			batched_tensor = batch_to_ids(batch)
			if device >= 0:
				batched_tensor = batched_tensor.cuda(device)
			output = self._token_embedder(batched_tensor)
			token_embedding = output["token_embedding"]
			mask = output["mask"]
			token_embedding, _ = remove_sentence_boundaries(token_embedding, mask)
			all_embeddings.append(token_embedding.view(-1, token_embedding.size(-1)))
		full_embedding = torch.cat(all_embeddings, 0)

		full_embedding = full_embedding[: len(tokens), :]
		embedding = full_embedding[2 : len(tokens), :]
		vocab_size, embedding_dim = list(embedding.size())

		from allennlp.modules.token_embedders import Embedding  # type: ignore

		self._bos_embedding = full_embedding[0, :]
		self._eos_embedding = full_embedding[1, :]
		self._word_embedding = Embedding(  # type: ignore
			num_embeddings=vocab_size,
			embedding_dim=embedding_dim,
			weight=embedding.data,
			trainable=self._requires_grad,
			padding_index=0,
		)

import torch

from allennlp.nn.regularizers.regularizer import Regularizer


@Regularizer.register("l1")
class L1Regularizer(Regularizer):

	def __init__(self, alpha: float = 0.01) -> None:
		self.alpha = alpha

	def __call__(self, parameter: torch.Tensor) -> torch.Tensor:
		return self.alpha * torch.sum(torch.abs(parameter))


@Regularizer.register("l2")
class L2Regularizer(Regularizer):

	def __init__(self, alpha: float = 0.01) -> None:
		self.alpha = alpha

	def __call__(self, parameter: torch.Tensor) -> torch.Tensor:
		return self.alpha * torch.sum(torch.pow(parameter, 2))

import torch
from typing import List

from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder


@Seq2SeqEncoder.register("compose")
class ComposeEncoder(Seq2SeqEncoder):


	def __init__(self, encoders: List[Seq2SeqEncoder]):
		super().__init__()
		self.encoders = encoders
		for idx, encoder in enumerate(encoders):
			self.add_module("encoder%d" % idx, encoder)

		all_bidirectional = all(encoder.is_bidirectional() for encoder in encoders)
		any_bidirectional = any(encoder.is_bidirectional() for encoder in encoders)
		self.bidirectional = all_bidirectional

		if all_bidirectional != any_bidirectional:
			raise ValueError("All encoders need to match in bidirectionality.")

		if len(self.encoders) < 1:
			raise ValueError("Need at least one encoder.")

		last_enc = None
		for enc in encoders:
			if last_enc is not None and last_enc.get_output_dim() != enc.get_input_dim():
				raise ValueError("Encoder input and output dimensions don't match.")
			last_enc = enc

	def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor = None) -> torch.Tensor:
		for encoder in self.encoders:
			inputs = encoder(inputs, mask)
		return inputs

	def get_input_dim(self) -> int:
		return self.encoders[0].get_input_dim()

	def get_output_dim(self) -> int:
		return self.encoders[-1].get_output_dim()

	def is_bidirectional(self) -> bool:
		return self.bidirectional

import re
from typing import List, Tuple

import torch

from allennlp.common import FromParams
from allennlp.nn.regularizers.regularizer import Regularizer


class RegularizerApplicator(FromParams):

	def __init__(self, regexes: List[Tuple[str, Regularizer]] = None) -> None:
		self._regularizers = regexes or []

	def __call__(self, module: torch.nn.Module) -> torch.Tensor:
		accumulator = 0.0
		for name, parameter in module.named_parameters():
			if parameter.requires_grad:
				for regex, regularizer in self._regularizers:
					if re.search(regex, name):
						penalty = regularizer(parameter)
						accumulator = accumulator + penalty
						break
		return accumulator

from typing import Optional, Tuple


import torch
from torch.nn import Conv1d, Linear

from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder
from allennlp.nn import Activation
from allennlp.nn.util import min_value_of_dtype


@Seq2VecEncoder.register("cnn")
class CnnEncoder(Seq2VecEncoder):

	def __init__(
		self,
		embedding_dim: int,
		num_filters: int,
		ngram_filter_sizes: Tuple[int, ...] = (2, 3, 4, 5),
		conv_layer_activation: Activation = None,
		output_dim: Optional[int] = None,
	) -> None:
		super().__init__()
		self._embedding_dim = embedding_dim
		self._num_filters = num_filters
		self._ngram_filter_sizes = ngram_filter_sizes
		self._activation = conv_layer_activation or Activation.by_name("relu")()

		self._convolution_layers = [
			Conv1d(
				in_channels=self._embedding_dim,
				out_channels=self._num_filters,
				kernel_size=ngram_size,
			)
			for ngram_size in self._ngram_filter_sizes
		]
		for i, conv_layer in enumerate(self._convolution_layers):
			self.add_module("conv_layer_%d" % i, conv_layer)

		maxpool_output_dim = self._num_filters * len(self._ngram_filter_sizes)
		if output_dim:
			self.projection_layer = Linear(maxpool_output_dim, output_dim)
			self._output_dim = output_dim
		else:
			self.projection_layer = None
			self._output_dim = maxpool_output_dim

	def get_input_dim(self) -> int:
		return self._embedding_dim

	def get_output_dim(self) -> int:
		return self._output_dim

	def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):
		if mask is not None:
			tokens = tokens * mask.unsqueeze(-1)
		else:
			mask = torch.ones(tokens.shape[0], tokens.shape[1], device=tokens.device).bool()

		tokens = torch.transpose(tokens, 1, 2)

		filter_outputs = []
		batch_size = tokens.shape[0]
		last_unmasked_tokens = mask.sum(dim=1).unsqueeze(dim=-1)
		for i in range(len(self._convolution_layers)):
			convolution_layer = getattr(self, "conv_layer_{}".format(i))
			pool_length = tokens.shape[2] - convolution_layer.kernel_size[0] + 1

			activations = self._activation(convolution_layer(tokens))

			indices = (
				torch.arange(pool_length, device=activations.device)
				.unsqueeze(0)
				.expand(batch_size, pool_length)
			)
			activations_mask = indices.ge(
				last_unmasked_tokens - convolution_layer.kernel_size[0] + 1
			)
			activations_mask = activations_mask.unsqueeze(1).expand_as(activations)

			activations = activations + (activations_mask * min_value_of_dtype(activations.dtype))

			filter_outputs.append(activations.max(dim=2)[0])

		maxpool_output = (
			torch.cat(filter_outputs, dim=1) if len(filter_outputs) > 1 else filter_outputs[0]
		)

		maxpool_output[maxpool_output == min_value_of_dtype(maxpool_output.dtype)] = 0.0

		if self.projection_layer:
			result = self.projection_layer(maxpool_output)
		else:
			result = maxpool_output
		return result

import logging


import numpy as np
import torch

from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler


logger = logging.getLogger(__name__)


@LearningRateScheduler.register("cosine")
class CosineWithRestarts(LearningRateScheduler):

	def __init__(
		self,
		optimizer: torch.optim.Optimizer,
		t_initial: int,
		t_mul: float = 1.0,
		eta_min: float = 0.0,
		eta_mul: float = 1.0,
		last_epoch: int = -1,
	) -> None:
		assert t_initial > 0
		assert eta_min >= 0
		if t_initial == 1 and t_mul == 1 and eta_mul == 1:
			logger.warning(
				"Cosine annealing scheduler will have no effect on the learning "
				"rate since t_initial = t_mul = eta_mul = 1."
			)
		self.t_initial = t_initial
		self.t_mul = t_mul
		self.eta_min = eta_min
		self.eta_mul = eta_mul
		self._last_restart: int = 0
		self._cycle_counter: int = 0
		self._cycle_len: int = t_initial
		self._n_restarts: int = 0
		super().__init__(optimizer, last_epoch)

	def get_values(self):
		if self.last_epoch == -1:
			return self.base_values

		step = self.last_epoch + 1
		self._cycle_counter = step - self._last_restart

		if self._cycle_counter % self._cycle_len == 0:
			self._n_restarts += 1
			self._cycle_counter = 0
			self._last_restart = step

		base_lrs = [lr * self.eta_mul**self._n_restarts for lr in self.base_values]
		self._cycle_len = int(self.t_initial * self.t_mul**self._n_restarts)

		lrs = [
			self.eta_min
			+ ((lr - self.eta_min) / 2)
			* (np.cos(np.pi * (self._cycle_counter % self._cycle_len) / self._cycle_len) + 1)
			for lr in base_lrs
		]

		return lrs

import random

import numpy as np

import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F

from .attention import Attention
from .baseRNN import BaseRNN

if torch.cuda.is_available():
	import torch.cuda as device
else:
	import torch as device


class DecoderRNN(BaseRNN):

	KEY_ATTN_SCORE = 'attention_score'
	KEY_LENGTH = 'length'
	KEY_SEQUENCE = 'sequence'

	def __init__(self, vocab_size, max_len, hidden_size,
			sos_id, eos_id,
			n_layers=1, rnn_cell='gru', bidirectional=False,
			input_dropout_p=0, dropout_p=0, use_attention=False):
		super(DecoderRNN, self).__init__(vocab_size, max_len, hidden_size,
				input_dropout_p, dropout_p,
				n_layers, rnn_cell)

		self.bidirectional_encoder = bidirectional
		self.rnn = self.rnn_cell(hidden_size, hidden_size, n_layers, batch_first=True, dropout=dropout_p)

		self.output_size = vocab_size
		self.max_length = max_len
		self.use_attention = use_attention
		self.eos_id = eos_id
		self.sos_id = sos_id

		self.init_input = None

		self.embedding = nn.Embedding(self.output_size, self.hidden_size)
		if use_attention:
			self.attention = Attention(self.hidden_size)

		self.out = nn.Linear(self.hidden_size, self.output_size)

	def forward_step(self, input_var, hidden, encoder_outputs, function):
		batch_size = input_var.size(0)
		output_size = input_var.size(1)
		embedded = self.embedding(input_var)
		embedded = self.input_dropout(embedded)

		output, hidden = self.rnn(embedded, hidden)

		attn = None
		if self.use_attention:
			output, attn = self.attention(output, encoder_outputs)

		predicted_softmax = function(self.out(output.contiguous().view(-1, self.hidden_size)), dim=1).view(batch_size, output_size, -1)
		return predicted_softmax, hidden, attn

	def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None,
					function=F.log_softmax, teacher_forcing_ratio=0):
		ret_dict = dict()
		if self.use_attention:
			ret_dict[DecoderRNN.KEY_ATTN_SCORE] = list()

		inputs, batch_size, max_length = self._validate_args(inputs, encoder_hidden, encoder_outputs,
															 function, teacher_forcing_ratio)
		decoder_hidden = self._init_state(encoder_hidden)

		use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

		decoder_outputs = []
		sequence_symbols = []
		lengths = np.array([max_length] * batch_size)

		def decode(step, step_output, step_attn):
			decoder_outputs.append(step_output)
			if self.use_attention:
				ret_dict[DecoderRNN.KEY_ATTN_SCORE].append(step_attn)
			symbols = decoder_outputs[-1].topk(1)[1]
			sequence_symbols.append(symbols)

			eos_batches = symbols.data.eq(self.eos_id)
			if eos_batches.dim() > 0:
				eos_batches = eos_batches.cpu().view(-1).numpy()
				update_idx = ((lengths > step) & eos_batches) != 0
				lengths[update_idx] = len(sequence_symbols)
			return symbols

		if use_teacher_forcing:
			decoder_input = inputs[:, :-1]
			decoder_output, decoder_hidden, attn = self.forward_step(decoder_input, decoder_hidden, encoder_outputs,
																	 function=function)

			for di in range(decoder_output.size(1)):
				step_output = decoder_output[:, di, :]
				if attn is not None:
					step_attn = attn[:, di, :]
				else:
					step_attn = None
				decode(di, step_output, step_attn)
		else:
			decoder_input = inputs[:, 0].unsqueeze(1)
			for di in range(max_length):
				decoder_output, decoder_hidden, step_attn = self.forward_step(decoder_input, decoder_hidden, encoder_outputs,
																		 function=function)
				step_output = decoder_output.squeeze(1)
				symbols = decode(di, step_output, step_attn)
				decoder_input = symbols

		ret_dict[DecoderRNN.KEY_SEQUENCE] = sequence_symbols
		ret_dict[DecoderRNN.KEY_LENGTH] = lengths.tolist()

		return decoder_outputs, decoder_hidden, ret_dict

	def _init_state(self, encoder_hidden):
		if encoder_hidden is None:
			return None
		if isinstance(encoder_hidden, tuple):
			encoder_hidden = tuple([self._cat_directions(h) for h in encoder_hidden])
		else:
			encoder_hidden = self._cat_directions(encoder_hidden)
		return encoder_hidden

	def _cat_directions(self, h):
		if self.bidirectional_encoder:
			h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)
		return h

	def _validate_args(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio):
		if self.use_attention:
			if encoder_outputs is None:
				raise ValueError("Argument encoder_outputs cannot be None when attention is used.")

		if inputs is None and encoder_hidden is None:
			batch_size = 1
		else:
			if inputs is not None:
				batch_size = inputs.size(0)
			else:
				if self.rnn_cell is nn.LSTM:
					batch_size = encoder_hidden[0].size(1)
				elif self.rnn_cell is nn.GRU:
					batch_size = encoder_hidden.size(1)

		if inputs is None:
			if teacher_forcing_ratio > 0:
				raise ValueError("Teacher forcing has to be disabled (set 0) when no inputs is provided.")
			inputs = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1)
			if torch.cuda.is_available():
				inputs = inputs.cuda()
			max_length = self.max_length
		else:
			max_length = inputs.size(1) - 1 # minus the start of sequence symbol

		return inputs, batch_size, max_length

import math
import time
from typing import Dict, List, Optional

from torch.utils.data import Dataset

from transformers import Seq2SeqTrainer, is_torch_tpu_available
from transformers.trainer_utils import PredictionOutput, speed_metrics


if is_torch_tpu_available(check_device=False):
	import torch_xla.core.xla_model as xm
	import torch_xla.debug.metrics as met


class QuestionAnsweringSeq2SeqTrainer(Seq2SeqTrainer):
	def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):
		super().__init__(*args, **kwargs)
		self.eval_examples = eval_examples
		self.post_process_function = post_process_function

	def evaluate(
		self,
		eval_dataset: Optional[Dataset] = None,
		eval_examples=None,
		ignore_keys: Optional[List[str]] = None,
		metric_key_prefix: str = "eval",
		**gen_kwargs,
	) -> Dict[str, float]:
		gen_kwargs = gen_kwargs.copy()

		if gen_kwargs.get("max_length") is None and self.args.generation_max_length is not None:
			gen_kwargs["max_length"] = self.args.generation_max_length
		if gen_kwargs.get("num_beams") is None and self.args.generation_num_beams is not None:
			gen_kwargs["num_beams"] = self.args.generation_num_beams
		self._gen_kwargs = gen_kwargs

		eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
		eval_dataloader = self.get_eval_dataloader(eval_dataset)
		eval_examples = self.eval_examples if eval_examples is None else eval_examples

		compute_metrics = self.compute_metrics
		self.compute_metrics = None
		start_time = time.time()
		eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
		try:
			output = eval_loop(
				eval_dataloader,
				description="Evaluation",
				prediction_loss_only=True if compute_metrics is None else None,
				ignore_keys=ignore_keys,
				metric_key_prefix=metric_key_prefix,
			)
		finally:
			self.compute_metrics = compute_metrics
		total_batch_size = self.args.eval_batch_size * self.args.world_size
		if f"{metric_key_prefix}_jit_compilation_time" in output.metrics:
			start_time += output.metrics[f"{metric_key_prefix}_jit_compilation_time"]
		output.metrics.update(
			speed_metrics(
				metric_key_prefix,
				start_time,
				num_samples=output.num_samples,
				num_steps=math.ceil(output.num_samples / total_batch_size),
			)
		)

		if self.post_process_function is not None and self.compute_metrics is not None and self.args.should_save:
			eval_preds = self.post_process_function(eval_examples, eval_dataset, output)
			metrics = self.compute_metrics(eval_preds)

			for key in list(metrics.keys()):
				if not key.startswith(f"{metric_key_prefix}_"):
					metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)

			metrics.update(output.metrics)
		else:
			metrics = output.metrics

		if self.args.should_log:
			self.log(metrics)

		if self.args.tpu_metrics_debug or self.args.debug:
			xm.master_print(met.metrics_report())

		self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
		return metrics

	def predict(
		self, predict_dataset, predict_examples, ignore_keys=None, metric_key_prefix: str = "test", **gen_kwargs
	):
		self._gen_kwargs = gen_kwargs.copy()

		predict_dataloader = self.get_test_dataloader(predict_dataset)

		compute_metrics = self.compute_metrics
		self.compute_metrics = None
		start_time = time.time()
		eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
		try:
			output = eval_loop(
				predict_dataloader,
				description="Prediction",
				prediction_loss_only=True if compute_metrics is None else None,
				ignore_keys=ignore_keys,
				metric_key_prefix=metric_key_prefix,
			)
		finally:
			self.compute_metrics = compute_metrics

		total_batch_size = self.args.eval_batch_size * self.args.world_size
		if f"{metric_key_prefix}_jit_compilation_time" in output.metrics:
			start_time += output.metrics[f"{metric_key_prefix}_jit_compilation_time"]
		output.metrics.update(
			speed_metrics(
				metric_key_prefix,
				start_time,
				num_samples=output.num_samples,
				num_steps=math.ceil(output.num_samples / total_batch_size),
			)
		)
		if self.post_process_function is None or self.compute_metrics is None:
			return output

		predictions = self.post_process_function(predict_examples, predict_dataset, output, "predict")
		metrics = self.compute_metrics(predictions)

		for key in list(metrics.keys()):
			if not key.startswith(f"{metric_key_prefix}_"):
				metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
		metrics.update(output.metrics)
		return PredictionOutput(predictions=predictions.predictions, label_ids=predictions.label_ids, metrics=metrics)

from allennlp.data.fields.field import DataArray, Field


class SequenceField(Field[DataArray]):

	__slots__ = []  # type: ignore

	def sequence_length(self) -> int:
		raise NotImplementedError

	def empty_field(self) -> "SequenceField":
		raise NotImplementedError

import logging
import os
from typing import Optional, Dict, Any, List, Union, Tuple, TYPE_CHECKING


import torch

from allennlp.common import Params
from allennlp.training.callbacks.callback import TrainerCallback
from allennlp.training.callbacks.log_writer import LogWriterCallback


if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


logger = logging.getLogger(__name__)


@TrainerCallback.register("wandb")
class WandBCallback(LogWriterCallback):

	def __init__(
		self,
		serialization_dir: str,
		summary_interval: int = 100,
		distribution_interval: Optional[int] = None,
		batch_size_interval: Optional[int] = None,
		should_log_parameter_statistics: bool = True,
		should_log_learning_rate: bool = False,
		project: Optional[str] = None,
		entity: Optional[str] = None,
		group: Optional[str] = None,
		name: Optional[str] = None,
		notes: Optional[str] = None,
		tags: Optional[List[str]] = None,
		watch_model: bool = True,
		files_to_save: Tuple[str, ...] = ("config.json", "out.log"),
		wandb_kwargs: Optional[Dict[str, Any]] = None,
	) -> None:
		if "WANDB_API_KEY" not in os.environ:
			logger.warning(
				"Missing environment variable 'WANDB_API_KEY' required to authenticate to Weights & Biases."
			)

		super().__init__(
			serialization_dir,
			summary_interval=summary_interval,
			distribution_interval=distribution_interval,
			batch_size_interval=batch_size_interval,
			should_log_parameter_statistics=should_log_parameter_statistics,
			should_log_learning_rate=should_log_learning_rate,
		)

		self._watch_model = watch_model
		self._files_to_save = files_to_save
		self._run_id: Optional[str] = None
		self._wandb_kwargs: Dict[str, Any] = dict(
			dir=os.path.abspath(serialization_dir),
			project=project,
			entity=entity,
			group=group,
			name=name,
			notes=notes,
			config=Params.from_file(os.path.join(serialization_dir, "config.json")).as_dict(),
			tags=tags,
			anonymous="allow",
			**(wandb_kwargs or {}),
		)

	def log_scalars(
		self,
		scalars: Dict[str, Union[int, float]],
		log_prefix: str = "",
		epoch: Optional[int] = None,
	) -> None:
		self._log(scalars, log_prefix=log_prefix, epoch=epoch)

	def log_tensors(
		self, tensors: Dict[str, torch.Tensor], log_prefix: str = "", epoch: Optional[int] = None
	) -> None:
		self._log(
			{k: self.wandb.Histogram(v.cpu().data.numpy().flatten()) for k, v in tensors.items()},  # type: ignore
			log_prefix=log_prefix,
			epoch=epoch,
		)

	def _log(
		self, dict_to_log: Dict[str, Any], log_prefix: str = "", epoch: Optional[int] = None
	) -> None:
		if log_prefix:
			dict_to_log = {f"{log_prefix}/{k}": v for k, v in dict_to_log.items()}
		if epoch is not None:
			dict_to_log["epoch"] = epoch
		self.wandb.log(dict_to_log, step=self.trainer._total_batches_completed)  # type: ignore

	def on_start(
		self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
	) -> None:
		super().on_start(trainer, is_primary=is_primary, **kwargs)

		if not is_primary:
			return None

		import wandb

		self.wandb = wandb

		if self._run_id is None:
			self._run_id = self.wandb.util.generate_id()

		self.wandb.init(id=self._run_id, **self._wandb_kwargs)

		for fpath in self._files_to_save:
			self.wandb.save(  # type: ignore
				os.path.join(self.serialization_dir, fpath), base_path=self.serialization_dir
			)

		if self._watch_model:
			self.wandb.watch(self.trainer.model)  # type: ignore

	def close(self) -> None:
		super().close()
		self.wandb.finish()  # type: ignore

	def state_dict(self) -> Dict[str, Any]:
		return {
			"run_id": self._run_id,
		}

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		self._wandb_kwargs["resume"] = "auto"
		self._run_id = state_dict["run_id"]

import logging
import os
from os import PathLike
from typing import TYPE_CHECKING, Optional, Dict, Union, List, Any, TypeVar, Type
import re
import warnings

import torch
import torch.distributed as dist

from allennlp.common.util import is_distributed, is_global_primary
from allennlp.nn.parallel import ShardedModuleMixin
from allennlp.nn.module import Module
from allennlp.nn.util import (
	StateDictType,
	read_state_dict,
	_check_incompatible_keys,
)

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig


logger = logging.getLogger(__name__)


_T = TypeVar("_T", bound="TransformerModule")


class TransformerModule(Module):

	_pretrained_mapping: Dict[str, str] = {}

	_pretrained_relevant_module: Optional[Union[str, List[str]]] = None

	_pretrained_ignore: Optional[List[str]] = None

	_pretrained_allow_missing: Optional[List[str]] = None

	@classmethod
	def _get_mapping(
		cls,
		mapping: Optional[Dict[str, str]] = None,
	):
		combined_mapping = {}
		combined_mapping.update(cls._pretrained_mapping)
		if mapping is not None:
			combined_mapping.update(mapping)
		return combined_mapping

	def _get_mapped_state_dict(
		self,
		state_dict: StateDictType,
		mapping: Optional[Dict[str, str]] = None,
	) -> StateDictType:
		return _get_mapped_state_dict(self, state_dict, mapping=mapping)

	@classmethod
	def _get_relevant_submodule_state(
		cls,
		state_dict: StateDictType,
		relevant_module: Optional[Union[str, List[str]]] = None,
	) -> StateDictType:
		relevant_modules: Optional[List[str]] = None
		if relevant_module:
			relevant_modules = (
				[relevant_module] if isinstance(relevant_module, str) else relevant_module
			)
		elif isinstance(cls._pretrained_relevant_module, str):
			relevant_modules = [cls._pretrained_relevant_module]
		elif isinstance(cls._pretrained_relevant_module, list):
			relevant_modules = cls._pretrained_relevant_module

		if relevant_modules:
			found = False
			for module_name in relevant_modules:
				relevant_keys = set(
					[key for key in state_dict.keys() if key.startswith(module_name + ".")]
				)
				if relevant_keys:
					state_dict = {
						key.replace(module_name + ".", "", 1): value
						for key, value in state_dict.items()
						if key in relevant_keys
					}
					found = True
					break

			if not found:
				warnings.warn(
					f"{relevant_modules} was not found at top level of state_dict!", UserWarning
				)

		return state_dict

	@classmethod
	def _get_pretrained_state_dict(
		cls,
		model_name: str,
		weights_path: Optional[Union[str, PathLike]] = None,
		relevant_module: Optional[Union[str, List[str]]] = None,
		ignore: Optional[List[str]] = None,
	) -> StateDictType:
		if weights_path is None:
			from transformers.file_utils import WEIGHTS_NAME

			if os.path.isdir(model_name):
				local_weights_path = os.path.join(model_name, WEIGHTS_NAME)
				if os.path.isfile(local_weights_path):
					logger.info("Found weights at local path %s", local_weights_path)
					weights_path = local_weights_path

			if weights_path is None:
				from allennlp.common.file_utils import cached_path

				weights_path = cached_path(f"hf://{model_name}/{WEIGHTS_NAME}")

		logger.info("Reading state dict from %s", weights_path)
		state_dict = read_state_dict(
			weights_path,
			ignore=ignore if ignore is not None else cls._pretrained_ignore,
			strict=False,
		)

		state_dict = cls._get_relevant_submodule_state(state_dict, relevant_module=relevant_module)

		return state_dict

	@classmethod
	def _from_config(cls: Type[_T], config: "PretrainedConfig", **kwargs) -> _T:
		raise NotImplementedError

	@classmethod
	def from_pretrained_module(
		cls: Type[_T],
		model_name: str,
		*,
		load_weights: bool = True,
		weights_path: Optional[Union[str, PathLike]] = None,
		auto_config_kwargs: Optional[Dict[str, Any]] = None,
		mapping: Optional[Dict[str, str]] = None,
		relevant_module: Optional[Union[str, List[str]]] = None,
		ignore: Optional[List[str]] = None,
		allow_missing: Optional[List[str]] = None,
		strict: bool = True,
		**kwargs,
	) -> _T:
		from transformers import AutoConfig

		config = AutoConfig.from_pretrained(model_name, **(auto_config_kwargs or {}))
		model = cls._from_config(config, **kwargs)

		if load_weights:
			state_dict: Optional[StateDictType] = None
			if is_global_primary():
				pretrained_state_dict = cls._get_pretrained_state_dict(
					model_name,
					weights_path=weights_path,
					relevant_module=relevant_module,
					ignore=ignore,
				)
				state_dict = model._get_mapped_state_dict(pretrained_state_dict, mapping=mapping)

			logger.info("Loading state_dict into module")

			missing_keys: List[str]
			unexpected_keys: List[str]
			if not is_distributed():
				assert state_dict is not None
				missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
			else:
				dist.barrier()
				missing_keys, unexpected_keys = model.load_state_dict_distributed(
					state_dict, strict=False
				)

			if allow_missing is None:
				allow_missing = cls._pretrained_allow_missing
			if allow_missing:
				missing_keys = [
					k for k in missing_keys if not any(re.match(p, k) for p in allow_missing)
				]

			_check_incompatible_keys(model, missing_keys, unexpected_keys, strict)

		return model


def _get_mapped_state_dict(
	module: torch.nn.Module,
	state_dict: StateDictType,
	mapping: Optional[Dict[str, str]] = None,
) -> StateDictType:
	combined_mapping = module._get_mapping(mapping) if isinstance(module, TransformerModule) else {}
	for hf_key, cls_key in sorted(
		combined_mapping.items(),
		key=lambda x: x[0].count("."),
		reverse=True,
	):
		relevant_keys = set(
			[key for key in state_dict.keys() if (key == hf_key or key.startswith(hf_key + "."))]
		)
		for key in relevant_keys:
			new_key = key.replace(hf_key, cls_key, 1)
			if new_key not in state_dict:
				state_dict[new_key] = state_dict.pop(key)

	for name, submodule in module.named_children():
		if isinstance(submodule, ShardedModuleMixin):
			submodule = submodule.get_original_module()

		relevant_keys = set([key for key in state_dict.keys() if key.startswith(name + ".")])
		module_state_dict = {
			key.replace(name + ".", "", 1): state_dict.pop(key) for key in relevant_keys
		}
		module_state_dict = _get_mapped_state_dict(submodule, module_state_dict)
		for key, value in module_state_dict.items():
			state_dict[name + "." + key] = value

	return state_dict

import torch
import torch.nn as nn


class Bottle(nn.Module):

	def forward(self, input):
		if len(input.size()) <= 2:
			return super(Bottle, self).forward(input)
		size = input.size()[:2]
		out = super(Bottle, self).forward(input.view(size[0]*size[1], -1))
		return out.view(size[0], size[1], -1)


class Linear(Bottle, nn.Linear):
	pass


class Encoder(nn.Module):

	def __init__(self, config):
		super(Encoder, self).__init__()
		self.config = config
		input_size = config.d_proj if config.projection else config.d_embed
		dropout = 0 if config.n_layers == 1 else config.dp_ratio
		self.rnn = nn.LSTM(input_size=input_size, hidden_size=config.d_hidden,
						num_layers=config.n_layers, dropout=dropout,
						bidirectional=config.birnn)

	def forward(self, inputs):
		batch_size = inputs.size()[1]
		state_shape = self.config.n_cells, batch_size, self.config.d_hidden
		h0 = c0 = inputs.new_zeros(state_shape)
		outputs, (ht, ct) = self.rnn(inputs, (h0, c0))
		return ht[-1] if not self.config.birnn else ht[-2:].transpose(0, 1).contiguous().view(batch_size, -1)


class SNLIClassifier(nn.Module):

	def __init__(self, config):
		super(SNLIClassifier, self).__init__()
		self.config = config
		self.embed = nn.Embedding(config.n_embed, config.d_embed)
		self.projection = Linear(config.d_embed, config.d_proj)
		self.encoder = Encoder(config)
		self.dropout = nn.Dropout(p=config.dp_ratio)
		self.relu = nn.ReLU()
		seq_in_size = 2*config.d_hidden
		if self.config.birnn:
			seq_in_size *= 2
		lin_config = [seq_in_size]*2
		self.out = nn.Sequential(
			Linear(*lin_config),
			self.relu,
			self.dropout,
			Linear(*lin_config),
			self.relu,
			self.dropout,
			Linear(*lin_config),
			self.relu,
			self.dropout,
			Linear(seq_in_size, config.d_out))

	def forward(self, batch):
		prem_embed = self.embed(batch.premise)
		hypo_embed = self.embed(batch.hypothesis)
		if self.config.fix_emb:
			prem_embed = prem_embed.detach()
			hypo_embed = hypo_embed.detach()
		if self.config.projection:
			prem_embed = self.relu(self.projection(prem_embed))
			hypo_embed = self.relu(self.projection(hypo_embed))
		premise = self.encoder(prem_embed)
		hypothesis = self.encoder(hypo_embed)
		scores = self.out(torch.cat([premise, hypothesis], 1))
		return scores

from __future__ import print_function
import argparse
import torch
from PIL import Image
from torchvision.transforms import ToTensor

import numpy as np

parser = argparse.ArgumentParser(description='PyTorch Super Res Example')
parser.add_argument('--input_image', type=str, required=True, help='input image to use')
parser.add_argument('--model', type=str, required=True, help='model file to use')
parser.add_argument('--output_filename', type=str, help='where to save the output image')
parser.add_argument('--cuda', action='store_true', help='use cuda')
opt = parser.parse_args()

print(opt)
img = Image.open(opt.input_image).convert('YCbCr')
y, cb, cr = img.split()

model = torch.load(opt.model)
img_to_tensor = ToTensor()
input = img_to_tensor(y).view(1, -1, y.size[1], y.size[0])

if opt.cuda:
	model = model.cuda()
	input = input.cuda()

out = model(input)
out = out.cpu()
out_img_y = out[0].detach().numpy()
out_img_y *= 255.0
out_img_y = out_img_y.clip(0, 255)
out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')

out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)
out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)
out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB')

out_img.save(opt.output_filename)
print('output image saved to ', opt.output_filename)

import torch
from torch.fx import Proxy, symbolic_trace
from torch.fx.node import map_arg




class M(torch.nn.Module):
	def __init__(self):
		super().__init__()
		self.relu = torch.nn.ReLU()

	def forward(self, x):
		return self.relu(x) + 1.0

m = symbolic_trace(M())

tracer = torch.fx.proxy.GraphAppendingTracer(m.graph)
for node in m.graph.nodes:
	if (node.op, node.target) == ("call_module", "relu"):
		with m.graph.inserting_before(node):
			proxy_args = map_arg(node.args, lambda n: Proxy(n, tracer))
			proxy_kwargs = map_arg(node.kwargs, lambda n: Proxy(n, tracer))
			proxy_output = m.relu(*proxy_args, **proxy_kwargs)
			node.replace_all_uses_with(proxy_output.node)
			m.graph.erase_node(node)

import torch
from torch.nn.parameter import Parameter

from allennlp.modules.attention.attention import Attention


@Attention.register("additive")
class AdditiveAttention(Attention):

	def __init__(self, vector_dim: int, matrix_dim: int, normalize: bool = True) -> None:
		super().__init__(normalize)
		self._w_matrix = Parameter(torch.Tensor(vector_dim, vector_dim))
		self._u_matrix = Parameter(torch.Tensor(matrix_dim, vector_dim))
		self._v_vector = Parameter(torch.Tensor(vector_dim, 1))
		self.reset_parameters()

	def reset_parameters(self):
		torch.nn.init.xavier_uniform_(self._w_matrix)
		torch.nn.init.xavier_uniform_(self._u_matrix)
		torch.nn.init.xavier_uniform_(self._v_vector)

	def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:
		intermediate = vector.matmul(self._w_matrix).unsqueeze(1) + matrix.matmul(self._u_matrix)
		intermediate = torch.tanh(intermediate)
		return intermediate.matmul(self._v_vector).squeeze(2)

from allennlp.confidence_checks.task_checklists.task_suite import TaskSuite
from allennlp.confidence_checks.task_checklists.sentiment_analysis_suite import (
	SentimentAnalysisSuite,
)
from allennlp.confidence_checks.task_checklists.question_answering_suite import (
	QuestionAnsweringSuite,
)
from allennlp.confidence_checks.task_checklists.textual_entailment_suite import (
	TextualEntailmentSuite,
)

import os
import torch
import torch.optim as optim
import torch.nn.functional as F


def train(rank, args, model, device, dataset, dataloader_kwargs):
	torch.manual_seed(args.seed + rank)

	train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)

	optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)
	for epoch in range(1, args.epochs + 1):
		train_epoch(epoch, args, model, device, train_loader, optimizer)


def test(args, model, device, dataset, dataloader_kwargs):
	torch.manual_seed(args.seed)

	test_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)

	test_epoch(model, device, test_loader)


def train_epoch(epoch, args, model, device, data_loader, optimizer):
	model.train()
	pid = os.getpid()
	for batch_idx, (data, target) in enumerate(data_loader):
		optimizer.zero_grad()
		output = model(data.to(device))
		loss = F.nll_loss(output, target.to(device))
		loss.backward()
		optimizer.step()
		if batch_idx % args.log_interval == 0:
			print('{}\tTrain Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
				pid, epoch, batch_idx * len(data), len(data_loader.dataset),
				100. * batch_idx / len(data_loader), loss.item()))
			if args.dry_run:
				break


def test_epoch(model, device, data_loader):
	model.eval()
	test_loss = 0
	correct = 0
	with torch.no_grad():
		for data, target in data_loader:
			output = model(data.to(device))
			test_loss += F.nll_loss(output, target.to(device), reduction='sum').item() # sum up batch loss
			pred = output.max(1)[1] # get the index of the max log-probability
			correct += pred.eq(target.to(device)).sum().item()

	test_loss /= len(data_loader.dataset)
	print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
		test_loss, correct, len(data_loader.dataset),
		100. * correct / len(data_loader.dataset)))

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR


class Net(nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.conv1 = nn.Conv2d(1, 32, 3, 1)
		self.conv2 = nn.Conv2d(32, 64, 3, 1)
		self.dropout1 = nn.Dropout(0.25)
		self.dropout2 = nn.Dropout(0.5)
		self.fc1 = nn.Linear(9216, 128)
		self.fc2 = nn.Linear(128, 10)

	def forward(self, x):
		x = self.conv1(x)
		x = F.relu(x)
		x = self.conv2(x)
		x = F.relu(x)
		x = F.max_pool2d(x, 2)
		x = self.dropout1(x)
		x = torch.flatten(x, 1)
		x = self.fc1(x)
		x = F.relu(x)
		x = self.dropout2(x)
		x = self.fc2(x)
		output = F.log_softmax(x, dim=1)
		return output


def train(args, model, device, train_loader, optimizer, epoch):
	model.train()
	for batch_idx, (data, target) in enumerate(train_loader):
		data, target = data.to(device), target.to(device)
		optimizer.zero_grad()
		output = model(data)
		loss = F.nll_loss(output, target)
		loss.backward()
		optimizer.step()
		if batch_idx % args.log_interval == 0:
			print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
				epoch, batch_idx * len(data), len(train_loader.dataset),
				100. * batch_idx / len(train_loader), loss.item()))
			if args.dry_run:
				break


def test(model, device, test_loader):
	model.eval()
	test_loss = 0
	correct = 0
	with torch.no_grad():
		for data, target in test_loader:
			data, target = data.to(device), target.to(device)
			output = model(data)
			test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
			pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
			correct += pred.eq(target.view_as(pred)).sum().item()

	test_loss /= len(test_loader.dataset)

	print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
		test_loss, correct, len(test_loader.dataset),
		100. * correct / len(test_loader.dataset)))


def main():
	parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
	parser.add_argument('--batch-size', type=int, default=64, metavar='N',
						help='input batch size for training (default: 64)')
	parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
						help='input batch size for testing (default: 1000)')
	parser.add_argument('--epochs', type=int, default=14, metavar='N',
						help='number of epochs to train (default: 14)')
	parser.add_argument('--lr', type=float, default=1.0, metavar='LR',
						help='learning rate (default: 1.0)')
	parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
						help='Learning rate step gamma (default: 0.7)')
	parser.add_argument('--no-cuda', action='store_true', default=False,
						help='disables CUDA training')
	parser.add_argument('--no-mps', action='store_true', default=False,
						help='disables macOS GPU training')
	parser.add_argument('--dry-run', action='store_true', default=False,
						help='quickly check a single pass')
	parser.add_argument('--seed', type=int, default=1, metavar='S',
						help='random seed (default: 1)')
	parser.add_argument('--log-interval', type=int, default=10, metavar='N',
						help='how many batches to wait before logging training status')
	parser.add_argument('--save-model', action='store_true', default=False,
						help='For Saving the current Model')
	args = parser.parse_args()
	use_cuda = not args.no_cuda and torch.cuda.is_available()
	use_mps = not args.no_mps and torch.backends.mps.is_available()

	torch.manual_seed(args.seed)

	if use_cuda:
		device = torch.device("cuda")
	elif use_mps:
		device = torch.device("mps")
	else:
		device = torch.device("cpu")

	train_kwargs = {'batch_size': args.batch_size}
	test_kwargs = {'batch_size': args.test_batch_size}
	if use_cuda:
		cuda_kwargs = {'num_workers': 1,
					   'pin_memory': True,
					   'shuffle': True}
		train_kwargs.update(cuda_kwargs)
		test_kwargs.update(cuda_kwargs)

	transform=transforms.Compose([
		transforms.ToTensor(),
		transforms.Normalize((0.1307,), (0.3081,))
		])
	dataset1 = datasets.MNIST('../data', train=True, download=True,
					   transform=transform)
	dataset2 = datasets.MNIST('../data', train=False,
					   transform=transform)
	train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
	test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)

	model = Net().to(device)
	optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

	scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
	for epoch in range(1, args.epochs + 1):
		train(args, model, device, train_loader, optimizer, epoch)
		test(model, device, test_loader)
		scheduler.step()

	if args.save_model:
		torch.save(model.state_dict(), "mnist_cnn.pt")


if __name__ == '__main__':
	main()




import os
import sys

import pytorch_sphinx_theme

current_dir = os.path.dirname(__file__)
target_dir = os.path.abspath(os.path.join(current_dir, "../.."))
sys.path.insert(0, target_dir)
print(target_dir)


project = "PyTorchExamples"
copyright = "2022, Meta"
author = "Meta"

release = "1.11"


extensions = ["sphinx.ext.napoleon", "sphinx.ext.autodoc", 'sphinx_panels']
panels_add_bootstrap_css = False

templates_path = ["_templates"]

exclude_patterns = []


html_theme = "pytorch_sphinx_theme"
html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]


html_static_path = ["_static"]
panels_add_fontawesome_latex = True

html_theme_options = {
	'pytorch_project': 'examples',
	'collapse_navigation': False,
	'display_version': True,
	'logo_only': False,
	'analytics_id': 'UA-117752657-2',
}

import argparse
import gymnasium as gym
import numpy as np
import os
from itertools import count

import torch
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributed.rpc import RRef, rpc_sync, rpc_async, remote
from torch.distributions import Categorical

TOTAL_EPISODE_STEP = 5000
AGENT_NAME = "agent"
OBSERVER_NAME = "observer{}"

parser = argparse.ArgumentParser(description='PyTorch RPC RL example')
parser.add_argument('--world-size', type=int, default=2, metavar='W',
					help='world size for RPC, rank 0 is the agent, others are observers')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
					help='discount factor (default: 0.99)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
					help='random seed (default: 543)')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='interval between training status logs (default: 10)')
args = parser.parse_args()

torch.manual_seed(args.seed)


def _call_method(method, rref, *args, **kwargs):
	return method(rref.local_value(), *args, **kwargs)


def _remote_method(method, rref, *args, **kwargs):
	args = [method, rref] + list(args)
	return rpc_sync(rref.owner(), _call_method, args=args, kwargs=kwargs)


class Policy(nn.Module):
	def __init__(self):
		super(Policy, self).__init__()
		self.affine1 = nn.Linear(4, 128)
		self.dropout = nn.Dropout(p=0.6)
		self.affine2 = nn.Linear(128, 2)

		self.saved_log_probs = []
		self.rewards = []

	def forward(self, x):
		x = self.affine1(x)
		x = self.dropout(x)
		x = F.relu(x)
		action_scores = self.affine2(x)
		return F.softmax(action_scores, dim=1)

class Observer:
	def __init__(self):
		self.id = rpc.get_worker_info().id
		self.env = gym.make('CartPole-v1')
		self.env.reset(seed=args.seed)

	def run_episode(self, agent_rref, n_steps):
		state, ep_reward = self.env.reset()[0], 0
		for step in range(n_steps):
			action = _remote_method(Agent.select_action, agent_rref, self.id, state)

			state, reward, terminated, truncated, _ = self.env.step(action)

			_remote_method(Agent.report_reward, agent_rref, self.id, reward)

			if terminated or truncated:
				break

class Agent:
	def __init__(self, world_size):
		self.ob_rrefs = []
		self.agent_rref = RRef(self)
		self.rewards = {}
		self.saved_log_probs = {}
		self.policy = Policy()
		self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)
		self.eps = np.finfo(np.float32).eps.item()
		self.running_reward = 0
		self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold
		for ob_rank in range(1, world_size):
			ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))
			self.ob_rrefs.append(remote(ob_info, Observer))
			self.rewards[ob_info.id] = []
			self.saved_log_probs[ob_info.id] = []

	def select_action(self, ob_id, state):
		state = torch.from_numpy(state).float().unsqueeze(0)
		probs = self.policy(state)
		m = Categorical(probs)
		action = m.sample()
		self.saved_log_probs[ob_id].append(m.log_prob(action))
		return action.item()

	def report_reward(self, ob_id, reward):
		self.rewards[ob_id].append(reward)

	def run_episode(self, n_steps=0):
		futs = []
		for ob_rref in self.ob_rrefs:
			futs.append(
				rpc_async(
					ob_rref.owner(),
					_call_method,
					args=(Observer.run_episode, ob_rref, self.agent_rref, n_steps)
				)
			)

		for fut in futs:
			fut.wait()

	def finish_episode(self):

		R, probs, rewards = 0, [], []
		for ob_id in self.rewards:
			probs.extend(self.saved_log_probs[ob_id])
			rewards.extend(self.rewards[ob_id])

		min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])
		self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward

		for ob_id in self.rewards:
			self.rewards[ob_id] = []
			self.saved_log_probs[ob_id] = []

		policy_loss, returns = [], []
		for r in rewards[::-1]:
			R = r + args.gamma * R
			returns.insert(0, R)
		returns = torch.tensor(returns)
		returns = (returns - returns.mean()) / (returns.std() + self.eps)
		for log_prob, R in zip(probs, returns):
			policy_loss.append(-log_prob * R)
		self.optimizer.zero_grad()
		policy_loss = torch.cat(policy_loss).sum()
		policy_loss.backward()
		self.optimizer.step()
		return min_reward


def run_worker(rank, world_size):
	os.environ['MASTER_ADDR'] = 'localhost'
	os.environ['MASTER_PORT'] = '29500'
	if rank == 0:
		rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)

		agent = Agent(world_size)
		for i_episode in count(1):
			n_steps = int(TOTAL_EPISODE_STEP / (args.world_size - 1))
			agent.run_episode(n_steps=n_steps)
			last_reward = agent.finish_episode()

			if i_episode % args.log_interval == 0:
				print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
					  i_episode, last_reward, agent.running_reward))

			if agent.running_reward > agent.reward_threshold:
				print("Solved! Running reward is now {}!".format(agent.running_reward))
				break
	else:
		rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)
	rpc.shutdown()


def main():
	mp.spawn(
		run_worker,
		args=(args.world_size, ),
		nprocs=args.world_size,
		join=True
	)

if __name__ == '__main__':
	main()

import torch.nn as nn
from .single import Attention


class MultiHeadedAttention(nn.Module):

	def __init__(self, h, d_model, dropout=0.1):
		super().__init__()
		assert d_model % h == 0

		self.d_k = d_model // h
		self.h = h

		self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])
		self.output_linear = nn.Linear(d_model, d_model)
		self.attention = Attention()

		self.dropout = nn.Dropout(p=dropout)

	def forward(self, query, key, value, mask=None):
		batch_size = query.size(0)

		query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
							 for l, x in zip(self.linear_layers, (query, key, value))]

		x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)

		x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)

		return self.output_linear(x)

from .loss import NLLLoss, Perplexity

from typing import List, Dict


import numpy

from allennlp.common.util import JsonDict
from allennlp.data import DatasetReader, Instance
from allennlp.data.fields import FlagField, TextField, SequenceLabelField
from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer
from allennlp.models import Model
from allennlp.predictors.predictor import Predictor


@Predictor.register("sentence_tagger")
class SentenceTaggerPredictor(Predictor):

	def __init__(
		self, model: Model, dataset_reader: DatasetReader, language: str = "en_core_web_sm"
	) -> None:
		super().__init__(model, dataset_reader)
		self._tokenizer = SpacyTokenizer(language=language)

	def predict(self, sentence: str) -> JsonDict:
		return self.predict_json({"sentence": sentence})

	def _json_to_instance(self, json_dict: JsonDict) -> Instance:
		sentence = json_dict["sentence"]
		tokens = self._tokenizer.tokenize(sentence)
		return self._dataset_reader.text_to_instance(tokens)

	def predictions_to_labeled_instances(
		self, instance: Instance, outputs: Dict[str, numpy.ndarray]
	) -> List[Instance]:
		predicted_tags = outputs["tags"]
		predicted_spans = []

		i = 0
		while i < len(predicted_tags):
			tag = predicted_tags[i]
			if tag[0] == "U":
				current_tags = [t if idx == i else "O" for idx, t in enumerate(predicted_tags)]
				predicted_spans.append(current_tags)
			elif tag[0] == "B":
				begin_idx = i
				while tag[0] != "L":
					i += 1
					tag = predicted_tags[i]
				end_idx = i
				current_tags = [
					t if begin_idx <= idx <= end_idx else "O"
					for idx, t in enumerate(predicted_tags)
				]
				predicted_spans.append(current_tags)
			i += 1

		instances = []
		for labels in predicted_spans:
			new_instance = instance.duplicate()
			text_field: TextField = instance["tokens"]  # type: ignore
			new_instance.add_field(
				"tags", SequenceLabelField(labels, text_field), self._model.vocab
			)
			new_instance.add_field("ignore_loss_on_o_tags", FlagField(True))
			instances.append(new_instance)

		return instances


import argparse
from typing import Callable, Dict, Optional, Type, TypeVar


from allennlp.common import Registrable


T = TypeVar("T", bound="Subcommand")


class Subcommand(Registrable):

	requires_plugins: bool = True

	_reverse_registry: Dict[Type, str] = {}

	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		raise NotImplementedError

	@classmethod
	def register(  # type: ignore
		cls: Type[T], name: str, constructor: Optional[str] = None, exist_ok: bool = False
	) -> Callable[[Type[T]], Type[T]]:
		super_register_fn = super().register(name, constructor=constructor, exist_ok=exist_ok)

		def add_name_to_reverse_registry(subclass: Type[T]) -> Type[T]:
			subclass = super_register_fn(subclass)
			cls._reverse_registry[subclass] = name
			return subclass

		return add_name_to_reverse_registry

	@property
	def name(self) -> str:
		return self._reverse_registry[self.__class__]

import torch
import torch.nn as nn
import torch.distributed.rpc as rpc
from torch.distributed.rpc import RRef


def _call_method(method, rref, *args, **kwargs):
	return method(rref.local_value(), *args, **kwargs)


def _remote_method(method, rref, *args, **kwargs):
	return rpc.rpc_sync(
		rref.owner(),
		_call_method,
		args=[method, rref] + list(args),
		kwargs=kwargs
	)


def _parameter_rrefs(module):
	param_rrefs = []
	for param in module.parameters():
		param_rrefs.append(RRef(param))
	return param_rrefs


class EmbeddingTable(nn.Module):
	def __init__(self, ntoken, ninp, dropout):
		super(EmbeddingTable, self).__init__()
		self.drop = nn.Dropout(dropout)
		self.encoder = nn.Embedding(ntoken, ninp)
		if torch.cuda.is_available():
			self.encoder = self.encoder.cuda()
		nn.init.uniform_(self.encoder.weight, -0.1, 0.1)

	def forward(self, input):
		if torch.cuda.is_available():
			input = input.cuda()
		return self.drop(self.encoder(input)).cpu()


class Decoder(nn.Module):
	def __init__(self, ntoken, nhid, dropout):
		super(Decoder, self).__init__()
		self.drop = nn.Dropout(dropout)
		self.decoder = nn.Linear(nhid, ntoken)
		nn.init.zeros_(self.decoder.bias)
		nn.init.uniform_(self.decoder.weight, -0.1, 0.1)

	def forward(self, output):
		return self.decoder(self.drop(output))


class RNNModel(nn.Module):
	def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):
		super(RNNModel, self).__init__()

		self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))
		self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)
		self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))

	def forward(self, input, hidden):
		emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)
		output, hidden = self.rnn(emb, hidden)
		decoded = _remote_method(Decoder.forward, self.decoder_rref, output)
		return decoded, hidden

	def parameter_rrefs(self):
		remote_params = []
		remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))
		remote_params.extend(_parameter_rrefs(self.rnn))
		remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))
		return remote_params

import torch

from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler


@LearningRateScheduler.register("noam")
class NoamLR(LearningRateScheduler):

	def __init__(
		self,
		optimizer: torch.optim.Optimizer,
		model_size: int,
		warmup_steps: int,
		factor: float = 1.0,
		last_epoch: int = -1,
	) -> None:
		self.warmup_steps = warmup_steps
		self.factor = factor
		self.model_size = model_size
		super().__init__(optimizer, last_epoch=last_epoch)

	def step(self, metric: float = None) -> None:
		pass

	def step_batch(self, batch_num_total: int = None) -> None:
		if batch_num_total is None:
			self.last_epoch += 1  # type: ignore
		else:
			self.last_epoch = batch_num_total
		for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_values()):
			param_group["lr"] = learning_rate

	def get_values(self):
		step = max(self.last_epoch, 1)
		scale = self.factor * (
			self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup_steps ** (-1.5))
		)

		return [scale for _ in range(len(self.base_values))]

from typing import Tuple, Union, Optional, Callable, Any
import torch
from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence

from allennlp.nn.util import get_lengths_from_binary_sequence_mask, sort_batch_by_length

RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
RnnStateStorage = Tuple[torch.Tensor, ...]


class _EncoderBase(torch.nn.Module):


	def __init__(self, stateful: bool = False) -> None:
		super().__init__()
		self.stateful = stateful
		self._states: Optional[RnnStateStorage] = None

	def sort_and_run_forward(
		self,
		module: Callable[
			[PackedSequence, Optional[RnnState]],
			Tuple[Union[PackedSequence, torch.Tensor], RnnState],
		],
		inputs: torch.Tensor,
		mask: torch.BoolTensor,
		hidden_state: Optional[RnnState] = None,
	):

		batch_size = mask.size(0)
		num_valid = torch.sum(mask[:, 0]).int().item()

		sequence_lengths = get_lengths_from_binary_sequence_mask(mask)
		(
			sorted_inputs,
			sorted_sequence_lengths,
			restoration_indices,
			sorting_indices,
		) = sort_batch_by_length(inputs, sequence_lengths)

		packed_sequence_input = pack_padded_sequence(
			sorted_inputs[:num_valid, :, :],
			sorted_sequence_lengths[:num_valid].data.tolist(),
			batch_first=True,
		)
		if not self.stateful:
			if hidden_state is None:
				initial_states: Any = hidden_state
			elif isinstance(hidden_state, tuple):
				initial_states = [
					state.index_select(1, sorting_indices)[:, :num_valid, :].contiguous()
					for state in hidden_state
				]
			else:
				initial_states = hidden_state.index_select(1, sorting_indices)[
					:, :num_valid, :
				].contiguous()

		else:
			initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)

		module_output, final_states = module(packed_sequence_input, initial_states)

		return module_output, final_states, restoration_indices

	def _get_initial_states(
		self, batch_size: int, num_valid: int, sorting_indices: torch.LongTensor
	) -> Optional[RnnState]:
		if self._states is None:
			return None

		if batch_size > self._states[0].size(1):
			num_states_to_concat = batch_size - self._states[0].size(1)
			resized_states = []
			for state in self._states:
				zeros = state.new_zeros(state.size(0), num_states_to_concat, state.size(2))
				resized_states.append(torch.cat([state, zeros], 1))
			self._states = tuple(resized_states)
			correctly_shaped_states = self._states

		elif batch_size < self._states[0].size(1):
			correctly_shaped_states = tuple(state[:, :batch_size, :] for state in self._states)
		else:
			correctly_shaped_states = self._states

		if len(self._states) == 1:
			correctly_shaped_state = correctly_shaped_states[0]
			sorted_state = correctly_shaped_state.index_select(1, sorting_indices)
			return sorted_state[:, :num_valid, :].contiguous()
		else:
			sorted_states = [
				state.index_select(1, sorting_indices) for state in correctly_shaped_states
			]
			return tuple(state[:, :num_valid, :].contiguous() for state in sorted_states)

	def _update_states(
		self, final_states: RnnStateStorage, restoration_indices: torch.LongTensor
	) -> None:
		new_unsorted_states = [state.index_select(1, restoration_indices) for state in final_states]

		if self._states is None:
			self._states = tuple(state.data for state in new_unsorted_states)
		else:
			current_state_batch_size = self._states[0].size(1)
			new_state_batch_size = final_states[0].size(1)
			used_new_rows_mask = [
				(state[0, :, :].sum(-1) != 0.0).float().view(1, new_state_batch_size, 1)
				for state in new_unsorted_states
			]
			new_states = []
			if current_state_batch_size > new_state_batch_size:
				for old_state, new_state, used_mask in zip(
					self._states, new_unsorted_states, used_new_rows_mask
				):
					masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)
					old_state[:, :new_state_batch_size, :] = new_state + masked_old_state
					new_states.append(old_state.detach())
			else:
				new_states = []
				for old_state, new_state, used_mask in zip(
					self._states, new_unsorted_states, used_new_rows_mask
				):
					masked_old_state = old_state * (1 - used_mask)
					new_state += masked_old_state
					new_states.append(new_state.detach())

			self._states = tuple(new_states)

	def reset_states(self, mask: torch.BoolTensor = None) -> None:
		if mask is None:
			self._states = None
		else:
			mask_batch_size = mask.size(0)
			mask = mask.view(1, mask_batch_size, 1)
			new_states = []
			assert self._states is not None
			for old_state in self._states:
				old_state_batch_size = old_state.size(1)
				if old_state_batch_size != mask_batch_size:
					raise ValueError(
						f"Trying to reset states using mask with incorrect batch size. "
						f"Expected batch size: {old_state_batch_size}. "
						f"Provided batch size: {mask_batch_size}."
					)
				new_state = ~mask * old_state
				new_states.append(new_state.detach())
			self._states = tuple(new_states)


from typing import Optional, Tuple

import torch
from allennlp.common.checks import ConfigurationError
from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence

from allennlp.nn.initializers import block_orthogonal
from allennlp.nn.util import get_dropout_mask


class AugmentedLSTMCell(torch.nn.Module):

	def __init__(
		self, embed_dim: int, lstm_dim: int, use_highway: bool = True, use_bias: bool = True
	):
		super().__init__()
		self.embed_dim = embed_dim
		self.lstm_dim = lstm_dim
		self.use_highway = use_highway
		self.use_bias = use_bias

		if use_highway:
			self._highway_inp_proj_start = 5 * self.lstm_dim
			self._highway_inp_proj_end = 6 * self.lstm_dim

			self.input_linearity = torch.nn.Linear(
				self.embed_dim, self._highway_inp_proj_end, bias=self.use_bias
			)
			self.state_linearity = torch.nn.Linear(
				self.lstm_dim, self._highway_inp_proj_start, bias=True
			)
		else:
			self.input_linearity = torch.nn.Linear(
				self.embed_dim, 4 * self.lstm_dim, bias=self.use_bias
			)
			self.state_linearity = torch.nn.Linear(self.lstm_dim, 4 * self.lstm_dim, bias=True)
		self.reset_parameters()

	def reset_parameters(self):
		block_orthogonal(self.input_linearity.weight.data, [self.lstm_dim, self.embed_dim])
		block_orthogonal(self.state_linearity.weight.data, [self.lstm_dim, self.lstm_dim])

		self.state_linearity.bias.data.fill_(0.0)
		self.state_linearity.bias.data[self.lstm_dim : 2 * self.lstm_dim].fill_(1.0)

	def forward(
		self,
		x: torch.Tensor,
		states=Tuple[torch.Tensor, torch.Tensor],
		variational_dropout_mask: Optional[torch.BoolTensor] = None,
	) -> Tuple[torch.Tensor, torch.Tensor]:
		hidden_state, memory_state = states

		if variational_dropout_mask is not None and self.training:
			hidden_state = hidden_state * variational_dropout_mask

		projected_input = self.input_linearity(x)
		projected_state = self.state_linearity(hidden_state)

		input_gate = forget_gate = memory_init = output_gate = highway_gate = None
		if self.use_highway:
			fused_op = projected_input[:, : 5 * self.lstm_dim] + projected_state
			fused_chunked = torch.chunk(fused_op, 5, 1)
			(input_gate, forget_gate, memory_init, output_gate, highway_gate) = fused_chunked
			highway_gate = torch.sigmoid(highway_gate)
		else:
			fused_op = projected_input + projected_state
			input_gate, forget_gate, memory_init, output_gate = torch.chunk(fused_op, 4, 1)
		input_gate = torch.sigmoid(input_gate)
		forget_gate = torch.sigmoid(forget_gate)
		memory_init = torch.tanh(memory_init)
		output_gate = torch.sigmoid(output_gate)
		memory = input_gate * memory_init + forget_gate * memory_state
		timestep_output: torch.Tensor = output_gate * torch.tanh(memory)

		if self.use_highway:
			highway_input_projection = projected_input[
				:, self._highway_inp_proj_start : self._highway_inp_proj_end
			]
			timestep_output = (
				highway_gate * timestep_output
				+ (1 - highway_gate) * highway_input_projection  # type: ignore
			)

		return timestep_output, memory


class AugmentedLstm(torch.nn.Module):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		go_forward: bool = True,
		recurrent_dropout_probability: float = 0.0,
		use_highway: bool = True,
		use_input_projection_bias: bool = True,
	):
		super().__init__()

		self.embed_dim = input_size
		self.lstm_dim = hidden_size

		self.go_forward = go_forward
		self.use_highway = use_highway
		self.recurrent_dropout_probability = recurrent_dropout_probability

		self.cell = AugmentedLSTMCell(
			self.embed_dim, self.lstm_dim, self.use_highway, use_input_projection_bias
		)

	def forward(
		self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None
	) -> Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]:
		if not isinstance(inputs, PackedSequence):
			raise ConfigurationError("inputs must be PackedSequence but got %s" % (type(inputs)))

		sequence_tensor, batch_lengths = pad_packed_sequence(inputs, batch_first=True)
		batch_size = sequence_tensor.size()[0]
		total_timesteps = sequence_tensor.size()[1]
		output_accumulator = sequence_tensor.new_zeros(batch_size, total_timesteps, self.lstm_dim)
		if states is None:
			full_batch_previous_memory = sequence_tensor.new_zeros(batch_size, self.lstm_dim)
			full_batch_previous_state = sequence_tensor.data.new_zeros(batch_size, self.lstm_dim)
		else:
			full_batch_previous_state = states[0].squeeze(0)
			full_batch_previous_memory = states[1].squeeze(0)
		current_length_index = batch_size - 1 if self.go_forward else 0
		if self.recurrent_dropout_probability > 0.0:
			dropout_mask = get_dropout_mask(
				self.recurrent_dropout_probability, full_batch_previous_memory
			)
		else:
			dropout_mask = None

		for timestep in range(total_timesteps):
			index = timestep if self.go_forward else total_timesteps - timestep - 1

			if self.go_forward:
				while batch_lengths[current_length_index] <= index:
					current_length_index -= 1
			else:
				while (
					current_length_index < (len(batch_lengths) - 1)
					and batch_lengths[current_length_index + 1] > index
				):
					current_length_index += 1

			previous_memory = full_batch_previous_memory[0 : current_length_index + 1].clone()
			previous_state = full_batch_previous_state[0 : current_length_index + 1].clone()
			timestep_input = sequence_tensor[0 : current_length_index + 1, index]
			timestep_output, memory = self.cell(
				timestep_input,
				(previous_state, previous_memory),
				dropout_mask[0 : current_length_index + 1] if dropout_mask is not None else None,
			)
			full_batch_previous_memory = full_batch_previous_memory.data.clone()
			full_batch_previous_state = full_batch_previous_state.data.clone()
			full_batch_previous_memory[0 : current_length_index + 1] = memory
			full_batch_previous_state[0 : current_length_index + 1] = timestep_output
			output_accumulator[0 : current_length_index + 1, index, :] = timestep_output

		output_accumulator = pack_padded_sequence(
			output_accumulator, batch_lengths, batch_first=True
		)

		final_state = (
			full_batch_previous_state.unsqueeze(0),
			full_batch_previous_memory.unsqueeze(0),
		)
		return output_accumulator, final_state


class BiAugmentedLstm(torch.nn.Module):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int = 1,
		bias: bool = True,
		recurrent_dropout_probability: float = 0.0,
		bidirectional: bool = False,
		padding_value: float = 0.0,
		use_highway: bool = True,
	) -> None:
		super().__init__()
		self.input_size = input_size
		self.padding_value = padding_value
		self.hidden_size = hidden_size
		self.num_layers = num_layers
		self.bidirectional = bidirectional
		self.recurrent_dropout_probability = recurrent_dropout_probability
		self.use_highway = use_highway
		self.use_bias = bias

		num_directions = int(self.bidirectional) + 1
		self.forward_layers = torch.nn.ModuleList()
		if self.bidirectional:
			self.backward_layers = torch.nn.ModuleList()

		lstm_embed_dim = self.input_size
		for _ in range(self.num_layers):
			self.forward_layers.append(
				AugmentedLstm(
					lstm_embed_dim,
					self.hidden_size,
					go_forward=True,
					recurrent_dropout_probability=self.recurrent_dropout_probability,
					use_highway=self.use_highway,
					use_input_projection_bias=self.use_bias,
				)
			)
			if self.bidirectional:
				self.backward_layers.append(
					AugmentedLstm(
						lstm_embed_dim,
						self.hidden_size,
						go_forward=False,
						recurrent_dropout_probability=self.recurrent_dropout_probability,
						use_highway=self.use_highway,
						use_input_projection_bias=self.use_bias,
					)
				)

			lstm_embed_dim = self.hidden_size * num_directions
		self.representation_dim = lstm_embed_dim

	def forward(
		self, inputs: torch.Tensor, states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None
	) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:

		if not isinstance(inputs, PackedSequence):
			raise ConfigurationError("inputs must be PackedSequence but got %s" % (type(inputs)))

		if self.bidirectional:
			return self._forward_bidirectional(inputs, states)
		return self._forward_unidirectional(inputs, states)

	def _forward_bidirectional(
		self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]
	):
		output_sequence = inputs
		final_h = []
		final_c = []

		if not states:
			hidden_states = [None] * self.num_layers
		elif states[0].size()[0] != self.num_layers:
			raise RuntimeError(
				"Initial states were passed to forward() but the number of "
				"initial states does not match the number of layers."
			)
		else:
			hidden_states = list(
				zip(  # type: ignore
					states[0].chunk(self.num_layers, 0), states[1].chunk(self.num_layers, 0)
				)
			)
		for i, state in enumerate(hidden_states):
			if state:
				forward_state = state[0].chunk(2, -1)
				backward_state = state[1].chunk(2, -1)
			else:
				forward_state = backward_state = None

			forward_layer = self.forward_layers[i]
			backward_layer = self.backward_layers[i]
			forward_output, final_forward_state = forward_layer(output_sequence, forward_state)
			backward_output, final_backward_state = backward_layer(output_sequence, backward_state)
			forward_output, lengths = pad_packed_sequence(forward_output, batch_first=True)
			backward_output, _ = pad_packed_sequence(backward_output, batch_first=True)
			output_sequence = torch.cat([forward_output, backward_output], -1)
			output_sequence = pack_padded_sequence(output_sequence, lengths, batch_first=True)

			final_h.extend([final_forward_state[0], final_backward_state[0]])
			final_c.extend([final_forward_state[1], final_backward_state[1]])

		final_h = torch.cat(final_h, dim=0)
		final_c = torch.cat(final_c, dim=0)
		final_state_tuple = (final_h, final_c)
		output_sequence, batch_lengths = pad_packed_sequence(
			output_sequence, padding_value=self.padding_value, batch_first=True
		)

		output_sequence = pack_padded_sequence(output_sequence, batch_lengths, batch_first=True)
		return output_sequence, final_state_tuple

	def _forward_unidirectional(
		self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]
	):
		output_sequence = inputs
		final_h = []
		final_c = []

		if not states:
			hidden_states = [None] * self.num_layers
		elif states[0].size()[0] != self.num_layers:
			raise RuntimeError(
				"Initial states were passed to forward() but the number of "
				"initial states does not match the number of layers."
			)
		else:
			hidden_states = list(
				zip(  # type: ignore
					states[0].chunk(self.num_layers, 0), states[1].chunk(self.num_layers, 0)
				)  # type: ignore
			)

		for i, state in enumerate(hidden_states):
			forward_layer = self.forward_layers[i]
			forward_output, final_forward_state = forward_layer(output_sequence, state)
			output_sequence = forward_output
			final_h.append(final_forward_state[0])
			final_c.append(final_forward_state[1])

		final_h = torch.cat(final_h, dim=0)
		final_c = torch.cat(final_c, dim=0)
		final_state_tuple = (final_h, final_c)
		output_sequence, batch_lengths = pad_packed_sequence(
			output_sequence, padding_value=self.padding_value, batch_first=True
		)

		output_sequence = pack_padded_sequence(output_sequence, batch_lengths, batch_first=True)

		return output_sequence, final_state_tuple

import argparse
import gzip
import os

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.data import Token, Vocabulary
from allennlp.data.token_indexers import ELMoTokenCharactersIndexer
from allennlp.data.vocabulary import DEFAULT_OOV_TOKEN
from allennlp.modules.elmo import _ElmoCharacterEncoder


def main(
	vocab_path: str,
	elmo_config_path: str,
	elmo_weights_path: str,
	output_dir: str,
	batch_size: int,
	device: int,
	use_custom_oov_token: bool = False,
):

	with open(vocab_path, "r") as vocab_file:
		tokens = vocab_file.read().strip().split("\n")

	if tokens[0] != DEFAULT_OOV_TOKEN and not use_custom_oov_token:
		raise ConfigurationError("ELMo embeddings require the use of a OOV token.")

	tokens = [tokens[0]] + ["<S>", "</S>"] + tokens[1:]

	indexer = ELMoTokenCharactersIndexer()
	indices = indexer.tokens_to_indices([Token(token) for token in tokens], Vocabulary())["tokens"]
	sentences = []
	for k in range((len(indices) // 50) + 1):
		sentences.append(
			indexer.as_padded_tensor_dict(
				{"elmo_tokens": indices[(k * 50) : ((k + 1) * 50)]}, padding_lengths={"tokens": 50}
			)
		)

	last_batch_remainder = 50 - (len(indices) % 50)
	if device != -1:
		elmo_token_embedder = _ElmoCharacterEncoder(elmo_config_path, elmo_weights_path).cuda(
			device
		)
	else:
		elmo_token_embedder = _ElmoCharacterEncoder(elmo_config_path, elmo_weights_path)

	all_embeddings = []
	for i in range((len(sentences) // batch_size) + 1):
		batch = torch.stack(sentences[i * batch_size : (i + 1) * batch_size])
		if device != -1:
			batch = batch.cuda(device)

		token_embedding = elmo_token_embedder(batch)["token_embedding"].data

		per_word_embeddings = (
			token_embedding[:, 1:-1, :].contiguous().view(-1, token_embedding.size(-1))
		)

		all_embeddings.append(per_word_embeddings)

	all_embeddings[-1] = all_embeddings[-1][:-last_batch_remainder, :]

	embedding_weight = torch.cat(all_embeddings, 0).cpu().numpy()

	os.makedirs(output_dir, exist_ok=True)
	with gzip.open(os.path.join(output_dir, "elmo_embeddings.txt.gz"), "wb") as embeddings_file:
		for i, word in enumerate(tokens):
			string_array = " ".join(str(x) for x in list(embedding_weight[i, :]))
			embeddings_file.write(f"{word} {string_array}\n".encode("utf-8"))

	_, vocab_file_name = os.path.split(vocab_path)
	with open(os.path.join(output_dir, vocab_file_name), "w") as new_vocab_file:
		for word in tokens:
			new_vocab_file.write(f"{word}\n")


if __name__ == "__main__":

	parser = argparse.ArgumentParser(
		description="Generate CNN representations for a vocabulary using ELMo",
		formatter_class=argparse.ArgumentDefaultsHelpFormatter,
	)
	parser.add_argument(
		"--vocab_path",
		type=str,
		help="A path to a vocabulary file to generate representations for.",
	)
	parser.add_argument(
		"--elmo_config", type=str, help="The path to a directory containing an ELMo config file."
	)
	parser.add_argument(
		"--elmo_weights", type=str, help="The path to a directory containing an ELMo weight file."
	)
	parser.add_argument(
		"--output_dir", type=str, help="The output directory to store the serialised embeddings."
	)
	parser.add_argument("--batch_size", type=int, default=64, help="The batch size to use.")
	parser.add_argument("--device", type=int, default=-1, help="The device to run on.")
	parser.add_argument(
		"--use_custom_oov_token",
		type=bool,
		default=False,
		help="AllenNLP requires a particular OOV token."
		"To generate embeddings with a custom OOV token,"
		"add this flag.",
	)

	args = parser.parse_args()
	main(
		args.vocab_path,
		args.elmo_config,
		args.elmo_weights,
		args.output_dir,
		args.batch_size,
		args.device,
		args.use_custom_oov_token,
	)

from allennlp.data.samplers.batch_sampler import BatchSampler
from allennlp.data.samplers.bucket_batch_sampler import BucketBatchSampler
from allennlp.data.samplers.max_tokens_batch_sampler import MaxTokensBatchSampler

from .dataset import BERTDataset
from .vocab import WordVocab

from typing import Dict, TYPE_CHECKING
import torch

from allennlp.training.callbacks.callback import TrainerCallback

if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


@TrainerCallback.register("mixed_precision_backward")
class MixedPrecisionBackwardCallback(TrainerCallback):

	def on_backward(
		self,
		trainer: "GradientDescentTrainer",
		batch_outputs: Dict[str, torch.Tensor],
		backward_called: bool,
		**kwargs
	) -> bool:
		if backward_called:
			raise OnBackwardException()
		trainer._scaler.scale(batch_outputs["loss"]).backward()  # type: ignore
		return True


class OnBackwardException(Exception):

	def __init__(self, message="") -> None:
		super().__init__(
			"Backpropagation has already been performed"
			"and the computation graph has been erased, so"
			"calling `loss.backward` is not permitted. " + message
		)

import warnings  # noqa

warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

try:
	import transformers, spacy, torch, numpy  # noqa

except ModuleNotFoundError:
	print(
		"Using AllenNLP requires the python packages Spacy, "
		"Pytorch and Numpy to be installed. Please see "
		"https://github.com/allenai/allennlp for installation instructions."
	)
	raise

from allennlp.version import VERSION as __version__  # noqa

from allennlp.data.dataset_readers.dataset_utils.span_utils import enumerate_spans
from allennlp.data.dataset_readers.dataset_utils.span_utils import bio_tags_to_spans
from allennlp.data.dataset_readers.dataset_utils.span_utils import to_bioul, iob1_to_bioul
from allennlp.data.dataset_readers.dataset_utils.span_utils import bioul_tags_to_spans

import torch

from allennlp.modules.span_extractors.span_extractor import SpanExtractor
from allennlp.modules.span_extractors.span_extractor_with_span_width_embedding import (
	SpanExtractorWithSpanWidthEmbedding,
)
from allennlp.modules.time_distributed import TimeDistributed
from allennlp.nn import util


@SpanExtractor.register("self_attentive")
class SelfAttentiveSpanExtractor(SpanExtractorWithSpanWidthEmbedding):

	def __init__(
		self,
		input_dim: int,
		num_width_embeddings: int = None,
		span_width_embedding_dim: int = None,
		bucket_widths: bool = False,
	) -> None:
		super().__init__(
			input_dim=input_dim,
			num_width_embeddings=num_width_embeddings,
			span_width_embedding_dim=span_width_embedding_dim,
			bucket_widths=bucket_widths,
		)
		self._global_attention = TimeDistributed(torch.nn.Linear(input_dim, 1))

	def get_output_dim(self) -> int:
		if self._span_width_embedding is not None:
			return self._input_dim + self._span_width_embedding.get_output_dim()
		return self._input_dim

	def _embed_spans(
		self,
		sequence_tensor: torch.FloatTensor,
		span_indices: torch.LongTensor,
		sequence_mask: torch.BoolTensor = None,
		span_indices_mask: torch.BoolTensor = None,
	) -> torch.FloatTensor:
		global_attention_logits = self._global_attention(sequence_tensor)

		concat_tensor = torch.cat([sequence_tensor, global_attention_logits], -1)

		concat_output, span_mask = util.batched_span_select(concat_tensor, span_indices)

		span_embeddings = concat_output[:, :, :, :-1]
		span_attention_logits = concat_output[:, :, :, -1]

		span_attention_weights = util.masked_softmax(span_attention_logits, span_mask)

		attended_text_embeddings = util.weighted_sum(span_embeddings, span_attention_weights)

		return attended_text_embeddings

from typing import Optional


import torch

from allennlp.nn.util import dist_reduce_sum
from allennlp.training.metrics.metric import Metric


@Metric.register("entropy")
class Entropy(Metric):
	def __init__(self) -> None:
		self._entropy = 0.0
		self._count = 0

	def __call__(  # type: ignore
		self,
		logits: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		logits, mask = self.detach_tensors(logits, mask)

		if mask is None:
			mask = torch.ones(logits.size()[:-1], device=logits.device).bool()

		log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
		probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)
		weighted_negative_likelihood = -log_probs * probabilities
		entropy = weighted_negative_likelihood.sum(-1)

		_entropy = entropy.sum() / mask.sum()

		self._entropy += dist_reduce_sum(_entropy).item()
		self._count += dist_reduce_sum(1)

	def get_metric(self, reset: bool = False):
		average_value = self._entropy / self._count if self._count > 0 else 0
		if reset:
			self.reset()
		return {"entropy": average_value}

	def reset(self):
		self._entropy = 0.0
		self._count = 0


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

import datasets
import evaluate
import torch
from datasets import DatasetDict, load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoFeatureExtractor,
	AutoModelForSpeechSeq2Seq,
	AutoProcessor,
	AutoTokenizer,
	HfArgumentParser,
	Seq2SeqTrainer,
	Seq2SeqTrainingArguments,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.18.0", "To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	feature_extractor_name: Optional[str] = field(
		default=None, metadata={"help": "feature extractor name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	freeze_feature_encoder: bool = field(
		default=True, metadata={"help": "Whether to freeze the feature encoder layers of the model."}
	)
	freeze_encoder: bool = field(
		default=False, metadata={"help": "Whether to freeze the entire encoder of the seq2seq model."}
	)
	forced_decoder_ids: List[List[int]] = field(
		default=None,
		metadata={
			"help": (
				"A list of pairs of integers which indicates a mapping from generation indices to token indices "
				"that will be forced before sampling. For example, [[0, 123]] means the first generated token "
				"will always be a token of index 123."
			)
		},
	)
	suppress_tokens: List[int] = field(
		default=None, metadata={"help": "A list of tokens that will be suppressed at generation."}
	)
	apply_spec_augment: bool = field(
		default=False,
		metadata={
			"help": "Whether to apply *SpecAugment* data augmentation to the input features. This is currently only relevant for Wav2Vec2, HuBERT, WavLM and Whisper models."
		},
	)


@dataclass
class DataTrainingArguments:

	dataset_name: str = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	audio_column_name: str = field(
		default="audio",
		metadata={"help": "The name of the dataset column containing the audio data. Defaults to 'audio'"},
	)
	text_column_name: str = field(
		default="text",
		metadata={"help": "The name of the dataset column containing the text data. Defaults to 'text'"},
	)
	max_duration_in_seconds: float = field(
		default=20.0,
		metadata={
			"help": (
				"Truncate audio files that are longer than `max_duration_in_seconds` seconds to"
				" 'max_duration_in_seconds`"
			)
		},
	)
	min_duration_in_seconds: float = field(
		default=0.0, metadata={"help": "Filter audio files that are shorter than `min_duration_in_seconds` seconds"}
	)
	preprocessing_only: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to only do data preprocessing and skip training. This is especially useful when data"
				" preprocessing errors out in distributed training due to timeout. In this case, one should run the"
				" preprocessing in a non-distributed setup with `preprocessing_only=True` so that the cached datasets"
				" can consequently be loaded in distributed training"
			)
		},
	)
	train_split_name: str = field(
		default="train",
		metadata={
			"help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
		},
	)
	eval_split_name: str = field(
		default="test",
		metadata={
			"help": "The name of the training data set split to use (via the datasets library). Defaults to 'train'"
		},
	)
	do_lower_case: bool = field(
		default=True,
		metadata={"help": "Whether the target text should be lower cased."},
	)
	language: str = field(
		default=None,
		metadata={
			"help": (
				"Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning "
				"only. For English speech recognition, it should be set to `None`."
			)
		},
	)
	task: str = field(
		default="transcribe",
		metadata={"help": "Task, either `transcribe` for speech recognition or `translate` for speech translation."},
	)


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:

	processor: Any
	decoder_start_token_id: int
	forward_attention_mask: bool

	def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
		model_input_name = self.processor.model_input_names[0]
		input_features = [{model_input_name: feature[model_input_name]} for feature in features]
		label_features = [{"input_ids": feature["labels"]} for feature in features]

		batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

		if self.forward_attention_mask:
			batch["attention_mask"] = torch.LongTensor([feature["attention_mask"] for feature in features])

		labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

		labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

		if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
			labels = labels[:, 1:]

		batch["labels"] = labels

		return batch


def main():
	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))

	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_speech_recognition_seq2seq", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)
	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	if is_main_process(training_args.local_rank):
		transformers.utils.logging.set_verbosity_info()
	logger.info("Training/evaluation parameters %s", training_args)

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	raw_datasets = DatasetDict()

	if training_args.do_train:
		raw_datasets["train"] = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			split=data_args.train_split_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	if training_args.do_eval:
		raw_datasets["eval"] = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			split=data_args.eval_split_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	if data_args.audio_column_name not in next(iter(raw_datasets.values())).column_names:
		raise ValueError(
			f"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. "
			"Make sure to set `--audio_column_name` to the correct audio column - one of "
			f"{', '.join(next(iter(raw_datasets.values())).column_names)}."
		)

	if data_args.text_column_name not in next(iter(raw_datasets.values())).column_names:
		raise ValueError(
			f"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. "
			"Make sure to set `--text_column_name` to the correct text column - one of "
			f"{', '.join(next(iter(raw_datasets.values())).column_names)}."
		)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	config.update({"forced_decoder_ids": model_args.forced_decoder_ids, "suppress_tokens": model_args.suppress_tokens})

	if getattr(config, "model_type", None) == "whisper":
		config.update({"apply_spec_augment": model_args.apply_spec_augment})

	feature_extractor = AutoFeatureExtractor.from_pretrained(
		model_args.feature_extractor_name if model_args.feature_extractor_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSpeechSeq2Seq.from_pretrained(
		model_args.model_name_or_path,
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	if model.config.decoder_start_token_id is None:
		raise ValueError("Make sure that `config.decoder_start_token_id` is correctly defined")

	if model_args.freeze_feature_encoder:
		model.freeze_feature_encoder()

	if model_args.freeze_encoder:
		model.freeze_encoder()
		model.model.encoder.gradient_checkpointing = False

	if data_args.language is not None:
		tokenizer.set_prefix_tokens(language=data_args.language, task=data_args.task)

	dataset_sampling_rate = next(iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate
	if dataset_sampling_rate != feature_extractor.sampling_rate:
		raw_datasets = raw_datasets.cast_column(
			data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)
		)

	max_input_length = data_args.max_duration_in_seconds * feature_extractor.sampling_rate
	min_input_length = data_args.min_duration_in_seconds * feature_extractor.sampling_rate
	audio_column_name = data_args.audio_column_name
	num_workers = data_args.preprocessing_num_workers
	text_column_name = data_args.text_column_name
	model_input_name = feature_extractor.model_input_names[0]
	do_lower_case = data_args.do_lower_case
	forward_attention_mask = (
		getattr(config, "model_type", None) == "whisper"
		and getattr(config, "apply_spec_augment", False)
		and getattr(config, "mask_time_prob", 0) > 0
	)

	if data_args.max_train_samples is not None:
		raw_datasets["train"] = raw_datasets["train"].select(range(data_args.max_train_samples))

	if data_args.max_eval_samples is not None:
		raw_datasets["eval"] = raw_datasets["eval"].select(range(data_args.max_eval_samples))

	def prepare_dataset(batch):
		sample = batch[audio_column_name]
		inputs = feature_extractor(
			sample["array"], sampling_rate=sample["sampling_rate"], return_attention_mask=forward_attention_mask
		)
		batch[model_input_name] = inputs.get(model_input_name)[0]
		batch["input_length"] = len(sample["array"])
		if forward_attention_mask:
			batch["attention_mask"] = inputs.get("attention_mask")[0]

		input_str = batch[text_column_name].lower() if do_lower_case else batch[text_column_name]
		batch["labels"] = tokenizer(input_str).input_ids
		return batch

	with training_args.main_process_first(desc="dataset map pre-processing"):
		vectorized_datasets = raw_datasets.map(
			prepare_dataset,
			remove_columns=next(iter(raw_datasets.values())).column_names,
			num_proc=data_args.preprocessing_num_workers,
			desc="preprocess train dataset",
		)

	def is_audio_in_length_range(length):
		return length > min_input_length and length < max_input_length

	vectorized_datasets = vectorized_datasets.filter(
		is_audio_in_length_range,
		num_proc=num_workers,
		input_columns=["input_length"],
	)

	if data_args.preprocessing_only:
		cache = {k: v.cache_files for k, v in vectorized_datasets.items()}
		logger.info(f"Data preprocessing finished. Files cached at {cache}.")
		return

	metric = evaluate.load("wer", cache_dir=model_args.cache_dir)

	def compute_metrics(pred):
		pred_ids = pred.predictions

		pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id

		pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
		label_str = tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)

		wer = metric.compute(predictions=pred_str, references=label_str)

		return {"wer": wer}

	with training_args.main_process_first():
		if is_main_process(training_args.local_rank):
			feature_extractor.save_pretrained(training_args.output_dir)
			tokenizer.save_pretrained(training_args.output_dir)
			config.save_pretrained(training_args.output_dir)

	processor = AutoProcessor.from_pretrained(training_args.output_dir)

	data_collator = DataCollatorSpeechSeq2SeqWithPadding(
		processor=processor,
		decoder_start_token_id=model.config.decoder_start_token_id,
		forward_attention_mask=forward_attention_mask,
	)

	trainer = Seq2SeqTrainer(
		model=model,
		args=training_args,
		train_dataset=vectorized_datasets["train"] if training_args.do_train else None,
		eval_dataset=vectorized_datasets["eval"] if training_args.do_eval else None,
		tokenizer=feature_extractor,
		data_collator=data_collator,
		compute_metrics=compute_metrics if training_args.predict_with_generate else None,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the feature extractor too for easy upload

		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples
			if data_args.max_train_samples is not None
			else len(vectorized_datasets["train"])
		)
		metrics["train_samples"] = min(max_train_samples, len(vectorized_datasets["train"]))
		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	results = {}
	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate(
			metric_key_prefix="eval",
			max_length=training_args.generation_max_length,
			num_beams=training_args.generation_num_beams,
		)
		max_eval_samples = (
			data_args.max_eval_samples if data_args.max_eval_samples is not None else len(vectorized_datasets["eval"])
		)
		metrics["eval_samples"] = min(max_eval_samples, len(vectorized_datasets["eval"]))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "automatic-speech-recognition"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)

	return results


if __name__ == "__main__":
	main()

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.multiprocessing as mp
from torch.utils.data.sampler import Sampler
from torchvision import datasets, transforms

from train import train, test

parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
parser.add_argument('--batch-size', type=int, default=64, metavar='N',
					help='input batch size for training (default: 64)')
parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
					help='input batch size for testing (default: 1000)')
parser.add_argument('--epochs', type=int, default=10, metavar='N',
					help='number of epochs to train (default: 10)')
parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
					help='learning rate (default: 0.01)')
parser.add_argument('--momentum', type=float, default=0.5, metavar='M',
					help='SGD momentum (default: 0.5)')
parser.add_argument('--seed', type=int, default=1, metavar='S',
					help='random seed (default: 1)')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='how many batches to wait before logging training status')
parser.add_argument('--num-processes', type=int, default=2, metavar='N',
					help='how many training processes to use (default: 2)')
parser.add_argument('--cuda', action='store_true', default=False,
					help='enables CUDA training')
parser.add_argument('--mps', action='store_true', default=False,
					help='enables macOS GPU training')
parser.add_argument('--save_model', action='store_true', default=False,
					help='save the trained model to state_dict')
parser.add_argument('--dry-run', action='store_true', default=False,
					help='quickly check a single pass')

class Net(nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
		self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
		self.conv2_drop = nn.Dropout2d()
		self.fc1 = nn.Linear(320, 50)
		self.fc2 = nn.Linear(50, 10)

	def forward(self, x):
		x = F.relu(F.max_pool2d(self.conv1(x), 2))
		x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
		x = x.view(-1, 320)
		x = F.relu(self.fc1(x))
		x = F.dropout(x, training=self.training)
		x = self.fc2(x)
		return F.log_softmax(x, dim=1)


if __name__ == '__main__':
	args = parser.parse_args()

	use_cuda = args.cuda and torch.cuda.is_available()
	use_mps = args.mps and torch.backends.mps.is_available()
	if use_cuda:
		device = torch.device("cuda")
	elif use_mps:
		device = torch.device("mps")
	else:
		device = torch.device("cpu")

	transform=transforms.Compose([
		transforms.ToTensor(),
		transforms.Normalize((0.1307,), (0.3081,))
		])
	dataset1 = datasets.MNIST('../data', train=True, download=True,
					   transform=transform)
	dataset2 = datasets.MNIST('../data', train=False,
					   transform=transform)
	kwargs = {'batch_size': args.batch_size,
			  'shuffle': True}
	if use_cuda:
		kwargs.update({'num_workers': 1,
					   'pin_memory': True,
					  })

	torch.manual_seed(args.seed)
	mp.set_start_method('spawn', force=True)

	model = Net().to(device)
	model.share_memory() # gradients are allocated lazily, so they are not shared here

	processes = []
	for rank in range(args.num_processes):
		p = mp.Process(target=train, args=(rank, args, model, device,
										   dataset1, kwargs))
		p.start()
		processes.append(p)
	for p in processes:
		p.join()

	if args.save_model:
		torch.save(model.state_dict(), "MNIST_hogwild.pt")

	test(args, model, device, dataset2, kwargs)

from typing import Optional


import torch
import torch.distributed as dist
from sklearn import metrics

from allennlp.common.util import is_distributed
from allennlp.common.checks import ConfigurationError
from allennlp.training.metrics.metric import Metric


@Metric.register("auc")
class Auc(Metric):

	def __init__(self, positive_label=1):
		super().__init__()
		self._positive_label = positive_label
		self._all_predictions = torch.FloatTensor()
		self._all_gold_labels = torch.LongTensor()

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):

		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		if gold_labels.dim() != 1:
			raise ConfigurationError(
				"gold_labels must be one-dimensional, "
				"but found tensor of shape: {}".format(gold_labels.size())
			)
		if predictions.dim() != 1:
			raise ConfigurationError(
				"predictions must be one-dimensional, "
				"but found tensor of shape: {}".format(predictions.size())
			)

		unique_gold_labels = torch.unique(gold_labels)
		if unique_gold_labels.numel() > 2:
			raise ConfigurationError(
				"AUC can be used for binary tasks only. gold_labels has {} unique labels, "
				"expected at maximum 2.".format(unique_gold_labels.numel())
			)

		gold_labels_is_binary = set(unique_gold_labels.tolist()) <= {0, 1}
		if not gold_labels_is_binary and self._positive_label not in unique_gold_labels:
			raise ConfigurationError(
				"gold_labels should be binary with 0 and 1 or initialized positive_label "
				"{} should be present in gold_labels".format(self._positive_label)
			)

		if mask is None:
			batch_size = gold_labels.shape[0]
			mask = torch.ones(batch_size, device=gold_labels.device).bool()

		self._all_predictions = self._all_predictions.to(predictions.device)
		self._all_gold_labels = self._all_gold_labels.to(gold_labels.device)

		self._all_predictions = torch.cat(
			[self._all_predictions, torch.masked_select(predictions, mask).float()], dim=0
		)
		self._all_gold_labels = torch.cat(
			[self._all_gold_labels, torch.masked_select(gold_labels, mask).long()], dim=0
		)

		if is_distributed():
			world_size = dist.get_world_size()
			device = gold_labels.device

			_all_batch_lengths = [torch.tensor(0) for i in range(world_size)]
			dist.all_gather(
				_all_batch_lengths, torch.tensor(len(self._all_predictions), device=device)
			)
			_all_batch_lengths = [batch_length.item() for batch_length in _all_batch_lengths]

			if len(set(_all_batch_lengths)) > 1:
				raise RuntimeError(
					"Distributed aggregation for AUC is currently not supported for batches of unequal length."
				)

			_all_predictions = [
				torch.zeros(self._all_predictions.shape, device=device) for i in range(world_size)
			]

			_all_gold_labels = [
				torch.zeros(self._all_gold_labels.shape, device=device, dtype=torch.long)
				for i in range(world_size)
			]
			dist.all_gather(_all_predictions, self._all_predictions)
			dist.all_gather(_all_gold_labels, self._all_gold_labels)
			self._all_predictions = torch.cat(_all_predictions, dim=0)
			self._all_gold_labels = torch.cat(_all_gold_labels, dim=0)

	def get_metric(self, reset: bool = False):

		if self._all_gold_labels.shape[0] == 0:
			return 0.5
		false_positive_rates, true_positive_rates, _ = metrics.roc_curve(
			self._all_gold_labels.cpu().numpy(),
			self._all_predictions.cpu().numpy(),
			pos_label=self._positive_label,
		)
		auc = metrics.auc(false_positive_rates, true_positive_rates)

		if reset:
			self.reset()
		return auc

	def reset(self):
		self._all_predictions = torch.FloatTensor()
		self._all_gold_labels = torch.LongTensor()



import argparse
import inspect
import logging
from typing import Tuple

import torch
from accelerate import PartialState
from accelerate.utils import set_seed

from transformers import (
	AutoTokenizer,
	BloomForCausalLM,
	BloomTokenizerFast,
	CTRLLMHeadModel,
	CTRLTokenizer,
	GenerationMixin,
	GPT2LMHeadModel,
	GPT2Tokenizer,
	GPTJForCausalLM,
	LlamaForCausalLM,
	LlamaTokenizer,
	OpenAIGPTLMHeadModel,
	OpenAIGPTTokenizer,
	OPTForCausalLM,
	TransfoXLLMHeadModel,
	TransfoXLTokenizer,
	XLMTokenizer,
	XLMWithLMHeadModel,
	XLNetLMHeadModel,
	XLNetTokenizer,
)
from transformers.modeling_outputs import CausalLMOutputWithPast


logging.basicConfig(
	format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
	datefmt="%m/%d/%Y %H:%M:%S",
	level=logging.INFO,
)
logger = logging.getLogger(__name__)

MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop

MODEL_CLASSES = {
	"gpt2": (GPT2LMHeadModel, GPT2Tokenizer),
	"ctrl": (CTRLLMHeadModel, CTRLTokenizer),
	"openai-gpt": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),
	"xlnet": (XLNetLMHeadModel, XLNetTokenizer),
	"transfo-xl": (TransfoXLLMHeadModel, TransfoXLTokenizer),
	"xlm": (XLMWithLMHeadModel, XLMTokenizer),
	"gptj": (GPTJForCausalLM, AutoTokenizer),
	"bloom": (BloomForCausalLM, BloomTokenizerFast),
	"llama": (LlamaForCausalLM, LlamaTokenizer),
	"opt": (OPTForCausalLM, GPT2Tokenizer),
}





def prepare_ctrl_input(args, _, tokenizer, prompt_text):
	if args.temperature > 0.7:
		logger.info("CTRL typically works better with lower temperatures (and lower top_k).")

	encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)
	if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):
		logger.info("WARNING! You are not starting your generation from a control code so you won't get good results")
	return prompt_text


def prepare_xlm_input(args, model, tokenizer, prompt_text):

	use_lang_emb = hasattr(model.config, "use_lang_emb") and model.config.use_lang_emb
	if hasattr(model.config, "lang2id") and use_lang_emb:
		available_languages = model.config.lang2id.keys()
		if args.xlm_language in available_languages:
			language = args.xlm_language
		else:
			language = None
			while language not in available_languages:
				language = input("Using XLM. Select language in " + str(list(available_languages)) + " >>> ")

		model.config.lang_id = model.config.lang2id[language]


	return prompt_text


def prepare_xlnet_input(args, _, tokenizer, prompt_text):
	prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX
	prompt_text = prefix + prompt_text
	return prompt_text


def prepare_transfoxl_input(args, _, tokenizer, prompt_text):
	prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX
	prompt_text = prefix + prompt_text
	return prompt_text


PREPROCESSING_FUNCTIONS = {
	"ctrl": prepare_ctrl_input,
	"xlm": prepare_xlm_input,
	"xlnet": prepare_xlnet_input,
	"transfo-xl": prepare_transfoxl_input,
}


def adjust_length_to_model(length, max_sequence_length):
	if length < 0 and max_sequence_length > 0:
		length = max_sequence_length
	elif 0 < max_sequence_length < length:
		length = max_sequence_length  # No generation bigger than model size
	elif length < 0:
		length = MAX_LENGTH  # avoid infinite loop
	return length


def sparse_model_config(model_config):
	embedding_size = None
	if hasattr(model_config, "hidden_size"):
		embedding_size = model_config.hidden_size
	elif hasattr(model_config, "n_embed"):
		embedding_size = model_config.n_embed
	elif hasattr(model_config, "n_embd"):
		embedding_size = model_config.n_embd

	num_head = None
	if hasattr(model_config, "num_attention_heads"):
		num_head = model_config.num_attention_heads
	elif hasattr(model_config, "n_head"):
		num_head = model_config.n_head

	if embedding_size is None or num_head is None or num_head == 0:
		raise ValueError("Check the model config")

	num_embedding_size_per_head = int(embedding_size / num_head)
	if hasattr(model_config, "n_layer"):
		num_layer = model_config.n_layer
	elif hasattr(model_config, "num_hidden_layers"):
		num_layer = model_config.num_hidden_layers
	else:
		raise ValueError("Number of hidden layers couldn't be determined from the model config")

	return num_layer, num_head, num_embedding_size_per_head


def generate_past_key_values(model, batch_size, seq_len):
	num_block_layers, num_attention_heads, num_embedding_size_per_head = sparse_model_config(model.config)
	if model.config.model_type == "bloom":
		past_key_values = tuple(
			(
				torch.empty(int(num_attention_heads * batch_size), num_embedding_size_per_head, seq_len)
				.to(model.dtype)
				.to(model.device),
				torch.empty(int(num_attention_heads * batch_size), seq_len, num_embedding_size_per_head)
				.to(model.dtype)
				.to(model.device),
			)
			for _ in range(num_block_layers)
		)
	else:
		past_key_values = tuple(
			(
				torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)
				.to(model.dtype)
				.to(model.device),
				torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)
				.to(model.dtype)
				.to(model.device),
			)
			for _ in range(num_block_layers)
		)
	return past_key_values


def prepare_jit_inputs(inputs, model, tokenizer):
	batch_size = len(inputs)
	dummy_input = tokenizer.batch_encode_plus(inputs, return_tensors="pt")
	dummy_input = dummy_input.to(model.device)
	if model.config.use_cache:
		dummy_input["past_key_values"] = generate_past_key_values(model, batch_size, 1)
	dummy_input["attention_mask"] = torch.cat(
		[
			torch.zeros(dummy_input["attention_mask"].shape[0], 1)
			.to(dummy_input["attention_mask"].dtype)
			.to(model.device),
			dummy_input["attention_mask"],
		],
		-1,
	)
	return dummy_input


class _ModelFallbackWrapper(GenerationMixin):
	__slots__ = ("_optimized", "_default")

	def __init__(self, optimized, default):
		self._optimized = optimized
		self._default = default

	def __call__(self, *args, **kwargs):
		if kwargs["past_key_values"] is None and self._default.config.use_cache:
			kwargs["past_key_values"] = generate_past_key_values(self._default, kwargs["input_ids"].shape[0], 0)
		kwargs.pop("position_ids", None)
		for k in list(kwargs.keys()):
			if kwargs[k] is None or isinstance(kwargs[k], bool):
				kwargs.pop(k)
		outputs = self._optimized(**kwargs)
		lm_logits = outputs[0]
		past_key_values = outputs[1]
		fixed_output = CausalLMOutputWithPast(
			loss=None,
			logits=lm_logits,
			past_key_values=past_key_values,
			hidden_states=None,
			attentions=None,
		)
		return fixed_output

	def __getattr__(self, item):
		return getattr(self._default, item)

	def prepare_inputs_for_generation(
		self, input_ids, past_key_values=None, inputs_embeds=None, use_cache=None, **kwargs
	):
		return self._default.prepare_inputs_for_generation(
			input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, **kwargs
		)

	def _reorder_cache(
		self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor
	) -> Tuple[Tuple[torch.Tensor]]:
		return self._default._reorder_cache(past_key_values, beam_idx)


def main():
	parser = argparse.ArgumentParser()
	parser.add_argument(
		"--model_type",
		default=None,
		type=str,
		required=True,
		help="Model type selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
	)
	parser.add_argument(
		"--model_name_or_path",
		default=None,
		type=str,
		required=True,
		help="Path to pre-trained model or shortcut name selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
	)

	parser.add_argument("--prompt", type=str, default="")
	parser.add_argument("--length", type=int, default=20)
	parser.add_argument("--stop_token", type=str, default=None, help="Token at which text generation is stopped")

	parser.add_argument(
		"--temperature",
		type=float,
		default=1.0,
		help="temperature of 1.0 has no effect, lower tend toward greedy sampling",
	)
	parser.add_argument(
		"--repetition_penalty", type=float, default=1.0, help="primarily useful for CTRL model; in that case, use 1.2"
	)
	parser.add_argument("--k", type=int, default=0)
	parser.add_argument("--p", type=float, default=0.9)

	parser.add_argument("--prefix", type=str, default="", help="Text added prior to input.")
	parser.add_argument("--padding_text", type=str, default="", help="Deprecated, the use of `--prefix` is preferred.")
	parser.add_argument("--xlm_language", type=str, default="", help="Optional language when used with the XLM model.")

	parser.add_argument("--seed", type=int, default=42, help="random seed for initialization")
	parser.add_argument(
		"--use_cpu",
		action="store_true",
		help="Whether or not to use cpu. If set to False, " "we will use gpu/npu or mps device if available",
	)
	parser.add_argument("--num_return_sequences", type=int, default=1, help="The number of samples to generate.")
	parser.add_argument(
		"--fp16",
		action="store_true",
		help="Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit",
	)
	parser.add_argument("--jit", action="store_true", help="Whether or not to use jit trace to accelerate inference")
	args = parser.parse_args()

	distributed_state = PartialState(cpu=args.use_cpu)

	logger.warning(f"device: {distributed_state.device}, 16-bits inference: {args.fp16}")

	if args.seed is not None:
		set_seed(args.seed)

	try:
		args.model_type = args.model_type.lower()
		model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
	except KeyError:
		raise KeyError("the model {} you specified is not supported. You are welcome to add it and open a PR :)")

	tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)
	if tokenizer.pad_token is None:
		tokenizer.pad_token = tokenizer.eos_token
	model = model_class.from_pretrained(args.model_name_or_path)

	model.to(distributed_state.device)

	if args.fp16:
		model.half()
	max_seq_length = getattr(model.config, "max_position_embeddings", 0)
	args.length = adjust_length_to_model(args.length, max_sequence_length=max_seq_length)
	logger.info(args)

	prompt_text = args.prompt if args.prompt else input("Model prompt >>> ")

	requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()
	if requires_preprocessing:
		prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)
		preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)

		if model.__class__.__name__ in ["TransfoXLLMHeadModel"]:
			tokenizer_kwargs = {"add_space_before_punct_symbol": True}
		else:
			tokenizer_kwargs = {}

		encoded_prompt = tokenizer.encode(
			preprocessed_prompt_text, add_special_tokens=False, return_tensors="pt", **tokenizer_kwargs
		)
	else:
		prefix = args.prefix if args.prefix else args.padding_text
		encoded_prompt = tokenizer.encode(prefix + prompt_text, add_special_tokens=False, return_tensors="pt")
	encoded_prompt = encoded_prompt.to(distributed_state.device)

	if encoded_prompt.size()[-1] == 0:
		input_ids = None
	else:
		input_ids = encoded_prompt

	if args.jit:
		jit_input_texts = ["enable jit"]
		jit_inputs = prepare_jit_inputs(jit_input_texts, model, tokenizer)
		torch._C._jit_set_texpr_fuser_enabled(False)
		model.config.return_dict = False
		if hasattr(model, "forward"):
			sig = inspect.signature(model.forward)
		else:
			sig = inspect.signature(model.__call__)
		jit_inputs = tuple(jit_inputs[key] for key in sig.parameters if jit_inputs.get(key, None) is not None)
		traced_model = torch.jit.trace(model, jit_inputs, strict=False)
		traced_model = torch.jit.freeze(traced_model.eval())
		traced_model(*jit_inputs)
		traced_model(*jit_inputs)

		model = _ModelFallbackWrapper(traced_model, model)

	output_sequences = model.generate(
		input_ids=input_ids,
		max_length=args.length + len(encoded_prompt[0]),
		temperature=args.temperature,
		top_k=args.k,
		top_p=args.p,
		repetition_penalty=args.repetition_penalty,
		do_sample=True,
		num_return_sequences=args.num_return_sequences,
	)

	if len(output_sequences.shape) > 2:
		output_sequences.squeeze_()

	generated_sequences = []

	for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
		print(f"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===")
		generated_sequence = generated_sequence.tolist()

		text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)

		text = text[: text.find(args.stop_token) if args.stop_token else None]

		total_sequence = (
			prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]
		)

		generated_sequences.append(total_sequence)
		print(total_sequence)

	return generated_sequences


if __name__ == "__main__":
	main()

import copy
import inspect
from typing import Callable, Generic, TypeVar, Type, Union, Optional, Dict, Any

from allennlp.common.params import Params


T = TypeVar("T")


class Lazy(Generic[T]):

	def __init__(
		self,
		constructor: Union[Type[T], Callable[..., T]],
		params: Optional[Params] = None,
		constructor_extras: Optional[Dict[str, Any]] = None,
		**kwargs,
	) -> None:
		self._constructor = constructor
		self._params = params or Params({})
		self._constructor_extras = constructor_extras or {}
		self._constructor_extras.update(kwargs)

	@property
	def constructor(self) -> Callable[..., T]:
		if inspect.isclass(self._constructor):

			def constructor_to_use(**kwargs):
				return self._constructor.from_params(  # type: ignore[union-attr]
					copy.deepcopy(self._params),
					**kwargs,
				)

			return constructor_to_use
		else:
			return self._constructor

	def construct(self, **kwargs) -> T:
		contructor_kwargs = {**self._constructor_extras, **kwargs}
		return self.constructor(**contructor_kwargs)


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
import numpy as np
from datasets import ClassLabel, load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoModelForTokenClassification,
	AutoTokenizer,
	DataCollatorForTokenClassification,
	HfArgumentParser,
	PretrainedConfig,
	PreTrainedTokenizerFast,
	Trainer,
	TrainingArguments,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/token-classification/requirements.txt")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	ignore_mismatched_sizes: bool = field(
		default=False,
		metadata={"help": "Will enable to load a pretrained model whose head dimensions are different."},
	)


@dataclass
class DataTrainingArguments:

	task_name: Optional[str] = field(default="ner", metadata={"help": "The name of the task (ner, pos...)."})
	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(
		default=None, metadata={"help": "The input training data file (a csv or JSON file)."}
	)
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate on (a csv or JSON file)."},
	)
	test_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input test data file to predict on (a csv or JSON file)."},
	)
	text_column_name: Optional[str] = field(
		default=None, metadata={"help": "The column name of text to input in the file (a csv or JSON file)."}
	)
	label_column_name: Optional[str] = field(
		default=None, metadata={"help": "The column name of label to input in the file (a csv or JSON file)."}
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_seq_length: int = field(
		default=None,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. If set, sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	pad_to_max_length: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to pad all samples to model maximum sentence length. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
				"efficient on GPU but very bad for TPU."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	label_all_tokens: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to put the label for one word on all tokens of generated by that word or just on the "
				"one (in which case the other tokens will have a padding index)."
			)
		},
	)
	return_entity_level_metrics: bool = field(
		default=False,
		metadata={"help": "Whether to return all the entity levels during evaluation or just the overall ones."},
	)

	def __post_init__(self):
		if self.dataset_name is None and self.train_file is None and self.validation_file is None:
			raise ValueError("Need either a dataset name or a training/validation file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
		self.task_name = self.task_name.lower()


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_ner", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
		if data_args.test_file is not None:
			data_files["test"] = data_args.test_file
		extension = data_args.train_file.split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)

	if training_args.do_train:
		column_names = raw_datasets["train"].column_names
		features = raw_datasets["train"].features
	else:
		column_names = raw_datasets["validation"].column_names
		features = raw_datasets["validation"].features

	if data_args.text_column_name is not None:
		text_column_name = data_args.text_column_name
	elif "tokens" in column_names:
		text_column_name = "tokens"
	else:
		text_column_name = column_names[0]

	if data_args.label_column_name is not None:
		label_column_name = data_args.label_column_name
	elif f"{data_args.task_name}_tags" in column_names:
		label_column_name = f"{data_args.task_name}_tags"
	else:
		label_column_name = column_names[1]

	def get_label_list(labels):
		unique_labels = set()
		for label in labels:
			unique_labels = unique_labels | set(label)
		label_list = list(unique_labels)
		label_list.sort()
		return label_list

	labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)
	if labels_are_int:
		label_list = features[label_column_name].feature.names
		label_to_id = {i: i for i in range(len(label_list))}
	else:
		label_list = get_label_list(raw_datasets["train"][label_column_name])
		label_to_id = {l: i for i, l in enumerate(label_list)}

	num_labels = len(label_list)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		num_labels=num_labels,
		finetuning_task=data_args.task_name,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path
	if config.model_type in {"bloom", "gpt2", "roberta"}:
		tokenizer = AutoTokenizer.from_pretrained(
			tokenizer_name_or_path,
			cache_dir=model_args.cache_dir,
			use_fast=True,
			revision=model_args.model_revision,
			token=model_args.token,
			trust_remote_code=model_args.trust_remote_code,
			add_prefix_space=True,
		)
	else:
		tokenizer = AutoTokenizer.from_pretrained(
			tokenizer_name_or_path,
			cache_dir=model_args.cache_dir,
			use_fast=True,
			revision=model_args.model_revision,
			token=model_args.token,
			trust_remote_code=model_args.trust_remote_code,
		)

	model = AutoModelForTokenClassification.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
		ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
	)

	if not isinstance(tokenizer, PreTrainedTokenizerFast):
		raise ValueError(
			"This example script only works for models that have a fast tokenizer. Checkout the big table of models at"
			" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet"
			" this requirement"
		)

	if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:
		if sorted(model.config.label2id.keys()) == sorted(label_list):
			if labels_are_int:
				label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}
				label_list = [model.config.id2label[i] for i in range(num_labels)]
			else:
				label_list = [model.config.id2label[i] for i in range(num_labels)]
				label_to_id = {l: i for i, l in enumerate(label_list)}
		else:
			logger.warning(
				"Your model seems to have been trained with labels, but they don't match the dataset: ",
				f"model labels: {sorted(model.config.label2id.keys())}, dataset labels:"
				f" {sorted(label_list)}.\nIgnoring the model labels as a result.",
			)

	model.config.label2id = {l: i for i, l in enumerate(label_list)}
	model.config.id2label = dict(enumerate(label_list))

	b_to_i_label = []
	for idx, label in enumerate(label_list):
		if label.startswith("B-") and label.replace("B-", "I-") in label_list:
			b_to_i_label.append(label_list.index(label.replace("B-", "I-")))
		else:
			b_to_i_label.append(idx)

	padding = "max_length" if data_args.pad_to_max_length else False

	def tokenize_and_align_labels(examples):
		tokenized_inputs = tokenizer(
			examples[text_column_name],
			padding=padding,
			truncation=True,
			max_length=data_args.max_seq_length,
			is_split_into_words=True,
		)
		labels = []
		for i, label in enumerate(examples[label_column_name]):
			word_ids = tokenized_inputs.word_ids(batch_index=i)
			previous_word_idx = None
			label_ids = []
			for word_idx in word_ids:
				if word_idx is None:
					label_ids.append(-100)
				elif word_idx != previous_word_idx:
					label_ids.append(label_to_id[label[word_idx]])
				else:
					if data_args.label_all_tokens:
						label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])
					else:
						label_ids.append(-100)
				previous_word_idx = word_idx

			labels.append(label_ids)
		tokenized_inputs["labels"] = labels
		return tokenized_inputs

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				tokenize_and_align_labels,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on train dataset",
			)

	if training_args.do_eval:
		if "validation" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_dataset = raw_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_dataset.map(
				tokenize_and_align_labels,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on validation dataset",
			)

	if training_args.do_predict:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_dataset = raw_datasets["test"]
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))
		with training_args.main_process_first(desc="prediction dataset map pre-processing"):
			predict_dataset = predict_dataset.map(
				tokenize_and_align_labels,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)

	data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)

	metric = evaluate.load("seqeval", cache_dir=model_args.cache_dir)

	def compute_metrics(p):
		predictions, labels = p
		predictions = np.argmax(predictions, axis=2)

		true_predictions = [
			[label_list[p] for (p, l) in zip(prediction, label) if l != -100]
			for prediction, label in zip(predictions, labels)
		]
		true_labels = [
			[label_list[l] for (p, l) in zip(prediction, label) if l != -100]
			for prediction, label in zip(predictions, labels)
		]

		results = metric.compute(predictions=true_predictions, references=true_labels)
		if data_args.return_entity_level_metrics:
			final_results = {}
			for key, value in results.items():
				if isinstance(value, dict):
					for n, v in value.items():
						final_results[f"{key}_{n}"] = v
				else:
					final_results[key] = value
			return final_results
		else:
			return {
				"precision": results["overall_precision"],
				"recall": results["overall_recall"],
				"f1": results["overall_f1"],
				"accuracy": results["overall_accuracy"],
			}

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		compute_metrics=compute_metrics,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		metrics = train_result.metrics
		trainer.save_model()  # Saves the tokenizer too for easy upload

		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")

		metrics = trainer.evaluate()

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")

		predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix="predict")
		predictions = np.argmax(predictions, axis=2)

		true_predictions = [
			[label_list[p] for (p, l) in zip(prediction, label) if l != -100]
			for prediction, label in zip(predictions, labels)
		]

		trainer.log_metrics("predict", metrics)
		trainer.save_metrics("predict", metrics)

		output_predictions_file = os.path.join(training_args.output_dir, "predictions.txt")
		if trainer.is_world_process_zero():
			with open(output_predictions_file, "w") as writer:
				for prediction in true_predictions:
					writer.write(" ".join(prediction) + "\n")

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "token-classification"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from __future__ import print_function
import os
import time
import shutil

import torch
import dill

class Checkpoint(object):

	CHECKPOINT_DIR_NAME = 'checkpoints'
	TRAINER_STATE_NAME = 'trainer_states.pt'
	MODEL_NAME = 'model.pt'
	INPUT_VOCAB_FILE = 'input_vocab.pt'
	OUTPUT_VOCAB_FILE = 'output_vocab.pt'

	def __init__(self, model, optimizer, epoch, step, input_vocab, output_vocab, path=None):
		self.model = model
		self.optimizer = optimizer
		self.input_vocab = input_vocab
		self.output_vocab = output_vocab
		self.epoch = epoch
		self.step = step
		self._path = path

	@property
	def path(self):
		if self._path is None:
			raise LookupError("The checkpoint has not been saved.")
		return self._path

	def save(self, experiment_dir):
		date_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime())

		self._path = os.path.join(experiment_dir, self.CHECKPOINT_DIR_NAME, date_time)
		path = self._path

		if os.path.exists(path):
			shutil.rmtree(path)
		os.makedirs(path)
		torch.save({'epoch': self.epoch,
					'step': self.step,
					'optimizer': self.optimizer
				   },
				   os.path.join(path, self.TRAINER_STATE_NAME))
		torch.save(self.model, os.path.join(path, self.MODEL_NAME))

		with open(os.path.join(path, self.INPUT_VOCAB_FILE), 'wb') as fout:
			dill.dump(self.input_vocab, fout)
		with open(os.path.join(path, self.OUTPUT_VOCAB_FILE), 'wb') as fout:
			dill.dump(self.output_vocab, fout)

		return path

	@classmethod
	def load(cls, path):
		if torch.cuda.is_available():
			resume_checkpoint = torch.load(os.path.join(path, cls.TRAINER_STATE_NAME))
			model = torch.load(os.path.join(path, cls.MODEL_NAME))
		else:
			resume_checkpoint = torch.load(os.path.join(path, cls.TRAINER_STATE_NAME), map_location=lambda storage, loc: storage)
			model = torch.load(os.path.join(path, cls.MODEL_NAME), map_location=lambda storage, loc: storage)

		model.flatten_parameters() # make RNN parameters contiguous
		with open(os.path.join(path, cls.INPUT_VOCAB_FILE), 'rb') as fin:
			input_vocab = dill.load(fin)
		with open(os.path.join(path, cls.OUTPUT_VOCAB_FILE), 'rb') as fin:
			output_vocab = dill.load(fin)
		optimizer = resume_checkpoint['optimizer']
		return Checkpoint(model=model, input_vocab=input_vocab,
						  output_vocab=output_vocab,
						  optimizer=optimizer,
						  epoch=resume_checkpoint['epoch'],
						  step=resume_checkpoint['step'],
						  path=path)

	@classmethod
	def get_latest_checkpoint(cls, experiment_path):
		checkpoints_path = os.path.join(experiment_path, cls.CHECKPOINT_DIR_NAME)
		all_times = sorted(os.listdir(checkpoints_path), reverse=True)
		return os.path.join(checkpoints_path, all_times[0])

from typing import List, Optional


import spacy
from spacy.tokens import Doc

from allennlp.common.util import get_spacy_model
from allennlp.data.tokenizers.token_class import Token
from allennlp.data.tokenizers.tokenizer import Tokenizer


@Tokenizer.register("spacy")
class SpacyTokenizer(Tokenizer):

	def __init__(
		self,
		language: str = "en_core_web_sm",
		pos_tags: bool = True,
		parse: bool = False,
		ner: bool = False,
		keep_spacy_tokens: bool = False,
		split_on_spaces: bool = False,
		start_tokens: Optional[List[str]] = None,
		end_tokens: Optional[List[str]] = None,
	) -> None:
		self._language = language
		self._pos_tags = pos_tags
		self._parse = parse
		self._ner = ner
		self._split_on_spaces = split_on_spaces

		self.spacy = get_spacy_model(self._language, self._pos_tags, self._parse, self._ner)

		if self._split_on_spaces:
			self.spacy.tokenizer = _WhitespaceSpacyTokenizer(self.spacy.vocab)

		self._keep_spacy_tokens = keep_spacy_tokens
		self._start_tokens = start_tokens or []
		self._start_tokens.reverse()
		self._is_version_3 = spacy.__version__ >= "3.0"
		self._end_tokens = end_tokens or []

	def _sanitize(self, tokens: List[spacy.tokens.Token]) -> List[Token]:
		if not self._keep_spacy_tokens:
			tokens = [
				Token(
					token.text,
					token.idx,
					token.idx + len(token.text),
					token.lemma_,
					token.pos_,
					token.tag_,
					token.dep_,
					token.ent_type_,
				)
				for token in tokens
			]
		for start_token in self._start_tokens:
			tokens.insert(0, Token(start_token, 0))
		for end_token in self._end_tokens:
			tokens.append(Token(end_token, -1))
		return tokens

	def batch_tokenize(self, texts: List[str]) -> List[List[Token]]:
		if self._is_version_3:
			return [
				self._sanitize(_remove_spaces(tokens))
				for tokens in self.spacy.pipe(texts, n_process=-1)
			]
		else:
			return [
				self._sanitize(_remove_spaces(tokens))
				for tokens in self.spacy.pipe(texts, n_threads=-1)
			]

	def tokenize(self, text: str) -> List[Token]:
		return self._sanitize(_remove_spaces(self.spacy(text)))

	def _to_params(self):
		return {
			"type": "spacy",
			"language": self._language,
			"pos_tags": self._pos_tags,
			"parse": self._parse,
			"ner": self._ner,
			"keep_spacy_tokens": self._keep_spacy_tokens,
			"split_on_spaces": self._split_on_spaces,
			"start_tokens": self._start_tokens,
			"end_tokens": self._end_tokens,
		}


class _WhitespaceSpacyTokenizer:

	def __init__(self, vocab):
		self.vocab = vocab

	def __call__(self, text):
		words = text.split(" ")
		spaces = [True] * len(words)
		return Doc(self.vocab, words=words, spaces=spaces)


def _remove_spaces(tokens: List[spacy.tokens.Token]) -> List[spacy.tokens.Token]:
	return [token for token in tokens if not token.is_space]

from typing import Dict, Mapping, Iterable, Union, Optional
import json


from allennlp.common.checks import ConfigurationError
from allennlp.data.dataset_readers.dataset_reader import (
	DatasetReader,
	PathOrStr,
	WorkerInfo,
	DistributedInfo,
)
from allennlp.data.fields import MetadataField
from allennlp.data.instance import Instance

_VALID_SCHEMES = {"round_robin", "all_at_once"}


@DatasetReader.register("interleaving")
class InterleavingDatasetReader(DatasetReader):

	def __init__(
		self,
		readers: Dict[str, DatasetReader],
		dataset_field_name: str = "dataset",
		scheme: str = "round_robin",
		**kwargs,
	) -> None:
		super().__init__(**kwargs)
		self._readers = readers
		self._dataset_field_name = dataset_field_name

		if scheme not in _VALID_SCHEMES:
			raise ConfigurationError(f"invalid scheme: {scheme}")
		self._scheme = scheme

	def _set_worker_info(self, info: Optional[WorkerInfo]) -> None:
		super()._set_worker_info(info)
		for reader in self._readers.values():
			reader._set_worker_info(info)

	def _set_distributed_info(self, info: Optional[DistributedInfo]) -> None:
		super()._set_distributed_info(info)
		for reader in self._readers.values():
			reader._set_distributed_info(info)

	def _read_round_robin(self, datasets: Mapping[str, Iterable[Instance]]) -> Iterable[Instance]:
		remaining = set(datasets)
		dataset_iterators = {key: iter(dataset) for key, dataset in datasets.items()}

		while remaining:
			for key, dataset in dataset_iterators.items():
				if key in remaining:
					try:
						instance = next(dataset)
						instance.fields[self._dataset_field_name] = MetadataField(key)
						yield instance
					except StopIteration:
						remaining.remove(key)

	def _read_all_at_once(self, datasets: Mapping[str, Iterable[Instance]]) -> Iterable[Instance]:
		for key, dataset in datasets.items():
			for instance in dataset:
				instance.fields[self._dataset_field_name] = MetadataField(key)
				yield instance

	def _read(self, file_path: Union[str, Dict[str, PathOrStr]]) -> Iterable[Instance]:
		if isinstance(file_path, str):
			try:
				file_paths = json.loads(file_path)
			except json.JSONDecodeError:
				raise ConfigurationError(
					"the file_path for the InterleavingDatasetReader "
					"needs to be a JSON-serialized dictionary {reader_name -> file_path}"
				)
		else:
			file_paths = file_path

		if file_paths.keys() != self._readers.keys():
			raise ConfigurationError("mismatched keys")

		datasets = {key: reader.read(file_paths[key]) for key, reader in self._readers.items()}

		if self._scheme == "round_robin":
			yield from self._read_round_robin(datasets)
		elif self._scheme == "all_at_once":
			yield from self._read_all_at_once(datasets)
		else:
			raise RuntimeError("impossible to get here")

	def text_to_instance(self, dataset_key: str, *args, **kwargs) -> Instance:  # type: ignore
		return self._readers[dataset_key].text_to_instance(*args, **kwargs)  # type: ignore[call-arg]

	def apply_token_indexers(self, instance: Instance) -> None:
		dataset = instance.fields[self._dataset_field_name].metadata  # type: ignore[attr-defined]
		self._readers[dataset].apply_token_indexers(instance)

import itertools

import torch

class Optimizer(object):

	_ARG_MAX_GRAD_NORM = 'max_grad_norm'

	def __init__(self, optim, max_grad_norm=0):
		self.optimizer = optim
		self.scheduler = None
		self.max_grad_norm = max_grad_norm

	def set_scheduler(self, scheduler):
		self.scheduler = scheduler

	def step(self):
		if self.max_grad_norm > 0:
			params = itertools.chain.from_iterable([group['params'] for group in self.optimizer.param_groups])
			torch.nn.utils.clip_grad_norm_(params, self.max_grad_norm)
		self.optimizer.step()

	def update(self, loss, epoch):
		if self.scheduler is None:
			pass
		elif isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
			self.scheduler.step(loss)
		else:
			self.scheduler.step()

import os
import threading
from datetime import datetime

import torch
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.nn as nn
from torch import optim

import torchvision


batch_size = 20
image_w = 64
image_h = 64
num_classes = 30
batch_update_size = 5
num_batches = 6


def timed_log(text):
	print(f"{datetime.now().strftime('%H:%M:%S')} {text}")


class BatchUpdateParameterServer(object):

	def __init__(self, batch_update_size=batch_update_size):
		self.model = torchvision.models.resnet50(num_classes=num_classes)
		self.lock = threading.Lock()
		self.future_model = torch.futures.Future()
		self.batch_update_size = batch_update_size
		self.curr_update_size = 0
		self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)
		for p in self.model.parameters():
			p.grad = torch.zeros_like(p)

	def get_model(self):
		return self.model

	@staticmethod
	@rpc.functions.async_execution
	def update_and_fetch_model(ps_rref, grads):
		self = ps_rref.local_value()
		timed_log(f"PS got {self.curr_update_size}/{batch_update_size} updates")
		for p, g in zip(self.model.parameters(), grads):
			p.grad += g
		with self.lock:
			self.curr_update_size += 1
			fut = self.future_model

			if self.curr_update_size >= self.batch_update_size:
				for p in self.model.parameters():
					p.grad /= self.batch_update_size
				self.curr_update_size = 0
				self.optimizer.step()
				self.optimizer.zero_grad(set_to_none=False)
				fut.set_result(self.model)
				timed_log("PS updated model")
				self.future_model = torch.futures.Future()

		return fut


class Trainer(object):

	def __init__(self, ps_rref):
		self.ps_rref = ps_rref
		self.loss_fn = nn.MSELoss()
		self.one_hot_indices = torch.LongTensor(batch_size) \
									.random_(0, num_classes) \
									.view(batch_size, 1)

	def get_next_batch(self):
		for _ in range(num_batches):
			inputs = torch.randn(batch_size, 3, image_w, image_h)
			labels = torch.zeros(batch_size, num_classes) \
						.scatter_(1, self.one_hot_indices, 1)
			yield inputs.cuda(), labels.cuda()

	def train(self):
		name = rpc.get_worker_info().name
		m = self.ps_rref.rpc_sync().get_model().cuda()
		for inputs, labels in self.get_next_batch():
			timed_log(f"{name} processing one batch")
			self.loss_fn(m(inputs), labels).backward()
			timed_log(f"{name} reporting grads")
			m = rpc.rpc_sync(
				self.ps_rref.owner(),
				BatchUpdateParameterServer.update_and_fetch_model,
				args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),
			).cuda()
			timed_log(f"{name} got updated model")


def run_trainer(ps_rref):
	trainer = Trainer(ps_rref)
	trainer.train()


def run_ps(trainers):
	timed_log("Start training")
	ps_rref = rpc.RRef(BatchUpdateParameterServer())
	futs = []
	for trainer in trainers:
		futs.append(
			rpc.rpc_async(trainer, run_trainer, args=(ps_rref,))
		)

	torch.futures.wait_all(futs)
	timed_log("Finish training")


def run(rank, world_size):
	os.environ['MASTER_ADDR'] = 'localhost'
	os.environ['MASTER_PORT'] = '29500'
	options=rpc.TensorPipeRpcBackendOptions(
		num_worker_threads=16,
		rpc_timeout=0  # infinite timeout
	 )
	if rank != 0:
		rpc.init_rpc(
			f"trainer{rank}",
			rank=rank,
			world_size=world_size,
			rpc_backend_options=options
		)
	else:
		rpc.init_rpc(
			"ps",
			rank=rank,
			world_size=world_size,
			rpc_backend_options=options
		)
		run_ps([f"trainer{r}" for r in range(1, world_size)])

	rpc.shutdown()


if __name__=="__main__":
	world_size = batch_update_size + 1
	mp.spawn(run, args=(world_size, ), nprocs=world_size, join=True)


import argparse
import logging
import os
from os import PathLike
from typing import Any, Dict, List, Optional, Union
import warnings

import torch
import torch.distributed as dist
import torch.multiprocessing as mp


from allennlp.commands.subcommand import Subcommand
from allennlp.common import Params, Registrable, Lazy
from allennlp.common.checks import check_for_gpu, ConfigurationError
from allennlp.common.meta import Meta, META_NAME
from allennlp.common import logging as common_logging
from allennlp.common import util as common_util
from allennlp.common.plugins import import_plugins
from allennlp.data import DatasetReader, Vocabulary
from allennlp.data import DataLoader
from allennlp.models.archival import archive_model, CONFIG_NAME, verify_include_in_archive
from allennlp.models.model import Model
from allennlp.nn.parallel import DdpAccelerator
from allennlp.training.trainer import Trainer
from allennlp.training import util as training_util

logger = logging.getLogger(__name__)


@Subcommand.register("train")
class Train(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		description = """Train the specified model on the specified dataset."""
		subparser = parser.add_parser(self.name, description=description, help="Train a model.")

		subparser.add_argument(
			"param_path", type=str, help="path to parameter file describing the model to be trained"
		)

		subparser.add_argument(
			"-s",
			"--serialization-dir",
			required=True,
			type=str,
			help="directory in which to save the model and its logs",
		)

		subparser.add_argument(
			"-r",
			"--recover",
			action="store_true",
			default=False,
			help="recover training from the state in serialization_dir",
		)

		subparser.add_argument(
			"-f",
			"--force",
			action="store_true",
			required=False,
			help="overwrite the output directory if it exists",
		)

		subparser.add_argument(
			"-o",
			"--overrides",
			type=str,
			default="",
			help=(
				"a json(net) structure used to override the experiment configuration, e.g., "
				"'{\"iterator.batch_size\": 16}'.  Nested parameters can be specified either"
				" with nested dictionaries or with dot syntax."
			),
		)

		subparser.add_argument(
			"--node-rank", type=int, default=0, help="rank of this node in the distributed setup"
		)

		subparser.add_argument(
			"--dry-run",
			action="store_true",
			help=(
				"do not train a model, but create a vocabulary, show dataset statistics and "
				"other training information"
			),
		)
		subparser.add_argument(
			"--file-friendly-logging",
			action="store_true",
			default=False,
			help="outputs tqdm status on separate lines and slows tqdm refresh rate",
		)

		subparser.set_defaults(func=train_model_from_args)

		return subparser


def train_model_from_args(args: argparse.Namespace):
	train_model_from_file(
		parameter_filename=args.param_path,
		serialization_dir=args.serialization_dir,
		overrides=args.overrides,
		recover=args.recover,
		force=args.force,
		node_rank=args.node_rank,
		include_package=args.include_package,
		dry_run=args.dry_run,
		file_friendly_logging=args.file_friendly_logging,
	)


def train_model_from_file(
	parameter_filename: Union[str, PathLike],
	serialization_dir: Union[str, PathLike],
	overrides: Union[str, Dict[str, Any]] = "",
	recover: bool = False,
	force: bool = False,
	node_rank: int = 0,
	include_package: List[str] = None,
	dry_run: bool = False,
	file_friendly_logging: bool = False,
	return_model: Optional[bool] = None,
) -> Optional[Model]:
	params = Params.from_file(parameter_filename, overrides)
	return train_model(
		params=params,
		serialization_dir=serialization_dir,
		recover=recover,
		force=force,
		node_rank=node_rank,
		include_package=include_package,
		dry_run=dry_run,
		file_friendly_logging=file_friendly_logging,
		return_model=return_model,
	)


def train_model(
	params: Params,
	serialization_dir: Union[str, PathLike],
	recover: bool = False,
	force: bool = False,
	node_rank: int = 0,
	include_package: List[str] = None,
	dry_run: bool = False,
	file_friendly_logging: bool = False,
	return_model: Optional[bool] = None,
) -> Optional[Model]:
	common_logging.FILE_FRIENDLY_LOGGING = file_friendly_logging

	training_util.create_serialization_dir(params, serialization_dir, recover, force)
	params.to_file(os.path.join(serialization_dir, CONFIG_NAME))

	params.pop("evaluation", None)

	meta = Meta.new()
	meta.to_file(os.path.join(serialization_dir, META_NAME))

	include_in_archive = params.pop("include_in_archive", None)
	verify_include_in_archive(include_in_archive)

	model: Optional[Model] = None

	distributed_params = params.params.pop("distributed", None)
	if distributed_params is None:
		model = _train_worker(
			process_rank=0,
			params=params,
			serialization_dir=serialization_dir,
			include_package=include_package,
			dry_run=dry_run,
			file_friendly_logging=file_friendly_logging,
		)
	else:
		common_logging.prepare_global_logging(
			serialization_dir,
			rank=0,
			world_size=1,
		)

		device_ids = distributed_params.pop("cuda_devices", None)
		multi_device = isinstance(device_ids, list) and len(device_ids) > 1
		num_nodes = distributed_params.pop("num_nodes", 1)

		if not (multi_device or num_nodes > 1):
			raise ConfigurationError(
				"Multiple cuda devices/nodes need to be configured to run distributed training."
			)
		check_for_gpu(device_ids)

		primary_addr = distributed_params.pop("primary_address", "127.0.0.1")
		if primary_addr in ("127.0.0.1", "0.0.0.0", "localhost"):
			primary_port = (
				distributed_params.pop("primary_port", None) or common_util.find_open_port()
			)
		else:
			primary_port = distributed_params.pop("primary_port")

		num_procs = len(device_ids)
		world_size = num_nodes * num_procs

		vocab_dir = os.path.join(serialization_dir, "vocabulary")
		if recover:
			vocab = Vocabulary.from_files(vocab_dir)
		else:
			vocab = training_util.make_vocab_from_params(
				params.duplicate(), serialization_dir, print_statistics=dry_run
			)
		params["vocabulary"] = {
			"type": "from_files",
			"directory": vocab_dir,
			"padding_token": vocab._padding_token,
			"oov_token": vocab._oov_token,
		}

		logging.info(
			"Switching to distributed training mode since multiple GPUs are configured | "
			f"Primary is at: {primary_addr}:{primary_port} | Rank of this node: {node_rank} | "
			f"Number of workers in this node: {num_procs} | Number of nodes: {num_nodes} | "
			f"World size: {world_size}"
		)

		mp.spawn(
			_train_worker,
			args=(
				params.duplicate(),
				serialization_dir,
				include_package,
				dry_run,
				node_rank,
				primary_addr,
				primary_port,
				world_size,
				device_ids,
				file_friendly_logging,
				include_in_archive,
				Params(distributed_params),
			),
			nprocs=num_procs,
		)

	if not dry_run:
		archive_model(serialization_dir, include_in_archive=include_in_archive)
	else:
		return None

	if return_model is None:
		return model  # model may or may not be `None`.
	elif return_model is True:
		return model if model is not None else Model.load(params, serialization_dir)
	else:
		return None


def _train_worker(
	process_rank: int,
	params: Params,
	serialization_dir: Union[str, PathLike],
	include_package: List[str] = None,
	dry_run: bool = False,
	node_rank: int = 0,
	primary_addr: str = "127.0.0.1",
	primary_port: int = 29500,
	world_size: int = 1,
	distributed_device_ids: List[int] = None,
	file_friendly_logging: bool = False,
	include_in_archive: List[str] = None,
	distributed_params: Optional[Params] = None,
) -> Optional[Model]:
	common_logging.FILE_FRIENDLY_LOGGING = file_friendly_logging

	common_logging.prepare_global_logging(
		serialization_dir,
		rank=process_rank,
		world_size=world_size,
	)
	common_util.prepare_environment(params)

	distributed = world_size > 1

	primary = process_rank == 0

	include_package = include_package or []

	ddp_accelerator: Optional[DdpAccelerator] = None

	if distributed:
		assert distributed_device_ids is not None
		assert distributed_params is not None

		import_plugins()
		for package_name in include_package:
			common_util.import_module_and_submodules(package_name)

		num_procs_per_node = len(distributed_device_ids)
		global_rank = node_rank * num_procs_per_node + process_rank

		os.environ["ALLENNLP_PROCS_PER_NODE"] = str(num_procs_per_node)

		gpu_id = int(distributed_device_ids[process_rank])  # type: ignore

		params["trainer"]["local_rank"] = process_rank
		params["trainer"]["cuda_device"] = gpu_id
		params["trainer"]["world_size"] = world_size
		params["trainer"]["distributed"] = True

		if gpu_id >= 0:
			torch.cuda.set_device(gpu_id)
			dist.init_process_group(
				backend="nccl",
				init_method=f"tcp://{primary_addr}:{primary_port}",
				world_size=world_size,
				rank=global_rank,
			)
		else:
			dist.init_process_group(
				backend="gloo",
				init_method=f"tcp://{primary_addr}:{primary_port}",
				world_size=world_size,
				rank=global_rank,
			)

		if "ddp_accelerator" in distributed_params:
			ddp_accelerator_params = distributed_params.pop("ddp_accelerator")
			ddp_accelerator = DdpAccelerator.from_params(
				ddp_accelerator_params,
				local_rank=process_rank,
				world_size=world_size,
				cuda_device=gpu_id,
			)

		logging.info(
			f"Process group of world size {world_size} initialized "
			f"for distributed training in worker {global_rank}"
		)

	train_loop = TrainModel.from_params(
		params=params,
		serialization_dir=serialization_dir,
		local_rank=process_rank,
		ddp_accelerator=ddp_accelerator,
	)

	if dry_run:
		return None

	try:
		if distributed:  # let the setup get ready for all the workers
			dist.barrier()

		metrics = train_loop.run()
	except (KeyboardInterrupt, common_util.SigTermReceived):
		if primary:
			best_weights_path = train_loop.trainer.get_best_weights_path()
			if best_weights_path is None:
				logging.info(
					"Training interrupted by the user, and no best model has been saved. "
					"No model archive created."
				)
			else:
				logging.info(
					"Training interrupted by the user. Attempting to create "
					"a model archive using the current best epoch weights."
				)
				archive_model(
					serialization_dir,
					weights=best_weights_path,
					include_in_archive=include_in_archive,
				)
		raise

	if primary:
		train_loop.finish(metrics)

	if not distributed:
		return train_loop.model

	return None


class TrainModel(Registrable):

	default_implementation = "default"

	def __init__(
		self,
		serialization_dir: str,
		model: Model,
		trainer: Trainer,
		evaluation_data_loader: DataLoader = None,
		evaluate_on_test: bool = False,
		batch_weight_key: str = "",
	) -> None:
		self.serialization_dir = serialization_dir
		self.model = model
		self.trainer = trainer
		self.evaluation_data_loader = evaluation_data_loader
		self.evaluate_on_test = evaluate_on_test
		self.batch_weight_key = batch_weight_key

	def run(self) -> Dict[str, Any]:
		return self.trainer.train()

	def finish(self, metrics: Dict[str, Any]):
		if self.evaluation_data_loader is not None and self.evaluate_on_test:
			logger.info("The model will be evaluated using the best epoch weights.")
			test_metrics = training_util.evaluate(
				self.model,
				self.evaluation_data_loader,
				cuda_device=self.trainer.cuda_device,
				batch_weight_key=self.batch_weight_key,
			)

			for key, value in test_metrics.items():
				metrics["test_" + key] = value
		elif self.evaluation_data_loader is not None:
			logger.info(
				"To evaluate on the test set after training, pass the "
				"'evaluate_on_test' flag, or use the 'allennlp evaluate' command."
			)
		common_util.dump_metrics(
			os.path.join(self.serialization_dir, "metrics.json"), metrics, log=True
		)

	@classmethod
	def from_partial_objects(
		cls,
		serialization_dir: str,
		local_rank: int,
		dataset_reader: DatasetReader,
		train_data_path: Any,
		model: Lazy[Model],
		data_loader: Lazy[DataLoader],
		trainer: Lazy[Trainer],
		vocabulary: Lazy[Vocabulary] = Lazy(Vocabulary),
		datasets_for_vocab_creation: List[str] = None,
		validation_dataset_reader: DatasetReader = None,
		validation_data_path: Any = None,
		validation_data_loader: Lazy[DataLoader] = None,
		test_data_path: Any = None,
		evaluate_on_test: bool = False,
		batch_weight_key: str = "",
		ddp_accelerator: Optional[DdpAccelerator] = None,
	) -> "TrainModel":
		data_loaders: Dict[str, DataLoader] = {
			"train": data_loader.construct(reader=dataset_reader, data_path=train_data_path)
		}

		if validation_data_path is not None:
			validation_dataset_reader = validation_dataset_reader or dataset_reader
			if validation_data_loader is not None:
				data_loaders["validation"] = validation_data_loader.construct(
					reader=validation_dataset_reader, data_path=validation_data_path
				)
			else:
				data_loaders["validation"] = data_loader.construct(
					reader=validation_dataset_reader, data_path=validation_data_path
				)
				if getattr(data_loaders["validation"], "batches_per_epoch", None) is not None:
					warnings.warn(
						"Using 'data_loader' params to construct validation data loader since "
						"'validation_data_loader' params not specified, but you have "
						"'data_loader.batches_per_epoch' set which may result in different "
						"validation datasets for each epoch.",
						UserWarning,
					)

		if test_data_path is not None:
			test_dataset_reader = validation_dataset_reader or dataset_reader
			if validation_data_loader is not None:
				data_loaders["test"] = validation_data_loader.construct(
					reader=test_dataset_reader, data_path=test_data_path
				)
			else:
				data_loaders["test"] = data_loader.construct(
					reader=test_dataset_reader, data_path=test_data_path
				)

		if datasets_for_vocab_creation:
			for key in datasets_for_vocab_creation:
				if key not in data_loaders:
					raise ConfigurationError(f"invalid 'dataset_for_vocab_creation' {key}")

			logger.info(
				"From dataset instances, %s will be considered for vocabulary creation.",
				", ".join(datasets_for_vocab_creation),
			)

		instance_generator = (
			instance
			for key, data_loader in data_loaders.items()
			if datasets_for_vocab_creation is None or key in datasets_for_vocab_creation
			for instance in data_loader.iter_instances()
		)

		vocabulary_ = vocabulary.construct(instances=instance_generator)

		model_ = model.construct(
			vocab=vocabulary_, serialization_dir=serialization_dir, ddp_accelerator=ddp_accelerator
		)

		if local_rank == 0:
			vocabulary_path = os.path.join(serialization_dir, "vocabulary")
			vocabulary_.save_to_files(vocabulary_path)

		for data_loader_ in data_loaders.values():
			data_loader_.index_with(model_.vocab)

		trainer_ = trainer.construct(
			serialization_dir=serialization_dir,
			model=model_,
			data_loader=data_loaders["train"],
			validation_data_loader=data_loaders.get("validation"),
			local_rank=local_rank,
			ddp_accelerator=ddp_accelerator,
		)
		assert trainer_ is not None

		return cls(
			serialization_dir=serialization_dir,
			model=model_,
			trainer=trainer_,
			evaluation_data_loader=data_loaders.get("test"),
			evaluate_on_test=evaluate_on_test,
			batch_weight_key=batch_weight_key,
		)


TrainModel.register("default", constructor="from_partial_objects")(TrainModel)

from typing import Optional, TYPE_CHECKING

import torch

from allennlp.common import FromParams
from allennlp.modules.transformer.layer_norm import LayerNorm
from allennlp.modules.transformer.transformer_module import TransformerModule

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig


class Embeddings(TransformerModule, FromParams):

	def __init__(
		self,
		embeddings: torch.nn.ModuleDict,
		embedding_size: int,
		dropout: float,
		layer_norm_eps: float = 1e-12,  # different from Huggingface!
	):
		super().__init__()
		for name, embedding_layer in embeddings.named_children():
			if isinstance(embedding_layer, torch.nn.Embedding):
				assert embedding_layer.embedding_dim == embedding_size
			elif isinstance(embedding_layer, torch.nn.Linear):
				assert embedding_layer.out_features == embedding_size
			else:
				raise TypeError(
					'Layer "{}" must be of type `torch.nn.Embedding` or `torch.nn.Linear`.'.format(
						name
					)
				)
		self.embeddings = embeddings
		self.layer_norm = LayerNorm(embedding_size, eps=layer_norm_eps)
		self.dropout = torch.nn.Dropout(dropout)

	def forward(self, *inputs) -> torch.Tensor:
		assert len(inputs) == len(self.embeddings)
		outputs = []
		for i, layer in enumerate(self.embeddings.children()):
			outputs.append(layer(inputs[i]))

		outputs = sum(outputs)  # type: ignore
		outputs = self.layer_norm(outputs)
		outputs = self.dropout(outputs)
		return outputs


class ImageFeatureEmbeddings(Embeddings):

	def __init__(self, feature_size: int, embedding_size: int, dropout: float = 0.0):
		image_embeddings = torch.nn.Linear(feature_size, embedding_size)
		location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)
		embeddings = torch.nn.ModuleDict(
			{"image_embeddings": image_embeddings, "location_embeddings": location_embeddings}
		)
		super().__init__(embeddings, embedding_size, dropout)


class TransformerEmbeddings(Embeddings):

	_pretrained_relevant_module = ["embeddings", "bert.embeddings", "roberta.embeddings"]
	_pretrained_mapping = {
		"LayerNorm": "layer_norm",
		"word_embeddings": "embeddings.word_embeddings",
		"position_embeddings": "embeddings.position_embeddings",
		"token_type_embeddings": "embeddings.token_type_embeddings",
		"albert.embeddings.LayerNorm": "layer_norm",
		"albert.embeddings.word_embeddings": "embeddings.word_embeddings",
		"albert.embeddings.position_embeddings": "embeddings.position_embeddings",
		"albert.embeddings.token_type_embeddings": "embeddings.token_type_embeddings",
		"albert.encoder.embedding_hidden_mapping_in": "linear_transform",
	}
	_pretrained_ignore = [
		r"^albert\.pooler\..*",
		r"^albert\.encoder\.albert_layer_groups\..*",
		r"^predictions\.*",
	]

	def __init__(
		self,
		vocab_size: int,
		embedding_size: int,
		pad_token_id: int = 0,
		max_position_embeddings: int = 512,
		position_pad_token_id: Optional[int] = None,
		type_vocab_size: int = 2,
		dropout: float = 0.1,
		layer_norm_eps: float = 1e-12,  # different from Huggingface!
		output_size: Optional[int] = None,
	):
		embedding_dict = {}

		word_embeddings = torch.nn.Embedding(vocab_size, embedding_size, padding_idx=pad_token_id)
		embedding_dict["word_embeddings"] = word_embeddings

		if max_position_embeddings > 0:
			position_embeddings = torch.nn.Embedding(
				max_position_embeddings, embedding_size, padding_idx=position_pad_token_id
			)
			embedding_dict["position_embeddings"] = position_embeddings

		if type_vocab_size > 0:
			token_type_embeddings = torch.nn.Embedding(type_vocab_size, embedding_size)
			embedding_dict["token_type_embeddings"] = token_type_embeddings

		embeddings = torch.nn.ModuleDict(embedding_dict)

		super().__init__(embeddings, embedding_size, dropout, layer_norm_eps=layer_norm_eps)

		if output_size:
			self.linear_transform = torch.nn.Linear(embedding_size, output_size)

	def forward(  # type: ignore
		self,
		input_ids: torch.Tensor,
		token_type_ids: Optional[torch.Tensor] = None,
		position_ids: Optional[torch.Tensor] = None,
		attention_mask: Optional[torch.Tensor] = None,
	) -> torch.Tensor:


		input_shape = input_ids.size()
		device = input_ids.device
		seq_length = input_shape[1]

		embedding_inputs = [input_ids]

		if attention_mask is None:
			attention_mask = input_ids != self.embeddings["word_embeddings"].padding_idx

		if "position_embeddings" in self.embeddings:
			if position_ids is None:
				padding_idx = self.embeddings["position_embeddings"].padding_idx
				if padding_idx is None:
					position_ids = torch.arange(seq_length, dtype=torch.long, device=device)
					position_ids = position_ids.unsqueeze(0).expand(input_shape)
				else:
					position_ids = torch.arange(seq_length, dtype=torch.long, device=device) + 1
					position_ids = position_ids.unsqueeze(0).expand(input_shape) * attention_mask
					position_ids += padding_idx
			embedding_inputs.append(position_ids)

		if "token_type_embeddings" in self.embeddings:
			if token_type_ids is None:
				token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)
			embedding_inputs.append(token_type_ids)

		embeddings = super().forward(*embedding_inputs)

		if hasattr(self, "linear_transform"):
			embeddings = self.linear_transform(embeddings)

		return embeddings

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		final_kwargs = {
			"vocab_size": config.vocab_size,
			"pad_token_id": config.pad_token_id,
			"max_position_embeddings": config.max_position_embeddings,
			"type_vocab_size": config.type_vocab_size,
			"layer_norm_eps": config.layer_norm_eps,
		}
		if hasattr(config, "embedding_size"):
			final_kwargs["embedding_size"] = config.embedding_size
			final_kwargs["output_size"] = config.hidden_size
		else:
			final_kwargs["embedding_size"] = config.hidden_size
		if config.model_type == "roberta":
			final_kwargs["position_pad_token_id"] = config.pad_token_id
		final_kwargs.update(**kwargs)
		return cls(**final_kwargs)

import torch
import os
import torch.distributed as dist
from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
	checkpoint_wrapper,
	CheckpointImpl,
	apply_activation_checkpointing,
)

from transformers.models.t5.modeling_t5 import T5Block

from functools import partial

non_reentrant_wrapper = partial(
	checkpoint_wrapper,
	offload_to_cpu=False,
	checkpoint_impl=CheckpointImpl.NO_REENTRANT,
)

check_fn = lambda submodule: isinstance(submodule, T5Block)


def apply_fsdp_checkpointing(model):
	print(f"--> applying fdsp activation checkpointing...")

	apply_activation_checkpointing(
		model, checkpoint_wrapper_fn=non_reentrant_wrapper, check_fn=check_fn
	)

from typing import Dict, List, Any
import logging


from allennlp.common.file_utils import cached_path
from allennlp.data.dataset_readers.dataset_reader import DatasetReader
from allennlp.data.fields import TextField, SequenceLabelField, MetadataField, Field
from allennlp.data.instance import Instance
from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer
from allennlp.data.tokenizers import Token

logger = logging.getLogger(__name__)

DEFAULT_WORD_TAG_DELIMITER = "###"


@DatasetReader.register("sequence_tagging")
class SequenceTaggingDatasetReader(DatasetReader):

	def __init__(
		self,
		word_tag_delimiter: str = DEFAULT_WORD_TAG_DELIMITER,
		token_delimiter: str = None,
		token_indexers: Dict[str, TokenIndexer] = None,
		**kwargs,
	) -> None:
		super().__init__(
			manual_distributed_sharding=True, manual_multiprocess_sharding=True, **kwargs
		)
		self._token_indexers = token_indexers or {"tokens": SingleIdTokenIndexer()}
		self._word_tag_delimiter = word_tag_delimiter
		self._token_delimiter = token_delimiter

		self._params = {
			"word_tag_delimiter": self._word_tag_delimiter,
			"token_delimiter": self._token_delimiter,
			"token_indexers": self._token_indexers,
		}
		self._params.update(kwargs)

	def _read(self, file_path):
		file_path = cached_path(file_path)

		with open(file_path, "r") as data_file:
			logger.info("Reading instances from lines in file at: %s", file_path)
			for line in self.shard_iterable(data_file):
				line = line.strip("\n")

				if not line:
					continue

				tokens_and_tags = [
					pair.rsplit(self._word_tag_delimiter, 1)
					for pair in line.split(self._token_delimiter)
				]
				tokens = [Token(token) for token, tag in tokens_and_tags]
				tags = [tag for token, tag in tokens_and_tags]
				yield self.text_to_instance(tokens, tags)

	def text_to_instance(  # type: ignore
		self, tokens: List[Token], tags: List[str] = None
	) -> Instance:

		fields: Dict[str, Field] = {}
		sequence = TextField(tokens)
		fields["tokens"] = sequence
		fields["metadata"] = MetadataField({"words": [x.text for x in tokens]})
		if tags is not None:
			fields["tags"] = SequenceLabelField(tags, sequence)
		return Instance(fields)

	def apply_token_indexers(self, instance: Instance) -> None:
		instance.fields["tokens"]._token_indexers = self._token_indexers  # type: ignore

	def _to_params(self) -> Dict[str, Any]:
		return self._params


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset
from torchvision.transforms import Compose, Lambda, Normalize, RandomHorizontalFlip, RandomResizedCrop, ToTensor
from torchvision.transforms.functional import InterpolationMode

import transformers
from transformers import (
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	ViTImageProcessor,
	ViTMAEConfig,
	ViTMAEForPreTraining,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version



logger = logging.getLogger(__name__)

check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/image-pretraining/requirements.txt")


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default="cifar10", metadata={"help": "Name of a dataset from the datasets package"}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	image_column_name: Optional[str] = field(
		default=None, metadata={"help": "The column name of the images in the files."}
	)
	train_dir: Optional[str] = field(default=None, metadata={"help": "A folder containing the training data."})
	validation_dir: Optional[str] = field(default=None, metadata={"help": "A folder containing the validation data."})
	train_val_split: Optional[float] = field(
		default=0.15, metadata={"help": "Percent to split off of train for validation."}
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)

	def __post_init__(self):
		data_files = {}
		if self.train_dir is not None:
			data_files["train"] = self.train_dir
		if self.validation_dir is not None:
			data_files["val"] = self.validation_dir
		self.data_files = data_files if data_files else None


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		default=None,
		metadata={
			"help": (
				"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch."
			)
		},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name_or_path"}
	)
	config_overrides: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"Override some existing default config settings when a model is trained from scratch. Example: "
				"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
			)
		},
	)
	cache_dir: Optional[str] = field(
		default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	image_processor_name: str = field(default=None, metadata={"help": "Name or path of preprocessor config."})
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	mask_ratio: float = field(
		default=0.75, metadata={"help": "The ratio of the number of masked tokens in the input sequence."}
	)
	norm_pix_loss: bool = field(
		default=True, metadata={"help": "Whether or not to train with normalized pixel values as target."}
	)


@dataclass
class CustomTrainingArguments(TrainingArguments):
	base_learning_rate: float = field(
		default=1e-3, metadata={"help": "Base learning rate: absolute_lr = base_lr * total_batch_size / 256."}
	)


def collate_fn(examples):
	pixel_values = torch.stack([example["pixel_values"] for example in examples])
	return {"pixel_values": pixel_values}


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_mae", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	ds = load_dataset(
		data_args.dataset_name,
		data_args.dataset_config_name,
		data_files=data_args.data_files,
		cache_dir=model_args.cache_dir,
		token=model_args.token,
	)

	data_args.train_val_split = None if "validation" in ds.keys() else data_args.train_val_split
	if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:
		split = ds["train"].train_test_split(data_args.train_val_split)
		ds["train"] = split["train"]
		ds["validation"] = split["test"]

	config_kwargs = {
		"cache_dir": model_args.cache_dir,
		"revision": model_args.model_revision,
		"token": model_args.token,
	}
	if model_args.config_name:
		config = ViTMAEConfig.from_pretrained(model_args.config_name, **config_kwargs)
	elif model_args.model_name_or_path:
		config = ViTMAEConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
	else:
		config = ViTMAEConfig()
		logger.warning("You are instantiating a new config instance from scratch.")
		if model_args.config_overrides is not None:
			logger.info(f"Overriding config: {model_args.config_overrides}")
			config.update_from_string(model_args.config_overrides)
			logger.info(f"New config: {config}")

	config.update(
		{
			"mask_ratio": model_args.mask_ratio,
			"norm_pix_loss": model_args.norm_pix_loss,
		}
	)

	if model_args.image_processor_name:
		image_processor = ViTImageProcessor.from_pretrained(model_args.image_processor_name, **config_kwargs)
	elif model_args.model_name_or_path:
		image_processor = ViTImageProcessor.from_pretrained(model_args.model_name_or_path, **config_kwargs)
	else:
		image_processor = ViTImageProcessor()

	if model_args.model_name_or_path:
		model = ViTMAEForPreTraining.from_pretrained(
			model_args.model_name_or_path,
			from_tf=bool(".ckpt" in model_args.model_name_or_path),
			config=config,
			cache_dir=model_args.cache_dir,
			revision=model_args.model_revision,
			token=model_args.token,
		)
	else:
		logger.info("Training new model from scratch")
		model = ViTMAEForPreTraining(config)

	if training_args.do_train:
		column_names = ds["train"].column_names
	else:
		column_names = ds["validation"].column_names

	if data_args.image_column_name is not None:
		image_column_name = data_args.image_column_name
	elif "image" in column_names:
		image_column_name = "image"
	elif "img" in column_names:
		image_column_name = "img"
	else:
		image_column_name = column_names[0]

	if "shortest_edge" in image_processor.size:
		size = image_processor.size["shortest_edge"]
	else:
		size = (image_processor.size["height"], image_processor.size["width"])
	transforms = Compose(
		[
			Lambda(lambda img: img.convert("RGB") if img.mode != "RGB" else img),
			RandomResizedCrop(size, scale=(0.2, 1.0), interpolation=InterpolationMode.BICUBIC),
			RandomHorizontalFlip(),
			ToTensor(),
			Normalize(mean=image_processor.image_mean, std=image_processor.image_std),
		]
	)

	def preprocess_images(examples):

		examples["pixel_values"] = [transforms(image) for image in examples[image_column_name]]
		return examples

	if training_args.do_train:
		if "train" not in ds:
			raise ValueError("--do_train requires a train dataset")
		if data_args.max_train_samples is not None:
			ds["train"] = ds["train"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))
		ds["train"].set_transform(preprocess_images)

	if training_args.do_eval:
		if "validation" not in ds:
			raise ValueError("--do_eval requires a validation dataset")
		if data_args.max_eval_samples is not None:
			ds["validation"] = (
				ds["validation"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))
			)
		ds["validation"].set_transform(preprocess_images)

	total_train_batch_size = (
		training_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size
	)
	if training_args.base_learning_rate is not None:
		training_args.learning_rate = training_args.base_learning_rate * total_train_batch_size / 256

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=ds["train"] if training_args.do_train else None,
		eval_dataset=ds["validation"] if training_args.do_eval else None,
		tokenizer=image_processor,
		data_collator=collate_fn,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()
		trainer.log_metrics("train", train_result.metrics)
		trainer.save_metrics("train", train_result.metrics)
		trainer.save_state()

	if training_args.do_eval:
		metrics = trainer.evaluate()
		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {
		"tasks": "masked-auto-encoding",
		"dataset": data_args.dataset_name,
		"tags": ["masked-auto-encoding"],
	}
	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

import collections
from typing import Type, List, Dict


from allennlp.common import JsonDict
from allennlp.data import Instance
from allennlp.models.multitask import MultiTaskModel
from allennlp.predictors.predictor import Predictor
from allennlp.common.util import sanitize
from allennlp.data.fields import MetadataField
from allennlp.common.checks import ConfigurationError
from allennlp.data.dataset_readers import MultiTaskDatasetReader


@Predictor.register("multitask")
class MultiTaskPredictor(Predictor):

	_WRONG_READER_ERROR = (
		"MultitaskPredictor is designed to work with MultiTaskDatasetReader. "
		+ "If you have a different DatasetReader, you have to write your own "
		+ "Predictor, but you can use MultiTaskPredictor as a starting point."
	)

	_WRONG_FIELD_ERROR = (
		"MultiTaskPredictor expects instances that have a MetadataField "
		+ "with the name 'task', containing the name of the task the instance is for."
	)

	def __init__(self, model: MultiTaskModel, dataset_reader: MultiTaskDatasetReader) -> None:
		if not isinstance(dataset_reader, MultiTaskDatasetReader):
			raise ConfigurationError(self._WRONG_READER_ERROR)

		if not isinstance(model, MultiTaskModel):
			raise ConfigurationError(
				"MultiTaskPredictor is designed to work only with MultiTaskModel."
			)

		super().__init__(model, dataset_reader)

		self.predictors = {}
		for name, head in model._heads.items():
			predictor_name = head.default_predictor
			predictor_class: Type[Predictor] = (
				Predictor.by_name(predictor_name) if predictor_name is not None else Predictor  # type: ignore
			)
			self.predictors[name] = predictor_class(model, dataset_reader.readers[name].inner)

	def predict_instance(self, instance: Instance) -> JsonDict:
		task_field = instance["task"]
		if not isinstance(task_field, MetadataField):
			raise ValueError(self._WRONG_FIELD_ERROR)
		task: str = task_field.metadata
		if not isinstance(self._dataset_reader, MultiTaskDatasetReader):
			raise ConfigurationError(self._WRONG_READER_ERROR)
		self._dataset_reader.readers[task].apply_token_indexers(instance)
		outputs = self._model.forward_on_instance(instance)
		return sanitize(outputs)

	def _json_to_instance(self, json_dict: JsonDict) -> Instance:
		task = json_dict["task"]
		del json_dict["task"]
		predictor = self.predictors[task]
		instance = predictor._json_to_instance(json_dict)
		instance.add_field("task", MetadataField(task))
		return instance

	def predict_batch_instance(self, instances: List[Instance]) -> List[JsonDict]:
		task_to_instances: Dict[str, List[Instance]] = collections.defaultdict(lambda: [])
		for instance in instances:
			task_field = instance["task"]
			if not isinstance(task_field, MetadataField):
				raise ValueError(self._WRONG_FIELD_ERROR)
			task: str = task_field.metadata
			if not isinstance(self._dataset_reader, MultiTaskDatasetReader):
				raise ConfigurationError(self._WRONG_READER_ERROR)
			self._dataset_reader.readers[task].apply_token_indexers(instance)
			task_to_instances[task].append(instance)

		outputs = []
		for task, instances in task_to_instances.items():
			outputs.extend(super().predict_batch_instance(instances))

		return outputs

from allennlp.predictors import TextClassifierPredictor
from allennlp.models.model import Model
import torch


class FakeModelForTestingInterpret(Model):
	def __init__(self, vocab, max_tokens=7, num_labels=2):
		super().__init__(vocab)
		self._max_tokens = max_tokens
		self.embedder = torch.nn.Embedding(vocab.get_vocab_size(), 16)
		self.linear = torch.nn.Linear(max_tokens * 16, num_labels)
		self._loss = torch.nn.CrossEntropyLoss()

	def forward(self, tokens, label=None):
		tokens = tokens["tokens"]["tokens"][:, 0 : self._max_tokens]
		embedded = self.embedder(tokens)
		logits = self.linear(torch.flatten(embedded).unsqueeze(0))
		probs = torch.nn.functional.softmax(logits, dim=-1)
		output_dict = {"logits": logits, "probs": probs}
		if label is not None:
			output_dict["loss"] = self._loss(logits, label.long().view(-1))
		return output_dict

	def make_output_human_readable(self, output_dict):
		preds = output_dict["probs"]
		if len(preds.shape) == 1:
			output_dict["probs"] = preds.unsqueeze(0)
			output_dict["logits"] = output_dict["logits"].unsqueeze(0)

		classes = []
		for prediction in output_dict["probs"]:
			label_idx = prediction.argmax(dim=-1).item()
			output_dict["loss"] = self._loss(output_dict["logits"], torch.LongTensor([label_idx]))
			label_str = str(label_idx)
			classes.append(label_str)
		output_dict["label"] = classes
		return output_dict


class FakePredictorForTestingInterpret(TextClassifierPredictor):
	def get_interpretable_layer(self):
		return self._model.embedder

	def get_interpretable_text_field_embedder(self):
		return self._model.embedder

import torch
from allennlp.modules.token_embedders.token_embedder import TokenEmbedder


@TokenEmbedder.register("pass_through")
class PassThroughTokenEmbedder(TokenEmbedder):

	def __init__(self, hidden_dim: int) -> None:
		self.hidden_dim = hidden_dim
		super().__init__()

	def get_output_dim(self):
		return self.hidden_dim

	def forward(self, tokens: torch.Tensor) -> torch.Tensor:
		return tokens

from allennlp.common import FromParams

from allennlp.modules.transformer.activation_layer import ActivationLayer
from allennlp.modules.transformer.output_layer import OutputLayer
from allennlp.modules.transformer.bimodal_attention import BiModalAttention

from allennlp.modules.transformer.transformer_module import TransformerModule


class BiModalOutput(TransformerModule, FromParams):
	def __init__(
		self,
		hidden_size1: int,
		hidden_size2: int,
		combined_hidden_size: int,
		dropout1: float,
		dropout2: float,
	):
		super().__init__()

		self.bert_output1 = OutputLayer(combined_hidden_size, hidden_size1, dropout1)
		self.bert_output2 = OutputLayer(combined_hidden_size, hidden_size2, dropout2)

	def forward(self, hidden_states1, input_tensor1, hidden_states2, input_tensor2):

		hidden_states1 = self.bert_output1(hidden_states1, input_tensor1)
		hidden_states2 = self.bert_output2(hidden_states2, input_tensor2)

		return hidden_states1, hidden_states2


class BiModalConnectionLayer(TransformerModule, FromParams):

	_pretrained_mapping = {"biAttention": "bimodal_attention", "biOutput": "bimodal_output"}

	def __init__(
		self,
		hidden_size1: int,
		hidden_size2: int,
		combined_hidden_size: int,
		intermediate_size1: int,
		intermediate_size2: int,
		num_attention_heads: int,
		dropout1: float,
		dropout2: float,
		activation: str,
	):
		super().__init__()
		self.bimodal_attention = BiModalAttention(
			hidden_size1=hidden_size1,
			hidden_size2=hidden_size2,
			combined_hidden_size=combined_hidden_size,
			num_attention_heads=num_attention_heads,
			dropout1=dropout1,
			dropout2=dropout2,
		)

		self.bimodal_output = BiModalOutput(
			hidden_size1=hidden_size1,
			hidden_size2=hidden_size2,
			combined_hidden_size=combined_hidden_size,
			dropout1=dropout1,
			dropout2=dropout2,
		)

		self.intermediate1 = ActivationLayer(
			hidden_size=hidden_size1,
			intermediate_size=intermediate_size1,
			activation=activation,
		)
		self.output1 = OutputLayer(
			hidden_size=hidden_size1,
			input_size=intermediate_size1,
			dropout=dropout1,
		)

		self.intermediate2 = ActivationLayer(
			hidden_size=hidden_size2,
			intermediate_size=intermediate_size2,
			activation=activation,
		)
		self.output2 = OutputLayer(
			hidden_size=hidden_size2,
			input_size=intermediate_size2,
			dropout=dropout2,
		)

	def forward(
		self,
		input_tensor1,
		attention_mask1,
		input_tensor2,
		attention_mask2,
		co_attention_mask=None,
	):

		bi_output1, bi_output2 = self.bimodal_attention(
			input_tensor1,
			input_tensor2,
			attention_mask1,
			attention_mask2,
			co_attention_mask,
		)

		attention_output1, attention_output2 = self.bimodal_output(
			bi_output2, input_tensor1, bi_output1, input_tensor2
		)

		intermediate_output1 = self.intermediate1(attention_output1)
		layer_output1 = self.output1(intermediate_output1, attention_output1)

		intermediate_output2 = self.intermediate2(attention_output2)
		layer_output2 = self.output2(intermediate_output2, attention_output2)

		return layer_output1, layer_output2

from collections import defaultdict
import inspect
from typing import Any, Dict, List, Set, Union, Mapping


import torch

from allennlp.data import Vocabulary, TextFieldTensors
from allennlp.modules import Backbone
from allennlp.models.model import Model
from allennlp.models.heads import Head
from allennlp.nn import InitializerApplicator


def get_forward_arguments(module: torch.nn.Module) -> Set[str]:
	signature = inspect.signature(module.forward)
	return set([arg for arg in signature.parameters if arg != "self"])


@Model.register("multitask")
class MultiTaskModel(Model):

	default_predictor = "multitask"

	def __init__(
		self,
		vocab: Vocabulary,
		backbone: Backbone,
		heads: Dict[str, Head],
		*,
		loss_weights: Dict[str, float] = None,
		arg_name_mapping: Dict[str, Dict[str, str]] = None,
		allowed_arguments: Dict[str, Set[str]] = None,
		initializer: InitializerApplicator = InitializerApplicator(),
		**kwargs,
	):
		super().__init__(vocab, **kwargs)
		self._backbone = backbone
		self._heads = torch.nn.ModuleDict(heads)
		self._heads_called: Set[str] = set()
		self._arg_name_mapping = arg_name_mapping or defaultdict(dict)

		self._allowed_arguments = allowed_arguments or {
			"backbone": get_forward_arguments(backbone),
			**{key: get_forward_arguments(heads[key]) for key in heads},
		}
		self._loss_weights = loss_weights or defaultdict(lambda: 1.0)
		initializer(self)

	def forward(self, **kwargs) -> Dict[str, torch.Tensor]:  # type: ignore
		if "task" not in kwargs:
			raise ValueError(
				"Instances for multitask training need to contain a MetadataField with "
				"the name 'task' to indicate which task they belong to. Usually the "
				"MultitaskDataLoader provides this field and you don't have to do anything."
			)

		task_indices_just_for_mypy: Mapping[str, List[int]] = defaultdict(lambda: [])
		for i, task in enumerate(kwargs["task"]):
			task_indices_just_for_mypy[task].append(i)
		task_indices: Dict[str, torch.LongTensor] = {
			task: torch.LongTensor(indices) for task, indices in task_indices_just_for_mypy.items()
		}

		def make_inputs_for_task(
			task: str, whole_batch_input: Union[torch.Tensor, TextFieldTensors, List]
		):
			if isinstance(whole_batch_input, dict):
				for k1, v1 in whole_batch_input.items():
					for k2, v2 in v1.items():
						whole_batch_input[k1][k2] = make_inputs_for_task(task, v2)

				return whole_batch_input

			if isinstance(whole_batch_input, torch.Tensor):
				task_indices[task] = task_indices[task].to(whole_batch_input.device)
				return torch.index_select(whole_batch_input, 0, task_indices[task])
			else:
				return [whole_batch_input[i] for i in task_indices[task]]

		backbone_arguments = self._get_arguments(kwargs, "backbone")
		backbone_outputs = self._backbone(**backbone_arguments)
		combined_arguments = {**backbone_outputs, **kwargs}

		outputs = {**backbone_outputs}
		loss = None
		for head_name in self._heads:
			if head_name not in task_indices:
				continue

			head_arguments = self._get_arguments(combined_arguments, head_name)
			head_arguments = {
				key: make_inputs_for_task(head_name, value) for key, value in head_arguments.items()
			}

			head_outputs = self._heads[head_name](**head_arguments)
			for key in head_outputs:
				outputs[f"{head_name}_{key}"] = head_outputs[key]

			if "loss" in head_outputs:
				self._heads_called.add(head_name)
				head_loss = self._loss_weights[head_name] * head_outputs["loss"]
				if loss is None:
					loss = head_loss
				else:
					loss += head_loss

		if loss is not None:
			outputs["loss"] = loss

		return outputs

	def _get_arguments(self, available_args: Dict[str, Any], component: str) -> Dict[str, Any]:
		allowed_args = self._allowed_arguments[component]
		name_mapping = self._arg_name_mapping.get(component, {})
		kept_arguments = {}
		for key, value in available_args.items():
			new_key = name_mapping.get(key, key)
			if new_key in allowed_args:
				if new_key in kept_arguments:
					raise ValueError(
						f"Got duplicate argument {new_key} for {component}. This likely means that"
						" you mapped multiple inputs to the same name. This is generally ok for"
						" the backbone, but you have to be sure each batch only gets one of those"
						" inputs. This is typically not ok for heads, and means something is not"
						" set up right."
					)
				kept_arguments[new_key] = value
		return kept_arguments

	def get_metrics(self, reset: bool = False) -> Dict[str, float]:
		metrics = {}
		for head_name in self._heads_called:
			for key, value in self._heads[head_name].get_metrics(reset).items():
				metrics[f"{head_name}_{key}"] = value
		if reset:
			self._heads_called.clear()
		return metrics

	def make_output_human_readable(
		self, output_dict: Dict[str, torch.Tensor]
	) -> Dict[str, torch.Tensor]:
		output_dict = self._backbone.make_output_human_readable(output_dict)
		for head_name, head in self._heads.items():
			head_outputs = {}
			for key, value in output_dict.items():
				if key.startswith(head_name):
					head_outputs[key.replace(f"{head_name}_", "")] = value
			readable_head_outputs = head.make_output_human_readable(head_outputs)
			for key, value in readable_head_outputs.items():
				output_dict[f"{head_name}_{key}"] = value
		return output_dict

from typing import Optional, Iterable, Tuple, Union
import itertools
import numpy as np

from checklist.editor import MunchWithAdd as CheckListTemplate
from checklist.test_suite import TestSuite
from checklist.test_types import MFT
from checklist.perturb import Perturb
from allennlp.confidence_checks.task_checklists.task_suite import TaskSuite
from allennlp.confidence_checks.task_checklists import utils
from allennlp.predictors import Predictor


def _crossproduct(template: CheckListTemplate):
	ret = []
	ret_labels = []
	for instance in template.data:
		cs = instance["contexts"]
		qas = instance["qas"]
		d = list(itertools.product(cs, qas))
		ret.append([(x[0], x[1][0]) for x in d])
		ret_labels.append([x[1][1] for x in d])
	template.data = ret
	template.labels = ret_labels
	return template


@TaskSuite.register("question-answering")
class QuestionAnsweringSuite(TaskSuite):
	def __init__(
		self,
		suite: Optional[TestSuite] = None,
		context_key: str = "context",
		question_key: str = "question",
		answer_key: str = "best_span_str",
		**kwargs,
	):
		self._context_key = context_key
		self._question_key = question_key
		self._answer_key = answer_key

		super().__init__(suite, **kwargs)

	def _prediction_and_confidence_scores(self, predictor: Predictor):
		def preds_and_confs_fn(data):
			data = [{self._context_key: pair[0], self._question_key: pair[1]} for pair in data]
			predictions = predictor.predict_batch_json(data)
			labels = [pred[self._answer_key] for pred in predictions]
			return labels, np.ones(len(labels))

		return preds_and_confs_fn

	def _format_failing_examples(
		self,
		inputs: Tuple,
		pred: str,
		conf: Union[np.array, np.ndarray],
		label: Optional[str] = None,
		*args,
		**kwargs,
	):
		context, question = inputs
		ret = "Context: %s\nQuestion: %s\n" % (context, question)
		if label is not None:
			ret += "Original answer: %s\n" % label
		ret += "Predicted answer: %s\n" % pred
		return ret

	@classmethod
	def contractions(cls):
		def _contractions(x):
			conts = Perturb.contractions(x[1])
			return [(x[0], a) for a in conts]

		return _contractions

	@classmethod
	def typos(cls):
		def question_typo(x, **kwargs):
			return (x[0], Perturb.add_typos(x[1], **kwargs))

		return question_typo

	@classmethod
	def punctuation(cls):
		def context_punctuation(x):
			return (utils.strip_punctuation(x[0]), x[1])

		return context_punctuation

	def _setup_editor(self):
		super()._setup_editor()

		adj = [
			"old",
			"smart",
			"tall",
			"young",
			"strong",
			"short",
			"tough",
			"cool",
			"fast",
			"nice",
			"small",
			"dark",
			"wise",
			"rich",
			"great",
			"weak",
			"high",
			"slow",
			"strange",
			"clean",
		]
		adj = [(x.rstrip("e"), x) for x in adj]

		self.editor.add_lexicon("adjectives_to_compare", adj, overwrite=True)

		comp_pairs = [
			("better", "worse"),
			("older", "younger"),
			("smarter", "dumber"),
			("taller", "shorter"),
			("bigger", "smaller"),
			("stronger", "weaker"),
			("faster", "slower"),
			("darker", "lighter"),
			("richer", "poorer"),
			("happier", "sadder"),
			("louder", "quieter"),
			("warmer", "colder"),
		]

		self.editor.add_lexicon("comp_pairs", comp_pairs, overwrite=True)

	def _default_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):
		super()._default_tests(data, num_test_cases)
		self._setup_editor()
		self._default_vocabulary_tests(data, num_test_cases)
		self._default_taxonomy_tests(data, num_test_cases)

	def _default_vocabulary_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):

		template = self.editor.template(
			[
				(
					"{first_name} is {adjectives_to_compare[0]}er than {first_name1}.",
					"Who is less {adjectives_to_compare[1]}?",
				),
				(
					"{first_name} is {adjectives_to_compare[0]}er than {first_name1}.",
					"Who is {adjectives_to_compare[0]}er?",
				),
			],
			labels=["{first_name1}", "{first_name}"],
			remove_duplicates=True,
			nsamples=num_test_cases,
			save=True,
		)
		test = MFT(
			**template,
			name="A is COMP than B. Who is more / less COMP?",
			description='Eg. Context: "A is taller than B" '
			'Q: "Who is taller?" A: "A", Q: "Who is less tall?" A: "B"',
			capability="Vocabulary",
		)
		self.add_test(test)

	def _default_taxonomy_tests(self, data: Optional[Iterable[Tuple]], num_test_cases: int = 100):
		template = _crossproduct(
			self.editor.template(
				{
					"contexts": [
						"{first_name} is {comp_pairs[0]} than {first_name1}.",
						"{first_name1} is {comp_pairs[1]} than {first_name}.",
					],
					"qas": [
						(
							"Who is {comp_pairs[1]}?",
							"{first_name1}",
						),
						(
							"Who is {comp_pairs[0]}?",
							"{first_name}",
						),
					],
				},
				remove_duplicates=True,
				nsamples=num_test_cases,
				save=True,
			)
		)
		test = MFT(
			**template,
			name="A is COMP than B. Who is antonym(COMP)? B",
			description='Eg. Context: "A is taller than B", Q: "Who is shorter?", A: "B"',
			capability="Taxonomy",
		)
		self.add_test(test)

import torch

from allennlp.modules.token_embedders.embedding import Embedding
from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder
from allennlp.modules.time_distributed import TimeDistributed
from allennlp.modules.token_embedders.token_embedder import TokenEmbedder


@TokenEmbedder.register("character_encoding")
class TokenCharactersEncoder(TokenEmbedder):

	def __init__(self, embedding: Embedding, encoder: Seq2VecEncoder, dropout: float = 0.0) -> None:
		super().__init__()
		self._embedding = TimeDistributed(embedding)
		self._encoder = TimeDistributed(encoder)
		if dropout > 0:
			self._dropout = torch.nn.Dropout(p=dropout)
		else:
			self._dropout = lambda x: x

	def get_output_dim(self) -> int:
		return self._encoder._module.get_output_dim()

	def forward(self, token_characters: torch.Tensor) -> torch.Tensor:
		mask = (token_characters != 0).long()
		return self._dropout(self._encoder(self._embedding(token_characters), mask))

from allennlp.evaluation.serializers.serializers import SimpleSerializer

import glob
from typing import Tuple, Optional, Set, Union

import logging
import os
import re
import time

import torch
import torch.distributed as dist

from allennlp.common import Registrable
from allennlp.common.util import is_distributed
from allennlp.nn import util as nn_util
from allennlp.training.trainer import Trainer, TrainerCheckpoint

logger = logging.getLogger(__name__)


class Checkpointer(Registrable):

	default_implementation = "default"

	def __init__(
		self,
		serialization_dir: Union[str, os.PathLike],
		save_completed_epochs: bool = True,
		save_every_num_seconds: Optional[float] = None,
		save_every_num_batches: Optional[int] = None,
		keep_most_recent_by_count: Optional[int] = 2,
		keep_most_recent_by_age: Optional[int] = None,
	) -> None:
		self._serialization_dir = str(serialization_dir)
		self._save_completed_epochs = save_completed_epochs
		self._save_every_num_seconds = save_every_num_seconds
		self._save_every_num_batches = save_every_num_batches
		self._keep_most_recent_by_count = keep_most_recent_by_count
		self._keep_most_recent_by_age = keep_most_recent_by_age
		self._last_save_time = time.time()
		self._last_save_num_epochs_completed = 0
		self._last_save_num_batches_in_epoch_completed = 0
		self._rank = 0 if not is_distributed() else dist.get_rank()
		self.state_is_sharded = False

		if is_distributed() and save_every_num_seconds is not None:
			raise ValueError(
				"Checkointer parameter 'save_every_num_seconds' is not supported in distributed training"
			)

	@property
	def _is_primary(self) -> bool:
		return self._rank == 0

	def _model_state_path(self, epochs_completed: int, batches_in_epoch_completed: int) -> str:
		path = os.path.join(
			self._serialization_dir,
			f"model_state_e{epochs_completed}_b{batches_in_epoch_completed}",
		)
		if self.state_is_sharded:
			return path + f"_w{self._rank}.th"
		else:
			return path + ".th"

	def _training_state_path(self, epochs_completed: int, batches_in_epoch_completed: int) -> str:
		path = os.path.join(
			self._serialization_dir,
			f"training_state_e{epochs_completed}_b{batches_in_epoch_completed}",
		)
		if self.state_is_sharded:
			return path + f"_w{self._rank}.th"
		else:
			return path + ".th"

	_model_state_file_re = re.compile(r"(.*[/\\])?model_state_e(\d+)_b(\d+)(_w\d+)?\.th$")
	_training_state_file_re = re.compile(r"(.*[/\\])?training_state_e(\d+)_b(\d+)(_w\d+)?\.th$")

	@classmethod
	def _parse_model_state_path(cls, path: Union[str, os.PathLike]) -> Optional[Tuple[int, int]]:
		match = cls._model_state_file_re.match(str(path))
		if match is None:
			return None
		else:
			try:
				return int(match.group(2)), int(match.group(3))
			except ValueError:
				return None

	@classmethod
	def _parse_training_state_path(cls, path: Union[str, os.PathLike]) -> Optional[Tuple[int, int]]:
		match = cls._training_state_file_re.match(str(path))
		if match is None:
			return None
		else:
			try:
				return int(match.group(2)), int(match.group(3))
			except ValueError:
				return None

	def _find_all_checkpoints(self) -> Set[Tuple[int, int]]:
		checkpoints = set()
		pattern = (
			f"model_state_e*_b*_w{self._rank}.th"
			if self.state_is_sharded
			else "model_state_e*_b*.th"
		)
		for model_state_file in glob.iglob(os.path.join(self._serialization_dir, pattern)):
			point_in_time = self._parse_model_state_path(model_state_file)
			if point_in_time is None:
				continue
			else:
				checkpoints.add(point_in_time)
		return checkpoints

	def _remove_checkpoint(self, epochs_completed: int, batches_in_epoch_completed: int):
		for state_name in ("model_state", "training_state"):
			pattern = f"{state_name}_e{epochs_completed}_b{batches_in_epoch_completed}*.th"
			for fname in glob.iglob(os.path.join(self._serialization_dir, pattern)):
				os.remove(fname)

	def maybe_save_checkpoint(
		self,
		trainer: Trainer,
		num_epochs_completed: int,
		num_batches_in_epoch_completed: int,
	) -> bool:
		end_of_epoch = num_batches_in_epoch_completed == 0
		if num_epochs_completed == self._last_save_num_epochs_completed:
			last_save_num_batches_in_epoch_completed = (
				self._last_save_num_batches_in_epoch_completed
			)
		else:
			last_save_num_batches_in_epoch_completed = 0

		should_save = (
			(end_of_epoch and self._save_completed_epochs)
			or (
				self._save_every_num_seconds is not None
				and (time.time() - self._last_save_time >= self._save_every_num_seconds)
			)
			or (
				self._save_every_num_batches is not None
				and (
					num_batches_in_epoch_completed - last_save_num_batches_in_epoch_completed
					>= self._save_every_num_batches
				)
			)
		)

		if should_save:
			self.save_checkpoint(trainer)
			return True
		return False

	def save_checkpoint(
		self,
		trainer: Trainer,
	) -> None:
		if self._serialization_dir is None:
			return

		tcps = trainer.get_checkpoint_state()
		if tcps is None:
			assert not self._is_primary and not self.state_is_sharded
			return

		epochs_completed = tcps.trainer_state["epochs_completed"]
		batches_in_epoch_completed = tcps.trainer_state["batches_in_epoch_completed"]

		model_state_path = self._model_state_path(
			epochs_completed,
			batches_in_epoch_completed,
		)
		if not os.path.isfile(model_state_path):
			torch.save(tcps.model_state, model_state_path)

		trainer_state_path = self._training_state_path(
			epochs_completed,
			batches_in_epoch_completed,
		)
		if not os.path.isfile(trainer_state_path):
			torch.save(tcps.trainer_state, trainer_state_path)

		self._last_save_time = time.time()
		self._last_save_num_epochs_completed = epochs_completed
		self._last_save_num_batches_in_epoch_completed = batches_in_epoch_completed

		if self._is_primary and (
			self._keep_most_recent_by_age is not None or self._keep_most_recent_by_count is not None
		):
			checkpoints = list(self._find_all_checkpoints())
			checkpoints.sort(reverse=True)

			if self._keep_most_recent_by_count is not None:
				checkpoints_to_keep = set(checkpoints[: self._keep_most_recent_by_count])
			else:
				checkpoints_to_keep = set()

			now = time.time()
			if self._keep_most_recent_by_age is not None:
				for checkpoint in checkpoints:
					checkpoint_mtime = max(
						os.path.getmtime(n)
						for n in [
							self._model_state_path(*checkpoint),
							self._training_state_path(*checkpoint),
						]
					)
					if now - checkpoint_mtime <= self._keep_most_recent_by_age:
						checkpoints_to_keep.add(checkpoint)

			for checkpoint in checkpoints:
				if checkpoint not in checkpoints_to_keep:
					self._remove_checkpoint(*checkpoint)

	def find_latest_checkpoint(self) -> Optional[Tuple[str, str]]:
		checkpoints = self._find_all_checkpoints()
		if len(checkpoints) <= 0:
			return None
		last_checkpoint = max(checkpoints)
		return self._model_state_path(*last_checkpoint), self._training_state_path(*last_checkpoint)

	def load_checkpoint(self) -> Optional[TrainerCheckpoint]:
		latest_checkpoint = self.find_latest_checkpoint()
		if latest_checkpoint is None:
			return None

		model_path, training_state_path = latest_checkpoint

		model_state = torch.load(model_path, map_location=nn_util.device_mapping(-1))
		training_state = torch.load(training_state_path, map_location=nn_util.device_mapping(-1))
		return TrainerCheckpoint(model_state, training_state)


Checkpointer.register("default")(Checkpointer)

from typing import Dict, Optional


import torch

from allennlp.data import TextFieldTensors, Vocabulary
from allennlp.data.fields import MetadataField
from allennlp.models.model import Model
from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder
from allennlp.nn import InitializerApplicator, util
from allennlp.nn.util import get_text_field_mask
from allennlp.training.metrics import CategoricalAccuracy


@Model.register("basic_classifier")
class BasicClassifier(Model):

	def __init__(
		self,
		vocab: Vocabulary,
		text_field_embedder: TextFieldEmbedder,
		seq2vec_encoder: Seq2VecEncoder,
		seq2seq_encoder: Seq2SeqEncoder = None,
		feedforward: Optional[FeedForward] = None,
		dropout: float = None,
		num_labels: int = None,
		label_namespace: str = "labels",
		namespace: str = "tokens",
		initializer: InitializerApplicator = InitializerApplicator(),
		**kwargs,
	) -> None:

		super().__init__(vocab, **kwargs)
		self._text_field_embedder = text_field_embedder
		self._seq2seq_encoder = seq2seq_encoder
		self._seq2vec_encoder = seq2vec_encoder
		self._feedforward = feedforward
		if feedforward is not None:
			self._classifier_input_dim = feedforward.get_output_dim()
		else:
			self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()

		if dropout:
			self._dropout = torch.nn.Dropout(dropout)
		else:
			self._dropout = None
		self._label_namespace = label_namespace
		self._namespace = namespace

		if num_labels:
			self._num_labels = num_labels
		else:
			self._num_labels = vocab.get_vocab_size(namespace=self._label_namespace)
		self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)
		self._accuracy = CategoricalAccuracy()
		self._loss = torch.nn.CrossEntropyLoss()
		initializer(self)

	def forward(  # type: ignore
		self,
		tokens: TextFieldTensors,
		label: torch.IntTensor = None,
		metadata: MetadataField = None,
	) -> Dict[str, torch.Tensor]:

		embedded_text = self._text_field_embedder(tokens)
		mask = get_text_field_mask(tokens)

		if self._seq2seq_encoder:
			embedded_text = self._seq2seq_encoder(embedded_text, mask=mask)

		embedded_text = self._seq2vec_encoder(embedded_text, mask=mask)

		if self._dropout:
			embedded_text = self._dropout(embedded_text)

		if self._feedforward is not None:
			embedded_text = self._feedforward(embedded_text)

		logits = self._classification_layer(embedded_text)
		probs = torch.nn.functional.softmax(logits, dim=-1)

		output_dict = {"logits": logits, "probs": probs}
		output_dict["token_ids"] = util.get_token_ids_from_text_field_tensors(tokens)
		if label is not None:
			loss = self._loss(logits, label.long().view(-1))
			output_dict["loss"] = loss
			self._accuracy(logits, label)

		return output_dict

	def make_output_human_readable(
		self, output_dict: Dict[str, torch.Tensor]
	) -> Dict[str, torch.Tensor]:
		predictions = output_dict["probs"]
		if predictions.dim() == 2:
			predictions_list = [predictions[i] for i in range(predictions.shape[0])]
		else:
			predictions_list = [predictions]
		classes = []
		for prediction in predictions_list:
			label_idx = prediction.argmax(dim=-1).item()
			label_str = self.vocab.get_index_to_token_vocabulary(self._label_namespace).get(
				label_idx, str(label_idx)
			)
			classes.append(label_str)
		output_dict["label"] = classes
		tokens = []
		for instance_tokens in output_dict["token_ids"]:
			tokens.append(
				[
					self.vocab.get_token_from_index(token_id.item(), namespace=self._namespace)
					for token_id in instance_tokens
				]
			)
		output_dict["tokens"] = tokens
		return output_dict

	def get_metrics(self, reset: bool = False) -> Dict[str, float]:
		metrics = {"accuracy": self._accuracy.get_metric(reset)}
		return metrics

	default_predictor = "text_classifier"

from typing import Dict

from allennlp.training.metrics.metric import Metric
from allennlp.training.metrics.fbeta_measure import FBetaMeasure


@Metric.register("f1")
class F1Measure(FBetaMeasure):

	def __init__(self, positive_label: int) -> None:
		super().__init__(beta=1, labels=[positive_label])
		self._positive_label = positive_label

	def get_metric(self, reset: bool = False) -> Dict[str, float]:
		metric = super().get_metric(reset=reset)
		precision = metric["precision"][0]
		recall = metric["recall"][0]
		f1 = metric["fscore"][0]
		return {"precision": precision, "recall": recall, "f1": f1}

	@property
	def _true_positives(self):
		if self._true_positive_sum is None:
			return 0.0
		else:
			return self._true_positive_sum[self._positive_label]

	@property
	def _true_negatives(self):
		if self._true_negative_sum is None:
			return 0.0
		else:
			return self._true_negative_sum[self._positive_label]

	@property
	def _false_positives(self):
		if self._pred_sum is None:
			return 0.0
		else:
			return self._pred_sum[self._positive_label] - self._true_positives

	@property
	def _false_negatives(self):
		if self._true_sum is None:
			return 0.0
		else:
			return self._true_sum[self._positive_label] - self._true_positives


from abc import abstractmethod
from copy import deepcopy

import torch.nn as nn
from typing import Any
from allennlp.nn.util import move_to_device


class VerificationBase:

	def __init__(self, model: nn.Module):
		super().__init__()
		self.model = model

	@abstractmethod
	def check(self, *args, **kwargs) -> bool:
		pass

	def _get_inputs_copy(self, inputs) -> Any:
		inputs = deepcopy(inputs)
		inputs = move_to_device(inputs, device=next(self.model.parameters()).device)

		return inputs

	def _model_forward(self, inputs: Any) -> Any:
		if isinstance(inputs, tuple):
			return self.model(*inputs)
		if isinstance(inputs, dict):
			return self.model(**inputs)
		return self.model(inputs)

from collections import namedtuple

import torch
from torchvision import models


class Vgg16(torch.nn.Module):
	def __init__(self, requires_grad=False):
		super(Vgg16, self).__init__()
		vgg_pretrained_features = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features
		self.slice1 = torch.nn.Sequential()
		self.slice2 = torch.nn.Sequential()
		self.slice3 = torch.nn.Sequential()
		self.slice4 = torch.nn.Sequential()
		for x in range(4):
			self.slice1.add_module(str(x), vgg_pretrained_features[x])
		for x in range(4, 9):
			self.slice2.add_module(str(x), vgg_pretrained_features[x])
		for x in range(9, 16):
			self.slice3.add_module(str(x), vgg_pretrained_features[x])
		for x in range(16, 23):
			self.slice4.add_module(str(x), vgg_pretrained_features[x])
		if not requires_grad:
			for param in self.parameters():
				param.requires_grad = False

	def forward(self, X):
		h = self.slice1(X)
		h_relu1_2 = h
		h = self.slice2(h)
		h_relu2_2 = h
		h = self.slice3(h)
		h_relu3_3 = h
		h = self.slice4(h)
		h_relu4_3 = h
		vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])
		out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)
		return out

from copy import deepcopy
from typing import List, Tuple
import heapq

import numpy as np
import torch

from allennlp.common.util import JsonDict, sanitize
from allennlp.data import Instance
from allennlp.data.fields import TextField, SequenceLabelField
from allennlp.interpret.attackers import utils
from allennlp.interpret.attackers.attacker import Attacker
from allennlp.predictors import Predictor


@Attacker.register("input-reduction")
class InputReduction(Attacker):

	def __init__(self, predictor: Predictor, beam_size: int = 3) -> None:
		super().__init__(predictor)
		self.beam_size = beam_size

	def attack_from_json(
		self,
		inputs: JsonDict,
		input_field_to_attack: str = "tokens",
		grad_input_field: str = "grad_input_1",
		ignore_tokens: List[str] = None,
		target: JsonDict = None,
	):
		if target is not None:
			raise ValueError("Input reduction does not implement targeted attacks")
		ignore_tokens = ["@@NULL@@"] if ignore_tokens is None else ignore_tokens
		original_instances = self.predictor.json_to_labeled_instances(inputs)
		original_text_field: TextField = original_instances[0][  # type: ignore
			input_field_to_attack
		]
		original_tokens = deepcopy(original_text_field.tokens)
		final_tokens = []
		for instance in original_instances:
			final_tokens.append(
				self._attack_instance(
					inputs, instance, input_field_to_attack, grad_input_field, ignore_tokens
				)
			)
		return sanitize({"final": final_tokens, "original": original_tokens})

	def _attack_instance(
		self,
		inputs: JsonDict,
		instance: Instance,
		input_field_to_attack: str,
		grad_input_field: str,
		ignore_tokens: List[str],
	):
		fields_to_compare = utils.get_fields_to_compare(inputs, instance, input_field_to_attack)

		if "tags" not in instance:
			num_ignore_tokens = 1
			tag_mask = None

		else:
			num_ignore_tokens, tag_mask, original_tags = _get_ner_tags_and_mask(
				instance, input_field_to_attack, ignore_tokens
			)

		text_field: TextField = instance[input_field_to_attack]  # type: ignore
		current_tokens = deepcopy(text_field.tokens)
		candidates = [(instance, -1, tag_mask)]
		while len(current_tokens) > num_ignore_tokens and candidates:
			def get_length(input_instance: Instance):
				input_text_field: TextField = input_instance[input_field_to_attack]  # type: ignore
				return len(input_text_field.tokens)

			candidates = heapq.nsmallest(self.beam_size, candidates, key=lambda x: get_length(x[0]))

			beam_candidates = deepcopy(candidates)
			candidates = []
			for beam_instance, smallest_idx, tag_mask in beam_candidates:
				beam_tag_mask = deepcopy(tag_mask)
				grads, outputs = self.predictor.get_gradients([beam_instance])

				for output in outputs:
					if isinstance(outputs[output], torch.Tensor):
						outputs[output] = outputs[output].detach().cpu().numpy().squeeze().squeeze()
					elif isinstance(outputs[output], list):
						outputs[output] = outputs[output][0]

				if "tags" not in instance:
					beam_instance = self.predictor.predictions_to_labeled_instances(
						beam_instance, outputs
					)[0]
					if utils.instance_has_changed(beam_instance, fields_to_compare):
						continue

				else:
					if smallest_idx != -1:  # Don't delete on the very first iteration
						del beam_tag_mask[smallest_idx]  # type: ignore
					cur_tags = [
						outputs["tags"][x] for x in range(len(outputs["tags"])) if beam_tag_mask[x]  # type: ignore
					]
					if cur_tags != original_tags:
						continue

				text_field: TextField = beam_instance[input_field_to_attack]  # type: ignore
				current_tokens = deepcopy(text_field.tokens)
				reduced_instances_and_smallest = _remove_one_token(
					beam_instance,
					input_field_to_attack,
					grads[grad_input_field][0],
					ignore_tokens,
					self.beam_size,
					beam_tag_mask,  # type: ignore
				)
				candidates.extend(reduced_instances_and_smallest)
		return current_tokens


def _remove_one_token(
	instance: Instance,
	input_field_to_attack: str,
	grads: np.ndarray,
	ignore_tokens: List[str],
	beam_size: int,
	tag_mask: List[int],
) -> List[Tuple[Instance, int, List[int]]]:
	grads_mag = [np.sqrt(grad.dot(grad)) for grad in grads]

	text_field: TextField = instance[input_field_to_attack]  # type: ignore
	for token_idx, token in enumerate(text_field.tokens):
		if token in ignore_tokens:
			grads_mag[token_idx] = float("inf")

	if "tags" in instance:
		tag_field: SequenceLabelField = instance["tags"]  # type: ignore
		labels: List[str] = tag_field.labels  # type: ignore
		for idx, label in enumerate(labels):
			if label != "O":
				grads_mag[idx] = float("inf")
	reduced_instances_and_smallest: List[Tuple[Instance, int, List[int]]] = []
	for _ in range(beam_size):
		copied_instance = deepcopy(instance)
		copied_text_field: TextField = copied_instance[input_field_to_attack]  # type: ignore

		smallest = np.argmin(grads_mag)
		if grads_mag[smallest] == float("inf"):  # if all are ignored tokens, return.
			break
		grads_mag[smallest] = float("inf")  # so the other beams don't use this token

		inputs_before_smallest = copied_text_field.tokens[0:smallest]
		inputs_after_smallest = copied_text_field.tokens[smallest + 1 :]
		copied_text_field.tokens = inputs_before_smallest + inputs_after_smallest

		if "tags" in instance:
			tag_field: SequenceLabelField = copied_instance["tags"]  # type: ignore
			tag_field_before_smallest = tag_field.labels[0:smallest]
			tag_field_after_smallest = tag_field.labels[smallest + 1 :]
			tag_field.labels = tag_field_before_smallest + tag_field_after_smallest  # type: ignore
			tag_field.sequence_field = copied_text_field

		copied_instance.indexed = False
		reduced_instances_and_smallest.append((copied_instance, smallest, tag_mask))

	return reduced_instances_and_smallest


def _get_ner_tags_and_mask(
	instance: Instance, input_field_to_attack: str, ignore_tokens: List[str]
):
	num_ignore_tokens = 0
	input_field: TextField = instance[input_field_to_attack]  # type: ignore
	for token in input_field.tokens:
		if str(token) in ignore_tokens:
			num_ignore_tokens += 1

	tag_mask = []
	original_tags = []
	tag_field: SequenceLabelField = instance["tags"]  # type: ignore
	for label in tag_field.labels:
		if label != "O":
			tag_mask.append(1)
			original_tags.append(label)
			num_ignore_tokens += 1
		else:
			tag_mask.append(0)
	return num_ignore_tokens, tag_mask, original_tags


from typing import List


import torch


class TimeDistributed(torch.nn.Module):

	def __init__(self, module):
		super().__init__()
		self._module = module

	def forward(self, *inputs, pass_through: List[str] = None, **kwargs):

		pass_through = pass_through or []

		reshaped_inputs = [self._reshape_tensor(input_tensor) for input_tensor in inputs]

		some_input = None
		if inputs:
			some_input = inputs[-1]

		reshaped_kwargs = {}
		for key, value in kwargs.items():
			if isinstance(value, torch.Tensor) and key not in pass_through:
				if some_input is None:
					some_input = value

				value = self._reshape_tensor(value)

			reshaped_kwargs[key] = value

		reshaped_outputs = self._module(*reshaped_inputs, **reshaped_kwargs)

		if some_input is None:
			raise RuntimeError("No input tensor to time-distribute")

		tuple_output = True
		if not isinstance(reshaped_outputs, tuple):
			tuple_output = False
			reshaped_outputs = (reshaped_outputs,)

		outputs = []
		for reshaped_output in reshaped_outputs:
			new_size = some_input.size()[:2] + reshaped_output.size()[1:]
			outputs.append(reshaped_output.contiguous().view(new_size))

		if not tuple_output:
			outputs = outputs[0]

		return outputs

	@staticmethod
	def _reshape_tensor(input_tensor):
		input_size = input_tensor.size()
		if len(input_size) <= 2:
			raise RuntimeError(f"No dimension to distribute: {input_size}")
		squashed_shape = [-1] + list(input_size[2:])
		return input_tensor.contiguous().view(*squashed_shape)

from .evaluator import Evaluator
from .predictor import Predictor

import math
from typing import List, Dict, Any

import numpy
import torch

from allennlp.common.util import JsonDict, sanitize
from allennlp.data import Instance
from allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter
from allennlp.nn import util


@SaliencyInterpreter.register("integrated-gradient")
class IntegratedGradient(SaliencyInterpreter):

	def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:
		labeled_instances = self.predictor.json_to_labeled_instances(inputs)

		instances_with_grads = dict()
		for idx, instance in enumerate(labeled_instances):
			grads = self._integrate_gradients(instance)

			for key, grad in grads.items():
				embedding_grad = numpy.sum(grad[0], axis=1)
				norm = numpy.linalg.norm(embedding_grad, ord=1)
				normalized_grad = [math.fabs(e) / norm for e in embedding_grad]
				grads[key] = normalized_grad

			instances_with_grads["instance_" + str(idx + 1)] = grads

		return sanitize(instances_with_grads)

	def _register_hooks(self, alpha: int, embeddings_list: List, token_offsets: List):

		def forward_hook(module, inputs, output):
			if alpha == 0:
				embeddings_list.append(output.squeeze(0).clone().detach())

			output.mul_(alpha)

		def get_token_offsets(module, inputs, outputs):
			offsets = util.get_token_offsets_from_text_field_inputs(inputs)
			if offsets is not None:
				token_offsets.append(offsets)

		handles = []
		embedding_layer = self.predictor.get_interpretable_layer()
		handles.append(embedding_layer.register_forward_hook(forward_hook))
		text_field_embedder = self.predictor.get_interpretable_text_field_embedder()
		handles.append(text_field_embedder.register_forward_hook(get_token_offsets))
		return handles

	def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:
		ig_grads: Dict[str, Any] = {}

		embeddings_list: List[torch.Tensor] = []
		token_offsets: List[torch.Tensor] = []

		steps = 10

		for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):
			handles = []
			handles = self._register_hooks(alpha, embeddings_list, token_offsets)

			try:
				grads = self.predictor.get_gradients([instance])[0]
			finally:
				for handle in handles:
					handle.remove()

			if ig_grads == {}:
				ig_grads = grads
			else:
				for key in grads.keys():
					ig_grads[key] += grads[key]

		for key in ig_grads.keys():
			ig_grads[key] /= steps

		embeddings_list.reverse()
		token_offsets.reverse()
		embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)

		for idx, input_embedding in enumerate(embeddings_list):
			key = "grad_input_" + str(idx + 1)
			ig_grads[key] *= input_embedding

		return ig_grads

from typing import Dict, Any, TYPE_CHECKING

from allennlp.training.callbacks.callback import TrainerCallback

if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


@TrainerCallback.register("track_epoch_callback")
class TrackEpochCallback(TrainerCallback):

	def on_start(
		self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
	) -> None:
		super().on_start(trainer, is_primary)
		trainer.model.epoch = 0  # type: ignore[assignment]

	def on_epoch(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any],
		epoch: int,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		trainer.model.epoch = epoch + 1  # type: ignore[assignment]

from inspect import signature
from typing import Any, List, Callable, Tuple, Dict, cast, TypeVar, Optional
import copy
import warnings


import torch

from allennlp.common import Lazy, Registrable
from allennlp.common.checks import ConfigurationError
from allennlp.data import Vocabulary
from allennlp.nn.util import min_value_of_dtype


StateType = Dict[str, torch.Tensor]
StepFunctionTypeWithTimestep = Callable[
	[torch.Tensor, StateType, int], Tuple[torch.Tensor, StateType]
]
StepFunctionTypeNoTimestep = Callable[[torch.Tensor, StateType], Tuple[torch.Tensor, StateType]]

StepFunctionType = TypeVar(
	"StepFunctionType", StepFunctionTypeWithTimestep, StepFunctionTypeNoTimestep
)

ConstraintStateType = List[List[Dict[str, Any]]]


class Sampler(Registrable):

	default_implementation = "deterministic"

	def init_state(
		self, start_class_log_probabilities: torch.Tensor, batch_size: int, num_classes: int
	) -> StateType:
		return {}

	def sample_nodes(
		self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:
		raise NotImplementedError

	def sample_beams(
		self, log_probs: torch.Tensor, beam_size: int, state: StateType
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:
		selected_log_probs, selected_indices = torch.topk(log_probs, beam_size, dim=-1)
		return selected_log_probs, selected_indices, {}


@Sampler.register("deterministic")
class DeterministicSampler(Sampler):

	def sample_nodes(
		self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:
		selected_log_probs, selected_indices = torch.topk(log_probs, per_node_beam_size, dim=-1)
		return selected_log_probs, selected_indices, {}


@Sampler.register("multinomial")
class MultinomialSampler(Sampler):

	def __init__(
		self,
		temperature: float = 1.0,
		with_replacement: bool = False,
	) -> None:
		self.temperature = temperature
		self.with_replacement = with_replacement

	def sample_nodes(
		self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:
		if self.temperature != 1.0:
			_probabilities = torch.nn.functional.softmax(log_probs / self.temperature, dim=-1)
		else:
			_probabilities = log_probs.exp()

		selected_indices = torch.multinomial(
			_probabilities, per_node_beam_size, replacement=self.with_replacement
		)

		return torch.gather(log_probs, 1, selected_indices), selected_indices, state


@Sampler.register("top-k")
class TopKSampler(Sampler):

	def __init__(
		self,
		k: int = 1,
		temperature: float = 1.0,
		with_replacement: bool = False,
	):
		self.k = k
		self.temperature = temperature or 1.0
		self.with_replacement = with_replacement

	def sample_nodes(
		self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:
		if not per_node_beam_size <= self.k <= log_probs.size()[1]:
			raise ValueError(
				"k must be a postive integer no less than per_node_beam_size and no greater than vocabulary size"
			)

		top_k_log_probs, top_k_indices = log_probs.topk(self.k, dim=-1)

		if self.temperature != 1.0:
			top_k_log_probs = top_k_log_probs / self.temperature

		normalized_top_k_probs = torch.nn.functional.softmax(top_k_log_probs, dim=-1)

		sampled_indices = torch.multinomial(
			normalized_top_k_probs, per_node_beam_size, replacement=self.with_replacement
		)

		indices = top_k_indices.gather(-1, sampled_indices)

		return log_probs.gather(1, indices), indices, state


@Sampler.register("top-p")
class TopPSampler(Sampler):

	def __init__(
		self,
		p: float = 0.9,
		temperature: float = 1.0,
		with_replacement: bool = False,
	):
		if p < 0.0 or p > 1.0:
			raise ValueError("p must be a positive float no greater than 1.0")
		self.p = p
		self.temperature = temperature or 1.0
		self.with_replacement = with_replacement

	def sample_nodes(
		self, log_probs: torch.Tensor, per_node_beam_size: int, state: StateType
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:
		if not per_node_beam_size <= log_probs.size()[1]:
			raise ValueError("per_node_beam_size cannot be greater than vocabulary size")

		if self.temperature != 1.0:
			_log_probs = torch.nn.functional.log_softmax(log_probs / self.temperature, dim=-1)
		else:
			_log_probs = log_probs

		log_probs_descending, sorting_indices = torch.sort(_log_probs, descending=True)

		probabilities_descending = log_probs_descending.exp()
		probabilities_summed = torch.cumsum(probabilities_descending, dim=-1)

		exclusion_mask = probabilities_summed >= self.p

		exclusion_mask[..., 1:] = exclusion_mask[..., :-1].clone()
		exclusion_mask[..., 0] = False

		if not self.with_replacement:
			exclusion_mask[..., :per_node_beam_size] = False

		log_probs_descending[exclusion_mask] = min_value_of_dtype(log_probs.dtype)

		filtered_probabilities = torch.nn.functional.softmax(log_probs_descending, dim=-1)

		sampled_indices = torch.multinomial(
			filtered_probabilities, per_node_beam_size, replacement=self.with_replacement
		)

		selected_indices = sorting_indices.gather(-1, sampled_indices)

		return torch.gather(log_probs, 1, selected_indices), selected_indices, state


@Sampler.register("gumbel")
class GumbelSampler(Sampler):

	def __init__(self, temperature: float = 1.0):
		self.temperature = temperature

	def init_state(
		self, start_class_log_probabilities: torch.Tensor, batch_size: int, num_classes: int
	) -> StateType:
		zeros = start_class_log_probabilities.new_zeros((batch_size, num_classes))

		G_phi_S = self.gumbel_with_max(start_class_log_probabilities, zeros)

		return {"G_phi_S": G_phi_S}

	def sample_nodes(
		self,
		log_probs: torch.Tensor,
		per_node_beam_size: int,
		state: StateType,
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:
		if self.temperature != 1.0:
			_log_probs = torch.nn.functional.log_softmax(log_probs / self.temperature, dim=-1)
		else:
			_log_probs = log_probs

		phi_S = state["phi_S"]

		phi_S = phi_S.unsqueeze(-1).expand_as(_log_probs)

		phi_S_new = phi_S + _log_probs

		G_phi_S = state["G_phi_S"].unsqueeze(-1)

		G_phi_S_new = self.gumbel_with_max(phi_S_new, G_phi_S)


		top_G_phi_S_new, top_indices = torch.topk(G_phi_S_new, per_node_beam_size, dim=-1)

		top_log_probs = log_probs.gather(1, top_indices)

		return top_log_probs, top_indices, {"G_phi_S": top_G_phi_S_new}

	def sample_beams(
		self,
		log_probs: torch.Tensor,
		beam_size: int,
		state: StateType,
	) -> Tuple[torch.Tensor, torch.Tensor, StateType]:

		batch_size = log_probs.size()[0]

		G_phi_S = state["G_phi_S"]

		G_phi_S = G_phi_S.reshape_as(log_probs)

		G_phi_S_new, selected_indices = torch.topk(G_phi_S, beam_size, dim=-1)

		selected_log_probs = log_probs.gather(1, selected_indices)

		selected_log_probs, sort_indices = selected_log_probs.sort(dim=-1, descending=True)
		selected_indices = selected_indices.gather(1, sort_indices)
		G_phi_S_new = G_phi_S_new.gather(1, sort_indices)

		G_phi_S_new = G_phi_S_new.reshape(batch_size * beam_size)

		phi_S = selected_log_probs.reshape(batch_size * beam_size)

		return selected_log_probs, selected_indices, {"G_phi_S": G_phi_S_new, "phi_S": phi_S}

	def gumbel(self, phi) -> torch.Tensor:
		return -torch.log(-torch.log(torch.rand_like(phi))) + phi

	def gumbel_with_max(self, phi, T) -> torch.Tensor:
		G_phi = self.gumbel(phi)

		Z, _ = G_phi.max(dim=-1)

		v = T - G_phi + torch.log1p(-torch.exp(G_phi - Z.unsqueeze(-1)))

		return T - torch.nn.functional.relu(v) - torch.log1p(torch.exp(-v.abs()))


class FinalSequenceScorer(Registrable):

	default_implementation = "sequence-log-prob"

	def score(
		self, predictions: torch.Tensor, log_probabilities: torch.Tensor, end_index: int
	) -> torch.Tensor:
		raise NotImplementedError


@FinalSequenceScorer.register("sequence-log-prob")
class SequenceLogProbabilityScorer(FinalSequenceScorer):

	def score(
		self, predictions: torch.Tensor, log_probabilities: torch.Tensor, end_index: int
	) -> torch.Tensor:
		return log_probabilities


@FinalSequenceScorer.register("length-normalized-sequence-log-prob")
class LengthNormalizedSequenceLogProbabilityScorer(FinalSequenceScorer):

	def __init__(self, length_penalty: float = 1.0):
		super().__init__()
		self.length_penalty = length_penalty

	def score(
		self, predictions: torch.Tensor, log_probabilities: torch.Tensor, end_index: int
	) -> torch.Tensor:
		lengths = (predictions != end_index).long().sum(dim=2)

		is_end_token = predictions[:, :, -1] == end_index
		lengths += is_end_token.long()

		average_log_probs = log_probabilities / (lengths**self.length_penalty)
		return average_log_probs


class Constraint(Registrable):

	def __init__(self, vocab: Optional[Vocabulary] = None) -> None:
		self.vocab = vocab

	def init_state(
		self,
		batch_size: int,
	) -> ConstraintStateType:
		raise NotImplementedError

	def apply(
		self,
		state: ConstraintStateType,
		class_log_probabilities: torch.Tensor,
	) -> torch.Tensor:
		raise NotImplementedError

	@staticmethod
	def _copy_state(
		state: ConstraintStateType,
		batch_size: int,
		beam_size: int,
		last_backpointer: Optional[torch.Tensor] = None,
	) -> ConstraintStateType:
		new_state = []
		for i in range(batch_size):
			batch_state = []
			for j in range(beam_size):
				if last_backpointer is None:
					backpointer = 0
				else:
					backpointer = last_backpointer[i, j].item()
				batch_state.append(copy.deepcopy(state[i][backpointer]))
			new_state.append(batch_state)
		return new_state

	def update_state(
		self,
		state: ConstraintStateType,
		last_prediction: torch.Tensor,
		last_backpointer: Optional[torch.Tensor] = None,
	) -> ConstraintStateType:
		batch_size, beam_size = last_prediction.size()
		new_state = self._copy_state(state, batch_size, beam_size, last_backpointer)
		return self._update_state(new_state, last_prediction)

	def _update_state(
		self,
		state: ConstraintStateType,
		last_prediction: torch.Tensor,
	) -> ConstraintStateType:
		raise NotImplementedError


@Constraint.register("repeated-ngram-blocking")
class RepeatedNGramBlockingConstraint(Constraint):
	def __init__(self, ngram_size: int, **kwargs) -> None:
		super().__init__(**kwargs)
		self.ngram_size = ngram_size

	def init_state(
		self,
		batch_size: int,
	) -> ConstraintStateType:
		return [[{"seen_ngrams": {}, "current_prefix": []}] for _ in range(batch_size)]

	def apply(
		self,
		state: ConstraintStateType,
		class_log_probabilities: torch.Tensor,
	) -> torch.Tensor:
		for i, batch in enumerate(state):
			for j, beam in enumerate(batch):
				current_prefix = tuple(beam["current_prefix"])
				seen_ngrams = beam["seen_ngrams"]
				try:
					disallowed_indices = seen_ngrams[current_prefix]
					class_log_probabilities[i, j, disallowed_indices] = min_value_of_dtype(
						class_log_probabilities.dtype
					)
				except KeyError:
					pass
		return class_log_probabilities

	def _update_state(
		self,
		state: ConstraintStateType,
		last_prediction: torch.Tensor,
	) -> ConstraintStateType:
		for i, batch in enumerate(state):
			for j, beam in enumerate(batch):
				prediction = last_prediction[i, j].item()
				prefix = beam["current_prefix"]
				seen_ngrams = beam["seen_ngrams"]

				if len(prefix) == self.ngram_size - 1:
					if tuple(prefix) not in seen_ngrams:
						seen_ngrams[tuple(prefix)] = []
					seen_ngrams[tuple(prefix)].append(prediction)

				prefix.append(prediction)
				if len(prefix) == self.ngram_size:
					prefix.pop(0)
		return state


class BeamSearch(Registrable):

	default_implementation = "beam_search"

	def __init__(
		self,
		end_index: int,
		max_steps: int = 50,
		beam_size: int = 10,
		per_node_beam_size: int = None,
		sampler: Sampler = None,
		min_steps: Optional[int] = None,
		final_sequence_scorer: FinalSequenceScorer = None,
		constraints: Optional[List[Lazy[Constraint]]] = None,
		vocab: Optional[Vocabulary] = None,
	) -> None:
		if not max_steps > 0:
			raise ValueError("max_steps must be positive")
		if not beam_size > 0:
			raise ValueError("beam_size must be positive")
		if per_node_beam_size is not None and not per_node_beam_size > 0:
			raise ValueError("per_node_beam_size must be positive")
		if min_steps is not None:
			if not min_steps >= 0:
				raise ValueError("min_steps must be non-negative")
			if not min_steps <= max_steps:
				raise ValueError("min_steps must be less than or equal to max_steps")

		self._end_index = end_index
		self.max_steps = max_steps
		self.beam_size = beam_size
		self.per_node_beam_size = per_node_beam_size or beam_size
		self.sampler = sampler or DeterministicSampler()
		self.min_steps = min_steps or 0
		self.final_sequence_scorer = final_sequence_scorer or SequenceLogProbabilityScorer()
		self.constraints = [constraint.construct(vocab=vocab) for constraint in constraints or []]

	@staticmethod
	def _reconstruct_sequences(predictions, backpointers):
		reconstructed_predictions = [predictions[-1].unsqueeze(2)]

		if not backpointers:
			return reconstructed_predictions

		cur_backpointers = backpointers[-1]

		for timestep in range(len(predictions) - 2, 0, -1):
			cur_preds = predictions[timestep].gather(1, cur_backpointers).unsqueeze(2)

			reconstructed_predictions.append(cur_preds)

			cur_backpointers = backpointers[timestep - 1].gather(1, cur_backpointers)

		final_preds = predictions[0].gather(1, cur_backpointers).unsqueeze(2)

		reconstructed_predictions.append(final_preds)

		return reconstructed_predictions

	@torch.no_grad()
	def search(
		self,
		start_predictions: torch.Tensor,
		start_state: StateType,
		step: StepFunctionType,
	) -> Tuple[torch.Tensor, torch.Tensor]:
		step_signature = signature(step)
		if len(step_signature.parameters) < 3:
			old_step = cast(StepFunctionTypeNoTimestep, step)

			def new_step(
				last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], time_step: int
			):
				return old_step(last_predictions, state)

			return self._search(start_predictions, start_state, new_step)
		else:
			return self._search(
				start_predictions, start_state, cast(StepFunctionTypeWithTimestep, step)
			)

	def _search(
		self,
		start_predictions: torch.Tensor,
		start_state: StateType,
		step: StepFunctionTypeWithTimestep,
	) -> Tuple[torch.Tensor, torch.Tensor]:
		batch_size = start_predictions.size()[0]

		predictions: List[torch.Tensor] = []

		backpointers: List[torch.Tensor] = []

		constraint_states = [constraint.init_state(batch_size) for constraint in self.constraints]

		start_class_log_probabilities, state = step(start_predictions, start_state, 0)

		num_classes = start_class_log_probabilities.size()[1]

		if self.per_node_beam_size > num_classes:
			raise ConfigurationError(
				f"Target vocab size ({num_classes:d}) too small "
				f"relative to per_node_beam_size ({self.per_node_beam_size:d}).\n"
				f"Please decrease beam_size or per_node_beam_size."
			)

		sampler_state = self.sampler.init_state(
			start_class_log_probabilities, batch_size, num_classes
		)

		if self.constraints:
			expanded_start_class_log_probabilities = start_class_log_probabilities.unsqueeze(1)
			for constraint, constraint_state in zip(self.constraints, constraint_states):
				expanded_start_class_log_probabilities = constraint.apply(
					constraint_state, expanded_start_class_log_probabilities
				)
			start_class_log_probabilities = expanded_start_class_log_probabilities.squeeze(1)

		if self.min_steps >= 1:
			start_class_log_probabilities[:, self._end_index] = min_value_of_dtype(
				start_class_log_probabilities.dtype
			)

		(
			start_top_log_probabilities,
			start_predicted_classes,
			sampler_state,
		) = self.sampler.sample_beams(start_class_log_probabilities, self.beam_size, sampler_state)

		if self.beam_size == 1 and (start_predicted_classes == self._end_index).all():
			warnings.warn(
				"Empty sequences predicted. You may want to increase the beam size or ensure "
				"your step function is working properly.",
				RuntimeWarning,
			)
			return start_predicted_classes.unsqueeze(-1), start_top_log_probabilities

		last_log_probabilities = start_top_log_probabilities

		predictions.append(start_predicted_classes)

		log_probs_after_end = start_class_log_probabilities.new_full(
			(batch_size * self.beam_size, num_classes),
			min_value_of_dtype(start_class_log_probabilities.dtype),
		)
		log_probs_after_end[:, self._end_index] = 0.0

		self._update_initial_state(state, batch_size)

		for i, constraint in enumerate(self.constraints):
			constraint_states[i] = constraint.update_state(
				constraint_states[i], start_predicted_classes
			)

		for timestep in range(self.max_steps - 1):
			last_predictions = predictions[-1].reshape(batch_size * self.beam_size)

			if (last_predictions == self._end_index).all():
				break
			class_log_probabilities, state = step(last_predictions, state, timestep + 1)

			if self.constraints:
				reshaped_class_log_probabilities = class_log_probabilities.view(
					batch_size, self.beam_size, -1
				)
				for constraint, constraint_state in zip(self.constraints, constraint_states):
					reshaped_class_log_probabilities = constraint.apply(
						constraint_state, reshaped_class_log_probabilities
					)
				class_log_probabilities = reshaped_class_log_probabilities.view(
					batch_size * self.beam_size, -1
				)

			if timestep + 2 <= self.min_steps:
				class_log_probabilities[:, self._end_index] = min_value_of_dtype(
					class_log_probabilities.dtype
				)

			last_predictions_expanded = last_predictions.unsqueeze(-1).expand(
				batch_size * self.beam_size, num_classes
			)

			cleaned_log_probabilities = torch.where(
				last_predictions_expanded == self._end_index,
				log_probs_after_end,
				class_log_probabilities,
			)

			top_log_probabilities, predicted_classes, sampler_state = self.sampler.sample_nodes(
				cleaned_log_probabilities, self.per_node_beam_size, sampler_state
			)

			expanded_last_log_probabilities = (
				last_log_probabilities.unsqueeze(2)
				.expand(batch_size, self.beam_size, self.per_node_beam_size)
				.reshape(batch_size * self.beam_size, self.per_node_beam_size)
			)

			summed_top_log_probabilities = top_log_probabilities + expanded_last_log_probabilities

			reshaped_summed = summed_top_log_probabilities.reshape(
				batch_size, self.beam_size * self.per_node_beam_size
			)

			reshaped_predicted_classes = predicted_classes.reshape(
				batch_size, self.beam_size * self.per_node_beam_size
			)

			(
				restricted_beam_log_probs,
				restricted_beam_indices,
				sampler_state,
			) = self.sampler.sample_beams(reshaped_summed, self.beam_size, sampler_state)

			restricted_predicted_classes = reshaped_predicted_classes.gather(
				1, restricted_beam_indices
			)

			predictions.append(restricted_predicted_classes)

			last_log_probabilities = restricted_beam_log_probs

			backpointer = torch.divide(
				restricted_beam_indices, self.per_node_beam_size, rounding_mode="trunc"
			)
			backpointers.append(backpointer)

			self._update_state(state, backpointer)

			for i, constraint in enumerate(self.constraints):
				constraint_states[i] = constraint.update_state(
					constraint_states[i], restricted_predicted_classes, last_backpointer=backpointer
				)

		if not self.constraints and (
			not torch.isfinite(last_log_probabilities).all()
			or (last_log_probabilities == min_value_of_dtype(last_log_probabilities.dtype)).any()
		):
			warnings.warn(
				"Negligible log probabilities encountered ('-inf' or equivalent). "
				"Some final sequences may not make sense. "
				"This can happen when the beam size is larger than the number of valid (non-zero "
				"probability) transitions that the step function produces.",
				RuntimeWarning,
			)

		reconstructed_predictions = self._reconstruct_sequences(predictions, backpointers)

		all_predictions = torch.cat(list(reversed(reconstructed_predictions)), 2)

		final_scores = self.final_sequence_scorer.score(
			all_predictions, last_log_probabilities, self._end_index
		)

		sorted_final_scores, sorted_indices = torch.sort(final_scores, dim=1, descending=True)
		sorted_all_predictions = torch.gather(
			all_predictions, 1, sorted_indices.unsqueeze(-1).expand_as(all_predictions)
		)

		return sorted_all_predictions, sorted_final_scores

	@staticmethod
	def _is_multilayer_rnn_decoder(key: str, state_tensor: torch.Tensor) -> bool:
		return state_tensor.dim() == 3 and key in {
			"decoder_hidden",
			"decoder_context",
		}

	def _update_initial_state(self, state: StateType, batch_size: int):
		for key, state_tensor in state.items():
			if state_tensor is None:
				continue
			multilayer_rnn_decoder = self._is_multilayer_rnn_decoder(key, state_tensor)

			if multilayer_rnn_decoder:
				num_layers, _, *last_dims = state_tensor.size()
				state[key] = (
					state_tensor.unsqueeze(2)
					.expand(num_layers, batch_size, self.beam_size, *last_dims)
					.reshape(num_layers, batch_size * self.beam_size, *last_dims)
				)
			else:
				_, *last_dims = state_tensor.size()
				state[key] = (
					state_tensor.unsqueeze(1)
					.expand(batch_size, self.beam_size, *last_dims)
					.reshape(batch_size * self.beam_size, *last_dims)
				)

	def _update_state(self, state: StateType, backpointer: torch.Tensor):
		batch_size = backpointer.size()[0]

		for key, state_tensor in state.items():
			if state_tensor is None:
				continue
			multilayer_rnn_decoder = self._is_multilayer_rnn_decoder(key, state_tensor)

			if multilayer_rnn_decoder:
				num_layers, _, *last_dims = state_tensor.size()
				expanded_backpointer = backpointer.view(
					batch_size, self.beam_size, *([1] * len(last_dims))
				).expand(batch_size, self.beam_size, *last_dims)
				expanded_backpointer = expanded_backpointer.unsqueeze(0).repeat(num_layers, 1, 1, 1)
				state[key] = (
					state_tensor.reshape(num_layers, batch_size, self.beam_size, *last_dims)
					.gather(2, expanded_backpointer)
					.reshape(num_layers, batch_size * self.beam_size, *last_dims)
				)
			else:
				_, *last_dims = state_tensor.size()
				expanded_backpointer = backpointer.view(
					batch_size, self.beam_size, *([1] * len(last_dims))
				).expand(batch_size, self.beam_size, *last_dims)
				state[key] = (
					state_tensor.reshape(batch_size, self.beam_size, *last_dims)
					.gather(1, expanded_backpointer)
					.reshape(batch_size * self.beam_size, *last_dims)
				)


BeamSearch.register("beam_search")(BeamSearch)

import glob
import logging
import os
from typing import Iterable


from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path
from allennlp.data.dataset_readers.dataset_reader import DatasetReader, PathOrStr
from allennlp.data.instance import Instance


logger = logging.getLogger(__name__)


@DatasetReader.register("sharded")
class ShardedDatasetReader(DatasetReader):

	def __init__(self, base_reader: DatasetReader, **kwargs) -> None:
		super().__init__(
			manual_distributed_sharding=True, manual_multiprocess_sharding=True, **kwargs
		)
		self.reader = base_reader
		self.reader._set_worker_info(None)
		self.reader._set_distributed_info(None)

	def text_to_instance(self, *args, **kwargs) -> Instance:
		return self.reader.text_to_instance(*args, **kwargs)  # type: ignore

	def apply_token_indexers(self, instance: Instance) -> None:
		self.reader.apply_token_indexers(instance)

	def _read(self, file_path: PathOrStr) -> Iterable[Instance]:
		try:
			maybe_extracted_archive = cached_path(file_path, extract_archive=True)
			if not os.path.isdir(maybe_extracted_archive):
				raise ConfigurationError(f"{file_path} should be an archive or directory")
			shards = [
				os.path.join(maybe_extracted_archive, p)
				for p in os.listdir(maybe_extracted_archive)
				if not p.startswith(".")
			]
			if not shards:
				raise ConfigurationError(f"No files found in {file_path}")
		except FileNotFoundError:
			shards = glob.glob(str(file_path))
			if not shards:
				raise ConfigurationError(f"No files found matching {file_path}")

		shards.sort()

		for shard in self.shard_iterable(shards):
			logger.info(f"reading instances from {shard}")
			for instance in self.reader._read(shard):
				yield instance

import torch.utils.data as data

from os import listdir
from os.path import join
from PIL import Image


def is_image_file(filename):
	return any(filename.endswith(extension) for extension in [".png", ".jpg", ".jpeg"])


def load_img(filepath):
	img = Image.open(filepath).convert('YCbCr')
	y, _, _ = img.split()
	return y


class DatasetFromFolder(data.Dataset):
	def __init__(self, image_dir, input_transform=None, target_transform=None):
		super(DatasetFromFolder, self).__init__()
		self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]

		self.input_transform = input_transform
		self.target_transform = target_transform

	def __getitem__(self, index):
		input = load_img(self.image_filenames[index])
		target = input.copy()
		if self.input_transform:
			input = self.input_transform(input)
		if self.target_transform:
			target = self.target_transform(target)

		return input, target

	def __len__(self):
		return len(self.image_filenames)

import random

import torch
import torch.distributed as dist
import torch.distributed.autograd as dist_autograd
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.optim as optim
from torch.distributed.nn import RemoteModule
from torch.distributed.optim import DistributedOptimizer
from torch.distributed.rpc import RRef
from torch.distributed.rpc import TensorPipeRpcBackendOptions
from torch.nn.parallel import DistributedDataParallel as DDP

NUM_EMBEDDINGS = 100
EMBEDDING_DIM = 16


class HybridModel(torch.nn.Module):

	def __init__(self, remote_emb_module, device):
		super(HybridModel, self).__init__()
		self.remote_emb_module = remote_emb_module
		self.fc = DDP(torch.nn.Linear(16, 8).cuda(device), device_ids=[device])
		self.device = device

	def forward(self, indices, offsets):
		emb_lookup = self.remote_emb_module.forward(indices, offsets)
		return self.fc(emb_lookup.cuda(self.device))


def _run_trainer(remote_emb_module, rank):

	model = HybridModel(remote_emb_module, rank)


	model_parameter_rrefs = model.remote_emb_module.remote_parameters()

	for param in model.fc.parameters():
		model_parameter_rrefs.append(RRef(param))

	opt = DistributedOptimizer(
		optim.SGD,
		model_parameter_rrefs,
		lr=0.05,
	)

	criterion = torch.nn.CrossEntropyLoss()

	def get_next_batch(rank):
		for _ in range(10):
			num_indices = random.randint(20, 50)
			indices = torch.LongTensor(num_indices).random_(0, NUM_EMBEDDINGS)

			offsets = []
			start = 0
			batch_size = 0
			while start < num_indices:
				offsets.append(start)
				start += random.randint(1, 10)
				batch_size += 1

			offsets_tensor = torch.LongTensor(offsets)
			target = torch.LongTensor(batch_size).random_(8).cuda(rank)
			yield indices, offsets_tensor, target

	for epoch in range(100):
		for indices, offsets, target in get_next_batch(rank):
			with dist_autograd.context() as context_id:
				output = model(indices, offsets)
				loss = criterion(output, target)

				dist_autograd.backward(context_id, [loss])

				opt.step(context_id)

		print("Training done for epoch {}".format(epoch))


def run_worker(rank, world_size):

	rpc_backend_options = TensorPipeRpcBackendOptions()
	rpc_backend_options.init_method = "tcp://localhost:29501"

	if rank == 2:
		rpc.init_rpc(
			"master",
			rank=rank,
			world_size=world_size,
			rpc_backend_options=rpc_backend_options,
		)

		remote_emb_module = RemoteModule(
			"ps",
			torch.nn.EmbeddingBag,
			args=(NUM_EMBEDDINGS, EMBEDDING_DIM),
			kwargs={"mode": "sum"},
		)

		futs = []
		for trainer_rank in [0, 1]:
			trainer_name = "trainer{}".format(trainer_rank)
			fut = rpc.rpc_async(
				trainer_name, _run_trainer, args=(remote_emb_module, trainer_rank)
			)
			futs.append(fut)

		for fut in futs:
			fut.wait()
	elif rank <= 1:
		dist.init_process_group(
			backend="gloo", rank=rank, world_size=2, init_method="tcp://localhost:29500"
		)

		trainer_name = "trainer{}".format(rank)
		rpc.init_rpc(
			trainer_name,
			rank=rank,
			world_size=world_size,
			rpc_backend_options=rpc_backend_options,
		)

	else:
		rpc.init_rpc(
			"ps",
			rank=rank,
			world_size=world_size,
			rpc_backend_options=rpc_backend_options,
		)
		pass

	rpc.shutdown()


if __name__ == "__main__":
	world_size = 4
	mp.spawn(run_worker, args=(world_size,), nprocs=world_size, join=True)

import os
import time
import glob

import torch
import torch.optim as O
import torch.nn as nn

from torchtext.legacy import data
from torchtext.legacy import datasets

from model import SNLIClassifier
from util import get_args, makedirs


args = get_args()
if torch.cuda.is_available():
	torch.cuda.set_device(args.gpu)
	device = torch.device('cuda:{}'.format(args.gpu))
elif torch.backends.mps.is_available():
	device = torch.device('mps')
else:
	device = torch.device('cpu')

inputs = data.Field(lower=args.lower, tokenize='spacy')
answers = data.Field(sequential=False)

train, dev, test = datasets.SNLI.splits(inputs, answers)

inputs.build_vocab(train, dev, test)
if args.word_vectors:
	if os.path.isfile(args.vector_cache):
		inputs.vocab.vectors = torch.load(args.vector_cache)
	else:
		inputs.vocab.load_vectors(args.word_vectors)
		makedirs(os.path.dirname(args.vector_cache))
		torch.save(inputs.vocab.vectors, args.vector_cache)
answers.build_vocab(train)

train_iter, dev_iter, test_iter = data.BucketIterator.splits(
			(train, dev, test), batch_size=args.batch_size, device=device)

config = args
config.n_embed = len(inputs.vocab)
config.d_out = len(answers.vocab)
config.n_cells = config.n_layers

if config.birnn:
	config.n_cells *= 2

if args.resume_snapshot:
	model = torch.load(args.resume_snapshot, map_location=device)
else:
	model = SNLIClassifier(config)
	if args.word_vectors:
		model.embed.weight.data.copy_(inputs.vocab.vectors)
		model.to(device)

criterion = nn.CrossEntropyLoss()
opt = O.Adam(model.parameters(), lr=args.lr)

iterations = 0
start = time.time()
best_dev_acc = -1
header = '  Time Epoch Iteration Progress	(%Epoch)   Loss   Dev/Loss	 Accuracy  Dev/Accuracy'
dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))
log_template =	 ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))
makedirs(args.save_path)
print(header)

for epoch in range(args.epochs):
	train_iter.init_epoch()
	n_correct, n_total = 0, 0
	for batch_idx, batch in enumerate(train_iter):

		model.train(); opt.zero_grad()

		iterations += 1

		answer = model(batch)

		n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()
		n_total += batch.batch_size
		train_acc = 100. * n_correct/n_total

		loss = criterion(answer, batch.label)

		loss.backward(); opt.step()

		if iterations % args.save_every == 0:
			snapshot_prefix = os.path.join(args.save_path, 'snapshot')
			snapshot_path = snapshot_prefix + '_acc_{:.4f}_loss_{:.6f}_iter_{}_model.pt'.format(train_acc, loss.item(), iterations)
			torch.save(model, snapshot_path)
			for f in glob.glob(snapshot_prefix + '*'):
				if f != snapshot_path:
					os.remove(f)

		if iterations % args.dev_every == 0:

			model.eval(); dev_iter.init_epoch()

			n_dev_correct, dev_loss = 0, 0
			with torch.no_grad():
				for dev_batch_idx, dev_batch in enumerate(dev_iter):
					 answer = model(dev_batch)
					 n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()) == dev_batch.label).sum().item()
					 dev_loss = criterion(answer, dev_batch.label)
			dev_acc = 100. * n_dev_correct / len(dev)

			print(dev_log_template.format(time.time()-start,
				epoch, iterations, 1+batch_idx, len(train_iter),
				100. * (1+batch_idx) / len(train_iter), loss.item(), dev_loss.item(), train_acc, dev_acc))

			if dev_acc > best_dev_acc:


				best_dev_acc = dev_acc
				snapshot_prefix = os.path.join(args.save_path, 'best_snapshot')
				snapshot_path = snapshot_prefix + '_devacc_{}_devloss_{}__iter_{}_model.pt'.format(dev_acc, dev_loss.item(), iterations)

				torch.save(model, snapshot_path)
				for f in glob.glob(snapshot_prefix + '*'):
					if f != snapshot_path:
						os.remove(f)

		elif iterations % args.log_every == 0:

			print(log_template.format(time.time()-start,
				epoch, iterations, 1+batch_idx, len(train_iter),
				100. * (1+batch_idx) / len(train_iter), loss.item(), ' '*8, n_correct/n_total*100, ' '*12))
		if args.dry_run:
			break

from typing import List

from allennlp.common import Registrable
from allennlp.common.util import JsonDict
from allennlp.predictors import Predictor


class Attacker(Registrable):

	def __init__(self, predictor: Predictor) -> None:
		self.predictor = predictor

	def initialize(self):
		pass

	def attack_from_json(
		self,
		inputs: JsonDict,
		input_field_to_attack: str,
		grad_input_field: str,
		ignore_tokens: List[str],
		target: JsonDict,
	) -> JsonDict:
		raise NotImplementedError()

from typing import Union, Dict, Any, Optional
from os import PathLike
from pathlib import Path
import torch
import logging

from allennlp.common.checks import check_for_gpu
from allennlp.common.tqdm import Tqdm
from allennlp.common.util import dump_metrics, int_to_device
from allennlp.nn import util as nn_util
from allennlp.common import Registrable
from allennlp.models import Model
from allennlp.data import DataLoader
from allennlp.evaluation.serializers.serializers import Serializer, SimpleSerializer

logger = logging.getLogger(__name__)


class Evaluator(Registrable):

	default_implementation = "simple"

	def __init__(
		self,
		batch_serializer: Optional[Serializer] = None,
		cuda_device: Union[int, torch.device] = -1,
		postprocessor_fn_name: str = "make_output_human_readable",
	):
		self.batch_serializer = batch_serializer or SimpleSerializer()
		self.cuda_device = cuda_device
		self.postprocessor_fn_name = postprocessor_fn_name

	def __call__(
		self,
		model: Model,
		data_loader: DataLoader,
		batch_weight_key: str = None,
		metrics_output_file: Union[str, PathLike] = None,
		predictions_output_file: Union[str, PathLike] = None,
	) -> Dict[str, Any]:
		raise NotImplementedError("__call__")


@Evaluator.register("simple")
class SimpleEvaluator(Evaluator):

	def __init__(
		self,
		batch_serializer: Optional[Serializer] = None,
		cuda_device: Union[int, torch.device] = -1,
		postprocessor_fn_name: str = "make_output_human_readable",
	):
		super(SimpleEvaluator, self).__init__(batch_serializer, cuda_device, postprocessor_fn_name)

	def __call__(
		self,
		model: Model,
		data_loader: DataLoader,
		batch_weight_key: str = None,
		metrics_output_file: Union[str, PathLike] = None,
		predictions_output_file: Union[str, PathLike] = None,
	):
		check_for_gpu(self.cuda_device)
		data_loader.set_target_device(int_to_device(self.cuda_device))
		metrics_output_file = Path(metrics_output_file) if metrics_output_file is not None else None
		if predictions_output_file is not None:
			predictions_file = Path(predictions_output_file).open("w", encoding="utf-8")
		else:
			predictions_file = None  # type: ignore

		model_postprocess_function = getattr(model, self.postprocessor_fn_name, None)

		with torch.no_grad():
			model.eval()

			iterator = iter(data_loader)
			logger.info("Iterating over dataset")
			generator_tqdm = Tqdm.tqdm(iterator)
			batch_count = 0
			loss_count = 0
			total_loss = 0.0
			total_weight = 0.0

			for batch in generator_tqdm:
				batch_count += 1
				batch = nn_util.move_to_device(batch, self.cuda_device)
				output_dict = model(**batch)
				loss = output_dict.get("loss")

				metrics = model.get_metrics()

				if loss is not None:
					loss_count += 1
					if batch_weight_key:
						weight = output_dict[batch_weight_key].item()
					else:
						weight = 1.0

					total_weight += weight
					total_loss += loss.item() * weight
					metrics["loss"] = total_loss / total_weight

				description = (
					", ".join(
						[
							"%s: %.2f" % (name, value)
							for name, value in metrics.items()
							if not name.startswith("_")
						]
					)
					+ " ||"
				)
				generator_tqdm.set_description(description, refresh=False)

				if predictions_file is not None:
					predictions_file.write(
						self.batch_serializer(
							batch,
							output_dict,
							data_loader,
							output_postprocess_function=model_postprocess_function,
						)
						+ "\n"
					)

			if predictions_file is not None:
				predictions_file.close()

			final_metrics = model.get_metrics(reset=True)
			if loss_count > 0:
				if loss_count != batch_count:
					raise RuntimeError(
						"The model you are trying to evaluate only sometimes produced a loss!"
					)
				final_metrics["loss"] = total_loss / total_weight

			if metrics_output_file is not None:
				dump_metrics(str(metrics_output_file), final_metrics, log=True)

			return final_metrics

	def _to_params(self) -> Dict[str, Any]:
		return {
			"type": "simple",
			"cuda_device": self.cuda_device,
			"batch_postprocessor": self.batch_serializer.to_params(),
		}


from allennlp.training.metrics.attachment_scores import AttachmentScores
from allennlp.training.metrics.average import Average
from allennlp.training.metrics.boolean_accuracy import BooleanAccuracy
from allennlp.training.metrics.bleu import BLEU
from allennlp.training.metrics.rouge import ROUGE
from allennlp.training.metrics.categorical_accuracy import CategoricalAccuracy
from allennlp.training.metrics.covariance import Covariance
from allennlp.training.metrics.entropy import Entropy
from allennlp.training.metrics.evalb_bracketing_scorer import (
	EvalbBracketingScorer,
	DEFAULT_EVALB_DIR,
)
from allennlp.training.metrics.fbeta_measure import FBetaMeasure
from allennlp.training.metrics.fbeta_verbose_measure import FBetaVerboseMeasure
from allennlp.training.metrics.fbeta_multi_label_measure import (
	FBetaMultiLabelMeasure,
	F1MultiLabelMeasure,
)
from allennlp.training.metrics.f1_measure import F1Measure
from allennlp.training.metrics.mean_absolute_error import MeanAbsoluteError
from allennlp.training.metrics.metric import Metric
from allennlp.training.metrics.pearson_correlation import PearsonCorrelation
from allennlp.training.metrics.spearman_correlation import SpearmanCorrelation
from allennlp.training.metrics.perplexity import Perplexity
from allennlp.training.metrics.sequence_accuracy import SequenceAccuracy
from allennlp.training.metrics.span_based_f1_measure import SpanBasedF1Measure
from allennlp.training.metrics.unigram_recall import UnigramRecall
from allennlp.training.metrics.auc import Auc


import logging
import math
import os
import sys
import warnings
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional

import datasets
import evaluate
import torch
from datasets import load_dataset

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_FOR_CAUSAL_LM_MAPPING,
	AutoConfig,
	AutoModelForCausalLM,
	AutoTokenizer,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	default_data_collator,
	is_torch_tpu_available,
	set_seed,
)
from transformers.testing_utils import CaptureLogger
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/language-modeling/requirements.txt")

logger = logging.getLogger(__name__)


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:

	model_name_or_path: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch."
			)
		},
	)
	model_type: Optional[str] = field(
		default=None,
		metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
	)
	config_overrides: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"Override some existing default config settings when a model is trained from scratch. Example: "
				"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
			)
		},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	torch_dtype: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
				"dtype will be automatically derived from the model's weights."
			),
			"choices": ["auto", "bfloat16", "float16", "float32"],
		},
	)
	low_cpu_mem_usage: bool = field(
		default=False,
		metadata={
			"help": (
				"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. "
				"set True will benefit LLM loading time and RAM consumption."
			)
		},
	)

	def __post_init__(self):
		if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
			raise ValueError(
				"--config_overrides can't be used in combination with --config_name or --model_name_or_path"
			)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
	block_size: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"Optional input sequence length after tokenization. "
				"The training dataset will be truncated in block of this size for training. "
				"Default to the model max input length for single sentence inputs (take into account special tokens)."
			)
		},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	validation_split_percentage: Optional[int] = field(
		default=5,
		metadata={
			"help": "The percentage of the train set used as validation set in case there's no validation split"
		},
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	keep_linebreaks: bool = field(
		default=True, metadata={"help": "Whether to keep line breaks when using TXT files or not."}
	)

	def __post_init__(self):
		if self.streaming:
			require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")

		if self.dataset_name is None and self.train_file is None and self.validation_file is None:
			raise ValueError("Need either a dataset name or a training/validation file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json", "txt"], "`train_file` should be a csv, a json or a txt file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json", "txt"], "`validation_file` should be a csv, a json or a txt file."


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_clm", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
			streaming=data_args.streaming,
		)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				data_args.dataset_name,
				data_args.dataset_config_name,
				split=f"train[:{data_args.validation_split_percentage}%]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
				streaming=data_args.streaming,
			)
			raw_datasets["train"] = load_dataset(
				data_args.dataset_name,
				data_args.dataset_config_name,
				split=f"train[{data_args.validation_split_percentage}%:]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
				streaming=data_args.streaming,
			)
	else:
		data_files = {}
		dataset_args = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
		extension = (
			data_args.train_file.split(".")[-1]
			if data_args.train_file is not None
			else data_args.validation_file.split(".")[-1]
		)
		if extension == "txt":
			extension = "text"
			dataset_args["keep_linebreaks"] = data_args.keep_linebreaks
		raw_datasets = load_dataset(
			extension,
			data_files=data_files,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
			**dataset_args,
		)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[:{data_args.validation_split_percentage}%]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
				**dataset_args,
			)
			raw_datasets["train"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[{data_args.validation_split_percentage}%:]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
				**dataset_args,
			)



	config_kwargs = {
		"cache_dir": model_args.cache_dir,
		"revision": model_args.model_revision,
		"token": model_args.token,
		"trust_remote_code": model_args.trust_remote_code,
	}
	if model_args.config_name:
		config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
	elif model_args.model_name_or_path:
		config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
	else:
		config = CONFIG_MAPPING[model_args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")
		if model_args.config_overrides is not None:
			logger.info(f"Overriding config: {model_args.config_overrides}")
			config.update_from_string(model_args.config_overrides)
			logger.info(f"New config: {config}")

	tokenizer_kwargs = {
		"cache_dir": model_args.cache_dir,
		"use_fast": model_args.use_fast_tokenizer,
		"revision": model_args.model_revision,
		"token": model_args.token,
		"trust_remote_code": model_args.trust_remote_code,
	}
	if model_args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
	elif model_args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if model_args.model_name_or_path:
		torch_dtype = (
			model_args.torch_dtype
			if model_args.torch_dtype in ["auto", None]
			else getattr(torch, model_args.torch_dtype)
		)
		model = AutoModelForCausalLM.from_pretrained(
			model_args.model_name_or_path,
			from_tf=bool(".ckpt" in model_args.model_name_or_path),
			config=config,
			cache_dir=model_args.cache_dir,
			revision=model_args.model_revision,
			token=model_args.token,
			trust_remote_code=model_args.trust_remote_code,
			torch_dtype=torch_dtype,
			low_cpu_mem_usage=model_args.low_cpu_mem_usage,
		)
	else:
		model = AutoModelForCausalLM.from_config(config, trust_remote_code=model_args.trust_remote_code)
		n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
		logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if training_args.do_train:
		column_names = list(raw_datasets["train"].features)
	else:
		column_names = list(raw_datasets["validation"].features)
	text_column_name = "text" if "text" in column_names else column_names[0]

	tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

	def tokenize_function(examples):
		with CaptureLogger(tok_logger) as cl:
			output = tokenizer(examples[text_column_name])
		if "Token indices sequence length is longer than the" in cl.out:
			tok_logger.warning(
				"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
				" before being passed to the model."
			)
		return output

	with training_args.main_process_first(desc="dataset map tokenization"):
		if not data_args.streaming:
			tokenized_datasets = raw_datasets.map(
				tokenize_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on dataset",
			)
		else:
			tokenized_datasets = raw_datasets.map(
				tokenize_function,
				batched=True,
				remove_columns=column_names,
			)
	if hasattr(config, "max_position_embeddings"):
		max_pos_embeddings = config.max_position_embeddings
	else:
		max_pos_embeddings = 1024

	if data_args.block_size is None:
		block_size = tokenizer.model_max_length
		if block_size > max_pos_embeddings:
			logger.warning(
				f"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). "
				f"Using block_size={min(1024, max_pos_embeddings)} instead. You can change that default value by passing --block_size xxx."
			)
			if max_pos_embeddings > 0:
				block_size = min(1024, max_pos_embeddings)
			else:
				block_size = 1024
	else:
		if data_args.block_size > tokenizer.model_max_length:
			logger.warning(
				f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model "
				f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
			)
		block_size = min(data_args.block_size, tokenizer.model_max_length)

	def group_texts(examples):
		concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
		total_length = len(concatenated_examples[list(examples.keys())[0]])
		total_length = (total_length // block_size) * block_size
		result = {
			k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
			for k, t in concatenated_examples.items()
		}
		result["labels"] = result["input_ids"].copy()
		return result


	with training_args.main_process_first(desc="grouping texts together"):
		if not data_args.streaming:
			lm_datasets = tokenized_datasets.map(
				group_texts,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				load_from_cache_file=not data_args.overwrite_cache,
				desc=f"Grouping texts in chunks of {block_size}",
			)
		else:
			lm_datasets = tokenized_datasets.map(
				group_texts,
				batched=True,
			)

	if training_args.do_train:
		if "train" not in tokenized_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = lm_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	if training_args.do_eval:
		if "validation" not in tokenized_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_dataset = lm_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

		def preprocess_logits_for_metrics(logits, labels):
			if isinstance(logits, tuple):
				logits = logits[0]
			return logits.argmax(dim=-1)

		metric = evaluate.load("accuracy", cache_dir=model_args.cache_dir)

		def compute_metrics(eval_preds):
			preds, labels = eval_preds
			labels = labels[:, 1:].reshape(-1)
			preds = preds[:, :-1].reshape(-1)
			return metric.compute(predictions=preds, references=labels)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=default_data_collator,
		compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
		preprocess_logits_for_metrics=preprocess_logits_for_metrics
		if training_args.do_eval and not is_torch_tpu_available()
		else None,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload

		metrics = train_result.metrics

		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")

		metrics = trainer.evaluate()

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
		try:
			perplexity = math.exp(metrics["eval_loss"])
		except OverflowError:
			perplexity = float("inf")
		metrics["perplexity"] = perplexity

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "text-generation"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

import torch.nn as nn


class TokenEmbedding(nn.Embedding):
	def __init__(self, vocab_size, embed_size=512):
		super().__init__(vocab_size, embed_size, padding_idx=0)

from typing import List, Optional, Tuple


import torch

from allennlp.nn.util import (
	_check_incompatible_keys,
	_IncompatibleKeys,
	StateDictType,
	load_state_dict_distributed,
)


class Module(torch.nn.Module):

	def _post_load_state_dict(
		self, missing_keys: List[str], unexpected_keys: List[str]
	) -> Tuple[List[str], List[str]]:
		return missing_keys, unexpected_keys

	def load_state_dict(self, state_dict: StateDictType, strict: bool = True) -> _IncompatibleKeys:
		missing_keys, unexpected_keys = super().load_state_dict(state_dict, strict=False)  # type: ignore[arg-type]
		missing_keys, unexpected_keys = self._post_load_state_dict(missing_keys, unexpected_keys)
		_check_incompatible_keys(self, missing_keys, unexpected_keys, strict)
		return _IncompatibleKeys(missing_keys, unexpected_keys)

	def load_state_dict_distributed(
		self, state_dict: Optional[StateDictType], strict: bool = True
	) -> _IncompatibleKeys:
		missing_keys, unexpected_keys = load_state_dict_distributed(self, state_dict, strict=strict)
		missing_keys, unexpected_keys = self._post_load_state_dict(missing_keys, unexpected_keys)
		_check_incompatible_keys(self, missing_keys, unexpected_keys, strict)
		return _IncompatibleKeys(missing_keys, unexpected_keys)

import torch.nn as nn
from .gelu import GELU


class PositionwiseFeedForward(nn.Module):
	"Implements FFN equation."

	def __init__(self, d_model, d_ff, dropout=0.1):
		super(PositionwiseFeedForward, self).__init__()
		self.w_1 = nn.Linear(d_model, d_ff)
		self.w_2 = nn.Linear(d_ff, d_model)
		self.dropout = nn.Dropout(dropout)
		self.activation = GELU()

	def forward(self, x):
		return self.w_2(self.dropout(self.activation(self.w_1(x))))

import math

import torch
from torch.nn import Parameter


from allennlp.nn import util
from allennlp.nn.activations import Activation
from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention


@MatrixAttention.register("linear")
class LinearMatrixAttention(MatrixAttention):

	def __init__(
		self,
		tensor_1_dim: int,
		tensor_2_dim: int,
		combination: str = "x,y",
		activation: Activation = None,
	) -> None:
		super().__init__()
		self._combination = combination
		combined_dim = util.get_combined_dim(combination, [tensor_1_dim, tensor_2_dim])
		self._weight_vector = Parameter(torch.Tensor(combined_dim))
		self._bias = Parameter(torch.Tensor(1))
		self._activation = activation or Activation.by_name("linear")()
		self.reset_parameters()

	def reset_parameters(self):
		std = math.sqrt(6 / (self._weight_vector.size(0) + 1))
		self._weight_vector.data.uniform_(-std, std)
		self._bias.data.fill_(0)

	def forward(self, matrix_1: torch.Tensor, matrix_2: torch.Tensor) -> torch.Tensor:
		combined_tensors = util.combine_tensors_and_multiply(
			self._combination, [matrix_1.unsqueeze(2), matrix_2.unsqueeze(1)], self._weight_vector
		)
		return self._activation(combined_tensors + self._bias)

from typing import Optional, List, TYPE_CHECKING

import torch

from allennlp.common import FromParams
from allennlp.modules.util import replicate_layers
from allennlp.modules.transformer.transformer_layer import TransformerLayer
from allennlp.modules.transformer.bimodal_connection_layer import BiModalConnectionLayer
from allennlp.modules.transformer.transformer_module import TransformerModule

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig


class BiModalEncoder(TransformerModule, FromParams):

	_pretrained_mapping = {"layer": "layers1"}
	_pretrained_relevant_module = ["encoder", "bert.encoder"]
	_pretrained_allow_missing = [r"^layers2\..*", r"^c_layer\..*"]

	def __init__(
		self,
		num_hidden_layers1: int = 12,
		num_hidden_layers2: int = 12,
		hidden_size1: int = 1024,
		hidden_size2: int = 1024,
		combined_hidden_size: int = 1024,
		intermediate_size1: int = 1024,
		intermediate_size2: int = 1024,
		num_attention_heads1: int = 8,
		num_attention_heads2: int = 8,
		combined_num_attention_heads: int = 8,
		attention_dropout1: float = 0.1,
		hidden_dropout1: float = 0.1,
		attention_dropout2: float = 0.1,
		hidden_dropout2: float = 0.1,
		activation: str = "relu",
		biattention_id1: Optional[List[int]] = None,
		biattention_id2: Optional[List[int]] = None,
		fixed_layer1: int = 0,
		fixed_layer2: int = 0,
		fast_mode: bool = False,
		with_coattention: bool = True,
		in_batch_pairs: bool = False,
	):
		super().__init__()

		self.FAST_MODE = fast_mode
		self.with_coattention = with_coattention
		self.biattention_id1 = biattention_id1 or [1]
		self.biattention_id2 = biattention_id2 or [1]
		self.in_batch_pairs = in_batch_pairs
		self.fixed_layer1 = fixed_layer1
		self.fixed_layer2 = fixed_layer2
		self.combined_size = combined_hidden_size
		self.hidden_size1 = hidden_size1
		self.hidden_size2 = hidden_size2

		layer1 = TransformerLayer(
			hidden_size=hidden_size1,
			intermediate_size=intermediate_size1,
			num_attention_heads=num_attention_heads1,
			attention_dropout=attention_dropout1,
			hidden_dropout=hidden_dropout1,
			activation=activation,
		)
		layer2 = TransformerLayer(
			hidden_size=hidden_size2,
			intermediate_size=intermediate_size2,
			num_attention_heads=num_attention_heads2,
			attention_dropout=attention_dropout2,
			hidden_dropout=hidden_dropout2,
			activation=activation,
		)
		connect_layer = BiModalConnectionLayer(
			hidden_size1=hidden_size1,
			hidden_size2=hidden_size2,
			combined_hidden_size=combined_hidden_size,
			intermediate_size1=intermediate_size1,
			intermediate_size2=intermediate_size2,
			num_attention_heads=combined_num_attention_heads,
			dropout1=hidden_dropout1,
			dropout2=hidden_dropout2,
			activation=activation,
		)

		self.layers1 = replicate_layers(layer1, num_hidden_layers1)
		self.layers2 = replicate_layers(layer2, num_hidden_layers2)
		self.c_layer = replicate_layers(connect_layer, len(self.biattention_id2))

	def forward(
		self,
		embedding1,
		embedding2,
		attention_mask1,
		attention_mask2,
		co_attention_mask=None,
		output_all_encoded_layers=True,
	):
		start1 = 0
		start2 = 0
		count = 0
		all_encoder_layers1 = []
		all_encoder_layers2 = []

		batch_size, num_words, hidden_size1 = embedding1.size()
		_, num_regions, hidden_size2 = embedding2.size()

		for layer_id2, layer_id1 in zip(self.biattention_id2, self.biattention_id1):
			end1 = layer_id1
			end2 = layer_id2

			assert self.fixed_layer1 <= end1
			assert self.fixed_layer2 <= end2

			for idx in range(start1, self.fixed_layer1):
				with torch.no_grad():
					embedding1 = self.layers1[idx](embedding1, attention_mask1).hidden_states
					start1 = self.fixed_layer1

			for idx in range(start1, end1):
				embedding1 = self.layers1[idx](embedding1, attention_mask1).hidden_states

			for idx in range(start2, self.fixed_layer2):
				with torch.no_grad():
					embedding2 = self.layers2[idx](embedding2, attention_mask2).hidden_states
					start2 = self.fixed_layer2

			for idx in range(start2, end2):
				embedding2 = self.layers2[idx](embedding2, attention_mask2).hidden_states

			if count == 0 and self.in_batch_pairs:
				embedding2 = (
					embedding2.unsqueeze(0)
					.expand(batch_size, batch_size, num_regions, hidden_size2)
					.contiguous()
					.view(batch_size * batch_size, num_regions, hidden_size2)
				)
				attention_mask2 = (
					attention_mask2.unsqueeze(0)
					.expand(batch_size, batch_size, 1, 1, num_regions)
					.contiguous()
					.view(batch_size * batch_size, 1, 1, num_regions)
				)

				embedding1 = (
					embedding1.unsqueeze(1)
					.expand(batch_size, batch_size, num_words, hidden_size1)
					.contiguous()
					.view(batch_size * batch_size, num_words, hidden_size1)
				)
				attention_mask1 = (
					attention_mask1.unsqueeze(1)
					.expand(batch_size, batch_size, 1, 1, num_words)
					.contiguous()
					.view(batch_size * batch_size, 1, 1, num_words)
				)
				if co_attention_mask is not None:
					co_attention_mask = (
						co_attention_mask.unsqueeze(1)
						.expand(batch_size, batch_size, 1, num_regions, num_words)
						.contiguous()
						.view(batch_size * batch_size, 1, num_regions, num_words)
					)

			if count == 0 and self.FAST_MODE:
				embedding1 = embedding1.expand(
					embedding2.size(0),
					embedding1.size(1),
					embedding1.size(2),
				)
				attention_mask1 = attention_mask1.expand(
					embedding2.size(0),
					attention_mask1.size(1),
					attention_mask1.size(2),
					attention_mask1.size(3),
				)

			if self.with_coattention:
				embedding1, embedding2 = self.c_layer[count](
					embedding1,
					attention_mask1,
					embedding2,
					attention_mask2,
					co_attention_mask,
				)

			start2 = end2
			start1 = end1
			count += 1

			if output_all_encoded_layers:
				all_encoder_layers1.append(embedding1)
				all_encoder_layers2.append(embedding2)

		for idx in range(start2, len(self.layers2)):
			embedding2 = self.layers2[idx](embedding2, attention_mask2).hidden_states

		for idx in range(start1, len(self.layers1)):
			embedding1 = self.layers1[idx](embedding1, attention_mask1).hidden_states

		if not output_all_encoded_layers:
			all_encoder_layers1.append(embedding1)
			all_encoder_layers2.append(embedding2)

		return (
			torch.stack(all_encoder_layers1, dim=-1),
			torch.stack(all_encoder_layers2, dim=-1),
		)

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		final_kwargs = {}
		final_kwargs["num_hidden_layers1"] = config.num_hidden_layers
		final_kwargs["hidden_size1"] = config.hidden_size
		final_kwargs["num_attention_heads1"] = config.num_attention_heads
		final_kwargs["attention_dropout1"] = config.attention_probs_dropout_prob
		final_kwargs["hidden_dropout1"] = config.hidden_dropout_prob
		final_kwargs["intermediate_size1"] = config.intermediate_size
		final_kwargs["activation"] = config.hidden_act
		final_kwargs.update(**kwargs)
		return cls(**final_kwargs)

import logging
import os
from typing import Any, Dict, Optional, Union, NamedTuple

import torch.optim.lr_scheduler

from allennlp.common import Registrable
from allennlp.common.checks import ConfigurationError, check_for_gpu
from allennlp.common.util import int_to_device

logger = logging.getLogger(__name__)


class TrainerCheckpoint(NamedTuple):
	model_state: Dict[str, Any]
	trainer_state: Dict[str, Any]


class Trainer(Registrable):

	default_implementation = "gradient_descent"

	def __init__(
		self,
		serialization_dir: Union[str, os.PathLike] = None,
		cuda_device: Optional[Union[int, torch.device]] = None,
		distributed: bool = False,
		local_rank: int = 0,
		world_size: int = 1,
	) -> None:
		if cuda_device is None:
			from torch import cuda

			if cuda.device_count() > 0:
				cuda_device = 0
			else:
				cuda_device = -1

		check_for_gpu(cuda_device)

		if serialization_dir is None:
			import tempfile

			self._serialization_dir = tempfile.mkdtemp()
		else:
			self._serialization_dir = str(serialization_dir)
		os.makedirs(self._serialization_dir, exist_ok=True)

		if isinstance(cuda_device, list):
			raise ConfigurationError(
				"In AllenNLP 1.0, the Trainer can only be assigned a single `cuda_device`. "
				"Instead, we use torch's DistributedDataParallel at the command level, meaning "
				"our Trainer always uses a single GPU per process."
			)

		if distributed and world_size <= 1:
			raise ConfigurationError(
				"Distributed training can be performed only with more than 1 device. Check "
				"`cuda_device` key in the experiment configuration."
			)

		self.cuda_device = int_to_device(cuda_device)

		self._distributed = distributed
		self._rank = local_rank
		self._primary = self._rank == 0
		self._world_size = world_size

	def train(self) -> Dict[str, Any]:
		raise NotImplementedError

	def get_checkpoint_state(self) -> Optional[TrainerCheckpoint]:
		raise NotImplementedError

	def get_best_weights_path(self) -> Optional[str]:
		return None

from typing import Union, Tuple
import torch
from allennlp.nn.util import min_value_of_dtype

FloatT = Union[torch.FloatTensor]
IntT = Union[torch.IntTensor]
BoolT = Union[torch.BoolTensor]


def apply_mask(
	values: torch.FloatTensor, mask: Union[torch.BoolTensor, torch.IntTensor, torch.FloatTensor]
) -> torch.FloatTensor:
	if mask.dim() == 2:
		mask = mask[:, None, None, :]
	elif mask.dim() == 3:
		mask = mask[:, None, :, :]
	mask = mask.to(values.dtype)
	mask = (1.0 - mask) * min_value_of_dtype(values.dtype)
	return values + mask


def get_extended_attention_mask(
	attention_mask: torch.Tensor,
	input_shape: Tuple[int, ...],
	dtype: torch.dtype,
	is_decoder: bool = False,
) -> torch.Tensor:

	if attention_mask.dim() == 3:
		extended_attention_mask = attention_mask[:, None, :, :]
	elif attention_mask.dim() == 2:
		if is_decoder:
			batch_size, seq_length = input_shape
			seq_ids = torch.arange(seq_length, device=attention_mask.device)
			causal_mask = (
				seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]
			)
			causal_mask = causal_mask.to(attention_mask.dtype)

			if causal_mask.shape[1] < attention_mask.shape[1]:
				prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]
				causal_mask = torch.cat(
					[
						torch.ones(
							(batch_size, seq_length, prefix_seq_len),
							device=attention_mask.device,
							dtype=causal_mask.dtype,
						),
						causal_mask,
					],
					axis=-1,
				)

			extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]
		else:
			extended_attention_mask = attention_mask[:, None, None, :]
	else:
		raise ValueError(
			"Wrong shape for input_ids (shape {}) or attention_mask (shape {})".format(
				input_shape, attention_mask.shape
			)
		)

	return extended_attention_mask

import argparse
import logging
from typing import Union, Dict, List, Tuple, NamedTuple, cast


import termcolor
import torch

from allennlp.commands.subcommand import Subcommand
from allennlp.common.file_utils import cached_path
from allennlp.nn.util import read_state_dict


logger = logging.getLogger(__name__)


@Subcommand.register("diff")
class Diff(Subcommand):
	requires_plugins: bool = False

	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		description = """Display a diff between two model checkpoints."""
		long_description = (
			description
		)
		subparser = parser.add_parser(
			self.name,
			description=long_description,
			help=description,
		)
		subparser.set_defaults(func=_diff)
		subparser.add_argument(
			"checkpoint1",
			type=str,
		)
		subparser.add_argument(
			"checkpoint2",
			type=str,
		)
		subparser.add_argument(
			"--strip-prefix-1",
			type=str,
			help="""a prefix to remove from all of the 1st checkpoint's keys.""",
		)
		subparser.add_argument(
			"--strip-prefix-2",
			type=str,
			help="""a prefix to remove from all of the 2nd checkpoint's keys.""",
		)
		subparser.add_argument(
			"--scale",
			type=float,
			default=1.0,
			help="""controls the scale of the distance calculation.""",
		)
		subparser.add_argument(
			"--threshold",
			type=float,
			default=1e-5,
		)
		return subparser


class Keep(NamedTuple):
	key: str
	shape: Tuple[int, ...]

	def display(self):
		termcolor.cprint(f" {self.key}, shape = {self.shape}")


class Insert(NamedTuple):
	key: str
	shape: Tuple[int, ...]

	def display(self):
		termcolor.cprint(f"+{self.key}, shape = {self.shape}", "green")


class Remove(NamedTuple):
	key: str
	shape: Tuple[int, ...]

	def display(self):
		termcolor.cprint(f"-{self.key}, shape = {self.shape}", "red")


class Modify(NamedTuple):
	key: str
	shape: Tuple[int, ...]
	distance: float

	def display(self):
		termcolor.cprint(
			f"!{self.key}, shape = {self.shape}, distance = {self.distance:.4f}", "yellow"
		)


class _Frontier(NamedTuple):
	x: int
	history: List[Union[Keep, Insert, Remove]]


def _finalize(
	history: List[Union[Keep, Insert, Remove]],
	state_dict_a: Dict[str, torch.Tensor],
	state_dict_b: Dict[str, torch.Tensor],
	scale: float,
	threshold: float,
) -> List[Union[Keep, Insert, Remove, Modify]]:
	out = cast(List[Union[Keep, Insert, Remove, Modify]], history)
	for i, step in enumerate(out):
		if isinstance(step, Keep):
			a_tensor = state_dict_a[step.key]
			b_tensor = state_dict_b[step.key]
			with torch.no_grad():
				dist = (scale * torch.nn.functional.mse_loss(a_tensor, b_tensor).sqrt()).item()
			if dist > threshold:
				out[i] = Modify(step.key, step.shape, dist)
	return out


def checkpoint_diff(
	state_dict_a: Dict[str, torch.Tensor],
	state_dict_b: Dict[str, torch.Tensor],
	scale: float,
	threshold: float,
) -> List[Union[Keep, Insert, Remove, Modify]]:
	param_list_a = [(k, tuple(v.shape)) for k, v in state_dict_a.items()]
	param_list_b = [(k, tuple(v.shape)) for k, v in state_dict_b.items()]

	frontier: Dict[int, _Frontier] = {1: _Frontier(0, [])}

	def one(idx):
		return idx - 1

	a_max = len(param_list_a)
	b_max = len(param_list_b)
	for d in range(0, a_max + b_max + 1):
		for k in range(-d, d + 1, 2):
			go_down = k == -d or (k != d and frontier[k - 1].x < frontier[k + 1].x)

			if go_down:
				old_x, history = frontier[k + 1]
				x = old_x
			else:
				old_x, history = frontier[k - 1]
				x = old_x + 1

			history = history[:]
			y = x - k

			if 1 <= y <= b_max and go_down:
				history.append(Insert(*param_list_b[one(y)]))
			elif 1 <= x <= a_max:
				history.append(Remove(*param_list_a[one(x)]))

			while x < a_max and y < b_max and param_list_a[one(x + 1)] == param_list_b[one(y + 1)]:
				x += 1
				y += 1
				history.append(Keep(*param_list_a[one(x)]))

			if x >= a_max and y >= b_max:
				return _finalize(history, state_dict_a, state_dict_b, scale, threshold)
			else:
				frontier[k] = _Frontier(x, history)

	assert False, "Could not find edit script"


def _get_checkpoint_path(checkpoint: str) -> str:
	if checkpoint.endswith(".tar.gz"):
		return cached_path(checkpoint + "!weights.th", extract_archive=True)
	elif ".tar.gz!" in checkpoint:
		return cached_path(checkpoint, extract_archive=True)
	else:
		return cached_path(checkpoint)


def _diff(args: argparse.Namespace):
	checkpoint_1_path = _get_checkpoint_path(args.checkpoint1)
	checkpoint_2_path = _get_checkpoint_path(args.checkpoint2)
	checkpoint_1 = read_state_dict(
		checkpoint_1_path, strip_prefix=args.strip_prefix_1, strict=False
	)
	checkpoint_2 = read_state_dict(
		checkpoint_2_path, strip_prefix=args.strip_prefix_2, strict=False
	)
	for step in checkpoint_diff(checkpoint_1, checkpoint_2, args.scale, args.threshold):
		step.display()


import argparse
import json
import logging
import math
import os
import random
from pathlib import Path

import datasets
import evaluate
import nltk
import numpy as np
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from filelock import FileLock
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_MAPPING,
	AutoConfig,
	AutoModelForSeq2SeqLM,
	AutoTokenizer,
	DataCollatorForSeq2Seq,
	SchedulerType,
	get_scheduler,
)
from transformers.utils import check_min_version, is_offline_mode, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)
require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/summarization/requirements.txt")

MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

try:
	nltk.data.find("tokenizers/punkt")
except (LookupError, OSError):
	if is_offline_mode():
		raise LookupError(
			"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files"
		)
	with FileLock(".lock") as lock:
		nltk.download("punkt", quiet=True)

summarization_name_mapping = {
	"amazon_reviews_multi": ("review_body", "review_title"),
	"big_patent": ("description", "abstract"),
	"cnn_dailymail": ("article", "highlights"),
	"orange_sum": ("text", "summary"),
	"pn_summary": ("article", "summary"),
	"psc": ("extract_text", "summary_text"),
	"samsum": ("dialogue", "summary"),
	"thaisum": ("body", "summary"),
	"xglue": ("news_body", "news_title"),
	"xsum": ("document", "summary"),
	"wiki_summary": ("article", "highlights"),
}


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a summarization task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--ignore_pad_token_for_loss",
		type=bool,
		default=True,
		help="Whether to ignore the tokens corresponding to padded labels in the loss computation or not.",
	)
	parser.add_argument(
		"--max_source_length",
		type=int,
		default=1024,
		help=(
			"The maximum total input sequence length after "
			"tokenization.Sequences longer than this will be truncated, sequences shorter will be padded."
		),
	)
	parser.add_argument(
		"--source_prefix",
		type=str,
		default=None,
		help="A prefix to add before every source text (useful for T5 models).",
	)
	parser.add_argument(
		"--preprocessing_num_workers",
		type=int,
		default=None,
		help="The number of processes to use for the preprocessing.",
	)
	parser.add_argument(
		"--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
	)
	parser.add_argument(
		"--max_target_length",
		type=int,
		default=128,
		help=(
			"The maximum total sequence length for target text after "
			"tokenization. Sequences longer than this will be truncated, sequences shorter will be padded. "
			"during ``evaluate`` and ``predict``."
		),
	)
	parser.add_argument(
		"--val_max_target_length",
		type=int,
		default=None,
		help=(
			"The maximum total sequence length for validation "
			"target text after tokenization.Sequences longer than this will be truncated, sequences shorter will be "
			"padded. Will default to `max_target_length`.This argument is also used to override the ``max_length`` "
			"param of ``model.generate``, which is used during ``evaluate`` and ``predict``."
		),
	)
	parser.add_argument(
		"--num_beams",
		type=int,
		default=None,
		help=(
			"Number of beams to use for evaluation. This argument will be "
			"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``."
		),
	)
	parser.add_argument(
		"--pad_to_max_length",
		action="store_true",
		help="If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=False,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--tokenizer_name",
		type=str,
		default=None,
		help="Pretrained tokenizer name or path if not the same as model_name",
	)
	parser.add_argument(
		"--text_column",
		type=str,
		default=None,
		help="The name of the column in the datasets containing the full texts (for summarization).",
	)
	parser.add_argument(
		"--summary_column",
		type=str,
		default=None,
		help="The name of the column in the datasets containing the summaries (for summarization).",
	)
	parser.add_argument(
		"--use_slow_tokenizer",
		action="store_true",
		help="If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="Model type to use if training from scratch.",
		choices=MODEL_TYPES,
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	args = parser.parse_args()

	if args.dataset_name is None and args.train_file is None and args.validation_file is None:
		raise ValueError("Need either a dataset name or a training/validation file.")
	else:
		if args.train_file is not None:
			extension = args.train_file.split(".")[-1]
			assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
		if args.validation_file is not None:
			extension = args.validation_file.split(".")[-1]
			assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


def main():
	args = parse_args()
	send_example_telemetry("run_summarization_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)
	if args.source_prefix is None and args.model_name_or_path in [
		"t5-small",
		"t5-base",
		"t5-large",
		"t5-3b",
		"t5-11b",
	]:
		logger.warning(
			"You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with "
			"`--source_prefix 'summarize: ' `"
		)
	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		extension = args.train_file.split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files)

	if args.config_name:
		config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")

	if args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(
			args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	elif args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(
			args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if args.model_name_or_path:
		model = AutoModelForSeq2SeqLM.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForSeq2SeqLM.from_config(config, trust_remote_code=args.trust_remote_code)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))
	if model.config.decoder_start_token_id is None:
		raise ValueError("Make sure that `config.decoder_start_token_id` is correctly defined")

	prefix = args.source_prefix if args.source_prefix is not None else ""

	column_names = raw_datasets["train"].column_names

	dataset_columns = summarization_name_mapping.get(args.dataset_name, None)
	if args.text_column is None:
		text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
	else:
		text_column = args.text_column
		if text_column not in column_names:
			raise ValueError(
				f"--text_column' value '{args.text_column}' needs to be one of: {', '.join(column_names)}"
			)
	if args.summary_column is None:
		summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
	else:
		summary_column = args.summary_column
		if summary_column not in column_names:
			raise ValueError(
				f"--summary_column' value '{args.summary_column}' needs to be one of: {', '.join(column_names)}"
			)

	if args.val_max_target_length is None:
		args.val_max_target_length = args.max_target_length

	max_target_length = args.max_target_length
	padding = "max_length" if args.pad_to_max_length else False

	def preprocess_function(examples):
		inputs = examples[text_column]
		targets = examples[summary_column]
		inputs = [prefix + inp for inp in inputs]
		model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)

		labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)

		if padding == "max_length" and args.ignore_pad_token_for_loss:
			labels["input_ids"] = [
				[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
			]

		model_inputs["labels"] = labels["input_ids"]
		return model_inputs

	with accelerator.main_process_first():
		train_dataset = raw_datasets["train"].map(
			preprocess_function,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on dataset",
		)

		max_target_length = args.val_max_target_length
		eval_dataset = raw_datasets["validation"].map(
			preprocess_function,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on dataset",
		)

	for index in random.sample(range(len(train_dataset)), 1):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id
	data_collator = DataCollatorForSeq2Seq(
		tokenizer,
		model=model,
		label_pad_token_id=label_pad_token_id,
		pad_to_multiple_of=8 if accelerator.use_fp16 else None,
	)

	def postprocess_text(preds, labels):
		preds = [pred.strip() for pred in preds]
		labels = [label.strip() for label in labels]

		preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
		labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

		return preds, labels

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)

	no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("summarization_no_trainer", experiment_config)

	metric = evaluate.load("rouge")

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0
	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()
				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()

		gen_kwargs = {
			"max_length": args.val_max_target_length,
			"num_beams": args.num_beams,
		}
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				generated_tokens = accelerator.unwrap_model(model).generate(
					batch["input_ids"],
					attention_mask=batch["attention_mask"],
					**gen_kwargs,
				)

				generated_tokens = accelerator.pad_across_processes(
					generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
				)
				labels = batch["labels"]
				if not args.pad_to_max_length:
					labels = accelerator.pad_across_processes(batch["labels"], dim=1, pad_index=tokenizer.pad_token_id)

				generated_tokens, labels = accelerator.gather_for_metrics((generated_tokens, labels))
				generated_tokens = generated_tokens.cpu().numpy()
				labels = labels.cpu().numpy()

				if args.ignore_pad_token_for_loss:
					labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
				if isinstance(generated_tokens, tuple):
					generated_tokens = generated_tokens[0]
				decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
				decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

				decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
				metric.add_batch(
					predictions=decoded_preds,
					references=decoded_labels,
				)
		result = metric.compute(use_stemmer=True)
		result = {k: round(v * 100, 4) for k, v in result.items()}

		logger.info(result)

		if args.with_tracking:
			result["train_loss"] = total_loss.item() / len(train_dataloader)
			result["epoch"] = epoch
			result["step"] = completed_steps
			accelerator.log(result, step=completed_steps)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			all_results = {f"eval_{k}": v for k, v in result.items()}
			with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
				json.dump(all_results, f)


if __name__ == "__main__":
	main()

import torch
import torch.nn as nn
import torch.nn.init as init


class Net(nn.Module):
	def __init__(self, upscale_factor):
		super(Net, self).__init__()

		self.relu = nn.ReLU()
		self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))
		self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))
		self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))
		self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))
		self.pixel_shuffle = nn.PixelShuffle(upscale_factor)

		self._initialize_weights()

	def forward(self, x):
		x = self.relu(self.conv1(x))
		x = self.relu(self.conv2(x))
		x = self.relu(self.conv3(x))
		x = self.pixel_shuffle(self.conv4(x))
		return x

	def _initialize_weights(self):
		init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))
		init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))
		init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))
		init.orthogonal_(self.conv4.weight)

import torch
from torch.fx import symbolic_trace
import operator


class M(torch.nn.Module):
	def forward(self, x, y):
		return x + y, torch.add(x, y), x.add(y)

traced = symbolic_trace(M())


patterns = set([operator.add, torch.add, "add"])

for n in traced.graph.nodes:
	if any(n.target == pattern for pattern in patterns):
		with traced.graph.inserting_after(n):
			new_node = traced.graph.call_function(torch.bitwise_and, n.args, n.kwargs)
			n.replace_all_uses_with(new_node)
		traced.graph.erase_node(n)

traced.recompile()


from allennlp.modules.seq2vec_encoders.bert_pooler import BertPooler
from allennlp.modules.seq2vec_encoders.boe_encoder import BagOfEmbeddingsEncoder
from allennlp.modules.seq2vec_encoders.cls_pooler import ClsPooler
from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder
from allennlp.modules.seq2vec_encoders.cnn_highway_encoder import CnnHighwayEncoder
from allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper import (
	AugmentedLstmSeq2VecEncoder,
	GruSeq2VecEncoder,
	LstmSeq2VecEncoder,
	PytorchSeq2VecWrapper,
	RnnSeq2VecEncoder,
	StackedAlternatingLstmSeq2VecEncoder,
	StackedBidirectionalLstmSeq2VecEncoder,
)
from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder


import argparse
import logging


from allennlp.commands.subcommand import Subcommand
from allennlp.common.params import Params


logger = logging.getLogger(__name__)


@Subcommand.register("count-instances")
class CountInstances(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		description = """Count the number of training instances in an experiment config file."""
		subparser = parser.add_parser(self.name, description=description, help=description)
		subparser.add_argument("param_path", type=str, help="path to an experiment config file")

		subparser.add_argument(
			"-o",
			"--overrides",
			type=str,
			default="",
			help=(
				"a json(net) structure used to override the experiment configuration, e.g., "
				"'{\"vocabulary.min_count.labels\": 10}'.  Nested parameters can be specified either"
				" with nested dictionaries or with dot syntax."
			),
		)

		subparser.set_defaults(func=count_instances_from_args)

		return subparser


def count_instances_from_args(args: argparse.Namespace):
	from allennlp.training.util import data_loaders_from_params

	params = Params.from_file(args.param_path)

	data_loaders = data_loaders_from_params(params, train=True, validation=False, test=False)
	instances = sum(
		1 for data_loader in data_loaders.values() for _ in data_loader.iter_instances()
	)

	print(f"Success! One epoch of training contains {instances} instances.")


from typing import Optional, Dict

from scipy.stats import wasserstein_distance

import torch
import torch.distributed as dist
from torch.distributions.categorical import Categorical
from torch.distributions.kl import kl_divergence

from allennlp.common.util import is_distributed
from allennlp.common.checks import ConfigurationError
from allennlp.training.metrics.metric import Metric


@Metric.register("independence")
class Independence(Metric):

	def __init__(
		self,
		num_classes: int,
		num_protected_variable_labels: int,
		dist_metric: str = "kl_divergence",
	) -> None:
		self._num_classes = num_classes
		self._num_protected_variable_labels = num_protected_variable_labels
		self._predicted_label_counts = torch.zeros(num_classes)
		self._total_predictions = torch.tensor(0)
		self._predicted_label_counts_by_protected_variable_label = torch.zeros(
			(num_protected_variable_labels, num_classes)
		)
		if dist_metric == "kl_divergence":
			self._dist_metric = kl_divergence
		elif dist_metric == "wasserstein":
			self._dist_metric = wasserstein_distance
		else:
			raise ConfigurationError(
				"supported distance metrics in initialization are 'kl_divergence' and 'wasserstein'"
			)

	def __call__(
		self,
		predicted_labels: torch.Tensor,
		protected_variable_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	) -> None:
		predicted_labels, protected_variable_labels, mask = self.detach_tensors(
			predicted_labels, protected_variable_labels, mask
		)

		if predicted_labels.size() != protected_variable_labels.size():
			raise ConfigurationError(
				"protected_variable_labels must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(protected_variable_labels.size())
			)
		if mask is not None and predicted_labels.size() != mask.size():
			raise ConfigurationError(
				"mask must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(mask.size())
			)
		if (predicted_labels >= self._num_classes).any():
			raise ConfigurationError(
				"predicted_labels contains an id >= {}, "
				"the number of classes.".format(self._num_classes)
			)
		if (protected_variable_labels >= self._num_protected_variable_labels).any():
			raise ConfigurationError(
				"protected_variable_labels contains an id >= {}, "
				"the number of protected variable labels.".format(
					self._num_protected_variable_labels
				)
			)

		device = predicted_labels.device
		self._predicted_label_counts = self._predicted_label_counts.to(device)
		self._predicted_label_counts_by_protected_variable_label = (
			self._predicted_label_counts_by_protected_variable_label.to(device)
		)
		self._total_predictions = self._total_predictions.to(device)

		if mask is not None:
			predicted_labels = predicted_labels[mask]
			protected_variable_labels = protected_variable_labels[mask]
		else:
			predicted_labels = predicted_labels.flatten()
			protected_variable_labels = protected_variable_labels.flatten()

		_predicted_label_counts = predicted_labels.float().histc(
			bins=self._num_classes, min=0, max=self._num_classes - 1
		)
		_total_predictions = torch.tensor(predicted_labels.nelement()).to(device)

		_predicted_label_counts_by_protected_variable_label = torch.zeros(
			(self._num_protected_variable_labels, self._num_classes)
		).to(device)
		for a in range(self._num_protected_variable_labels):
			_predicted_label_counts_by_protected_variable_label[a] = (
				predicted_labels[protected_variable_labels == a]
				.float()
				.histc(bins=self._num_classes, min=0, max=self._num_classes - 1)
			)

		if is_distributed():
			_predicted_label_counts = _predicted_label_counts.to(device)
			dist.all_reduce(_predicted_label_counts, op=dist.ReduceOp.SUM)

			_total_predictions = _total_predictions.to(device)
			dist.all_reduce(_total_predictions, op=dist.ReduceOp.SUM)

			_predicted_label_counts_by_protected_variable_label = (
				_predicted_label_counts_by_protected_variable_label.to(device)
			)
			dist.all_reduce(
				_predicted_label_counts_by_protected_variable_label, op=dist.ReduceOp.SUM
			)

		self._predicted_label_counts += _predicted_label_counts
		self._total_predictions += _total_predictions
		self._predicted_label_counts_by_protected_variable_label += (
			_predicted_label_counts_by_protected_variable_label
		)

	def get_metric(self, reset: bool = False) -> Dict[int, torch.FloatTensor]:
		distances: Dict[int, torch.FloatTensor] = {}
		if self._total_predictions == 0:
			distances = {
				a: torch.tensor(float("nan")) for a in range(self._num_protected_variable_labels)
			}
			return distances

		C_dist = Categorical(self._predicted_label_counts / self._total_predictions)
		if self._dist_metric == wasserstein_distance:
			C_dist = C_dist.probs
		for a in range(self._num_protected_variable_labels):
			C_given_a_dist = Categorical(
				self._predicted_label_counts_by_protected_variable_label[a]
				/ self._total_predictions
			)
			if self._dist_metric == kl_divergence:
				distances[a] = self._dist_metric(C_given_a_dist, C_dist)
			elif self._dist_metric == wasserstein_distance:
				C_given_a_dist = C_given_a_dist.probs
				label_values = torch.tensor(range(self._num_classes))
				distances[a] = self._dist_metric(label_values, label_values, C_given_a_dist, C_dist)
		if reset:
			self.reset()
		return distances

	def reset(self) -> None:
		self._predicted_label_counts = torch.zeros(self._num_classes)
		self._total_predictions = torch.tensor(0)
		self._predicted_label_counts_by_protected_variable_label = torch.zeros(
			(self._num_protected_variable_labels, self._num_classes)
		)


@Metric.register("separation")
class Separation(Metric):

	def __init__(
		self,
		num_classes: int,
		num_protected_variable_labels: int,
		dist_metric: str = "kl_divergence",
	) -> None:
		self._num_classes = num_classes
		self._num_protected_variable_labels = num_protected_variable_labels
		self._predicted_label_counts_by_gold_label = torch.zeros((num_classes, num_classes))
		self._total_predictions = torch.tensor(0)
		self._predicted_label_counts_by_gold_label_and_protected_variable_label = torch.zeros(
			(num_classes, num_protected_variable_labels, num_classes)
		)
		if dist_metric == "kl_divergence":
			self._dist_metric = kl_divergence
		elif dist_metric == "wasserstein":
			self._dist_metric = wasserstein_distance
		else:
			raise ConfigurationError(
				"supported distance metrics in initialization are 'kl_divergence' and 'wasserstein'"
			)

	def __call__(  # type: ignore
		self,
		predicted_labels: torch.Tensor,
		gold_labels: torch.Tensor,
		protected_variable_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	) -> None:
		predicted_labels, gold_labels, protected_variable_labels, mask = self.detach_tensors(
			predicted_labels, gold_labels, protected_variable_labels, mask
		)

		if predicted_labels.size() != protected_variable_labels.size():
			raise ConfigurationError(
				"protected_variable_labels must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(protected_variable_labels.size())
			)
		if predicted_labels.size() != gold_labels.size():
			raise ConfigurationError(
				"gold_labels must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(gold_labels.size())
			)
		if mask is not None and predicted_labels.size() != mask.size():
			raise ConfigurationError(
				"mask must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(mask.size())
			)
		if (predicted_labels >= self._num_classes).any():
			raise ConfigurationError(
				"predicted_labels contains an id >= {}, "
				"the number of classes.".format(self._num_classes)
			)
		if (gold_labels >= self._num_classes).any():
			raise ConfigurationError(
				"gold_labels contains an id >= {}, "
				"the number of classes.".format(self._num_classes)
			)
		if (protected_variable_labels >= self._num_protected_variable_labels).any():
			raise ConfigurationError(
				"protected_variable_labels contains an id >= {}, "
				"the number of protected variable labels.".format(
					self._num_protected_variable_labels
				)
			)

		device = predicted_labels.device
		self._predicted_label_counts_by_gold_label = self._predicted_label_counts_by_gold_label.to(
			device
		)
		self._predicted_label_counts_by_gold_label_and_protected_variable_label = (
			self._predicted_label_counts_by_gold_label_and_protected_variable_label.to(device)
		)
		self._total_predictions = self._total_predictions.to(device)

		if mask is not None:
			predicted_labels = predicted_labels[mask]
			gold_labels = gold_labels[mask]
			protected_variable_labels = protected_variable_labels[mask]
		else:
			predicted_labels = predicted_labels.flatten()
			gold_labels = gold_labels.flatten()
			protected_variable_labels = protected_variable_labels.flatten()

		_total_predictions = torch.tensor(predicted_labels.nelement()).to(device)
		_predicted_label_counts_by_gold_label = torch.zeros(
			(self._num_classes, self._num_classes)
		).to(device)
		_predicted_label_counts_by_gold_label_and_protected_variable_label = torch.zeros(
			(self._num_classes, self._num_protected_variable_labels, self._num_classes)
		).to(device)
		for y in range(self._num_classes):
			_predicted_label_counts_by_gold_label[y] = (
				predicted_labels[gold_labels == y]
				.float()
				.histc(bins=self._num_classes, min=0, max=self._num_classes - 1)
			)
			for a in range(self._num_protected_variable_labels):
				_predicted_label_counts_by_gold_label_and_protected_variable_label[y][a] = (
					predicted_labels[(gold_labels == y) & (protected_variable_labels == a)]
					.float()
					.histc(bins=self._num_classes, min=0, max=self._num_classes - 1)
				)

		if is_distributed():
			_total_predictions = _total_predictions.to(device)
			dist.all_reduce(_total_predictions, op=dist.ReduceOp.SUM)

			_predicted_label_counts_by_gold_label = _predicted_label_counts_by_gold_label.to(device)
			dist.all_reduce(_predicted_label_counts_by_gold_label[y], op=dist.ReduceOp.SUM)

			_predicted_label_counts_by_gold_label_and_protected_variable_label = (
				_predicted_label_counts_by_gold_label_and_protected_variable_label.to(device)
			)
			dist.all_reduce(
				_predicted_label_counts_by_gold_label_and_protected_variable_label,
				op=dist.ReduceOp.SUM,
			)

		self._total_predictions += _total_predictions
		self._predicted_label_counts_by_gold_label += _predicted_label_counts_by_gold_label
		self._predicted_label_counts_by_gold_label_and_protected_variable_label += (
			_predicted_label_counts_by_gold_label_and_protected_variable_label
		)

	def get_metric(self, reset: bool = False) -> Dict[int, Dict[int, torch.FloatTensor]]:
		distances: Dict[int, Dict[int, torch.FloatTensor]] = {}
		if self._total_predictions == 0:
			distances = {
				y: {
					a: torch.tensor(float("nan"))
					for a in range(self._num_protected_variable_labels)
				}
				for y in range(self._num_classes)
			}
			return distances

		for y in range(self._num_classes):
			probs = self._predicted_label_counts_by_gold_label[y] / self._total_predictions
			C_given_y_dist = Categorical(probs)
			if self._dist_metric == wasserstein_distance:
				C_given_y_dist = C_given_y_dist.probs
			distances[y] = {}
			for a in range(self._num_protected_variable_labels):
				probs = (
					self._predicted_label_counts_by_gold_label_and_protected_variable_label[y][a]
					/ self._total_predictions
				)
				if self._dist_metric == kl_divergence:
					if probs.sum() == 0:
						distances[y][a] = torch.tensor(float("nan"))
						continue
					C_given_a_and_y_dist = Categorical(probs)
					distances[y][a] = self._dist_metric(C_given_a_and_y_dist, C_given_y_dist)
				elif self._dist_metric == wasserstein_distance:
					C_given_a_and_y_dist = Categorical(probs).probs
					label_values = torch.tensor(range(self._num_classes))
					distances[y][a] = self._dist_metric(
						label_values, label_values, C_given_a_and_y_dist, C_given_y_dist
					)
		if reset:
			self.reset()
		return distances

	def reset(self) -> None:
		self._predicted_label_counts_by_gold_label = torch.zeros(
			(self._num_classes, self._num_classes)
		)
		self._total_predictions = torch.tensor(0)
		self._predicted_label_counts_by_gold_label_and_protected_variable_label = torch.zeros(
			(self._num_classes, self._num_protected_variable_labels, self._num_classes)
		)


@Metric.register("sufficiency")
class Sufficiency(Metric):

	def __init__(
		self,
		num_classes: int,
		num_protected_variable_labels: int,
		dist_metric: str = "kl_divergence",
	) -> None:
		self._num_classes = num_classes
		self._num_protected_variable_labels = num_protected_variable_labels
		self._gold_label_counts_by_predicted_label = torch.zeros((num_classes, num_classes))
		self._total_predictions = torch.tensor(0)
		self._gold_label_counts_by_predicted_label_and_protected_variable_label = torch.zeros(
			(num_classes, num_protected_variable_labels, num_classes)
		)
		if dist_metric == "kl_divergence":
			self._dist_metric = kl_divergence
		elif dist_metric == "wasserstein":
			self._dist_metric = wasserstein_distance
		else:
			raise ConfigurationError(
				"supported distance metrics in initialization are 'kl_divergence' and 'wasserstein'"
			)

	def __call__(  # type: ignore
		self,
		predicted_labels: torch.Tensor,
		gold_labels: torch.Tensor,
		protected_variable_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	) -> None:
		predicted_labels, gold_labels, protected_variable_labels, mask = self.detach_tensors(
			predicted_labels, gold_labels, protected_variable_labels, mask
		)

		if predicted_labels.size() != protected_variable_labels.size():
			raise ConfigurationError(
				"protected_variable_labels must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(protected_variable_labels.size())
			)
		if predicted_labels.size() != gold_labels.size():
			raise ConfigurationError(
				"gold_labels must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(gold_labels.size())
			)
		if mask is not None and predicted_labels.size() != mask.size():
			raise ConfigurationError(
				"mask must be of same size as predicted_labels but "
				"found tensor of shape: {}".format(mask.size())
			)
		if (predicted_labels >= self._num_classes).any():
			raise ConfigurationError(
				"predicted_labels contains an id >= {}, "
				"the number of classes.".format(self._num_classes)
			)
		if (gold_labels >= self._num_classes).any():
			raise ConfigurationError(
				"gold_labels contains an id >= {}, "
				"the number of classes.".format(self._num_classes)
			)
		if (protected_variable_labels >= self._num_protected_variable_labels).any():
			raise ConfigurationError(
				"protected_variable_labels contains an id >= {}, "
				"the number of protected variable labels.".format(
					self._num_protected_variable_labels
				)
			)

		device = predicted_labels.device
		self._gold_label_counts_by_predicted_label = self._gold_label_counts_by_predicted_label.to(
			device
		)
		self._gold_label_counts_by_predicted_label_and_protected_variable_label = (
			self._gold_label_counts_by_predicted_label_and_protected_variable_label.to(device)
		)
		self._total_predictions = self._total_predictions.to(device)

		if mask is not None:
			predicted_labels = predicted_labels[mask]
			gold_labels = gold_labels[mask]
			protected_variable_labels = protected_variable_labels[mask]
		else:
			predicted_labels = predicted_labels.flatten()
			gold_labels = gold_labels.flatten()
			protected_variable_labels = protected_variable_labels.flatten()

		_total_predictions = torch.tensor(predicted_labels.nelement()).to(device)
		_gold_label_counts_by_predicted_label = torch.zeros(
			(self._num_classes, self._num_classes)
		).to(device)
		_gold_label_counts_by_predicted_label_and_protected_variable_label = torch.zeros(
			(self._num_classes, self._num_protected_variable_labels, self._num_classes)
		).to(device)
		for c in range(self._num_classes):
			_gold_label_counts_by_predicted_label[c] = (
				gold_labels[predicted_labels == c]
				.float()
				.histc(bins=self._num_classes, min=0, max=self._num_classes - 1)
			)
			for a in range(self._num_protected_variable_labels):
				_gold_label_counts_by_predicted_label_and_protected_variable_label[c][a] = (
					gold_labels[(predicted_labels == c) & (protected_variable_labels == a)]
					.float()
					.histc(bins=self._num_classes, min=0, max=self._num_classes - 1)
				)

		if is_distributed():
			_total_predictions = _total_predictions.to(device)
			dist.all_reduce(_total_predictions, op=dist.ReduceOp.SUM)

			_gold_label_counts_by_predicted_label = _gold_label_counts_by_predicted_label.to(device)
			dist.all_reduce(_gold_label_counts_by_predicted_label[c], op=dist.ReduceOp.SUM)

			_gold_label_counts_by_predicted_label_and_protected_variable_label = (
				_gold_label_counts_by_predicted_label_and_protected_variable_label.to(device)
			)
			dist.all_reduce(
				_gold_label_counts_by_predicted_label_and_protected_variable_label,
				op=dist.ReduceOp.SUM,
			)

		self._total_predictions += _total_predictions
		self._gold_label_counts_by_predicted_label += _gold_label_counts_by_predicted_label
		self._gold_label_counts_by_predicted_label_and_protected_variable_label += (
			_gold_label_counts_by_predicted_label_and_protected_variable_label
		)

	def get_metric(self, reset: bool = False) -> Dict[int, Dict[int, torch.FloatTensor]]:
		distances: Dict[int, Dict[int, torch.FloatTensor]] = {}
		if self._total_predictions == 0:
			distances = {
				c: {
					a: torch.tensor(float("nan"))
					for a in range(self._num_protected_variable_labels)
				}
				for c in range(self._num_classes)
			}
			return distances

		for c in range(self._num_classes):
			probs = self._gold_label_counts_by_predicted_label[c] / self._total_predictions
			if self._dist_metric == kl_divergence:
				if probs.sum() == 0:
					distances[c] = {
						a: torch.tensor(float("nan"))
						for a in range(self._num_protected_variable_labels)
					}
					continue
			Y_given_c_dist = Categorical(probs)
			distances[c] = {}
			if self._dist_metric == wasserstein_distance:
				Y_given_c_dist = Y_given_c_dist.probs
			for a in range(self._num_protected_variable_labels):
				probs = (
					self._gold_label_counts_by_predicted_label_and_protected_variable_label[c][a]
					/ self._total_predictions
				)
				if self._dist_metric == kl_divergence:
					if probs.sum() == 0:
						distances[c][a] = torch.tensor(float("nan"))
						continue
					Y_given_a_and_c_dist = Categorical(probs)
					distances[c][a] = self._dist_metric(Y_given_a_and_c_dist, Y_given_c_dist)
				elif self._dist_metric == wasserstein_distance:
					Y_given_a_and_c_dist = Categorical(probs).probs
					label_values = torch.tensor(range(self._num_classes))
					distances[c][a] = self._dist_metric(
						label_values, label_values, Y_given_a_and_c_dist, Y_given_c_dist
					)
		if reset:
			self.reset()
		return distances

	def reset(self) -> None:
		self._gold_label_counts_by_predicted_label = torch.zeros(
			(self._num_classes, self._num_classes)
		)
		self._total_predictions = torch.tensor(0)
		self._gold_label_counts_by_predicted_label_and_protected_variable_label = torch.zeros(
			(self._num_classes, self._num_protected_variable_labels, self._num_classes)
		)

from typing import List, Dict, Any


import spacy

from allennlp.common import Registrable
from allennlp.common.util import get_spacy_model


class SentenceSplitter(Registrable):

	default_implementation = "spacy"

	def split_sentences(self, text: str) -> List[str]:
		raise NotImplementedError

	def batch_split_sentences(self, texts: List[str]) -> List[List[str]]:
		return [self.split_sentences(text) for text in texts]


@SentenceSplitter.register("spacy")
class SpacySentenceSplitter(SentenceSplitter):

	def __init__(self, language: str = "en_core_web_sm", rule_based: bool = False) -> None:
		self._language = language
		self._rule_based = rule_based

		self.spacy = get_spacy_model(self._language, parse=not self._rule_based, ner=False)
		self._is_version_3 = spacy.__version__ >= "3.0"
		if rule_based:
			sbd_name = "sbd" if spacy.__version__ < "2.1" else "sentencizer"
			if not self.spacy.has_pipe(sbd_name):
				if self._is_version_3:
					self.spacy.add_pipe(sbd_name)
				else:
					sbd = self.spacy.create_pipe(sbd_name)
					self.spacy.add_pipe(sbd)

	def split_sentences(self, text: str) -> List[str]:
		if self._is_version_3:
			return [sent.text.strip() for sent in self.spacy(text).sents]
		else:
			return [sent.string.strip() for sent in self.spacy(text).sents]

	def batch_split_sentences(self, texts: List[str]) -> List[List[str]]:
		if self._is_version_3:
			return [
				[sentence.text.strip() for sentence in doc.sents] for doc in self.spacy.pipe(texts)
			]
		return [
			[sentence.string.strip() for sentence in doc.sents] for doc in self.spacy.pipe(texts)
		]

	def _to_params(self) -> Dict[str, Any]:
		return {"type": "spacy", "language": self._language, "rule_based": self._rule_based}

from typing import Optional, Dict, Any


import torch
import torch.nn

from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder


@Seq2VecEncoder.register("bert_pooler")
class BertPooler(Seq2VecEncoder):

	def __init__(
		self,
		pretrained_model: str,
		*,
		override_weights_file: Optional[str] = None,
		override_weights_strip_prefix: Optional[str] = None,
		load_weights: bool = True,
		requires_grad: bool = True,
		dropout: float = 0.0,
		transformer_kwargs: Optional[Dict[str, Any]] = None,
	) -> None:
		super().__init__()

		from allennlp.common import cached_transformers

		model = cached_transformers.get(
			pretrained_model,
			False,
			override_weights_file=override_weights_file,
			override_weights_strip_prefix=override_weights_strip_prefix,
			load_weights=load_weights,
			**(transformer_kwargs or {}),
		)

		self._dropout = torch.nn.Dropout(p=dropout)

		import copy

		self.pooler = copy.deepcopy(model.pooler)
		for param in self.pooler.parameters():
			param.requires_grad = requires_grad
		self._embedding_dim = model.config.hidden_size

	def get_input_dim(self) -> int:
		return self._embedding_dim

	def get_output_dim(self) -> int:
		return self._embedding_dim

	def forward(
		self, tokens: torch.Tensor, mask: torch.BoolTensor = None, num_wrapping_dims: int = 0
	):
		pooler = self.pooler
		for _ in range(num_wrapping_dims):
			from allennlp.modules import TimeDistributed

			pooler = TimeDistributed(pooler)
		pooled = pooler(tokens)
		pooled = self._dropout(pooled)
		return pooled

from collections import defaultdict
from typing import Tuple, Dict, Set, Optional

import torch

from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("rouge")
class ROUGE(Metric):

	def __init__(
		self,
		ngram_size: int = 2,
		exclude_indices: Set[int] = None,
	) -> None:
		self._ngram_size = ngram_size
		self._exclude_indices = exclude_indices or set()

		self._total_rouge_n_recalls: Dict[int, float] = defaultdict(float)
		self._total_rouge_n_precisions: Dict[int, float] = defaultdict(float)
		self._total_rouge_n_f1s: Dict[int, float] = defaultdict(float)

		self._total_rouge_l_f1 = 0.0

		self._total_sequence_count = 0

	def reset(self) -> None:
		self._total_rouge_n_recalls = defaultdict(float)
		self._total_rouge_n_precisions = defaultdict(float)
		self._total_rouge_n_f1s = defaultdict(float)

		self._total_rouge_l_f1 = 0.0

		self._total_sequence_count = 0

	def _longest_common_subsequence(self, seq_1: torch.LongTensor, seq_2: torch.LongTensor):
		m = len(seq_1)
		n = len(seq_2)

		if m < n:
			seq_1, seq_2 = seq_2, seq_1
			m, n = n, m

		prev_lcs = torch.zeros(n + 1, dtype=torch.long)

		for i in range(m - 1, -1, -1):
			if seq_1[i].item() in self._exclude_indices:
				continue

			cur_lcs = torch.zeros_like(prev_lcs)
			for j in range(n - 1, -1, -1):
				if seq_1[i] == seq_2[j]:
					cur_lcs[j] = 1 + prev_lcs[j + 1]
				else:
					cur_lcs[j] = max(cur_lcs[j + 1], prev_lcs[j])
			prev_lcs = cur_lcs

		return prev_lcs[0].item()

	def _get_rouge_l_score(
		self, predicted_tokens: torch.LongTensor, reference_tokens: torch.LongTensor
	) -> float:
		total_f1 = 0.0

		for predicted_seq, reference_seq in zip(predicted_tokens, reference_tokens):
			from allennlp.training.util import get_valid_tokens_mask

			m = get_valid_tokens_mask(reference_seq, self._exclude_indices).sum().item()
			n = get_valid_tokens_mask(predicted_seq, self._exclude_indices).sum().item()

			lcs = self._longest_common_subsequence(reference_seq, predicted_seq)

			if lcs == 0:
				continue

			recall_lcs = lcs / m
			precision_lcs = lcs / n

			f1 = 2 * recall_lcs * precision_lcs / (recall_lcs + precision_lcs)

			total_f1 += f1

		return dist_reduce_sum(total_f1)

	def _get_rouge_n_stats(
		self,
		predicted_tokens: torch.LongTensor,
		reference_tokens: torch.LongTensor,
		ngram_size: int,
	) -> Tuple[float, float, float]:
		total_recall = 0.0
		total_precision = 0.0
		total_f1 = 0.0

		for predicted_seq, reference_seq in zip(predicted_tokens, reference_tokens):
			from allennlp.training.util import ngrams

			predicted_ngram_counts = ngrams(predicted_seq, ngram_size, self._exclude_indices)
			reference_ngram_counts = ngrams(reference_seq, ngram_size, self._exclude_indices)

			matches = 0
			total_reference_ngrams = 0
			for ngram, count in reference_ngram_counts.items():
				matches += min(predicted_ngram_counts[ngram], count)
				total_reference_ngrams += count

			total_predicted_ngrams = sum(predicted_ngram_counts.values())

			if total_reference_ngrams == 0 or total_predicted_ngrams == 0 or matches == 0:
				continue

			recall = matches / total_reference_ngrams
			precision = matches / total_predicted_ngrams

			f1 = 2.0 * recall * precision / (recall + precision)

			total_recall += recall
			total_precision += precision
			total_f1 += f1

		total_recall = dist_reduce_sum(total_recall)
		total_precision = dist_reduce_sum(total_precision)
		total_f1 = dist_reduce_sum(total_f1)

		return total_recall, total_precision, total_f1

	def __call__(
		self,  # type: ignore
		predictions: torch.LongTensor,
		gold_targets: torch.LongTensor,
		mask: Optional[torch.BoolTensor] = None,
	) -> None:
		if mask is not None:
			raise NotImplementedError("This metric does not support a mask.")

		predictions, gold_targets = self.detach_tensors(predictions, gold_targets)
		for n in range(1, self._ngram_size + 1):

			recall, precision, f1 = self._get_rouge_n_stats(predictions, gold_targets, n)
			self._total_rouge_n_recalls[n] += recall
			self._total_rouge_n_precisions[n] += precision
			self._total_rouge_n_f1s[n] += f1

		self._total_rouge_l_f1 += self._get_rouge_l_score(predictions, gold_targets)

		sequence_count = len(predictions)
		self._total_sequence_count += dist_reduce_sum(sequence_count)

	def _metric_mean(self, metric_sum):
		if self._total_sequence_count == 0:
			return 0.0
		return metric_sum / self._total_sequence_count

	def get_metric(self, reset: bool = False) -> Dict[str, float]:

		metrics = {}

		metrics.update(
			{
				f"ROUGE-{i}_R": self._metric_mean(self._total_rouge_n_recalls[i])
				for i in range(1, self._ngram_size + 1)
			}
		)

		metrics.update(
			{
				f"ROUGE-{i}_P": self._metric_mean(self._total_rouge_n_precisions[i])
				for i in range(1, self._ngram_size + 1)
			}
		)

		metrics.update(
			{
				f"ROUGE-{i}_F1": self._metric_mean(self._total_rouge_n_f1s[i])
				for i in range(1, self._ngram_size + 1)
			}
		)

		metrics["ROUGE-L"] = self._metric_mean(self._total_rouge_l_f1)

		if reset:
			self.reset()

		return metrics

from copy import deepcopy
import torch


def replicate_layers(layer: torch.nn.Module, num_copies: int):
	return torch.nn.ModuleList([deepcopy(layer) for _ in range(num_copies)])

import torch

from allennlp.modules.transformer.transformer_module import TransformerModule


class LayerNorm(torch.nn.LayerNorm, TransformerModule):
	_pretrained_mapping = {"gamma": "weight", "beta": "bias"}

from typing import Optional, Dict, Any, List, Union

from allennlp.common.checks import ConfigurationError


class MetricTracker:

	def __init__(
		self,
		metric_name: Union[str, List[str]],
		patience: Optional[int] = None,
	) -> None:
		self._patience = patience
		self._best_so_far: Optional[float] = None
		self._epochs_with_no_improvement = 0
		self._is_best_so_far = True
		self._epoch_number = 0
		self.best_epoch: Optional[int] = None
		self.best_epoch_metrics: Dict[str, float] = {}

		if isinstance(metric_name, str):
			metric_name = [metric_name]
		self.tracked_metrics = []
		for name in metric_name:
			if name.startswith("+"):
				self.tracked_metrics.append((1.0, name[1:]))
			elif name.startswith("-"):
				self.tracked_metrics.append((-1.0, name[1:]))
			else:
				raise ConfigurationError("metric_name must start with + or -")

	def clear(self) -> None:
		self._best_so_far = None
		self._epochs_with_no_improvement = 0
		self._is_best_so_far = True
		self._epoch_number = 0
		self.best_epoch = None
		self.best_epoch_metrics.clear()

	def state_dict(self) -> Dict[str, Any]:
		return {
			"best_so_far": self._best_so_far,
			"epochs_with_no_improvement": self._epochs_with_no_improvement,
			"is_best_so_far": self._is_best_so_far,
			"epoch_number": self._epoch_number,
			"best_epoch": self.best_epoch,
			"best_epoch_metrics": self.best_epoch_metrics,
		}

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		self._best_so_far = state_dict["best_so_far"]
		self._epochs_with_no_improvement = state_dict["epochs_with_no_improvement"]
		self._is_best_so_far = state_dict["is_best_so_far"]
		self._epoch_number = state_dict["epoch_number"]
		self.best_epoch = state_dict["best_epoch"]

		self.best_epoch_metrics = state_dict.get("best_epoch_metrics", {})

	def add_metrics(self, metrics: Dict[str, float]) -> None:
		combined_score = self.combined_score(metrics)

		new_best = (self._best_so_far is None) or (combined_score > self._best_so_far)

		if new_best:
			self._best_so_far = combined_score
			self._epochs_with_no_improvement = 0
			self._is_best_so_far = True
			self.best_epoch = self._epoch_number
		else:
			self._epochs_with_no_improvement += 1
			self._is_best_so_far = False
		self._epoch_number += 1

	def is_best_so_far(self) -> bool:
		return self._is_best_so_far

	def should_stop_early(self) -> bool:
		if self._patience is None:
			return False
		else:
			return self._epochs_with_no_improvement >= self._patience

	def combined_score(self, metrics: Dict[str, float]) -> float:
		try:
			return sum(
				factor * metrics[metric_name] for factor, metric_name in self.tracked_metrics
			)
		except KeyError as e:
			raise ConfigurationError(
				f"You configured the trainer to use the {e.args[0]} "
				"metric for early stopping, but the model did not produce that metric."
			)

import os
import torch
import torch.distributed as dist
from datetime import datetime
import tqdm
from transformers import AutoTokenizer, GPT2TokenizerFast
from transformers import T5Tokenizer, T5ForConditionalGeneration

g_gigabyte = 1024**3

def setup():
	dist.init_process_group("nccl")


def cleanup():
	dist.destroy_process_group()

def get_date_of_run():
	date_of_run = datetime.now().strftime("%Y-%m-%d-%I:%M:%S_%p")
	print(f"--> current date and time of run = {date_of_run}")
	return date_of_run



def format_metrics_to_gb(item):
	metric_num = item / g_gigabyte
	metric_num = round(metric_num, ndigits=4)
	return metric_num

def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):
	model.train()
	local_rank = int(os.environ['LOCAL_RANK'])
	fsdp_loss = torch.zeros(2).to(local_rank)
  
	if sampler:
		sampler.set_epoch(epoch)
	if rank==0:
		inner_pbar = tqdm.tqdm(
			range(len(train_loader)), colour="blue", desc="r0 Training Epoch"
		)
	for batch in train_loader:
		for key in batch.keys():
			batch[key] = batch[key].to(local_rank)
		optimizer.zero_grad()
		output = model(input_ids=batch["source_ids"],attention_mask=batch["source_mask"],labels=batch["target_ids"] )
		loss = output["loss"]
		loss.backward()
		optimizer.step()
		fsdp_loss[0] += loss.item()
		fsdp_loss[1] += len(batch)
		if rank==0:
			inner_pbar.update(1)

	dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)
	train_accuracy = fsdp_loss[0] / fsdp_loss[1]


	if rank == 0:
		inner_pbar.close()
		print(
				f"Train Epoch: \t{epoch}, Loss: \t{train_accuracy:.4f}"
			)
	return train_accuracy


def validation(model, rank, world_size, val_loader):
	model.eval()
	correct = 0
	local_rank = int(os.environ['LOCAL_RANK'])
	fsdp_loss = torch.zeros(2).to(local_rank)
	if rank == 0:
		inner_pbar = tqdm.tqdm(
			range(len(val_loader)), colour="green", desc="Validation Epoch"
		)
	with torch.no_grad():
		for batch in val_loader:
			for key in batch.keys():
				batch[key] = batch[key].to(local_rank)
			output = model(input_ids=batch["source_ids"],attention_mask=batch["source_mask"],labels=batch["target_ids"])
			fsdp_loss[0] += output["loss"].item()  # sum up batch loss
			fsdp_loss[1] += len(batch)

			if rank==0:
				inner_pbar.update(1)

	dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)
	val_loss = fsdp_loss[0] / fsdp_loss[1]
	if rank == 0:
		inner_pbar.close()
		print(f"Validation Loss: {val_loss:.4f}")
	return val_loss


def setup_model(model_name):
		model = T5ForConditionalGeneration.from_pretrained(model_name)
		tokenizer =  T5Tokenizer.from_pretrained(model_name)
		return model, tokenizer

import torch
import torch.nn as nn
import torch.nn.functional as F


class Attention(nn.Module):
	def __init__(self, dim):
		super(Attention, self).__init__()
		self.linear_out = nn.Linear(dim*2, dim)
		self.mask = None

	def set_mask(self, mask):
		self.mask = mask

	def forward(self, output, context):
		batch_size = output.size(0)
		hidden_size = output.size(2)
		input_size = context.size(1)
		attn = torch.bmm(output, context.transpose(1, 2))
		if self.mask is not None:
			attn.data.masked_fill_(self.mask, -float('inf'))
		attn = F.softmax(attn.view(-1, input_size), dim=1).view(batch_size, -1, input_size)

		mix = torch.bmm(attn, context)

		combined = torch.cat((mix, output), dim=2)
		output = F.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)

		return output, attn


from allennlp.models.heads.head import Head
from allennlp.models.heads.classifier_head import ClassifierHead

from typing import Optional


import torch

from allennlp.nn.util import dist_reduce_sum
from allennlp.training.metrics.metric import Metric


@Metric.register("boolean_accuracy")
class BooleanAccuracy(Metric):

	def __init__(self) -> None:
		self._correct_count = 0.0
		self._total_count = 0.0

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		if gold_labels.size() != predictions.size():
			raise ValueError(
				f"gold_labels must have shape == predictions.size() but "
				f"found tensor of shape: {gold_labels.size()}"
			)
		if mask is not None and mask.size() != predictions.size():
			raise ValueError(
				f"mask must have shape == predictions.size() but "
				f"found tensor of shape: {mask.size()}"
			)

		batch_size = predictions.size(0)

		if mask is not None:
			predictions = predictions * mask
			gold_labels = gold_labels * mask

			keep = mask.view(batch_size, -1).max(dim=1)[0]
		else:
			keep = torch.ones(batch_size, device=predictions.device).bool()

		predictions = predictions.view(batch_size, -1)
		gold_labels = gold_labels.view(batch_size, -1)

		correct = predictions.eq(gold_labels).prod(dim=1).float()

		_correct_count = (correct * keep).sum()
		_total_count = keep.sum()

		self._correct_count += dist_reduce_sum(_correct_count).item()
		self._total_count += dist_reduce_sum(_total_count).item()

	def get_metric(self, reset: bool = False):
		if self._total_count > 0:
			accuracy = float(self._correct_count) / float(self._total_count)
		else:
			accuracy = 0.0
		if reset:
			self.reset()
		return accuracy

	def reset(self):
		self._correct_count = 0.0
		self._total_count = 0.0

import torch

from allennlp.common.registrable import Registrable
from allennlp.training.scheduler import Scheduler


class MomentumScheduler(Scheduler, Registrable):
	def __init__(self, optimizer: torch.optim.Optimizer, last_epoch: int = -1) -> None:
		super().__init__(optimizer, "momentum", last_epoch)

	def get_values(self) -> None:
		raise NotImplementedError


from typing import Callable

import torch


class Highway(torch.nn.Module):

	def __init__(
		self,
		input_dim: int,
		num_layers: int = 1,
		activation: Callable[[torch.Tensor], torch.Tensor] = torch.nn.functional.relu,
	) -> None:
		super().__init__()
		self._input_dim = input_dim
		self._layers = torch.nn.ModuleList(
			[torch.nn.Linear(input_dim, input_dim * 2) for _ in range(num_layers)]
		)
		self._activation = activation
		for layer in self._layers:
			layer.bias[input_dim:].data.fill_(1)

	def forward(self, inputs: torch.Tensor) -> torch.Tensor:
		current_input = inputs
		for layer in self._layers:
			projected_input = layer(current_input)
			linear_part = current_input
			nonlinear_part, gate = projected_input.chunk(2, dim=-1)
			nonlinear_part = self._activation(nonlinear_part)
			gate = torch.sigmoid(gate)
			current_input = gate * linear_part + (1 - gate) * nonlinear_part
		return current_input

from typing import Optional, Iterable, List, Union, Tuple
import numpy as np

from checklist.test_suite import TestSuite
from checklist.test_types import MFT, INV, DIR, Expect
from checklist.perturb import Perturb
from allennlp.confidence_checks.task_checklists.task_suite import TaskSuite
from allennlp.confidence_checks.task_checklists import utils
from allennlp.data.instance import Instance
from allennlp.predictors import Predictor


def _add_phrase_function(phrases: List[str], num_samples: int = 10):

	def perturb_fn(inp):
		input_str = utils.strip_punctuation(inp)
		total = len(phrases)
		idx = np.random.choice(total, min(num_samples, total), replace=False)
		ret = [input_str + ". " + phrases[i] for i in idx]
		return ret

	return perturb_fn


@TaskSuite.register("sentiment-analysis")
class SentimentAnalysisSuite(TaskSuite):

	def __init__(
		self,
		suite: Optional[TestSuite] = None,
		positive: Optional[int] = 0,
		negative: Optional[int] = 1,
		**kwargs,
	):

		self._positive = positive
		self._negative = negative
		super().__init__(suite, **kwargs)

	def _prediction_and_confidence_scores(self, predictor: Predictor):
		def preds_and_confs_fn(data):
			labels = []
			confs = []
			if isinstance(data[0], Instance):
				predictions = predictor.predict_batch_instance(data)
			else:
				data = [{"sentence": sentence} for sentence in data]
				predictions = predictor.predict_batch_json(data)
			for pred in predictions:
				label = pred["probs"].index(max(pred["probs"]))
				labels.append(label)
				confs.append(pred["probs"])
			return np.array(labels), np.array(confs)

		return preds_and_confs_fn

	def _format_failing_examples(
		self,
		inputs: Tuple,
		pred: int,
		conf: Union[np.array, np.ndarray],
		label: Optional[int] = None,
		*args,
		**kwargs,
	):
		labels = {self._positive: "Positive", self._negative: "Negative"}
		ret = str(inputs)
		if label is not None:
			ret += " (Original: %s)" % labels[label]
		ret += "\nPrediction: %s (Confidence: %.1f)" % (labels[pred], conf[pred])

		return ret

	def _default_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):
		super()._default_tests(data, num_test_cases)
		self._setup_editor()
		self._default_vocabulary_tests(data, num_test_cases)
		self._default_ner_tests(data, num_test_cases)
		self._default_temporal_tests(data, num_test_cases)
		self._default_fairness_tests(data, num_test_cases)
		self._default_negation_tests(data, num_test_cases)

	def _setup_editor(self):
		super()._setup_editor()

		pos_adj = [
			"good",
			"great",
			"excellent",
			"amazing",
			"extraordinary",
			"beautiful",
			"fantastic",
			"nice",
			"incredible",
			"exceptional",
			"awesome",
			"perfect",
			"fun",
			"adorable",
			"brilliant",
			"exciting",
			"sweet",
			"wonderful",
		]
		neg_adj = [
			"awful",
			"bad",
			"horrible",
			"weird",
			"rough",
			"lousy",
			"average",
			"difficult",
			"poor",
			"sad",
			"frustrating",
			"lame",
			"nasty",
			"annoying",
			"boring",
			"creepy",
			"dreadful",
			"ridiculous",
			"terrible",
			"ugly",
			"unpleasant",
		]
		self.editor.add_lexicon("pos_adj", pos_adj, overwrite=True)
		self.editor.add_lexicon("neg_adj", neg_adj, overwrite=True)

		pos_verb_present = [
			"like",
			"enjoy",
			"appreciate",
			"love",
			"recommend",
			"admire",
			"value",
			"welcome",
		]
		neg_verb_present = ["hate", "dislike", "regret", "abhor", "dread", "despise"]
		pos_verb_past = [
			"liked",
			"enjoyed",
			"appreciated",
			"loved",
			"admired",
			"valued",
			"welcomed",
		]
		neg_verb_past = ["hated", "disliked", "regretted", "abhorred", "dreaded", "despised"]
		self.editor.add_lexicon("pos_verb_present", pos_verb_present, overwrite=True)
		self.editor.add_lexicon("neg_verb_present", neg_verb_present, overwrite=True)
		self.editor.add_lexicon("pos_verb_past", pos_verb_past, overwrite=True)
		self.editor.add_lexicon("neg_verb_past", neg_verb_past, overwrite=True)
		self.editor.add_lexicon("pos_verb", pos_verb_present + pos_verb_past, overwrite=True)
		self.editor.add_lexicon("neg_verb", neg_verb_present + neg_verb_past, overwrite=True)

		noun = [
			"airline",
			"movie",
			"product",
			"customer service",
			"restaurant",
			"hotel",
			"food",
			"staff",
			"company",
			"crew",
			"service",
		]
		self.editor.add_lexicon("noun", noun, overwrite=True)

		intens_adj = [
			"very",
			"really",
			"absolutely",
			"truly",
			"extremely",
			"quite",
			"incredibly",
			"amazingly",
			"especially",
			"exceptionally",
			"unbelievably",
			"utterly",
			"exceedingly",
			"rather",
			"totally",
			"particularly",
		]
		intens_verb = [
			"really",
			"absolutely",
			"truly",
			"extremely",
			"especially",
			"utterly",
			"totally",
			"particularly",
			"highly",
			"definitely",
			"certainly",
			"genuinely",
			"honestly",
			"strongly",
			"sure",
			"sincerely",
		]

		self.editor.add_lexicon("intens_adj", intens_adj, overwrite=True)
		self.editor.add_lexicon("intens_verb", intens_verb, overwrite=True)

		reducer_adj = [
			"somewhat",
			"kinda",
			"mostly",
			"probably",
			"generally",
			"reasonably",
			"a little",
			"a bit",
			"slightly",
		]

		self.editor.add_lexicon("reducer_adj", reducer_adj, overwrite=True)

		self.monotonic_label = Expect.monotonic(increasing=True, tolerance=0.1)
		self.monotonic_label_down = Expect.monotonic(increasing=False, tolerance=0.1)

	def _default_vocabulary_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):

		positive_words = (
			self.editor.lexicons["pos_adj"]
			+ self.editor.lexicons["pos_verb_present"]
			+ self.editor.lexicons["pos_verb_past"]
		)

		test = MFT(
			positive_words,
			labels=self._positive,
			name="Single Positive Words",
			capability="Vocabulary",
			description="Correctly recognizes positive words",
		)

		self.add_test(test)

		negative_words = (
			self.editor.lexicons["neg_adj"]
			+ self.editor.lexicons["neg_verb_present"]
			+ self.editor.lexicons["neg_verb_past"]
		)

		test = MFT(
			negative_words,
			labels=self._negative,
			name="Single Negative Words",
			capability="Vocabulary",
			description="Correctly recognizes negative words",
		)

		self.add_test(test)

		template = self.editor.template(
			"{it} {noun} {be} {pos_adj}.",
			it=["The", "This", "That"],
			be=["is", "was"],
			labels=self._positive,
			save=True,
		)
		template += self.editor.template(
			"{it} {be} {a:pos_adj} {noun}.",
			it=["It", "This", "That"],
			be=["is", "was"],
			labels=self._positive,
			save=True,
		)
		template += self.editor.template(
			"{i} {pos_verb} {the} {noun}.",
			i=["I", "We"],
			the=["this", "that", "the"],
			labels=self._positive,
			save=True,
		)
		template += self.editor.template(
			"{it} {noun} {be} {neg_adj}.",
			it=["That", "This", "The"],
			be=["is", "was"],
			labels=self._negative,
			save=True,
		)
		template += self.editor.template(
			"{it} {be} {a:neg_adj} {noun}.",
			it=["It", "This", "That"],
			be=["is", "was"],
			labels=self._negative,
			save=True,
		)
		template += self.editor.template(
			"{i} {neg_verb} {the} {noun}.",
			i=["I", "We"],
			the=["this", "that", "the"],
			labels=self._negative,
			save=True,
		)

		test = MFT(
			**template,
			name="Sentiment-laden words in context",
			capability="Vocabulary",
			description="Use positive and negative verbs and adjectives "
			"with nouns such as product, movie, airline, etc. "
			'E.g. "This was a bad movie"',
		)

		self.add_test(test)

		template = self.editor.template(
			["{it} {be} {a:pos_adj} {noun}.", "{it} {be} {a:intens_adj} {pos_adj} {noun}."],
			it=["It", "This", "That"],
			be=["is", "was"],
			nsamples=num_test_cases,
			save=True,
		)
		template += self.editor.template(
			["{i} {pos_verb} {the} {noun}.", "{i} {intens_verb} {pos_verb} {the} {noun}."],
			i=["I", "We"],
			the=["this", "that", "the"],
			nsamples=num_test_cases,
			save=True,
		)
		template += self.editor.template(
			["{it} {be} {a:neg_adj} {noun}.", "{it} {be} {a:intens_adj} {neg_adj} {noun}."],
			it=["It", "This", "That"],
			be=["is", "was"],
			nsamples=num_test_cases,
			save=True,
		)
		template += self.editor.template(
			["{i} {neg_verb} {the} {noun}.", "{i} {intens_verb} {neg_verb} {the} {noun}."],
			i=["I", "We"],
			the=["this", "that", "the"],
			nsamples=num_test_cases,
			save=True,
		)

		test = DIR(
			template.data,
			self.monotonic_label,
			templates=template.templates,
			name="Intensifiers",
			capability="Vocabulary",
			description="Test is composed of pairs of sentences (x1, x2), where we add an intensifier "
			"such as 'really',or 'very' to x2 and expect the confidence to NOT go down "
			"(with tolerance=0.1). e.g.:"
			"x1 = 'That was a good movie'"
			"x2 = 'That was a very good movie'",
		)

		self.add_test(test)

		template = self.editor.template(
			["{it} {noun} {be} {pos_adj}.", "{it} {noun} {be} {reducer_adj} {pos_adj}."],
			it=["The", "This", "That"],
			be=["is", "was"],
			nsamples=num_test_cases,
			save=True,
		)
		template += self.editor.template(
			["{it} {noun} {be} {neg_adj}.", "{it} {noun} {be} {reducer_adj} {neg_adj}."],
			it=["The", "This", "That"],
			be=["is", "was"],
			nsamples=num_test_cases,
			save=True,
		)
		test = DIR(
			template.data,
			self.monotonic_label_down,
			templates=template.templates,
			name="Reducers",
			capability="Vocabulary",
			description="Test is composed of pairs of sentences (x1, x2), where we add a reducer "
			"such as 'somewhat', or 'kinda' to x2 and expect the confidence to NOT go up "
			" (with tolerance=0.1). e.g.:"
			"x1 = 'The staff was good.'"
			"x2 = 'The staff was somewhat good.'",
		)

		self.add_test(test)

		if data:

			positive = self.editor.template("I {pos_verb_present} you.").data
			positive += self.editor.template("You are {pos_adj}.").data

			negative = self.editor.template("I {neg_verb_present} you.").data
			negative += self.editor.template("You are {neg_adj}.").data

			template = Perturb.perturb(
				data, _add_phrase_function(positive), nsamples=num_test_cases
			)
			test = DIR(
				template.data,
				Expect.pairwise(self._diff_up),
				name="Add positive phrases",
				capability="Vocabulary",
				description="Add very positive phrases (e.g. I love you) to the end of sentences, "
				"expect probability of positive to NOT go down (tolerance=0.1)",
			)

			self.add_test(test)

			template = Perturb.perturb(
				data, _add_phrase_function(negative), nsamples=num_test_cases
			)
			test = DIR(
				template.data,
				Expect.pairwise(self._diff_down),
				name="Add negative phrases",
				capability="Vocabulary",
				description="Add very negative phrases (e.g. I hate you) to the end of sentences, "
				"expect probability of positive to NOT go up (tolerance=0.1)",
			)

			self.add_test(test)

	def _default_robustness_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):

		template = Perturb.perturb(data, utils.add_random_strings, nsamples=num_test_cases)
		test = INV(
			template.data,
			name="Add random urls and handles",
			capability="Robustness",
			description="Add randomly generated urls and handles to the start or end of sentence",
		)

		self.add_test(test)

	def _default_ner_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):
		if data:
			template = Perturb.perturb(
				data, utils.spacy_wrap(Perturb.change_names, ner=True), nsamples=num_test_cases
			)
			test = INV(
				template.data,
				name="Change names",
				capability="NER",
				description="Replace names with other common names",
			)
			self.add_test(test)

			template = Perturb.perturb(
				data, utils.spacy_wrap(Perturb.change_location, ner=True), nsamples=num_test_cases
			)
			test = INV(
				template.data,
				name="Change locations",
				capability="NER",
				description="Replace city or country names with other cities or countries",
			)
			self.add_test(test)

			template = Perturb.perturb(
				data, utils.spacy_wrap(Perturb.change_number, ner=True), nsamples=num_test_cases
			)
			test = INV(
				template.data,
				name="Change numbers",
				capability="NER",
				description="Replace integers with random integers within a 20% radius of the original",
			)
			self.add_test(test)

	def _default_temporal_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):
		self._setup_editor()

		change = ["but", "even though", "although", ""]
		template = self.editor.template(
			[
				"I used to think this {noun} was {neg_adj}, {change} now I think it is {pos_adj}.",
				"I think this {noun} is {pos_adj}, {change} I used to think it was {neg_adj}.",
				"In the past I thought this {noun} was {neg_adj}, {change} now I think it is {pos_adj}.",
				"I think this {noun} is {pos_adj}, {change} in the past I thought it was {neg_adj}.",
			],
			change=change,
			unroll=True,
			nsamples=num_test_cases,
			save=True,
			labels=self._positive,
		)
		template += self.editor.template(
			[
				"I used to {neg_verb_present} this {noun}, {change} now I {pos_verb_present} it.",
				"I {pos_verb_present} this {noun}, {change} I used to {neg_verb_present} it.",
				"In the past I would {neg_verb_present} this {noun}, {change} now I {pos_verb} it.",
				"I {pos_verb_present} this {noun}, {change} in the past I would {neg_verb_present} it.",
			],
			change=change,
			unroll=True,
			nsamples=num_test_cases,
			save=True,
			labels=self._positive,
		)

		template += self.editor.template(
			[
				"I used to think this {noun} was {pos_adj}, {change} now I think it is {neg_adj}.",
				"I think this {noun} is {neg_adj}, {change} I used to think it was {pos_adj}.",
				"In the past I thought this {noun} was {pos_adj}, {change} now I think it is {neg_adj}.",
				"I think this {noun} is {neg_adj}, {change} in the past I thought it was {pos_adj}.",
			],
			change=change,
			unroll=True,
			nsamples=num_test_cases,
			save=True,
			labels=self._negative,
		)
		template += self.editor.template(
			[
				"I used to {pos_verb_present} this {noun}, {change} now I {neg_verb_present} it.",
				"I {neg_verb_present} this {noun}, {change} I used to {pos_verb_present} it.",
				"In the past I would {pos_verb_present} this {noun}, {change} now I {neg_verb_present} it.",
				"I {neg_verb_present} this {noun}, {change} in the past I would {pos_verb_present} it.",
			],
			change=change,
			unroll=True,
			nsamples=num_test_cases,
			save=True,
			labels=self._negative,
		)
		test = MFT(
			**template,
			name="Used to, but now",
			capability="Temporal",
			description="Have two conflicing statements, one about the past and "
			"one about the present."
			"Expect the present to carry the sentiment. Examples:\n"
			"I used to love this airline, now I hate it -> should be negative\n"
			"I love this airline, although I used to hate it -> should be positive",
		)

		self.add_test(test)

		adjectives = self.editor.lexicons["pos_adj"] + self.editor.lexicons["neg_adj"]
		verbs = self.editor.lexicons["pos_verb_present"] + self.editor.lexicons["neg_verb_present"]

		template = self.editor.template(
			["{it} {be} {a:adj} {noun}.", "I used to think {it} {be} {a:adj} {noun}."],
			it=["it", "this", "that"],
			be=["is", "was"],
			adj=adjectives,
			save=True,
			nsamples=num_test_cases,
		)
		template += self.editor.template(
			["{i} {verb} {the} {noun}.", "{i} used to {verb} {the} {noun}."],
			i=["I", "We"],
			the=["this", "that", "the"],
			verb=verbs,
			save=True,
			nsamples=num_test_cases,
		)
		test = DIR(
			template.data,
			self.monotonic_label_down,
			templates=template.templates,
			name="'Used to' should reduce",
			capability="Temporal",
			description="A model should not be more confident on 'I used to think X' "
			"when compared to 'X', e.g. 'I used to love this restaurant' "
			"should have less confidence than 'I love this restaurant'",
		)

		self.add_test(test)

	def _default_fairness_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):
		protected = {
			"race": ["a black", "a hispanic", "a white", "an asian"],  # add more here.
			"sexuality": self.editor.template("{a:sexual_adj}").data,
			"religion": self.editor.template("{a:religion_adj}").data,
			"nationality": self.editor.template("{a:nationality}").data,
		}

		for p, vals in protected.items():
			template = self.editor.template(
				["{male} is %s {profession}." % r for r in vals],
				return_maps=False,
				nsamples=num_test_cases,
				save=True,
			)
			template += self.editor.template(
				["{female} is %s {profession}." % r for r in vals],
				return_maps=False,
				nsamples=num_test_cases,
				save=True,
			)
			test = INV(
				template.data,
				threshold=0.1,
				templates=template.templates,
				name="Protected: %s" % p,
				capability="Fairness",
				description="Prediction should be the same for various adjectives within a protected class",
			)

			self.add_test(test)

	def _default_negation_tests(self, data: Optional[Iterable[str]], num_test_cases: int = 100):
		template = self.editor.template(
			"{it} {noun} {nt} {pos_adj}.",
			it=["This", "That", "The"],
			nt=["is not", "isn't"],
			save=True,
			nsamples=num_test_cases,
		)
		template += self.editor.template(
			"{it} {benot} {a:pos_adj} {noun}.",
			it=["It", "This", "That"],
			benot=["is not", "isn't", "was not", "wasn't"],
			save=True,
			nsamples=num_test_cases,
		)
		neg = ["I can't say I", "I don't", "I would never say I", "I don't think I", "I didn't"]
		template += self.editor.template(
			"{neg} {pos_verb_present} {the} {noun}.",
			neg=neg,
			the=["this", "that", "the"],
			save=True,
			nsamples=num_test_cases,
		)
		template += self.editor.template(
			"No one {pos_verb_present}s {the} {noun}.",
			neg=neg,
			the=["this", "that", "the"],
			save=True,
			nsamples=num_test_cases,
		)
		test = MFT(
			template.data,
			labels=self._negative,
			templates=template.templates,
			name="Simple negations: negative",
			capability="Negation",
			description="Very simple negations of positive statements",
		)

		self.add_test(test)

		template = self.editor.template(
			"I thought {it} {noun} would be {pos_adj}, but it {neg}.",
			neg=["was not", "wasn't"],
			it=["this", "that", "the"],
			nt=["is not", "isn't"],
			save=True,
			nsamples=num_test_cases,
		)
		template += self.editor.template(
			"I thought I would {pos_verb_present} {the} {noun}, but I {neg}.",
			neg=["did not", "didn't"],
			the=["this", "that", "the"],
			save=True,
			nsamples=num_test_cases,
		)
		test = MFT(
			template.data,
			labels=self._negative,
			templates=template.templates,
			name="Simple negations: I thought x was positive, but it was not",
			capability="Negation",
			description="",
		)
		self.add_test(test)

	def _positive_change(self, orig_conf: np.ndarray, conf: np.ndarray) -> float:
		return (
			orig_conf[self._negative]
			- conf[self._negative]
			+ conf[self._positive]
			- orig_conf[self._positive]
		)

	def _diff_up(
		self,
		orig_pred: int,
		pred: int,
		orig_conf: np.ndarray,
		conf: np.ndarray,
		labels: Optional[int] = None,
		meta: Optional[List] = None,
	) -> Union[bool, float]:
		tolerance = 0.1
		change = self._positive_change(orig_conf, conf)
		if change + tolerance >= 0:
			return True
		else:
			return change + tolerance

	def _diff_down(
		self,
		orig_pred: int,
		pred: int,
		orig_conf: np.ndarray,
		conf: np.ndarray,
		labels: Optional[int] = None,
		meta: Optional[List] = None,
	) -> Union[bool, float]:
		tolerance = 0.1
		change = self._positive_change(orig_conf, conf)
		if change - tolerance <= 0:
			return True
		else:
			return -(change - tolerance)


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import List, Optional, Tuple

import datasets
import evaluate
import numpy as np
from datasets import load_dataset
from trainer_seq2seq_qa import QuestionAnsweringSeq2SeqTrainer

import transformers
from transformers import (
	AutoConfig,
	AutoModelForSeq2SeqLM,
	AutoTokenizer,
	DataCollatorForSeq2Seq,
	HfArgumentParser,
	Seq2SeqTrainingArguments,
	set_seed,
)
from transformers.trainer_utils import EvalLoopOutput, EvalPrediction, get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/question-answering/requirements.txt")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Path to directory to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	context_column: Optional[str] = field(
		default="context",
		metadata={"help": "The name of the column in the datasets containing the contexts (for question answering)."},
	)
	question_column: Optional[str] = field(
		default="question",
		metadata={"help": "The name of the column in the datasets containing the questions (for question answering)."},
	)
	answer_column: Optional[str] = field(
		default="answers",
		metadata={"help": "The name of the column in the datasets containing the answers (for question answering)."},
	)
	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
	)
	test_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input test data file to evaluate the perplexity on (a text file)."},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_seq_length: int = field(
		default=384,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	max_answer_length: int = field(
		default=30,
		metadata={
			"help": (
				"The maximum length of an answer that can be generated. This is needed because the start "
				"and end predictions are not conditioned on one another."
			)
		},
	)
	val_max_answer_length: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"The maximum total sequence length for validation target text after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded. Will default to `max_answer_length`. "
				"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used "
				"during ``evaluate`` and ``predict``."
			)
		},
	)
	pad_to_max_length: bool = field(
		default=True,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when"
				" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU)."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	version_2_with_negative: bool = field(
		default=False, metadata={"help": "If true, some of the examples do not have an answer."}
	)
	null_score_diff_threshold: float = field(
		default=0.0,
		metadata={
			"help": (
				"The threshold used to select the null answer: if the best answer has a score that is less than "
				"the score of the null answer minus this threshold, the null answer is selected for this example. "
				"Only useful when `version_2_with_negative=True`."
			)
		},
	)
	doc_stride: int = field(
		default=128,
		metadata={"help": "When splitting up a long document into chunks, how much stride to take between chunks."},
	)
	n_best_size: int = field(
		default=20,
		metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
	)
	num_beams: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, "
				"which is used during ``evaluate`` and ``predict``."
			)
		},
	)
	ignore_pad_token_for_loss: bool = field(
		default=True,
		metadata={
			"help": "Whether to ignore the tokens corresponding to padded labels in the loss computation or not."
		},
	)

	def __post_init__(self):
		if (
			self.dataset_name is None
			and self.train_file is None
			and self.validation_file is None
			and self.test_file is None
		):
			raise ValueError("Need either a dataset name or a training/validation file/test_file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
			if self.test_file is not None:
				extension = self.test_file.split(".")[-1]
				assert extension in ["csv", "json"], "`test_file` should be a csv or a json file."
		if self.val_max_answer_length is None:
			self.val_max_answer_length = self.max_answer_length


question_answering_column_name_mapping = {
	"squad_v2": ("question", "context", "answer"),
}


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_seq2seq_qa", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
			extension = data_args.train_file.split(".")[-1]
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
			extension = data_args.validation_file.split(".")[-1]
		if data_args.test_file is not None:
			data_files["test"] = data_args.test_file
			extension = data_args.test_file.split(".")[-1]
		raw_datasets = load_dataset(
			extension,
			data_files=data_files,
			field="data",
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSeq2SeqLM.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if model.config.decoder_start_token_id is None:
		raise ValueError("Make sure that `config.decoder_start_token_id` is correctly defined")

	if training_args.do_train:
		column_names = raw_datasets["train"].column_names
	elif training_args.do_eval:
		column_names = raw_datasets["validation"].column_names
	elif training_args.do_predict:
		column_names = raw_datasets["test"].column_names
	else:
		logger.info("There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.")
		return

	dataset_columns = question_answering_column_name_mapping.get(data_args.dataset_name, None)
	if data_args.question_column is None:
		question_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
	else:
		question_column = data_args.question_column
		if question_column not in column_names:
			raise ValueError(
				f"--question_column' value '{data_args.question_column}' needs to be one of: {', '.join(column_names)}"
			)
	if data_args.context_column is None:
		context_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
	else:
		context_column = data_args.context_column
		if context_column not in column_names:
			raise ValueError(
				f"--context_column' value '{data_args.context_column}' needs to be one of: {', '.join(column_names)}"
			)
	if data_args.answer_column is None:
		answer_column = dataset_columns[2] if dataset_columns is not None else column_names[2]
	else:
		answer_column = data_args.answer_column
		if answer_column not in column_names:
			raise ValueError(
				f"--answer_column' value '{data_args.answer_column}' needs to be one of: {', '.join(column_names)}"
			)

	max_answer_length = data_args.max_answer_length
	padding = "max_length" if data_args.pad_to_max_length else False

	if training_args.label_smoothing_factor > 0 and not hasattr(model, "prepare_decoder_input_ids_from_labels"):
		logger.warning(
			"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for "
			f"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory"
		)

	if data_args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)
	max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	def preprocess_squad_batch(
		examples,
		question_column: str,
		context_column: str,
		answer_column: str,
	) -> Tuple[List[str], List[str]]:
		questions = examples[question_column]
		contexts = examples[context_column]
		answers = examples[answer_column]

		def generate_input(_question, _context):
			return " ".join(["question:", _question.lstrip(), "context:", _context.lstrip()])

		inputs = [generate_input(question, context) for question, context in zip(questions, contexts)]
		targets = [answer["text"][0] if len(answer["text"]) > 0 else "" for answer in answers]
		return inputs, targets

	def preprocess_function(examples):
		inputs, targets = preprocess_squad_batch(examples, question_column, context_column, answer_column)

		model_inputs = tokenizer(inputs, max_length=max_seq_length, padding=padding, truncation=True)
		labels = tokenizer(text_target=targets, max_length=max_answer_length, padding=padding, truncation=True)

		if padding == "max_length" and data_args.ignore_pad_token_for_loss:
			labels["input_ids"] = [
				[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
			]

		model_inputs["labels"] = labels["input_ids"]
		return model_inputs

	def preprocess_validation_function(examples):
		inputs, targets = preprocess_squad_batch(examples, question_column, context_column, answer_column)

		model_inputs = tokenizer(
			inputs,
			max_length=max_seq_length,
			padding=padding,
			truncation=True,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
		)
		labels = tokenizer(text_target=targets, max_length=max_answer_length, padding=padding, truncation=True)

		if padding == "max_length" and data_args.ignore_pad_token_for_loss:
			labels["input_ids"] = [
				[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
			]

		sample_mapping = model_inputs.pop("overflow_to_sample_mapping")

		model_inputs["example_id"] = []
		labels_out = []

		for i in range(len(model_inputs["input_ids"])):
			sample_index = sample_mapping[i]
			model_inputs["example_id"].append(examples["id"][sample_index])
			labels_out.append(labels["input_ids"][sample_index])

		model_inputs["labels"] = labels_out
		return model_inputs

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on train dataset",
			)
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	if training_args.do_eval:
		if "validation" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_examples = raw_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_examples), data_args.max_eval_samples)
			eval_examples = eval_examples.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_examples.map(
				preprocess_validation_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on validation dataset",
			)
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

	if training_args.do_predict:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_examples = raw_datasets["test"]
		if data_args.max_predict_samples is not None:
			predict_examples = predict_examples.select(range(data_args.max_predict_samples))
		with training_args.main_process_first(desc="prediction dataset map pre-processing"):
			predict_dataset = predict_examples.map(
				preprocess_validation_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))

	label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id
	data_collator = DataCollatorForSeq2Seq(
		tokenizer,
		model=model,
		label_pad_token_id=label_pad_token_id,
		pad_to_multiple_of=8 if training_args.fp16 else None,
	)

	metric = evaluate.load(
		"squad_v2" if data_args.version_2_with_negative else "squad", cache_dir=model_args.cache_dir
	)

	def compute_metrics(p: EvalPrediction):
		return metric.compute(predictions=p.predictions, references=p.label_ids)

	def post_processing_function(
		examples: datasets.Dataset, features: datasets.Dataset, outputs: EvalLoopOutput, stage="eval"
	):
		preds = outputs.predictions
		if isinstance(preds, tuple):
			preds = preds[0]
		preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
		decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

		example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
		feature_per_example = {example_id_to_index[feature["example_id"]]: i for i, feature in enumerate(features)}
		predictions = {}
		for example_index, example in enumerate(examples):
			feature_index = feature_per_example[example_index]
			predictions[example["id"]] = decoded_preds[feature_index]

		if data_args.version_2_with_negative:
			formatted_predictions = [
				{"id": k, "prediction_text": v, "no_answer_probability": 0.0} for k, v in predictions.items()
			]
		else:
			formatted_predictions = [{"id": k, "prediction_text": v} for k, v in predictions.items()]

		references = [{"id": ex["id"], "answers": ex[answer_column]} for ex in examples]
		return EvalPrediction(predictions=formatted_predictions, label_ids=references)

	trainer = QuestionAnsweringSeq2SeqTrainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		eval_examples=eval_examples if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		compute_metrics=compute_metrics if training_args.predict_with_generate else None,
		post_process_function=post_processing_function,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload

		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	results = {}
	max_length = (
		training_args.generation_max_length
		if training_args.generation_max_length is not None
		else data_args.val_max_answer_length
	)
	num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams
	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix="eval")

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")
		results = trainer.predict(predict_dataset, predict_examples)
		metrics = results.metrics

		max_predict_samples = (
			data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
		)
		metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))

		trainer.log_metrics("predict", metrics)
		trainer.save_metrics("predict", metrics)

	if training_args.push_to_hub:
		kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "question-answering"}
		if data_args.dataset_name is not None:
			kwargs["dataset_tags"] = data_args.dataset_name
			if data_args.dataset_config_name is not None:
				kwargs["dataset_args"] = data_args.dataset_config_name
				kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
			else:
				kwargs["dataset"] = data_args.dataset_name

		trainer.push_to_hub(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()


import torch


from allennlp.common.registrable import Registrable
from allennlp.nn.util import masked_softmax


class Attention(torch.nn.Module, Registrable):

	def __init__(self, normalize: bool = True) -> None:
		super().__init__()
		self._normalize = normalize

	def forward(
		self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.BoolTensor = None
	) -> torch.Tensor:
		similarities = self._forward_internal(vector, matrix)
		if self._normalize:
			return masked_softmax(similarities, matrix_mask)
		else:
			return similarities

	def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:
		raise NotImplementedError

from typing import Any, Dict, List, Mapping


from allennlp.data.fields.field import DataArray, Field


class MetadataField(Field[DataArray], Mapping[str, Any]):

	__slots__ = ["metadata"]

	def __init__(self, metadata: Any) -> None:
		self.metadata = metadata

	def __getitem__(self, key: str) -> Any:
		try:
			return self.metadata[key]  # type: ignore
		except TypeError:
			raise TypeError("your metadata is not a dict")

	def __iter__(self):
		try:
			return iter(self.metadata)
		except TypeError:
			raise TypeError("your metadata is not iterable")

	def __len__(self):
		try:
			return len(self.metadata)
		except TypeError:
			raise TypeError("your metadata has no length")

	def get_padding_lengths(self) -> Dict[str, int]:
		return {}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> DataArray:

		return self.metadata  # type: ignore

	def empty_field(self) -> "MetadataField":
		return MetadataField(None)

	def batch_tensors(self, tensor_list: List[DataArray]) -> List[DataArray]:  # type: ignore
		return tensor_list

	def __str__(self) -> str:
		return "MetadataField (print field.metadata to see specific information)."

	def human_readable_repr(self):
		if hasattr(self.metadata, "human_readable_repr"):
			return self.metadata.human_readable_repr()
		return self.metadata

import torch

from allennlp.modules.attention.attention import Attention
from allennlp.nn import util


@Attention.register("cosine")
class CosineAttention(Attention):

	def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:
		a_norm = vector / (
			vector.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(vector.dtype)
		)
		b_norm = matrix / (
			matrix.norm(p=2, dim=-1, keepdim=True) + util.tiny_value_of_dtype(matrix.dtype)
		)
		return torch.bmm(a_norm.unsqueeze(dim=1), b_norm.transpose(-1, -2)).squeeze(1)


from allennlp.modules.token_embedders.token_embedder import TokenEmbedder
from allennlp.modules.token_embedders.embedding import Embedding
from allennlp.modules.token_embedders.token_characters_encoder import TokenCharactersEncoder
from allennlp.modules.token_embedders.elmo_token_embedder import ElmoTokenEmbedder
from allennlp.modules.token_embedders.empty_embedder import EmptyEmbedder
from allennlp.modules.token_embedders.bag_of_word_counts_token_embedder import (
	BagOfWordCountsTokenEmbedder,
)
from allennlp.modules.token_embedders.pass_through_token_embedder import PassThroughTokenEmbedder
from allennlp.modules.token_embedders.pretrained_transformer_embedder import (
	PretrainedTransformerEmbedder,
)
from allennlp.modules.token_embedders.pretrained_transformer_mismatched_embedder import (
	PretrainedTransformerMismatchedEmbedder,
)

from allennlp.modules.encoder_base import _EncoderBase
from allennlp.common import Registrable


class Seq2SeqEncoder(_EncoderBase, Registrable):

	def get_input_dim(self) -> int:
		raise NotImplementedError

	def get_output_dim(self) -> int:
		raise NotImplementedError

	def is_bidirectional(self) -> bool:
		raise NotImplementedError

import logging
import math
from typing import Any, Dict, Optional, Tuple, Union

import torch
import torch.nn.functional as F
from allennlp.data.tokenizers import PretrainedTransformerTokenizer
from allennlp.modules.scalar_mix import ScalarMix
from allennlp.modules.token_embedders.token_embedder import TokenEmbedder
from allennlp.nn.util import batched_index_select
from transformers import XLNetConfig

logger = logging.getLogger(__name__)


@TokenEmbedder.register("pretrained_transformer")
class PretrainedTransformerEmbedder(TokenEmbedder):

	authorized_missing_keys = [r"position_ids$"]

	def __init__(
		self,
		model_name: str,
		*,
		max_length: int = None,
		sub_module: str = None,
		train_parameters: bool = True,
		eval_mode: bool = False,
		last_layer_only: bool = True,
		override_weights_file: Optional[str] = None,
		override_weights_strip_prefix: Optional[str] = None,
		reinit_modules: Optional[Union[int, Tuple[int, ...], Tuple[str, ...]]] = None,
		load_weights: bool = True,
		gradient_checkpointing: Optional[bool] = None,
		tokenizer_kwargs: Optional[Dict[str, Any]] = None,
		transformer_kwargs: Optional[Dict[str, Any]] = None,
	) -> None:
		super().__init__()
		from allennlp.common import cached_transformers

		self.transformer_model = cached_transformers.get(
			model_name,
			True,
			override_weights_file=override_weights_file,
			override_weights_strip_prefix=override_weights_strip_prefix,
			reinit_modules=reinit_modules,
			load_weights=load_weights,
			**(transformer_kwargs or {}),
		)

		if gradient_checkpointing is not None:
			self.transformer_model.config.update({"gradient_checkpointing": gradient_checkpointing})

		self.config = self.transformer_model.config
		if sub_module:
			assert hasattr(self.transformer_model, sub_module)
			self.transformer_model = getattr(self.transformer_model, sub_module)
		self._max_length = max_length

		self.output_dim = self.config.hidden_size

		self._scalar_mix: Optional[ScalarMix] = None
		if not last_layer_only:
			self._scalar_mix = ScalarMix(self.config.num_hidden_layers)
			self.config.output_hidden_states = True

		tokenizer = PretrainedTransformerTokenizer(
			model_name,
			tokenizer_kwargs=tokenizer_kwargs,
		)

		try:
			if self.transformer_model.get_input_embeddings().num_embeddings != len(
				tokenizer.tokenizer
			):
				self.transformer_model.resize_token_embeddings(len(tokenizer.tokenizer))
		except NotImplementedError:
			logger.warning(
				"Could not resize the token embedding matrix of the transformer model. "
				"This model does not support resizing."
			)

		self._num_added_start_tokens = len(tokenizer.single_sequence_start_tokens)
		self._num_added_end_tokens = len(tokenizer.single_sequence_end_tokens)
		self._num_added_tokens = self._num_added_start_tokens + self._num_added_end_tokens

		self.train_parameters = train_parameters
		if not train_parameters:
			for param in self.transformer_model.parameters():
				param.requires_grad = False

		self.eval_mode = eval_mode
		if eval_mode:
			self.transformer_model.eval()

	def train(self, mode: bool = True):
		self.training = mode
		for name, module in self.named_children():
			if self.eval_mode and name == "transformer_model":
				module.eval()
			else:
				module.train(mode)
		return self

	def get_output_dim(self):
		return self.output_dim

	def _number_of_token_type_embeddings(self):
		if isinstance(self.config, XLNetConfig):
			return 3  # XLNet has 3 type ids
		elif hasattr(self.config, "type_vocab_size"):
			return self.config.type_vocab_size
		else:
			return 0

	def forward(
		self,
		token_ids: torch.LongTensor,
		mask: torch.BoolTensor,
		type_ids: Optional[torch.LongTensor] = None,
		segment_concat_mask: Optional[torch.BoolTensor] = None,
	) -> torch.Tensor:  # type: ignore
		if type_ids is not None:
			max_type_id = type_ids.max()
			if max_type_id == 0:
				type_ids = None
			else:
				if max_type_id >= self._number_of_token_type_embeddings():
					raise ValueError("Found type ids too large for the chosen transformer model.")
				assert token_ids.shape == type_ids.shape

		fold_long_sequences = self._max_length is not None and token_ids.size(1) > self._max_length
		if fold_long_sequences:
			batch_size, num_segment_concat_wordpieces = token_ids.size()
			token_ids, segment_concat_mask, type_ids = self._fold_long_sequences(
				token_ids, segment_concat_mask, type_ids
			)

		transformer_mask = segment_concat_mask if self._max_length is not None else mask
		assert transformer_mask is not None

		parameters = {"input_ids": token_ids, "attention_mask": transformer_mask.float()}
		if type_ids is not None:
			parameters["token_type_ids"] = type_ids

		transformer_output = self.transformer_model(**parameters)
		if self._scalar_mix is not None:
			hidden_states = transformer_output.hidden_states[1:]
			embeddings = self._scalar_mix(hidden_states)
		else:
			embeddings = transformer_output.last_hidden_state

		if fold_long_sequences:
			embeddings = self._unfold_long_sequences(
				embeddings, segment_concat_mask, batch_size, num_segment_concat_wordpieces
			)

		return embeddings

	def _fold_long_sequences(
		self,
		token_ids: torch.LongTensor,
		mask: torch.BoolTensor,
		type_ids: Optional[torch.LongTensor] = None,
	) -> Tuple[torch.LongTensor, torch.LongTensor, Optional[torch.LongTensor]]:
		num_segment_concat_wordpieces = token_ids.size(1)
		num_segments = math.ceil(num_segment_concat_wordpieces / self._max_length)  # type: ignore
		padded_length = num_segments * self._max_length  # type: ignore
		length_to_pad = padded_length - num_segment_concat_wordpieces

		def fold(tensor):  # Shape: [batch_size, num_segment_concat_wordpieces]
			tensor = F.pad(tensor, [0, length_to_pad], value=0)
			return tensor.reshape(-1, self._max_length)

		return fold(token_ids), fold(mask), fold(type_ids) if type_ids is not None else None

	def _unfold_long_sequences(
		self,
		embeddings: torch.FloatTensor,
		mask: torch.BoolTensor,
		batch_size: int,
		num_segment_concat_wordpieces: int,
	) -> torch.FloatTensor:

		def lengths_to_mask(lengths, max_len, device):
			return torch.arange(max_len, device=device).expand(
				lengths.size(0), max_len
			) < lengths.unsqueeze(1)

		device = embeddings.device
		num_segments = int(embeddings.size(0) / batch_size)
		embedding_size = embeddings.size(2)

		num_wordpieces = num_segment_concat_wordpieces - (num_segments - 1) * self._num_added_tokens

		embeddings = embeddings.reshape(
			batch_size, num_segments * self._max_length, embedding_size  # type: ignore
		)
		mask = mask.reshape(batch_size, num_segments * self._max_length)  # type: ignore
		seq_lengths = mask.sum(-1)
		if not (lengths_to_mask(seq_lengths, mask.size(1), device) == mask).all():
			raise ValueError(
				"Long sequence splitting only supports masks with all 1s preceding all 0s."
			)
		end_token_indices = (
			seq_lengths.unsqueeze(-1) - torch.arange(self._num_added_end_tokens, device=device) - 1
		)

		start_token_embeddings = embeddings[:, : self._num_added_start_tokens, :]
		end_token_embeddings = batched_index_select(embeddings, end_token_indices)

		embeddings = embeddings.reshape(batch_size, num_segments, self._max_length, embedding_size)
		embeddings = embeddings[
			:, :, self._num_added_start_tokens : embeddings.size(2) - self._num_added_end_tokens, :
		]  # truncate segment-level start/end tokens
		embeddings = embeddings.reshape(batch_size, -1, embedding_size)  # flatten


		num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length
		num_removed_non_end_tokens = (
			num_effective_segments * self._num_added_tokens - self._num_added_end_tokens
		)
		end_token_indices -= num_removed_non_end_tokens.unsqueeze(-1)
		assert (end_token_indices >= self._num_added_start_tokens).all()
		embeddings = torch.cat([embeddings, torch.zeros_like(end_token_embeddings)], 1)
		embeddings.scatter_(
			1, end_token_indices.unsqueeze(-1).expand_as(end_token_embeddings), end_token_embeddings
		)

		embeddings = torch.cat([start_token_embeddings, embeddings], 1)

		embeddings = embeddings[:, :num_wordpieces, :]
		return embeddings

import io
import itertools
import logging
import re
import tarfile
import warnings
import zipfile
from typing import Any, cast, Iterator, NamedTuple, Optional, Sequence, Tuple, BinaryIO

import numpy
import torch

from torch.nn.functional import embedding

from allennlp.common import Tqdm
from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path, get_file_extension, is_url_or_existing_file
from allennlp.data.vocabulary import Vocabulary
from allennlp.modules.time_distributed import TimeDistributed
from allennlp.modules.token_embedders.token_embedder import TokenEmbedder
from allennlp.nn import util

with warnings.catch_warnings():
	warnings.filterwarnings("ignore", category=FutureWarning)
	import h5py

logger = logging.getLogger(__name__)


@TokenEmbedder.register("embedding")
class Embedding(TokenEmbedder):

	def __init__(
		self,
		embedding_dim: int,
		num_embeddings: int = None,
		projection_dim: int = None,
		weight: torch.FloatTensor = None,
		padding_index: int = None,
		trainable: bool = True,
		max_norm: float = None,
		norm_type: float = 2.0,
		scale_grad_by_freq: bool = False,
		sparse: bool = False,
		vocab_namespace: str = "tokens",
		pretrained_file: str = None,
		vocab: Vocabulary = None,
	) -> None:
		super().__init__()

		if num_embeddings is None and vocab is None:
			raise ConfigurationError(
				"Embedding must be constructed with either num_embeddings or a vocabulary."
			)

		_vocab_namespace: Optional[str] = vocab_namespace
		if num_embeddings is None:
			num_embeddings = vocab.get_vocab_size(_vocab_namespace)  # type: ignore
		else:
			_vocab_namespace = None  # type: ignore

		self.num_embeddings = num_embeddings
		self.padding_index = padding_index
		self.max_norm = max_norm
		self.norm_type = norm_type
		self.scale_grad_by_freq = scale_grad_by_freq
		self.sparse = sparse
		self._vocab_namespace = _vocab_namespace
		self._pretrained_file = pretrained_file

		self.output_dim = projection_dim or embedding_dim

		if weight is not None and pretrained_file:
			raise ConfigurationError(
				"Embedding was constructed with both a weight and a pretrained file."
			)

		elif pretrained_file is not None:

			if vocab is None:
				raise ConfigurationError(
					"To construct an Embedding from a pretrained file, you must also pass a vocabulary."
				)


			weight = _read_pretrained_embeddings_file(
				pretrained_file, embedding_dim, vocab, vocab_namespace
			)
			self.weight = torch.nn.Parameter(weight, requires_grad=trainable)

		elif weight is not None:
			self.weight = torch.nn.Parameter(weight, requires_grad=trainable)

		else:
			weight = torch.FloatTensor(num_embeddings, embedding_dim)
			self.weight = torch.nn.Parameter(weight, requires_grad=trainable)
			torch.nn.init.xavier_uniform_(self.weight)

		if self.weight.size() != (num_embeddings, embedding_dim):
			raise ConfigurationError(
				"A weight matrix was passed with contradictory embedding shapes."
			)

		if self.padding_index is not None:
			self.weight.data[self.padding_index].fill_(0)

		if projection_dim:
			self._projection = torch.nn.Linear(embedding_dim, projection_dim)
		else:
			self._projection = None

	def get_output_dim(self) -> int:
		return self.output_dim

	def forward(self, tokens: torch.Tensor) -> torch.Tensor:
		original_size = tokens.size()
		tokens = util.combine_initial_dims(tokens)

		embedded = embedding(
			tokens,
			self.weight,
			padding_idx=self.padding_index,
			max_norm=self.max_norm,
			norm_type=self.norm_type,
			scale_grad_by_freq=self.scale_grad_by_freq,
			sparse=self.sparse,
		)

		embedded = util.uncombine_initial_dims(embedded, original_size)

		if self._projection:
			projection = self._projection
			for _ in range(embedded.dim() - 2):
				projection = TimeDistributed(projection)
			embedded = projection(embedded)
		return embedded

	def extend_vocab(
		self,
		extended_vocab: Vocabulary,
		vocab_namespace: str = None,
		extension_pretrained_file: str = None,
		model_path: str = None,
	):

		vocab_namespace = vocab_namespace or self._vocab_namespace
		if not vocab_namespace:
			logger.info(
				"Loading a model trained before embedding extension was implemented; "
				"pass an explicit vocab namespace if you want to extend the vocabulary."
			)
			return

		extended_num_embeddings = extended_vocab.get_vocab_size(vocab_namespace)
		if extended_num_embeddings == self.num_embeddings:
			return

		if extended_num_embeddings < self.num_embeddings:
			raise ConfigurationError(
				f"Size of namespace, {vocab_namespace} for extended_vocab is smaller than "
				f"embedding. You likely passed incorrect vocab or namespace for extension."
			)

		if extension_pretrained_file and is_url_or_existing_file(extension_pretrained_file):
			pass
		elif extension_pretrained_file:
			raise ConfigurationError(
				f"You passed pretrained embedding file {extension_pretrained_file} "
				f"for model_path {model_path} but it's not available."
			)
		elif is_url_or_existing_file(self._pretrained_file):
			extension_pretrained_file = self._pretrained_file
		elif self._pretrained_file is not None:
			logger.warning(
				f"Embedding at model_path, {model_path} cannot locate the pretrained_file. "
				f"Originally pretrained_file was at '{self._pretrained_file}'."
			)
		else:
			logger.info(
				"If you are fine-tuning and want to use a pretrained_file for "
				"embedding extension, please pass the mapping by --embedding-sources argument."
			)

		embedding_dim = self.weight.data.shape[-1]
		if not extension_pretrained_file:
			extra_num_embeddings = extended_num_embeddings - self.num_embeddings
			extra_weight = torch.FloatTensor(extra_num_embeddings, embedding_dim)
			torch.nn.init.xavier_uniform_(extra_weight)
		else:
			whole_weight = _read_pretrained_embeddings_file(
				extension_pretrained_file, embedding_dim, extended_vocab, vocab_namespace
			)
			extra_weight = whole_weight[self.num_embeddings :, :]

		device = self.weight.data.device
		extended_weight = torch.cat([self.weight.data, extra_weight.to(device)], dim=0)
		self.weight = torch.nn.Parameter(extended_weight, requires_grad=self.weight.requires_grad)
		self.num_embeddings = extended_num_embeddings


def _read_pretrained_embeddings_file(
	file_uri: str, embedding_dim: int, vocab: Vocabulary, namespace: str = "tokens"
) -> torch.FloatTensor:
	file_ext = get_file_extension(file_uri)
	if file_ext in [".h5", ".hdf5"]:
		return _read_embeddings_from_hdf5(file_uri, embedding_dim, vocab, namespace)

	return _read_embeddings_from_text_file(file_uri, embedding_dim, vocab, namespace)


def _read_embeddings_from_text_file(
	file_uri: str, embedding_dim: int, vocab: Vocabulary, namespace: str = "tokens"
) -> torch.FloatTensor:
	tokens_to_keep = set(vocab.get_index_to_token_vocabulary(namespace).values())
	vocab_size = vocab.get_vocab_size(namespace)
	embeddings = {}

	logger.info("Reading pretrained embeddings from file")

	with EmbeddingsTextFile(file_uri) as embeddings_file:
		for line in Tqdm.tqdm(embeddings_file):
			token = line.split(" ", 1)[0]
			if token in tokens_to_keep:
				fields = line.rstrip().split(" ")
				if len(fields) - 1 != embedding_dim:
					logger.warning(
						"Found line with wrong number of dimensions (expected: %d; actual: %d): %s",
						embedding_dim,
						len(fields) - 1,
						line,
					)
					continue

				vector = numpy.asarray(fields[1:], dtype="float32")
				embeddings[token] = vector

	if not embeddings:
		raise ConfigurationError(
			"No embeddings of correct dimension found; you probably "
			"misspecified your embedding_dim parameter, or didn't "
			"pre-populate your Vocabulary"
		)

	all_embeddings = numpy.asarray(list(embeddings.values()))
	embeddings_mean = float(numpy.mean(all_embeddings))
	embeddings_std = float(numpy.std(all_embeddings))
	logger.info("Initializing pre-trained embedding layer")
	embedding_matrix = torch.FloatTensor(vocab_size, embedding_dim).normal_(
		embeddings_mean, embeddings_std
	)
	num_tokens_found = 0
	index_to_token = vocab.get_index_to_token_vocabulary(namespace)
	for i in range(vocab_size):
		token = index_to_token[i]

		if token in embeddings:
			embedding_matrix[i] = torch.FloatTensor(embeddings[token])
			num_tokens_found += 1
		else:
			logger.debug(
				"Token %s was not found in the embedding file. Initialising randomly.", token
			)

	logger.info(
		"Pretrained embeddings were found for %d out of %d tokens", num_tokens_found, vocab_size
	)

	return embedding_matrix


def _read_embeddings_from_hdf5(
	embeddings_filename: str, embedding_dim: int, vocab: Vocabulary, namespace: str = "tokens"
) -> torch.FloatTensor:
	with h5py.File(embeddings_filename, "r") as fin:
		embeddings = fin["embedding"][...]

	if list(embeddings.shape) != [vocab.get_vocab_size(namespace), embedding_dim]:
		raise ConfigurationError(
			"Read shape {0} embeddings from the file, but expected {1}".format(
				list(embeddings.shape), [vocab.get_vocab_size(namespace), embedding_dim]
			)
		)

	return torch.FloatTensor(embeddings)


def format_embeddings_file_uri(
	main_file_path_or_url: str, path_inside_archive: Optional[str] = None
) -> str:
	if path_inside_archive:
		return "({})#{}".format(main_file_path_or_url, path_inside_archive)
	return main_file_path_or_url


class EmbeddingsFileURI(NamedTuple):
	main_file_uri: str
	path_inside_archive: Optional[str] = None


def parse_embeddings_file_uri(uri: str) -> "EmbeddingsFileURI":
	match = re.fullmatch(r"\((.*)\)#(.*)", uri)
	if match:
		fields = cast(Tuple[str, str], match.groups())
		return EmbeddingsFileURI(*fields)
	else:
		return EmbeddingsFileURI(uri, None)


class EmbeddingsTextFile(Iterator[str]):

	DEFAULT_ENCODING = "utf-8"

	def __init__(
		self, file_uri: str, encoding: str = DEFAULT_ENCODING, cache_dir: str = None
	) -> None:

		self.uri = file_uri
		self._encoding = encoding
		self._cache_dir = cache_dir
		self._archive_handle: Any = None  # only if the file is inside an archive

		main_file_uri, path_inside_archive = parse_embeddings_file_uri(file_uri)
		main_file_local_path = cached_path(main_file_uri, cache_dir=cache_dir)

		if zipfile.is_zipfile(main_file_local_path):  # ZIP archive
			self._open_inside_zip(main_file_uri, path_inside_archive)

		elif tarfile.is_tarfile(main_file_local_path):  # TAR archive
			self._open_inside_tar(main_file_uri, path_inside_archive)

		else:  # all the other supported formats, including uncompressed files
			if path_inside_archive:
				raise ValueError("Unsupported archive format: %s" + main_file_uri)

			extension = get_file_extension(main_file_uri)

			package = None
			if extension in [".txt", ".vec"]:
				package = io
			elif extension == ".gz":
				import gzip

				package = gzip
			elif extension == ".bz2":
				import bz2

				package = bz2
			elif extension == ".xz":
				import lzma

				package = lzma

			if package is None:
				logger.warning(
					'The embeddings file has an unknown file extension "%s". '
					"We will assume the file is an (uncompressed) text file",
					extension,
				)
				package = io

			self._handle = package.open(  # type: ignore
				main_file_local_path, "rt", encoding=encoding
			)

		first_line = next(self._handle)  # this moves the iterator forward
		self.num_tokens = EmbeddingsTextFile._get_num_tokens_from_first_line(first_line)
		if self.num_tokens:
			self._iterator = self._handle
		else:
			self._iterator = itertools.chain([first_line], self._handle)

	def _open_inside_zip(self, archive_path: str, member_path: Optional[str] = None) -> None:
		cached_archive_path = cached_path(archive_path, cache_dir=self._cache_dir)
		archive = zipfile.ZipFile(cached_archive_path, "r")
		if member_path is None:
			members_list = archive.namelist()
			member_path = self._get_the_only_file_in_the_archive(members_list, archive_path)
		member_path = cast(str, member_path)
		member_file = cast(BinaryIO, archive.open(member_path, "r"))
		self._handle = io.TextIOWrapper(member_file, encoding=self._encoding)
		self._archive_handle = archive

	def _open_inside_tar(self, archive_path: str, member_path: Optional[str] = None) -> None:
		cached_archive_path = cached_path(archive_path, cache_dir=self._cache_dir)
		archive = tarfile.open(cached_archive_path, "r")
		if member_path is None:
			members_list = archive.getnames()
			member_path = self._get_the_only_file_in_the_archive(members_list, archive_path)
		member_path = cast(str, member_path)
		member = archive.getmember(member_path)  # raises exception if not present
		member_file = cast(BinaryIO, archive.extractfile(member))
		self._handle = io.TextIOWrapper(member_file, encoding=self._encoding)
		self._archive_handle = archive

	def read(self) -> str:
		return "".join(self._iterator)

	def readline(self) -> str:
		return next(self._iterator)

	def close(self) -> None:
		self._handle.close()
		if self._archive_handle:
			self._archive_handle.close()

	def __enter__(self) -> "EmbeddingsTextFile":
		return self

	def __exit__(self, exc_type, exc_val, exc_tb) -> None:
		self.close()

	def __iter__(self) -> "EmbeddingsTextFile":
		return self

	def __next__(self) -> str:
		return next(self._iterator)

	def __len__(self) -> Optional[int]:
		if self.num_tokens:
			return self.num_tokens
		raise AttributeError(
			"an object of type EmbeddingsTextFile implements `__len__` only if the underlying "
			"text file declares the number of tokens (i.e. the number of lines following)"
			"in the first line. That is not the case of this particular instance."
		)

	@staticmethod
	def _get_the_only_file_in_the_archive(members_list: Sequence[str], archive_path: str) -> str:
		if len(members_list) > 1:
			raise ValueError(
				"The archive %s contains multiple files, so you must select "
				"one of the files inside providing a uri of the type: %s."
				% (
					archive_path,
					format_embeddings_file_uri("path_or_url_to_archive", "path_inside_archive"),
				)
			)
		return members_list[0]

	@staticmethod
	def _get_num_tokens_from_first_line(line: str) -> Optional[int]:
		fields = line.split(" ")
		if 1 <= len(fields) <= 2:
			try:
				int_fields = [int(x) for x in fields]
			except ValueError:
				return None
			else:
				num_tokens = max(int_fields)
				logger.info(
					"Recognized a header line in the embedding file with number of tokens: %d",
					num_tokens,
				)
				return num_tokens
		return None




from pkg_resources import packaging
import torch
import torch.cuda.nccl as nccl
import torch.distributed as dist


def bfloat_support():
	return (
		torch.version.cuda
		and torch.cuda.is_bf16_supported()
		and packaging.version.parse(torch.version.cuda).release >= (11, 0)
		and dist.is_nccl_available()
		and nccl.version() >= (2, 10)
	)

import datetime
from typing import List, Dict, Any, Tuple, Callable
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from allennlp.common.checks import check_for_gpu


def init_process(
	process_rank: int,
	world_size: int,
	distributed_device_ids: List[int],
	func: Callable,
	func_args: Tuple = None,
	func_kwargs: Dict[str, Any] = None,
	primary_addr: str = "127.0.0.1",
	primary_port: int = 29500,
):
	assert world_size > 1

	global_rank = process_rank

	gpu_id = distributed_device_ids[process_rank]  # type: ignore

	if gpu_id >= 0:
		torch.cuda.set_device(int(gpu_id))
		dist.init_process_group(
			backend="nccl",
			init_method=f"tcp://{primary_addr}:{primary_port}",
			world_size=world_size,
			rank=global_rank,
		)
	else:
		dist.init_process_group(
			backend="gloo",
			init_method=f"tcp://{primary_addr}:{primary_port}",
			world_size=world_size,
			rank=global_rank,
			timeout=datetime.timedelta(seconds=120),
		)

	func(global_rank, world_size, gpu_id, *(func_args or []), **(func_kwargs or {}))

	dist.destroy_process_group()


def run_distributed_test(
	device_ids: List[int] = None,
	func: Callable = None,
	*args,
	**kwargs,
):
	device_ids = device_ids or [-1, -1]
	check_for_gpu(device_ids)
	if "start_method" in kwargs:
		start_method = kwargs.pop("start_method")
	else:
		start_method = "spawn" if any(x >= 0 for x in device_ids) else "fork"
	nprocs = world_size = len(device_ids)
	mp.start_processes(
		init_process,
		args=(world_size, device_ids, func, args, kwargs),
		nprocs=nprocs,
		start_method=start_method,
	)

from allennlp.nn.checkpoint.checkpoint_wrapper import CheckpointWrapper, TorchCheckpointWrapper
from allennlp.nn.checkpoint.fairscale_checkpoint_wrapper import FairScaleCheckpointWrapper

import torch
from torch.nn.parameter import Parameter

from allennlp.modules.attention.attention import Attention
from allennlp.nn import Activation


@Attention.register("bilinear")
class BilinearAttention(Attention):

	def __init__(
		self,
		vector_dim: int,
		matrix_dim: int,
		activation: Activation = None,
		normalize: bool = True,
	) -> None:
		super().__init__(normalize)
		self._weight_matrix = Parameter(torch.Tensor(vector_dim, matrix_dim))
		self._bias = Parameter(torch.Tensor(1))
		self._activation = activation or Activation.by_name("linear")()
		self.reset_parameters()

	def reset_parameters(self):
		torch.nn.init.xavier_uniform_(self._weight_matrix)
		self._bias.data.fill_(0)

	def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:
		intermediate = vector.mm(self._weight_matrix).unsqueeze(1)
		return self._activation(intermediate.bmm(matrix.transpose(1, 2)).squeeze(1) + self._bias)

from typing import Dict, List, Union, Set, Iterator
import logging
import textwrap


import torch

from allennlp.common.checks import ConfigurationError
from allennlp.common.util import pad_sequence_to_length
from allennlp.data.fields.field import Field
from allennlp.data.fields.sequence_field import SequenceField
from allennlp.data.vocabulary import Vocabulary

logger = logging.getLogger(__name__)


class SequenceLabelField(Field[torch.Tensor]):

	__slots__ = [
		"labels",
		"sequence_field",
		"_label_namespace",
		"_indexed_labels",
		"_skip_indexing",
	]

	_already_warned_namespaces: Set[str] = set()

	def __init__(
		self,
		labels: Union[List[str], List[int]],
		sequence_field: SequenceField,
		label_namespace: str = "labels",
	) -> None:
		self.labels = labels
		self.sequence_field = sequence_field
		self._label_namespace = label_namespace
		self._indexed_labels = None
		self._maybe_warn_for_namespace(label_namespace)
		if len(labels) != sequence_field.sequence_length():
			raise ConfigurationError(
				"Label length and sequence length "
				"don't match: %d and %d" % (len(labels), sequence_field.sequence_length())
			)

		self._skip_indexing = False
		if all(isinstance(x, int) for x in labels):
			self._indexed_labels = labels
			self._skip_indexing = True

		elif not all(isinstance(x, str) for x in labels):
			raise ConfigurationError(
				"SequenceLabelFields must be passed either all "
				"strings or all ints. Found labels {} with "
				"types: {}.".format(labels, [type(x) for x in labels])
			)

	def _maybe_warn_for_namespace(self, label_namespace: str) -> None:
		if not (self._label_namespace.endswith("labels") or self._label_namespace.endswith("tags")):
			if label_namespace not in self._already_warned_namespaces:
				logger.warning(
					"Your label namespace was '%s'. We recommend you use a namespace "
					"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by "
					"default to your vocabulary.  See documentation for "
					"`non_padded_namespaces` parameter in Vocabulary.",
					self._label_namespace,
				)
				self._already_warned_namespaces.add(label_namespace)

	def __iter__(self) -> Iterator[Union[str, int]]:
		return iter(self.labels)

	def __getitem__(self, idx: int) -> Union[str, int]:
		return self.labels[idx]

	def __len__(self) -> int:
		return len(self.labels)

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		if self._indexed_labels is None:
			for label in self.labels:
				counter[self._label_namespace][label] += 1  # type: ignore

	def index(self, vocab: Vocabulary):
		if not self._skip_indexing:
			self._indexed_labels = [
				vocab.get_token_index(label, self._label_namespace)  # type: ignore
				for label in self.labels
			]

	def get_padding_lengths(self) -> Dict[str, int]:
		return {"num_tokens": self.sequence_field.sequence_length()}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:
		if self._indexed_labels is None:
			raise ConfigurationError(
				"You must call .index(vocabulary) on a field before calling .as_tensor()"
			)
		desired_num_tokens = padding_lengths["num_tokens"]
		padded_tags = pad_sequence_to_length(self._indexed_labels, desired_num_tokens)
		tensor = torch.LongTensor(padded_tags)
		return tensor

	def empty_field(self) -> "SequenceLabelField":
		empty_list: List[str] = []
		sequence_label_field = SequenceLabelField(empty_list, self.sequence_field.empty_field())
		sequence_label_field._indexed_labels = empty_list
		return sequence_label_field

	def __str__(self) -> str:
		length = self.sequence_field.sequence_length()
		formatted_labels = "".join(
			"\t\t" + labels + "\n" for labels in textwrap.wrap(repr(self.labels), 100)
		)
		return (
			f"SequenceLabelField of length {length} with "
			f"labels:\n {formatted_labels} \t\tin namespace: '{self._label_namespace}'."
		)

	def human_readable_repr(self) -> Union[List[str], List[int]]:
		return self.labels


from enum import Enum, auto

import torch
from torch.fx import GraphModule, Node, Proxy, symbolic_trace



class M(torch.nn.Module):
	def __init__(self):
		super().__init__()

	def forward(self, x, y):
		y = torch.cat([x, y])
		return y

traced = symbolic_trace(M())

class ActivationFunction(Enum):
	RELU = auto()
	LEAKY_RELU = auto()
	PRELU = auto()

activation_functions = {
	ActivationFunction.RELU: torch.nn.ReLU(),
	ActivationFunction.LEAKY_RELU: torch.nn.LeakyReLU(),
	ActivationFunction.PRELU: torch.nn.PReLU(),
}

def wrap_in_activation_function(m: GraphModule, fn: ActivationFunction) -> GraphModule:
	output_node: Optional[Node] = None
	for n in reversed(m.graph.nodes):
		if n.op == "output":
			output_node = n
			break
	assert output_node

	assert len(output_node.all_input_nodes) == 1
	wrap_node = output_node.all_input_nodes[0]

	wrap_proxy = Proxy(wrap_node)

	fn_impl = activation_functions[fn]
	fn_impl_traced = symbolic_trace(fn_impl)

	with traced.graph.inserting_after(wrap_node):
		fn_impl_output_node = fn_impl_traced(wrap_proxy)
		new_args = (fn_impl_output_node.node,)
		output_node.args = new_args

	m.recompile()


x, y = torch.randn(5, 3), torch.randn(5, 3)
orig_output = traced(x, y)

wrap_in_activation_function(traced, ActivationFunction.LEAKY_RELU)
new_output = traced(x, y)

torch.testing.assert_close(new_output, torch.nn.LeakyReLU()(orig_output))

import torch
from torch.utils.data import Dataset
import fsspec
from dataclasses import dataclass


@dataclass
class DataConfig:
	path: str = None
	block_size: int = None
	train_split: float = None
	truncate: float = 1.0

class CharDataset(Dataset):

	def __init__(self, data_cfg: DataConfig): #data_path: str, block_size):
		data = fsspec.open(data_cfg.path).open().read().decode('utf-8')
		data = data[ : int(len(data) * data_cfg.truncate)]

		chars = sorted(list(set(data)))
		data_size, vocab_size = len(data), len(chars)
		print('Data has %d characters, %d unique.' % (data_size, vocab_size))

		self.stoi = {ch: i for i, ch in enumerate(chars)}
		self.itos = {i: ch for i, ch in enumerate(chars)}
		self.block_size = data_cfg.block_size
		self.vocab_size = vocab_size
		self.data = data

	def __len__(self):
		return len(self.data) - self.block_size

	def __getitem__(self, idx):
		chunk = self.data[idx:idx + self.block_size + 1]
		dix = [self.stoi[s] for s in chunk]
		x = torch.tensor(dix[:-1], dtype=torch.long)
		y = torch.tensor(dix[1:], dtype=torch.long)
		return x, y

import logging
from typing import List, Dict, Any, Optional, TYPE_CHECKING

import torch

from allennlp.training.callbacks.callback import TrainerCallback
from allennlp.training.util import get_train_and_validation_metrics
from allennlp.data import TensorDict

if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


logger = logging.getLogger(__name__)


@TrainerCallback.register("console_logger")
class ConsoleLoggerCallback(TrainerCallback):
	def __init__(
		self,
		serialization_dir: str,
		should_log_inputs: bool = True,
	) -> None:
		super().__init__(serialization_dir)
		self._should_log_inputs = should_log_inputs

	def on_batch(
		self,
		trainer: "GradientDescentTrainer",
		batch_inputs: List[TensorDict],
		batch_outputs: List[Dict[str, Any]],
		batch_metrics: Dict[str, Any],
		epoch: int,
		batch_number: int,
		is_training: bool,
		is_primary: bool = True,
		batch_grad_norm: Optional[float] = None,
		**kwargs,
	) -> None:

		if not is_primary:
			return None

		if batch_number == 1 and epoch == 0 and self._should_log_inputs:
			logger.info("Batch inputs")
			for b, batch in enumerate(batch_inputs):
				self._log_fields(batch, log_prefix="batch_input")  # type: ignore

	def _log_fields(self, fields: Dict, log_prefix: str = ""):
		for key, val in fields.items():
			key = log_prefix + "/" + key
			if isinstance(val, dict):
				self._log_fields(val, key)
			elif isinstance(val, torch.Tensor):
				torch.set_printoptions(threshold=2)
				logger.info("%s (Shape: %s)\n%s", key, " x ".join([str(x) for x in val.shape]), val)
				torch.set_printoptions(threshold=1000)
			elif isinstance(val, List):
				logger.info('Field : "%s" : (Length %d of type "%s")', key, len(val), type(val[0]))
			elif isinstance(val, str):
				logger.info('Field : "{}" : "{:20.20} ..."'.format(key, val))
			else:
				logger.info('Field : "%s" : %s', key, val)

	def on_epoch(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any],
		epoch: int,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		if not is_primary:
			return None

		train_metrics, val_metrics = get_train_and_validation_metrics(metrics)

		metric_names = set(train_metrics.keys())
		if val_metrics is not None:
			metric_names.update(val_metrics.keys())
		val_metrics = val_metrics or {}

		dual_message_template = "%s |  %8.3f  |  %8.3f"
		no_val_message_template = "%s |  %8.3f  |  %8s"
		no_train_message_template = "%s |  %8s  |  %8.3f"
		header_template = "%s |  %-10s"
		name_length = max(len(x) for x in metric_names)
		logger.info(header_template, "Training".rjust(name_length + 13), "Validation")

		for name in sorted(metric_names):
			train_metric = train_metrics.get(name)
			val_metric = val_metrics.get(name)

			if val_metric is not None and train_metric is not None:
				logger.info(
					dual_message_template, name.ljust(name_length), train_metric, val_metric
				)
			elif val_metric is not None:
				logger.info(no_train_message_template, name.ljust(name_length), "N/A", val_metric)
			elif train_metric is not None:
				logger.info(no_val_message_template, name.ljust(name_length), train_metric, "N/A")

import argparse
import os
import random
import shutil
import time
import warnings
from enum import Enum

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.parallel
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
import torchvision.datasets as datasets
import torchvision.models as models
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Subset

model_names = sorted(name for name in models.__dict__
	if name.islower() and not name.startswith("__")
	and callable(models.__dict__[name]))

parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')
parser.add_argument('data', metavar='DIR', nargs='?', default='imagenet',
					help='path to dataset (default: imagenet)')
parser.add_argument('-a', '--arch', metavar='ARCH', default='resnet18',
					choices=model_names,
					help='model architecture: ' +
						' | '.join(model_names) +
						' (default: resnet18)')
parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',
					help='number of data loading workers (default: 4)')
parser.add_argument('--epochs', default=90, type=int, metavar='N',
					help='number of total epochs to run')
parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
					help='manual epoch number (useful on restarts)')
parser.add_argument('-b', '--batch-size', default=256, type=int,
					metavar='N',
					help='mini-batch size (default: 256), this is the total '
						 'batch size of all GPUs on the current node when '
						 'using Data Parallel or Distributed Data Parallel')
parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,
					metavar='LR', help='initial learning rate', dest='lr')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
					help='momentum')
parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,
					metavar='W', help='weight decay (default: 1e-4)',
					dest='weight_decay')
parser.add_argument('-p', '--print-freq', default=10, type=int,
					metavar='N', help='print frequency (default: 10)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
					help='path to latest checkpoint (default: none)')
parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',
					help='evaluate model on validation set')
parser.add_argument('--pretrained', dest='pretrained', action='store_true',
					help='use pre-trained model')
parser.add_argument('--world-size', default=-1, type=int,
					help='number of nodes for distributed training')
parser.add_argument('--rank', default=-1, type=int,
					help='node rank for distributed training')
parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,
					help='url used to set up distributed training')
parser.add_argument('--dist-backend', default='nccl', type=str,
					help='distributed backend')
parser.add_argument('--seed', default=None, type=int,
					help='seed for initializing training. ')
parser.add_argument('--gpu', default=None, type=int,
					help='GPU id to use.')
parser.add_argument('--multiprocessing-distributed', action='store_true',
					help='Use multi-processing distributed training to launch '
						 'N processes per node, which has N GPUs. This is the '
						 'fastest way to use PyTorch for either single node or '
						 'multi node data parallel training')
parser.add_argument('--dummy', action='store_true', help="use fake data to benchmark")

best_acc1 = 0


def main():
	args = parser.parse_args()

	if args.seed is not None:
		random.seed(args.seed)
		torch.manual_seed(args.seed)
		cudnn.deterministic = True
		cudnn.benchmark = False
		warnings.warn('You have chosen to seed training. '
					  'This will turn on the CUDNN deterministic setting, '
					  'which can slow down your training considerably! '
					  'You may see unexpected behavior when restarting '
					  'from checkpoints.')

	if args.gpu is not None:
		warnings.warn('You have chosen a specific GPU. This will completely '
					  'disable data parallelism.')

	if args.dist_url == "env://" and args.world_size == -1:
		args.world_size = int(os.environ["WORLD_SIZE"])

	args.distributed = args.world_size > 1 or args.multiprocessing_distributed

	if torch.cuda.is_available():
		ngpus_per_node = torch.cuda.device_count()
	else:
		ngpus_per_node = 1
	if args.multiprocessing_distributed:
		args.world_size = ngpus_per_node * args.world_size
		mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))
	else:
		main_worker(args.gpu, ngpus_per_node, args)


def main_worker(gpu, ngpus_per_node, args):
	global best_acc1
	args.gpu = gpu

	if args.gpu is not None:
		print("Use GPU: {} for training".format(args.gpu))

	if args.distributed:
		if args.dist_url == "env://" and args.rank == -1:
			args.rank = int(os.environ["RANK"])
		if args.multiprocessing_distributed:
			args.rank = args.rank * ngpus_per_node + gpu
		dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
								world_size=args.world_size, rank=args.rank)
	if args.pretrained:
		print("=> using pre-trained model '{}'".format(args.arch))
		model = models.__dict__[args.arch](pretrained=True)
	else:
		print("=> creating model '{}'".format(args.arch))
		model = models.__dict__[args.arch]()

	if not torch.cuda.is_available() and not torch.backends.mps.is_available():
		print('using CPU, this will be slow')
	elif args.distributed:
		if torch.cuda.is_available():
			if args.gpu is not None:
				torch.cuda.set_device(args.gpu)
				model.cuda(args.gpu)
				args.batch_size = int(args.batch_size / ngpus_per_node)
				args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)
				model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
			else:
				model.cuda()
				model = torch.nn.parallel.DistributedDataParallel(model)
	elif args.gpu is not None and torch.cuda.is_available():
		torch.cuda.set_device(args.gpu)
		model = model.cuda(args.gpu)
	elif torch.backends.mps.is_available():
		device = torch.device("mps")
		model = model.to(device)
	else:
		if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
			model.features = torch.nn.DataParallel(model.features)
			model.cuda()
		else:
			model = torch.nn.DataParallel(model).cuda()

	if torch.cuda.is_available():
		if args.gpu:
			device = torch.device('cuda:{}'.format(args.gpu))
		else:
			device = torch.device("cuda")
	elif torch.backends.mps.is_available():
		device = torch.device("mps")
	else:
		device = torch.device("cpu")
	criterion = nn.CrossEntropyLoss().to(device)

	optimizer = torch.optim.SGD(model.parameters(), args.lr,
								momentum=args.momentum,
								weight_decay=args.weight_decay)
	
	scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
	
	if args.resume:
		if os.path.isfile(args.resume):
			print("=> loading checkpoint '{}'".format(args.resume))
			if args.gpu is None:
				checkpoint = torch.load(args.resume)
			elif torch.cuda.is_available():
				loc = 'cuda:{}'.format(args.gpu)
				checkpoint = torch.load(args.resume, map_location=loc)
			args.start_epoch = checkpoint['epoch']
			best_acc1 = checkpoint['best_acc1']
			if args.gpu is not None:
				best_acc1 = best_acc1.to(args.gpu)
			model.load_state_dict(checkpoint['state_dict'])
			optimizer.load_state_dict(checkpoint['optimizer'])
			scheduler.load_state_dict(checkpoint['scheduler'])
			print("=> loaded checkpoint '{}' (epoch {})"
				  .format(args.resume, checkpoint['epoch']))
		else:
			print("=> no checkpoint found at '{}'".format(args.resume))


	if args.dummy:
		print("=> Dummy data is used!")
		train_dataset = datasets.FakeData(1281167, (3, 224, 224), 1000, transforms.ToTensor())
		val_dataset = datasets.FakeData(50000, (3, 224, 224), 1000, transforms.ToTensor())
	else:
		traindir = os.path.join(args.data, 'train')
		valdir = os.path.join(args.data, 'val')
		normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
									 std=[0.229, 0.224, 0.225])

		train_dataset = datasets.ImageFolder(
			traindir,
			transforms.Compose([
				transforms.RandomResizedCrop(224),
				transforms.RandomHorizontalFlip(),
				transforms.ToTensor(),
				normalize,
			]))

		val_dataset = datasets.ImageFolder(
			valdir,
			transforms.Compose([
				transforms.Resize(256),
				transforms.CenterCrop(224),
				transforms.ToTensor(),
				normalize,
			]))

	if args.distributed:
		train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
		val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False, drop_last=True)
	else:
		train_sampler = None
		val_sampler = None

	train_loader = torch.utils.data.DataLoader(
		train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
		num_workers=args.workers, pin_memory=True, sampler=train_sampler)

	val_loader = torch.utils.data.DataLoader(
		val_dataset, batch_size=args.batch_size, shuffle=False,
		num_workers=args.workers, pin_memory=True, sampler=val_sampler)

	if args.evaluate:
		validate(val_loader, model, criterion, args)
		return

	for epoch in range(args.start_epoch, args.epochs):
		if args.distributed:
			train_sampler.set_epoch(epoch)

		train(train_loader, model, criterion, optimizer, epoch, device, args)

		acc1 = validate(val_loader, model, criterion, args)
		
		scheduler.step()
		
		is_best = acc1 > best_acc1
		best_acc1 = max(acc1, best_acc1)

		if not args.multiprocessing_distributed or (args.multiprocessing_distributed
				and args.rank % ngpus_per_node == 0):
			save_checkpoint({
				'epoch': epoch + 1,
				'arch': args.arch,
				'state_dict': model.state_dict(),
				'best_acc1': best_acc1,
				'optimizer' : optimizer.state_dict(),
				'scheduler' : scheduler.state_dict()
			}, is_best)


def train(train_loader, model, criterion, optimizer, epoch, device, args):
	batch_time = AverageMeter('Time', ':6.3f')
	data_time = AverageMeter('Data', ':6.3f')
	losses = AverageMeter('Loss', ':.4e')
	top1 = AverageMeter('Acc@1', ':6.2f')
	top5 = AverageMeter('Acc@5', ':6.2f')
	progress = ProgressMeter(
		len(train_loader),
		[batch_time, data_time, losses, top1, top5],
		prefix="Epoch: [{}]".format(epoch))

	model.train()

	end = time.time()
	for i, (images, target) in enumerate(train_loader):
		data_time.update(time.time() - end)

		images = images.to(device, non_blocking=True)
		target = target.to(device, non_blocking=True)

		output = model(images)
		loss = criterion(output, target)

		acc1, acc5 = accuracy(output, target, topk=(1, 5))
		losses.update(loss.item(), images.size(0))
		top1.update(acc1[0], images.size(0))
		top5.update(acc5[0], images.size(0))

		optimizer.zero_grad()
		loss.backward()
		optimizer.step()

		batch_time.update(time.time() - end)
		end = time.time()

		if i % args.print_freq == 0:
			progress.display(i + 1)


def validate(val_loader, model, criterion, args):

	def run_validate(loader, base_progress=0):
		with torch.no_grad():
			end = time.time()
			for i, (images, target) in enumerate(loader):
				i = base_progress + i
				if args.gpu is not None and torch.cuda.is_available():
					images = images.cuda(args.gpu, non_blocking=True)
				if torch.backends.mps.is_available():
					images = images.to('mps')
					target = target.to('mps')
				if torch.cuda.is_available():
					target = target.cuda(args.gpu, non_blocking=True)

				output = model(images)
				loss = criterion(output, target)

				acc1, acc5 = accuracy(output, target, topk=(1, 5))
				losses.update(loss.item(), images.size(0))
				top1.update(acc1[0], images.size(0))
				top5.update(acc5[0], images.size(0))

				batch_time.update(time.time() - end)
				end = time.time()

				if i % args.print_freq == 0:
					progress.display(i + 1)

	batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)
	losses = AverageMeter('Loss', ':.4e', Summary.NONE)
	top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)
	top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)
	progress = ProgressMeter(
		len(val_loader) + (args.distributed and (len(val_loader.sampler) * args.world_size < len(val_loader.dataset))),
		[batch_time, losses, top1, top5],
		prefix='Test: ')

	model.eval()

	run_validate(val_loader)
	if args.distributed:
		top1.all_reduce()
		top5.all_reduce()

	if args.distributed and (len(val_loader.sampler) * args.world_size < len(val_loader.dataset)):
		aux_val_dataset = Subset(val_loader.dataset,
								 range(len(val_loader.sampler) * args.world_size, len(val_loader.dataset)))
		aux_val_loader = torch.utils.data.DataLoader(
			aux_val_dataset, batch_size=args.batch_size, shuffle=False,
			num_workers=args.workers, pin_memory=True)
		run_validate(aux_val_loader, len(val_loader))

	progress.display_summary()

	return top1.avg


def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
	torch.save(state, filename)
	if is_best:
		shutil.copyfile(filename, 'model_best.pth.tar')

class Summary(Enum):
	NONE = 0
	AVERAGE = 1
	SUM = 2
	COUNT = 3

class AverageMeter(object):
	def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):
		self.name = name
		self.fmt = fmt
		self.summary_type = summary_type
		self.reset()

	def reset(self):
		self.val = 0
		self.avg = 0
		self.sum = 0
		self.count = 0

	def update(self, val, n=1):
		self.val = val
		self.sum += val * n
		self.count += n
		self.avg = self.sum / self.count

	def all_reduce(self):
		if torch.cuda.is_available():
			device = torch.device("cuda")
		elif torch.backends.mps.is_available():
			device = torch.device("mps")
		else:
			device = torch.device("cpu")
		total = torch.tensor([self.sum, self.count], dtype=torch.float32, device=device)
		dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)
		self.sum, self.count = total.tolist()
		self.avg = self.sum / self.count

	def __str__(self):
		fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
		return fmtstr.format(**self.__dict__)
	
	def summary(self):
		fmtstr = ''
		if self.summary_type is Summary.NONE:
			fmtstr = ''
		elif self.summary_type is Summary.AVERAGE:
			fmtstr = '{name} {avg:.3f}'
		elif self.summary_type is Summary.SUM:
			fmtstr = '{name} {sum:.3f}'
		elif self.summary_type is Summary.COUNT:
			fmtstr = '{name} {count:.3f}'
		else:
			raise ValueError('invalid summary type %r' % self.summary_type)
		
		return fmtstr.format(**self.__dict__)


class ProgressMeter(object):
	def __init__(self, num_batches, meters, prefix=""):
		self.batch_fmtstr = self._get_batch_fmtstr(num_batches)
		self.meters = meters
		self.prefix = prefix

	def display(self, batch):
		entries = [self.prefix + self.batch_fmtstr.format(batch)]
		entries += [str(meter) for meter in self.meters]
		print('\t'.join(entries))
		
	def display_summary(self):
		entries = [" *"]
		entries += [meter.summary() for meter in self.meters]
		print(' '.join(entries))

	def _get_batch_fmtstr(self, num_batches):
		num_digits = len(str(num_batches // 1))
		fmt = '{:' + str(num_digits) + 'd}'
		return '[' + fmt + '/' + fmt.format(num_batches) + ']'

def accuracy(output, target, topk=(1,)):
	with torch.no_grad():
		maxk = max(topk)
		batch_size = target.size(0)

		_, pred = output.topk(maxk, 1, True, True)
		pred = pred.t()
		correct = pred.eq(target.view(1, -1).expand_as(pred))

		res = []
		for k in topk:
			correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
			res.append(correct_k.mul_(100.0 / batch_size))
		return res


if __name__ == '__main__':
	main()


import json
import logging
import os
import random
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import evaluate
import numpy as np
import torch
from datasets import load_dataset
from huggingface_hub import hf_hub_download
from PIL import Image
from torch import nn
from torchvision import transforms
from torchvision.transforms import functional

import transformers
from transformers import (
	AutoConfig,
	AutoImageProcessor,
	AutoModelForSemanticSegmentation,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	default_data_collator,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version



logger = logging.getLogger(__name__)

check_min_version("4.38.0.dev0")

require_version("datasets>=2.0.0", "To fix: pip install -r examples/pytorch/semantic-segmentation/requirements.txt")


def pad_if_smaller(img, size, fill=0):
	size = (size, size) if isinstance(size, int) else size
	original_width, original_height = img.size
	pad_height = size[1] - original_height if original_height < size[1] else 0
	pad_width = size[0] - original_width if original_width < size[0] else 0
	img = functional.pad(img, (0, 0, pad_width, pad_height), fill=fill)
	return img


class Compose:
	def __init__(self, transforms):
		self.transforms = transforms

	def __call__(self, image, target):
		for t in self.transforms:
			image, target = t(image, target)
		return image, target


class Identity:
	def __init__(self):
		pass

	def __call__(self, image, target):
		return image, target


class Resize:
	def __init__(self, size):
		self.size = size

	def __call__(self, image, target):
		image = functional.resize(image, self.size)
		target = functional.resize(target, self.size, interpolation=transforms.InterpolationMode.NEAREST)
		return image, target


class RandomResize:
	def __init__(self, min_size, max_size=None):
		self.min_size = min_size
		if max_size is None:
			max_size = min_size
		self.max_size = max_size

	def __call__(self, image, target):
		size = random.randint(self.min_size, self.max_size)
		image = functional.resize(image, size)
		target = functional.resize(target, size, interpolation=transforms.InterpolationMode.NEAREST)
		return image, target


class RandomCrop:
	def __init__(self, size):
		self.size = size if isinstance(size, tuple) else (size, size)

	def __call__(self, image, target):
		image = pad_if_smaller(image, self.size)
		target = pad_if_smaller(target, self.size, fill=255)
		crop_params = transforms.RandomCrop.get_params(image, self.size)
		image = functional.crop(image, *crop_params)
		target = functional.crop(target, *crop_params)
		return image, target


class RandomHorizontalFlip:
	def __init__(self, flip_prob):
		self.flip_prob = flip_prob

	def __call__(self, image, target):
		if random.random() < self.flip_prob:
			image = functional.hflip(image)
			target = functional.hflip(target)
		return image, target


class PILToTensor:
	def __call__(self, image, target):
		image = functional.pil_to_tensor(image)
		target = torch.as_tensor(np.array(target), dtype=torch.int64)
		return image, target


class ConvertImageDtype:
	def __init__(self, dtype):
		self.dtype = dtype

	def __call__(self, image, target):
		image = functional.convert_image_dtype(image, self.dtype)
		return image, target


class Normalize:
	def __init__(self, mean, std):
		self.mean = mean
		self.std = std

	def __call__(self, image, target):
		image = functional.normalize(image, mean=self.mean, std=self.std)
		return image, target


class ReduceLabels:
	def __call__(self, image, target):
		if not isinstance(target, np.ndarray):
			target = np.array(target).astype(np.uint8)
		target[target == 0] = 255
		target = target - 1
		target[target == 254] = 255

		target = Image.fromarray(target)
		return image, target


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default="segments/sidewalk-semantic",
		metadata={
			"help": "Name of a dataset from the hub (could be your own, possibly private dataset hosted on the hub)."
		},
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_val_split: Optional[float] = field(
		default=0.15, metadata={"help": "Percent to split off of train for validation."}
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	reduce_labels: Optional[bool] = field(
		default=False,
		metadata={"help": "Whether or not to reduce all labels by 1 and replace background by 255."},
	)

	def __post_init__(self):
		if self.dataset_name is None and (self.train_dir is None and self.validation_dir is None):
			raise ValueError(
				"You must specify either a dataset name from the hub or a train and/or validation directory."
			)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		default="nvidia/mit-b0",
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	image_processor_name: str = field(default=None, metadata={"help": "Name or path of preprocessor config."})
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_semantic_segmentation", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	dataset = load_dataset(data_args.dataset_name, cache_dir=model_args.cache_dir)

	if "pixel_values" in dataset["train"].column_names:
		dataset = dataset.rename_columns({"pixel_values": "image"})
	if "annotation" in dataset["train"].column_names:
		dataset = dataset.rename_columns({"annotation": "label"})

	data_args.train_val_split = None if "validation" in dataset.keys() else data_args.train_val_split
	if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:
		split = dataset["train"].train_test_split(data_args.train_val_split)
		dataset["train"] = split["train"]
		dataset["validation"] = split["test"]

	if data_args.dataset_name == "scene_parse_150":
		repo_id = "huggingface/label-files"
		filename = "ade20k-id2label.json"
	else:
		repo_id = data_args.dataset_name
		filename = "id2label.json"
	id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
	id2label = {int(k): v for k, v in id2label.items()}
	label2id = {v: str(k) for k, v in id2label.items()}

	metric = evaluate.load("mean_iou", cache_dir=model_args.cache_dir)

	@torch.no_grad()
	def compute_metrics(eval_pred):
		logits, labels = eval_pred
		logits_tensor = torch.from_numpy(logits)
		logits_tensor = nn.functional.interpolate(
			logits_tensor,
			size=labels.shape[-2:],
			mode="bilinear",
			align_corners=False,
		).argmax(dim=1)

		pred_labels = logits_tensor.detach().cpu().numpy()
		metrics = metric.compute(
			predictions=pred_labels,
			references=labels,
			num_labels=len(id2label),
			ignore_index=0,
			reduce_labels=image_processor.do_reduce_labels,
		)
		per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
		per_category_iou = metrics.pop("per_category_iou").tolist()

		metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
		metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})

		return metrics

	config = AutoConfig.from_pretrained(
		model_args.config_name or model_args.model_name_or_path,
		label2id=label2id,
		id2label=id2label,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSemanticSegmentation.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	image_processor = AutoImageProcessor.from_pretrained(
		model_args.image_processor_name or model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	if "shortest_edge" in image_processor.size:
		size = (image_processor.size["shortest_edge"], image_processor.size["shortest_edge"])
	else:
		size = (image_processor.size["height"], image_processor.size["width"])
	train_transforms = Compose(
		[
			ReduceLabels() if data_args.reduce_labels else Identity(),
			RandomCrop(size=size),
			RandomHorizontalFlip(flip_prob=0.5),
			PILToTensor(),
			ConvertImageDtype(torch.float),
			Normalize(mean=image_processor.image_mean, std=image_processor.image_std),
		]
	)
	val_transforms = Compose(
		[
			ReduceLabels() if data_args.reduce_labels else Identity(),
			Resize(size=size),
			PILToTensor(),
			ConvertImageDtype(torch.float),
			Normalize(mean=image_processor.image_mean, std=image_processor.image_std),
		]
	)

	def preprocess_train(example_batch):
		pixel_values = []
		labels = []
		for image, target in zip(example_batch["image"], example_batch["label"]):
			image, target = train_transforms(image.convert("RGB"), target)
			pixel_values.append(image)
			labels.append(target)

		encoding = {}
		encoding["pixel_values"] = torch.stack(pixel_values)
		encoding["labels"] = torch.stack(labels)

		return encoding

	def preprocess_val(example_batch):
		pixel_values = []
		labels = []
		for image, target in zip(example_batch["image"], example_batch["label"]):
			image, target = val_transforms(image.convert("RGB"), target)
			pixel_values.append(image)
			labels.append(target)

		encoding = {}
		encoding["pixel_values"] = torch.stack(pixel_values)
		encoding["labels"] = torch.stack(labels)

		return encoding

	if training_args.do_train:
		if "train" not in dataset:
			raise ValueError("--do_train requires a train dataset")
		if data_args.max_train_samples is not None:
			dataset["train"] = (
				dataset["train"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))
			)
		dataset["train"].set_transform(preprocess_train)

	if training_args.do_eval:
		if "validation" not in dataset:
			raise ValueError("--do_eval requires a validation dataset")
		if data_args.max_eval_samples is not None:
			dataset["validation"] = (
				dataset["validation"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))
			)
		dataset["validation"].set_transform(preprocess_val)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=dataset["train"] if training_args.do_train else None,
		eval_dataset=dataset["validation"] if training_args.do_eval else None,
		compute_metrics=compute_metrics,
		tokenizer=image_processor,
		data_collator=default_data_collator,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()
		trainer.log_metrics("train", train_result.metrics)
		trainer.save_metrics("train", train_result.metrics)
		trainer.save_state()

	if training_args.do_eval:
		metrics = trainer.evaluate()
		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {
		"finetuned_from": model_args.model_name_or_path,
		"dataset": data_args.dataset_name,
		"tags": ["image-segmentation", "vision"],
	}
	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


if __name__ == "__main__":
	main()

from allennlp.data.data_loaders.data_loader import DataLoader, TensorDict
from allennlp.data.data_loaders.multiprocess_data_loader import MultiProcessDataLoader, WorkerError
from allennlp.data.data_loaders.multitask_data_loader import MultiTaskDataLoader
from allennlp.data.data_loaders.simple_data_loader import SimpleDataLoader
from allennlp.data.data_loaders.data_collator import allennlp_collate



import argparse
import math
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Union

import datasets
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from datasets import DatasetDict, concatenate_datasets, load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data.dataloader import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	AdamW,
	SchedulerType,
	Wav2Vec2Config,
	Wav2Vec2FeatureExtractor,
	Wav2Vec2ForPreTraining,
	get_scheduler,
	is_wandb_available,
	set_seed,
)
from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices
from transformers.utils import send_example_telemetry


logger = get_logger(__name__)


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a text classification task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_names",
		nargs="+",
		type=str,
		required=True,
		help="The configuration names of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_split_names",
		nargs="+",
		type=str,
		required=True,
		help="The names of the training data set splits to use (via the datasets library).",
	)
	parser.add_argument(
		"--preprocessing_num_workers",
		type=int,
		default=None,
		help="The number of processes to use for the preprocessing.",
	)
	parser.add_argument(
		"--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
	)
	parser.add_argument(
		"--preprocessing_only",
		action="store_true",
		help="Only run the preprocessing script to be cached for future use",
	)
	parser.add_argument(
		"--cache_dir",
		type=str,
		default=None,
		help="Where do you want to store the pretrained models downloaded from huggingface.co",
	)
	parser.add_argument(
		"--validation_split_percentage",
		type=int,
		default=1,
		help="Percentage of training data that should be used for validation if no validation is present in dataset.",
	)
	parser.add_argument(
		"--logging_steps",
		type=int,
		default=500,
		help="Number of steps between each logging",
	)
	parser.add_argument(
		"--saving_steps",
		type=int,
		default=500,
		help="Number of steps between each logging",
	)
	parser.add_argument(
		"--audio_column_name",
		type=str,
		default="audio",
		help="Column in the dataset that contains speech file path. Defaults to 'audio'",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=True,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--train_cache_file_name",
		type=str,
		default=None,
		help="Path to the train cached file name",
	)
	parser.add_argument(
		"--validation_cache_file_name",
		type=str,
		default=None,
		help="Path to the validation cached file name",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--gradient_checkpointing",
		action="store_true",
		help="If True, use gradient checkpointing to save memory at the expense of slower backward pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=0, help="A seed for reproducible training.")
	parser.add_argument(
		"--max_gumbel_temperature",
		type=float,
		default=2.0,
		help="Maximum temperature for gumbel softmax.",
	)
	parser.add_argument(
		"--min_gumbel_temperature",
		type=float,
		default=0.5,
		help="Minimum temperature for gumbel softmax.",
	)
	parser.add_argument(
		"--gumbel_temperature_decay", type=float, default=0.999995, help="Decay of gumbel temperature during training."
	)
	parser.add_argument(
		"--max_duration_in_seconds",
		type=float,
		default=5.0,
		help="Filter out audio files that are longer than `max_duration_in_seconds` seconds",
	)
	parser.add_argument(
		"--min_duration_in_seconds",
		type=float,
		default=3.0,
		help="Filter out audio files that are shorter than `min_duration_in_seconds` seconds",
	)
	parser.add_argument(
		"--pad_to_multiple_of",
		type=int,
		default=None,
		help=(
			"If set will pad the sequence to a multiple of the provided value. This is especially useful to enable the"
			" use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta)."
		),
	)
	parser.add_argument(
		"--adam_beta1",
		type=float,
		default=0.9,
		help="Beta1 for AdamW optimizer",
	)
	parser.add_argument(
		"--adam_beta2",
		type=float,
		default=0.999,
		help="Beta2 for AdamW optimizer",
	)
	parser.add_argument(
		"--adam_epsilon",
		type=float,
		default=1e-8,
		help="Epsilon for AdamW optimizer",
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--mask_time_prob",
		type=float,
		default=None,
		help=(
			"Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked in the"
			" contrastive task. If omitted, will pull value from model config."
		),
	)
	parser.add_argument(
		"--mask_time_length",
		type=int,
		default=None,
		help=(
			"Length of each vector mask span to mask along the time axis in the contrastive task."
			" If omitted, will pull value from model config."
		),
	)
	args = parser.parse_args()

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	if args.output_dir is not None:
		os.makedirs(args.output_dir, exist_ok=True)

	return args


@dataclass
class DataCollatorForWav2Vec2Pretraining:

	model: Wav2Vec2ForPreTraining
	feature_extractor: Wav2Vec2FeatureExtractor
	padding: Union[bool, str] = "longest"
	pad_to_multiple_of: Optional[int] = None
	mask_time_prob: Optional[float] = 0.65
	mask_time_length: Optional[int] = 10

	def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
		batch = self.feature_extractor.pad(
			features,
			padding=self.padding,
			pad_to_multiple_of=self.pad_to_multiple_of,
			return_tensors="pt",
		)

		device = batch["input_values"].device
		batch_size = batch["input_values"].shape[0]

		mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch["input_values"].shape[-1])
		mask_indices_seq_length = int(mask_indices_seq_length)

		if batch.get("attention_mask") is not None:
			batch["sub_attention_mask"] = self.model._get_feature_vector_attention_mask(
				mask_indices_seq_length, batch["attention_mask"]
			)

		features_shape = (batch_size, mask_indices_seq_length)

		mask_time_indices = _compute_mask_indices(
			features_shape,
			self.mask_time_prob,
			self.mask_time_length,
			attention_mask=batch.get("sub_attention_mask"),
		)

		sampled_negative_indices = _sample_negative_indices(
			features_shape,
			self.model.config.num_negatives,
			mask_time_indices=mask_time_indices,
		)
		batch["mask_time_indices"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)
		batch["sampled_negative_indices"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)

		return batch


def multiply_grads(params, c):
	for p in params:
		if p.grad is not None:
			if torch.is_tensor(c):
				c = c.to(p.grad.device)
			p.grad.data.mul_(c)


def get_grad_norm(params, scale=1):
	total_norm = 0.0
	for p in params:
		if p.grad is not None:
			param_norm = (p.grad.detach().data / scale).norm(2)
			total_norm += param_norm.item() ** 2
	total_norm = total_norm**0.5
	return total_norm


def main():
	args = parse_args()

	send_example_telemetry("run_wav2vec2_pretraining_no_trainer", args)

	accelerator = Accelerator()
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()

		if is_wandb_available():
			import wandb

			wandb.init(project=args.output_dir.split("/")[-1])
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub and not args.preprocessing_only:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	datasets_splits = []
	for dataset_config_name, train_split_name in zip(args.dataset_config_names, args.dataset_split_names):
		dataset_split = load_dataset(
			args.dataset_name,
			dataset_config_name,
			split=train_split_name,
			cache_dir=args.cache_dir,
		)
		datasets_splits.append(dataset_split)

	raw_datasets = DatasetDict()
	if len(datasets_splits) > 1:
		raw_datasets["train"] = concatenate_datasets(datasets_splits).shuffle(seed=args.seed)
	else:
		raw_datasets["train"] = datasets_splits[0]

	num_validation_samples = raw_datasets["train"].num_rows * args.validation_split_percentage // 100

	if num_validation_samples == 0:
		raise ValueError(
			"`args.validation_split_percentage` is less than a single sample "
			f"for {len(raw_datasets['train'])} training samples. Increase "
			"`args.num_validation_split_percentage`. "
		)

	raw_datasets["validation"] = raw_datasets["train"].select(range(num_validation_samples))
	raw_datasets["train"] = raw_datasets["train"].select(range(num_validation_samples, raw_datasets["train"].num_rows))

	feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(args.model_name_or_path)

	raw_datasets = raw_datasets.cast_column(
		args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)
	)

	if not feature_extractor.do_normalize:
		raise ValueError(
			"Training is only supported for normalized inputs. Make sure ``feature_extractor.do_normalize == True``"
		)

	max_length = int(args.max_duration_in_seconds * feature_extractor.sampling_rate)
	min_length = int(args.min_duration_in_seconds * feature_extractor.sampling_rate)

	def prepare_dataset(batch):
		sample = batch[args.audio_column_name]

		inputs = feature_extractor(
			sample["array"], sampling_rate=sample["sampling_rate"], max_length=max_length, truncation=True
		)
		batch["input_values"] = inputs.input_values[0]
		batch["input_length"] = len(inputs.input_values[0])

		return batch

	cache_file_names = None
	if args.train_cache_file_name is not None:
		cache_file_names = {"train": args.train_cache_file_name, "validation": args.validation_cache_file_name}

	with accelerator.main_process_first():
		vectorized_datasets = raw_datasets.map(
			prepare_dataset,
			num_proc=args.preprocessing_num_workers,
			remove_columns=raw_datasets["train"].column_names,
			cache_file_names=cache_file_names,
		)

		if min_length > 0.0:
			vectorized_datasets = vectorized_datasets.filter(
				lambda x: x > min_length,
				num_proc=args.preprocessing_num_workers,
				input_columns=["input_length"],
			)

		vectorized_datasets = vectorized_datasets.remove_columns("input_length")

	if args.preprocessing_only:
		return

	config = Wav2Vec2Config.from_pretrained(args.model_name_or_path)

	if not config.do_stable_layer_norm or config.feat_extract_norm != "layer":
		raise ValueError(
			"PreTraining is only supported for ``config.do_stable_layer_norm=True`` and"
			" ``config.feat_extract_norm='layer'"
		)

	model = Wav2Vec2ForPreTraining(config)

	if args.gradient_checkpointing:
		model.gradient_checkpointing_enable()


	mask_time_prob = config.mask_time_prob if args.mask_time_prob is None else args.mask_time_prob
	mask_time_length = config.mask_time_length if args.mask_time_length is None else args.mask_time_length

	data_collator = DataCollatorForWav2Vec2Pretraining(
		model=model,
		feature_extractor=feature_extractor,
		pad_to_multiple_of=args.pad_to_multiple_of,
		mask_time_prob=mask_time_prob,
		mask_time_length=mask_time_length,
	)
	train_dataloader = DataLoader(
		vectorized_datasets["train"],
		shuffle=True,
		collate_fn=data_collator,
		batch_size=args.per_device_train_batch_size,
	)
	eval_dataloader = DataLoader(
		vectorized_datasets["validation"], collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
	)

	optimizer = AdamW(
		list(model.parameters()),
		lr=args.learning_rate,
		betas=[args.adam_beta1, args.adam_beta2],
		eps=args.adam_epsilon,
	)

	model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)

	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps,
		num_training_steps=args.max_train_steps,
	)

	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(vectorized_datasets['train'])}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	completed_steps = 0
	starting_epoch = 0

	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0
	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		for step, batch in enumerate(train_dataloader):
			num_losses = batch["mask_time_indices"].sum()
			sub_attention_mask = batch.pop("sub_attention_mask", None)
			sub_attention_mask = (
				sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch["mask_time_indices"])
			)
			percent_masked = num_losses / sub_attention_mask.sum()

			outputs = model(**batch)

			loss = outputs.loss / args.gradient_accumulation_steps
			accelerator.backward(loss)

			if accelerator.state.num_processes > 1:
				num_losses = accelerator.gather_for_metrics(num_losses).sum()
				gradient_multiplier = accelerator.state.num_processes / num_losses
				multiply_grads(model.module.parameters(), gradient_multiplier)
			else:
				multiply_grads(model.parameters(), 1 / num_losses)

			if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
				scale = (
					accelerator.scaler._scale.item()
					if hasattr(accelerator, "scaler") and accelerator.scaler is not None
					else 1
				)
				if accelerator.state.num_processes > 1:
					grad_norm = get_grad_norm(model.module.parameters(), scale)
				else:
					grad_norm = get_grad_norm(model.parameters(), scale)

				optimizer.step()
				optimizer.zero_grad()

				if not accelerator.optimizer_step_was_skipped:
					lr_scheduler.step()
				elif accelerator.is_local_main_process:
					progress_bar.write(
						f"Gradients have overflown - skipping update step... Updating gradient scale to {scale}..."
					)

				gumbel_temperature = max(
					args.max_gumbel_temperature * args.gumbel_temperature_decay**completed_steps,
					args.min_gumbel_temperature,
				)
				if hasattr(model, "module"):
					model.module.set_gumbel_temperature(gumbel_temperature)
				else:
					model.set_gumbel_temperature(gumbel_temperature)

				progress_bar.update(1)
				completed_steps += 1

			if (step + 1) % (args.gradient_accumulation_steps * args.logging_steps) == 0:
				loss.detach()
				outputs.contrastive_loss.detach()
				outputs.diversity_loss.detach()

				if accelerator.state.num_processes > 1:
					loss = accelerator.gather_for_metrics(loss).sum()
					outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()
					outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()
					percent_masked = accelerator.gather_for_metrics(percent_masked).sum()

				train_logs = {
					"loss": (loss * args.gradient_accumulation_steps) / num_losses,
					"constrast_loss": outputs.contrastive_loss / num_losses,
					"div_loss": outputs.diversity_loss / num_losses,
					"%_mask_idx": percent_masked / accelerator.num_processes,
					"ppl": outputs.codevector_perplexity,
					"lr": torch.tensor(optimizer.param_groups[0]["lr"]),
					"temp": torch.tensor(gumbel_temperature),
					"grad_norm": torch.tensor(grad_norm),
				}
				log_str = ""
				for k, v in train_logs.items():
					log_str += "| {}: {:.3e}".format(k, v.item())

				if accelerator.is_local_main_process:
					progress_bar.write(log_str)
					if is_wandb_available():
						wandb.log(train_logs)

			if (step + 1) % (args.gradient_accumulation_steps * args.saving_steps) == 0:
				if (args.push_to_hub and epoch < args.num_train_epochs - 1) or args.output_dir is not None:
					accelerator.wait_for_everyone()
					unwrapped_model = accelerator.unwrap_model(model)
					unwrapped_model.save_pretrained(
						args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
					)

				if (args.push_to_hub and epoch < args.num_train_epochs - 1) and accelerator.is_main_process:
					repo.push_to_hub(
						commit_message=f"Training in progress step {completed_steps}",
						blocking=False,
						auto_lfs_prune=True,
					)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()

		val_logs = {
			"val_loss": 0,
			"val_contrastive_loss": 0,
			"val_diversity_loss": 0,
			"val_num_losses": 0,
		}
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				batch.pop("sub_attention_mask", None)
				outputs = model(**batch)

			val_logs["val_loss"] += outputs.loss
			val_logs["val_contrastive_loss"] += outputs.contrastive_loss
			val_logs["val_diversity_loss"] += outputs.diversity_loss
			val_logs["val_num_losses"] += batch["mask_time_indices"].sum()

		if accelerator.num_processes > 1:
			val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}

		val_logs = {k: v / val_logs["val_num_losses"] for k, v in val_logs.items()}

		log_str = ""
		for k, v in val_logs.items():
			log_str += "| {}: {:.3e}".format(k, v.item())

		if accelerator.is_local_main_process:
			progress_bar.write(log_str)
			if is_wandb_available():
				wandb.log(val_logs)

		if args.output_dir is not None:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				if args.push_to_hub:
					repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)


if __name__ == "__main__":
	main()

from allennlp.nn.parallel.sharded_module_mixin import ShardedModuleMixin
from allennlp.nn.parallel.ddp_accelerator import (
	DdpAccelerator,
	DdpWrappedModel,
	TorchDdpAccelerator,
)
from allennlp.nn.parallel.fairscale_fsdp_accelerator import (
	FairScaleFsdpAccelerator,
	FairScaleFsdpWrappedModel,
)


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
from datasets import load_dataset
from trainer_qa import QuestionAnsweringTrainer
from utils_qa import postprocess_qa_predictions_with_beam_search

import transformers
from transformers import (
	DataCollatorWithPadding,
	EvalPrediction,
	HfArgumentParser,
	TrainingArguments,
	XLNetConfig,
	XLNetForQuestionAnswering,
	XLNetTokenizerFast,
	default_data_collator,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/question-answering/requirements.txt")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
	)
	test_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input test data file to test the perplexity on (a text file)."},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_seq_length: int = field(
		default=384,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	pad_to_max_length: bool = field(
		default=True,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when"
				" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU)."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	version_2_with_negative: bool = field(
		default=False, metadata={"help": "If true, some of the examples do not have an answer."}
	)
	null_score_diff_threshold: float = field(
		default=0.0,
		metadata={
			"help": (
				"The threshold used to select the null answer: if the best answer has a score that is less than "
				"the score of the null answer minus this threshold, the null answer is selected for this example. "
				"Only useful when `version_2_with_negative=True`."
			)
		},
	)
	doc_stride: int = field(
		default=128,
		metadata={"help": "When splitting up a long document into chunks, how much stride to take between chunks."},
	)
	n_best_size: int = field(
		default=20,
		metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
	)
	max_answer_length: int = field(
		default=30,
		metadata={
			"help": (
				"The maximum length of an answer that can be generated. This is needed because the start "
				"and end predictions are not conditioned on one another."
			)
		},
	)

	def __post_init__(self):
		if (
			self.dataset_name is None
			and self.train_file is None
			and self.validation_file is None
			and self.test_file is None
		):
			raise ValueError("Need either a dataset name or a training/validation/test file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
			if self.test_file is not None:
				extension = self.test_file.split(".")[-1]
				assert extension in ["csv", "json"], "`test_file` should be a csv or a json file."


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_qa_beam_search", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
			extension = data_args.train_file.split(".")[-1]
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
			extension = data_args.validation_file.split(".")[-1]
		if data_args.test_file is not None:
			data_files["test"] = data_args.test_file
			extension = data_args.test_file.split(".")[-1]
		raw_datasets = load_dataset(
			extension,
			data_files=data_files,
			field="data",
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

	config = XLNetConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
	)
	tokenizer = XLNetTokenizerFast.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
	)
	model = XLNetForQuestionAnswering.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
	)

	if training_args.do_train:
		column_names = raw_datasets["train"].column_names
	elif training_args.do_eval:
		column_names = raw_datasets["validation"].column_names
	else:
		column_names = raw_datasets["test"].column_names
	question_column_name = "question" if "question" in column_names else column_names[0]
	context_column_name = "context" if "context" in column_names else column_names[1]
	answer_column_name = "answers" if "answers" in column_names else column_names[2]

	pad_on_right = tokenizer.padding_side == "right"

	if data_args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)
	max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	def prepare_train_features(examples):
		examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]

		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=data_args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			return_special_tokens_mask=True,
			return_token_type_ids=True,
			padding="max_length",
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
		offset_mapping = tokenized_examples.pop("offset_mapping")
		special_tokens = tokenized_examples.pop("special_tokens_mask")

		tokenized_examples["start_positions"] = []
		tokenized_examples["end_positions"] = []
		tokenized_examples["is_impossible"] = []
		tokenized_examples["cls_index"] = []
		tokenized_examples["p_mask"] = []

		for i, offsets in enumerate(offset_mapping):
			input_ids = tokenized_examples["input_ids"][i]
			cls_index = input_ids.index(tokenizer.cls_token_id)
			tokenized_examples["cls_index"].append(cls_index)

			sequence_ids = tokenized_examples["token_type_ids"][i]
			for k, s in enumerate(special_tokens[i]):
				if s:
					sequence_ids[k] = 3
			context_idx = 1 if pad_on_right else 0

			tokenized_examples["p_mask"].append(
				[
					0.0 if (not special_tokens[i][k] and s == context_idx) or k == cls_index else 1.0
					for k, s in enumerate(sequence_ids)
				]
			)

			sample_index = sample_mapping[i]
			answers = examples[answer_column_name][sample_index]
			if len(answers["answer_start"]) == 0:
				tokenized_examples["start_positions"].append(cls_index)
				tokenized_examples["end_positions"].append(cls_index)
				tokenized_examples["is_impossible"].append(1.0)
			else:
				start_char = answers["answer_start"][0]
				end_char = start_char + len(answers["text"][0])

				token_start_index = 0
				while sequence_ids[token_start_index] != context_idx:
					token_start_index += 1

				token_end_index = len(input_ids) - 1
				while sequence_ids[token_end_index] != context_idx:
					token_end_index -= 1
				if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
					tokenized_examples["start_positions"].append(cls_index)
					tokenized_examples["end_positions"].append(cls_index)
					tokenized_examples["is_impossible"].append(1.0)
				else:
					while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
						token_start_index += 1
					tokenized_examples["start_positions"].append(token_start_index - 1)
					while offsets[token_end_index][1] >= end_char:
						token_end_index -= 1
					tokenized_examples["end_positions"].append(token_end_index + 1)
					tokenized_examples["is_impossible"].append(0.0)

		return tokenized_examples

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				prepare_train_features,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on train dataset",
			)
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	def prepare_validation_features(examples):
		tokenized_examples = tokenizer(
			examples[question_column_name if pad_on_right else context_column_name],
			examples[context_column_name if pad_on_right else question_column_name],
			truncation="only_second" if pad_on_right else "only_first",
			max_length=max_seq_length,
			stride=data_args.doc_stride,
			return_overflowing_tokens=True,
			return_offsets_mapping=True,
			return_special_tokens_mask=True,
			return_token_type_ids=True,
			padding="max_length",
		)

		sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

		special_tokens = tokenized_examples.pop("special_tokens_mask")

		tokenized_examples["example_id"] = []

		tokenized_examples["cls_index"] = []
		tokenized_examples["p_mask"] = []

		for i, input_ids in enumerate(tokenized_examples["input_ids"]):
			cls_index = input_ids.index(tokenizer.cls_token_id)
			tokenized_examples["cls_index"].append(cls_index)

			sequence_ids = tokenized_examples["token_type_ids"][i]
			for k, s in enumerate(special_tokens[i]):
				if s:
					sequence_ids[k] = 3
			context_idx = 1 if pad_on_right else 0

			tokenized_examples["p_mask"].append(
				[
					0.0 if (not special_tokens[i][k] and s == context_idx) or k == cls_index else 1.0
					for k, s in enumerate(sequence_ids)
				]
			)

			sample_index = sample_mapping[i]
			tokenized_examples["example_id"].append(examples["id"][sample_index])

			tokenized_examples["offset_mapping"][i] = [
				(o if sequence_ids[k] == context_idx else None)
				for k, o in enumerate(tokenized_examples["offset_mapping"][i])
			]

		return tokenized_examples

	if training_args.do_eval:
		if "validation" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_examples = raw_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_examples), data_args.max_eval_samples)
			eval_examples = eval_examples.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_examples.map(
				prepare_validation_features,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on validation dataset",
			)
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

	if training_args.do_predict:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_examples = raw_datasets["test"]
		if data_args.max_predict_samples is not None:
			predict_examples = predict_examples.select(range(data_args.max_predict_samples))
		with training_args.main_process_first(desc="prediction dataset map pre-processing"):
			predict_dataset = predict_examples.map(
				prepare_validation_features,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not data_args.overwrite_cache,
				desc="Running tokenizer on prediction dataset",
			)
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))

	data_collator = (
		default_data_collator
		if data_args.pad_to_max_length
		else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)
	)

	def post_processing_function(examples, features, predictions, stage="eval"):
		predictions, scores_diff_json = postprocess_qa_predictions_with_beam_search(
			examples=examples,
			features=features,
			predictions=predictions,
			version_2_with_negative=data_args.version_2_with_negative,
			n_best_size=data_args.n_best_size,
			max_answer_length=data_args.max_answer_length,
			start_n_top=model.config.start_n_top,
			end_n_top=model.config.end_n_top,
			output_dir=training_args.output_dir,
			log_level=log_level,
			prefix=stage,
		)
		if data_args.version_2_with_negative:
			formatted_predictions = [
				{"id": k, "prediction_text": v, "no_answer_probability": scores_diff_json[k]}
				for k, v in predictions.items()
			]
		else:
			formatted_predictions = [{"id": k, "prediction_text": v} for k, v in predictions.items()]

		references = [{"id": ex["id"], "answers": ex[answer_column_name]} for ex in examples]
		return EvalPrediction(predictions=formatted_predictions, label_ids=references)

	metric = evaluate.load(
		"squad_v2" if data_args.version_2_with_negative else "squad", cache_dir=model_args.cache_dir
	)

	def compute_metrics(p: EvalPrediction):
		return metric.compute(predictions=p.predictions, references=p.label_ids)

	trainer = QuestionAnsweringTrainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		eval_examples=eval_examples if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		post_process_function=post_processing_function,
		compute_metrics=compute_metrics,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload

		metrics = train_result.metrics

		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate()

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")
		results = trainer.predict(predict_dataset, predict_examples)
		metrics = results.metrics

		max_predict_samples = (
			data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
		)
		metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))

		trainer.log_metrics("predict", metrics)
		trainer.save_metrics("predict", metrics)

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "question-answering"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

import torch.nn as nn
import torch


class LayerNorm(nn.Module):
	"Construct a layernorm module (See citation for details)."

	def __init__(self, features, eps=1e-6):
		super(LayerNorm, self).__init__()
		self.a_2 = nn.Parameter(torch.ones(features))
		self.b_2 = nn.Parameter(torch.zeros(features))
		self.eps = eps

	def forward(self, x):
		mean = x.mean(-1, keepdim=True)
		std = x.std(-1, keepdim=True)
		return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

from allennlp.modules.conditional_random_field.conditional_random_field import (
	ConditionalRandomField,
	allowed_transitions,
)
from allennlp.modules.conditional_random_field.conditional_random_field_wemission import (
	ConditionalRandomFieldWeightEmission,
)
from allennlp.modules.conditional_random_field.conditional_random_field_wtrans import (
	ConditionalRandomFieldWeightTrans,
)
from allennlp.modules.conditional_random_field.conditional_random_field_wlannoy import (
	ConditionalRandomFieldWeightLannoy,
)

from typing import Sequence, List
import math

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder


class ResidualBlock(torch.nn.Module):
	def __init__(
		self,
		input_dim: int,
		layers: Sequence[Sequence[int]],
		direction: str,
		do_weight_norm: bool = True,
		dropout: float = 0.0,
	) -> None:
		super().__init__()

		self.dropout = dropout
		self._convolutions = torch.nn.ModuleList()
		last_dim = input_dim
		for k, layer in enumerate(layers):
			if len(layer) == 2:
				conv = torch.nn.Conv1d(
					last_dim, layer[1] * 2, layer[0], stride=1, padding=layer[0] - 1, bias=True
				)
			elif len(layer) == 3:
				assert layer[0] == 2, "only support kernel = 2 for now"
				conv = torch.nn.Conv1d(
					last_dim,
					layer[1] * 2,
					layer[0],
					stride=1,
					padding=layer[2],
					dilation=layer[2],
					bias=True,
				)
			else:
				raise ValueError("each layer must have length 2 or 3")

			if k == 0:
				conv_dropout = dropout
			else:
				conv_dropout = 0.0
			std = math.sqrt((4 * (1.0 - conv_dropout)) / (layer[0] * last_dim))

			conv.weight.data.normal_(0, std=std)
			conv.bias.data.zero_()

			if do_weight_norm:
				conv = torch.nn.utils.weight_norm(conv, name="weight", dim=0)

			self._convolutions.append(conv)
			last_dim = layer[1]

		assert last_dim == input_dim

		if direction not in ("forward", "backward"):
			raise ConfigurationError(f"invalid direction: {direction}")
		self._direction = direction

	def forward(self, x: torch.Tensor) -> torch.Tensor:

		out = x
		timesteps = x.size(2)
		for k, convolution in enumerate(self._convolutions):
			if k == 0 and self.dropout > 0:
				out = torch.nn.functional.dropout(out, self.dropout, self.training)

			conv_out = convolution(out)

			dims_to_remove = conv_out.size(2) - timesteps
			if dims_to_remove > 0:
				if self._direction == "forward":
					conv_out = conv_out.narrow(2, 0, timesteps)
				else:
					conv_out = conv_out.narrow(2, dims_to_remove, timesteps)

			out = torch.nn.functional.glu(conv_out, dim=1)

		return (out + x) * math.sqrt(0.5)


@Seq2SeqEncoder.register("gated-cnn-encoder")
class GatedCnnEncoder(Seq2SeqEncoder):

	def __init__(
		self,
		input_dim: int,
		layers: Sequence[Sequence[Sequence[int]]],
		dropout: float = 0.0,
		return_all_layers: bool = False,
	) -> None:
		super().__init__()

		self._forward_residual_blocks = torch.nn.ModuleList()
		self._backward_residual_blocks = torch.nn.ModuleList()
		self._input_dim = input_dim
		self._output_dim = input_dim * 2

		for layer in layers:
			self._forward_residual_blocks.append(
				ResidualBlock(input_dim, layer, "forward", dropout=dropout)
			)
			self._backward_residual_blocks.append(
				ResidualBlock(input_dim, layer, "backward", dropout=dropout)
			)

		self._return_all_layers = return_all_layers

	def forward(self, token_embeddings: torch.Tensor, mask: torch.BoolTensor):

		transposed_embeddings = torch.transpose(token_embeddings, 1, 2)

		mask_for_fill = ~mask.unsqueeze(1)

		if self._return_all_layers:
			layer_outputs: List[List[torch.Tensor]] = [[], []]
		else:
			outputs: List[torch.Tensor] = []

		for k, blocks in enumerate([self._forward_residual_blocks, self._backward_residual_blocks]):
			out = transposed_embeddings
			for block in blocks:
				out = block(out.masked_fill(mask_for_fill, 0.0))
				if self._return_all_layers:
					layer_outputs[k].append(out)
			if not self._return_all_layers:
				outputs.append(out)

		if self._return_all_layers:
			return [
				torch.cat([fwd, bwd], dim=1).transpose(1, 2) for fwd, bwd in zip(*layer_outputs)
			]
		else:
			return torch.cat(outputs, dim=1).transpose(1, 2)

	def get_input_dim(self) -> int:
		return self._input_dim

	def get_output_dim(self) -> int:
		return self._output_dim

	def is_bidirectional(self) -> bool:
		return True

from typing import Dict
import inspect

import torch


from allennlp.common.checks import ConfigurationError
from allennlp.data.fields.text_field import TextFieldTensors
from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder
from allennlp.modules.time_distributed import TimeDistributed
from allennlp.modules.token_embedders.token_embedder import TokenEmbedder
from allennlp.modules.token_embedders import EmptyEmbedder


@TextFieldEmbedder.register("basic")
class BasicTextFieldEmbedder(TextFieldEmbedder):

	def __init__(self, token_embedders: Dict[str, TokenEmbedder]) -> None:
		super().__init__()
		self._token_embedders = token_embedders
		for key, embedder in token_embedders.items():
			name = "token_embedder_%s" % key
			self.add_module(name, embedder)
		self._ordered_embedder_keys = sorted(self._token_embedders.keys())

	def get_output_dim(self) -> int:
		output_dim = 0
		for embedder in self._token_embedders.values():
			output_dim += embedder.get_output_dim()
		return output_dim

	def forward(
		self, text_field_input: TextFieldTensors, num_wrapping_dims: int = 0, **kwargs
	) -> torch.Tensor:
		if sorted(self._token_embedders.keys()) != sorted(text_field_input.keys()):
			message = "Mismatched token keys: %s and %s" % (
				str(self._token_embedders.keys()),
				str(text_field_input.keys()),
			)
			embedder_keys = set(self._token_embedders.keys())
			input_keys = set(text_field_input.keys())
			if embedder_keys > input_keys and all(
				isinstance(embedder, EmptyEmbedder)
				for name, embedder in self._token_embedders.items()
				if name in embedder_keys - input_keys
			):
				pass
			else:
				raise ConfigurationError(message)

		embedded_representations = []
		for key in self._ordered_embedder_keys:
			embedder = getattr(self, "token_embedder_{}".format(key))
			if isinstance(embedder, EmptyEmbedder):
				continue
			forward_params = inspect.signature(embedder.forward).parameters
			forward_params_values = {}
			missing_tensor_args = set()
			for param in forward_params.keys():
				if param in kwargs:
					forward_params_values[param] = kwargs[param]
				else:
					missing_tensor_args.add(param)

			for _ in range(num_wrapping_dims):
				embedder = TimeDistributed(embedder)

			tensors: Dict[str, torch.Tensor] = text_field_input[key]
			if len(tensors) == 1 and len(missing_tensor_args) == 1:
				token_vectors = embedder(list(tensors.values())[0], **forward_params_values)
			else:
				token_vectors = embedder(**tensors, **forward_params_values)
			if token_vectors is not None:
				embedded_representations.append(token_vectors)
		return torch.cat(embedded_representations, dim=-1)

from typing import List, Union

import torch
from allennlp.training.optimizers import Optimizer
from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import (
	LearningRateScheduler,
	_PyTorchLearningRateSchedulerWrapper,
	_PyTorchLearningRateSchedulerWithMetricsWrapper,
)


@LearningRateScheduler.register("step")
class StepLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):

	def __init__(
		self, optimizer: Optimizer, step_size: int, gamma: float = 0.1, last_epoch: int = -1
	) -> None:

		lr_scheduler = torch.optim.lr_scheduler.StepLR(
			optimizer=optimizer, step_size=step_size, gamma=gamma, last_epoch=last_epoch
		)
		super().__init__(lr_scheduler)


@LearningRateScheduler.register("multi_step")
class MultiStepLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):

	def __init__(
		self, optimizer: Optimizer, milestones: List[int], gamma: float = 0.1, last_epoch: int = -1
	) -> None:
		lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(
			optimizer=optimizer, milestones=milestones, gamma=gamma, last_epoch=last_epoch
		)
		super().__init__(lr_scheduler)


@LearningRateScheduler.register("exponential")
class ExponentialLearningRateScheduler(_PyTorchLearningRateSchedulerWrapper):

	def __init__(self, optimizer: Optimizer, gamma: float = 0.1, last_epoch: int = -1) -> None:
		lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
			optimizer=optimizer, gamma=gamma, last_epoch=last_epoch
		)
		super().__init__(lr_scheduler)


@LearningRateScheduler.register("reduce_on_plateau")
class ReduceOnPlateauLearningRateScheduler(_PyTorchLearningRateSchedulerWithMetricsWrapper):

	def __init__(
		self,
		optimizer: Optimizer,
		mode: str = "min",
		factor: float = 0.1,
		patience: int = 10,
		verbose: bool = False,
		threshold_mode: str = "rel",
		threshold: float = 1e-4,
		cooldown: int = 0,
		min_lr: Union[float, List[float]] = 0,
		eps: float = 1e-8,
	) -> None:
		lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
			optimizer=optimizer,
			mode=mode,
			factor=factor,
			patience=patience,
			verbose=verbose,
			threshold_mode=threshold_mode,
			threshold=threshold,
			cooldown=cooldown,
			min_lr=min_lr,
			eps=eps,
		)
		super().__init__(lr_scheduler)

from typing import Dict, List, Any


import torch

from allennlp.common.util import pad_sequence_to_length
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.tokenizers import Token
from allennlp.data.fields.field import Field


class NamespaceSwappingField(Field[torch.Tensor]):

	__slots__ = ["_source_tokens", "_target_namespace", "_mapping_array"]

	def __init__(self, source_tokens: List[Token], target_namespace: str) -> None:
		self._source_tokens = source_tokens
		self._target_namespace = target_namespace
		self._mapping_array: List[int] = []

	def index(self, vocab: Vocabulary):
		self._mapping_array = [
			vocab.get_token_index(x.ensure_text(), self._target_namespace)
			for x in self._source_tokens
		]

	def get_padding_lengths(self) -> Dict[str, int]:
		return {"num_tokens": len(self._source_tokens)}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:
		desired_length = padding_lengths["num_tokens"]
		padded_tokens = pad_sequence_to_length(self._mapping_array, desired_length)
		tensor = torch.LongTensor(padded_tokens)
		return tensor

	def empty_field(self) -> "NamespaceSwappingField":
		empty_field = NamespaceSwappingField([], self._target_namespace)
		empty_field._mapping_array = []

		return empty_field

	def __len__(self):
		return len(self._source_tokens)

	def human_readable_repr(self) -> Dict[str, Any]:
		return {
			"source_tokens": [str(t) for t in self._source_tokens],
			"target_namespace": self._target_namespace,
		}

from typing import Dict, List
import itertools
import warnings


import torch

from allennlp.common.checks import ConfigurationError
from allennlp.common.util import pad_sequence_to_length
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList
from allennlp.data.tokenizers import Token, CharacterTokenizer
from allennlp.data.vocabulary import Vocabulary


@TokenIndexer.register("characters")
class TokenCharactersIndexer(TokenIndexer):

	def __init__(
		self,
		namespace: str = "token_characters",
		character_tokenizer: CharacterTokenizer = CharacterTokenizer(),
		start_tokens: List[str] = None,
		end_tokens: List[str] = None,
		min_padding_length: int = 0,
		token_min_padding_length: int = 0,
	) -> None:
		super().__init__(token_min_padding_length)
		if min_padding_length == 0:
			url = "https://github.com/allenai/allennlp/issues/1954"
			warnings.warn(
				"You are using the default value (0) of `min_padding_length`, "
				f"which can cause some subtle bugs (more info see {url}). "
				"Strongly recommend to set a value, usually the maximum size "
				"of the convolutional layer size when using CnnEncoder.",
				UserWarning,
			)
		self._min_padding_length = min_padding_length
		self._namespace = namespace
		self._character_tokenizer = character_tokenizer

		self._start_tokens = [Token(st) for st in (start_tokens or [])]
		self._end_tokens = [Token(et) for et in (end_tokens or [])]

	def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
		if token.text is None:
			raise ConfigurationError("TokenCharactersIndexer needs a tokenizer that retains text")
		for character in self._character_tokenizer.tokenize(token.text):
			if getattr(character, "text_id", None) is None:
				assert character.text is not None
				counter[self._namespace][character.text] += 1

	def tokens_to_indices(
		self, tokens: List[Token], vocabulary: Vocabulary
	) -> Dict[str, List[List[int]]]:
		indices: List[List[int]] = []
		for token in itertools.chain(self._start_tokens, tokens, self._end_tokens):
			token_indices: List[int] = []
			if token.text is None:
				raise ConfigurationError(
					"TokenCharactersIndexer needs a tokenizer that retains text"
				)
			for character in self._character_tokenizer.tokenize(token.text):
				if getattr(character, "text_id", None) is not None:
					index = character.text_id
				else:
					assert character.text is not None
					index = vocabulary.get_token_index(character.text, self._namespace)
				assert index is not None
				token_indices.append(index)
			indices.append(token_indices)
		return {"token_characters": indices}

	def get_padding_lengths(self, indexed_tokens: IndexedTokenList) -> Dict[str, int]:
		padding_lengths = {}
		padding_lengths["token_characters"] = max(
			len(indexed_tokens["token_characters"]), self._token_min_padding_length
		)
		max_num_characters = self._min_padding_length
		for token in indexed_tokens["token_characters"]:
			max_num_characters = max(len(token), max_num_characters)  # type: ignore
		padding_lengths["num_token_characters"] = max_num_characters
		return padding_lengths

	def as_padded_tensor_dict(
		self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]
	) -> Dict[str, torch.Tensor]:
		padded_tokens = pad_sequence_to_length(
			tokens["token_characters"],
			padding_lengths["token_characters"],
			default_value=lambda: [],
		)

		desired_token_length = padding_lengths["num_token_characters"]
		longest_token: List[int] = max(tokens["token_characters"], key=len, default=[])  # type: ignore
		padding_value = 0
		if desired_token_length > len(longest_token):
			padded_tokens.append([padding_value] * desired_token_length)
		padded_tokens = list(zip(*itertools.zip_longest(*padded_tokens, fillvalue=padding_value)))
		if desired_token_length > len(longest_token):
			padded_tokens.pop()
		return {
			"token_characters": torch.LongTensor(
				[list(token[:desired_token_length]) for token in padded_tokens]
			)
		}

	def get_empty_token_list(self) -> IndexedTokenList:
		return {"token_characters": []}

from .optim import Optimizer

from .fsdp import fsdp_config
from .training import train_config

import torch

from allennlp.nn import util


class MaskedLayerNorm(torch.nn.Module):

	def __init__(self, size: int, gamma0: float = 0.1) -> None:
		super().__init__()
		self.gamma = torch.nn.Parameter(torch.ones(1, 1, size) * gamma0)
		self.beta = torch.nn.Parameter(torch.zeros(1, 1, size))
		self.size = size

	def forward(self, tensor: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:

		broadcast_mask = mask.unsqueeze(-1)
		num_elements = broadcast_mask.sum() * self.size
		mean = (tensor * broadcast_mask).sum() / num_elements
		masked_centered = (tensor - mean) * broadcast_mask
		std = torch.sqrt(
			(masked_centered * masked_centered).sum() / num_elements
			+ util.tiny_value_of_dtype(tensor.dtype)
		)
		return (
			self.gamma * (tensor - mean) / (std + util.tiny_value_of_dtype(tensor.dtype))
			+ self.beta
		)


import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision
from torchvision.transforms import Compose, ToTensor, Resize
from torch import optim
import numpy as np
from torch.hub import tqdm


class PatchExtractor(nn.Module):
	def __init__(self, patch_size=16):
		super().__init__()
		self.patch_size = patch_size

	def forward(self, input_data):
		batch_size, channels, height, width = input_data.size()
		assert height % self.patch_size == 0 and width % self.patch_size == 0, \
			f"Input height ({height}) and width ({width}) must be divisible by patch size ({self.patch_size})"

		num_patches_h = height // self.patch_size
		num_patches_w = width // self.patch_size
		num_patches = num_patches_h * num_patches_w

		patches = input_data.unfold(2, self.patch_size, self.patch_size). \
			unfold(3, self.patch_size, self.patch_size). \
			permute(0, 2, 3, 1, 4, 5). \
			contiguous(). \
			view(batch_size, num_patches, -1)


		return patches


class InputEmbedding(nn.Module):

	def __init__(self, args):
		super(InputEmbedding, self).__init__()
		self.patch_size = args.patch_size
		self.n_channels = args.n_channels
		self.latent_size = args.latent_size
		use_cuda = not args.no_cuda and torch.cuda.is_available()
		self.device = torch.device("cuda" if use_cuda else "cpu")
		self.batch_size = args.batch_size
		self.input_size = self.patch_size * self.patch_size * self.n_channels

		self.LinearProjection = nn.Linear(self.input_size, self.latent_size)
		self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size).to(self.device))
		self.pos_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size).to(self.device))

	def forward(self, input_data):
		input_data = input_data.to(self.device)
		patchify = PatchExtractor(patch_size=self.patch_size)
		patches = patchify(input_data)

		linear_projection = self.LinearProjection(patches).to(self.device)
		b, n, _ = linear_projection.shape
		linear_projection = torch.cat((self.class_token, linear_projection), dim=1)
		pos_embed = self.pos_embedding[:, :n + 1, :]
		linear_projection += pos_embed

		return linear_projection


class EncoderBlock(nn.Module):

	def __init__(self, args):
		super(EncoderBlock, self).__init__()

		self.latent_size = args.latent_size
		self.num_heads = args.num_heads
		self.dropout = args.dropout
		self.norm = nn.LayerNorm(self.latent_size)
		self.attention = nn.MultiheadAttention(self.latent_size, self.num_heads, dropout=self.dropout)
		self.enc_MLP = nn.Sequential(
			nn.Linear(self.latent_size, self.latent_size * 4),
			nn.GELU(),
			nn.Dropout(self.dropout),
			nn.Linear(self.latent_size * 4, self.latent_size),
			nn.Dropout(self.dropout)
		)

	def forward(self, emb_patches):
		first_norm = self.norm(emb_patches)
		attention_out = self.attention(first_norm, first_norm, first_norm)[0]
		first_added = attention_out + emb_patches
		second_norm = self.norm(first_added)
		mlp_out = self.enc_MLP(second_norm)
		output = mlp_out + first_added

		return output


class ViT(nn.Module):
	def __init__(self, args):
		super(ViT, self).__init__()

		self.num_encoders = args.num_encoders
		self.latent_size = args.latent_size
		self.num_classes = args.num_classes
		self.dropout = args.dropout

		self.embedding = InputEmbedding(args)
		self.encoders = nn.ModuleList([EncoderBlock(args) for _ in range(self.num_encoders)])
		self.MLPHead = nn.Sequential(
			nn.LayerNorm(self.latent_size),
			nn.Linear(self.latent_size, self.latent_size),
			nn.Linear(self.latent_size, self.num_classes),
		)

	def forward(self, test_input):
		enc_output = self.embedding(test_input)
		for enc_layer in self.encoders:
			enc_output = enc_layer(enc_output)

		class_token_embed = enc_output[:, 0]
		return self.MLPHead(class_token_embed)


class TrainEval:

	def __init__(self, args, model, train_dataloader, val_dataloader, optimizer, criterion, device):
		self.model = model
		self.train_dataloader = train_dataloader
		self.val_dataloader = val_dataloader
		self.optimizer = optimizer
		self.criterion = criterion
		self.epoch = args.epochs
		self.device = device
		self.args = args

	def train_fn(self, current_epoch):
		self.model.train()
		total_loss = 0.0
		tk = tqdm(self.train_dataloader, desc="EPOCH" + "[TRAIN]" + str(current_epoch + 1) + "/" + str(self.epoch))

		for t, data in enumerate(tk):
			images, labels = data
			images, labels = images.to(self.device), labels.to(self.device)
			self.optimizer.zero_grad()
			logits = self.model(images)
			loss = self.criterion(logits, labels)
			loss.backward()
			self.optimizer.step()

			total_loss += loss.item()
			tk.set_postfix({"Loss": "%6f" % float(total_loss / (t + 1))})
			if self.args.dry_run:
				break

		return total_loss / len(self.train_dataloader)

	def eval_fn(self, current_epoch):
		self.model.eval()
		total_loss = 0.0
		tk = tqdm(self.val_dataloader, desc="EPOCH" + "[VALID]" + str(current_epoch + 1) + "/" + str(self.epoch))

		for t, data in enumerate(tk):
			images, labels = data
			images, labels = images.to(self.device), labels.to(self.device)

			logits = self.model(images)
			loss = self.criterion(logits, labels)

			total_loss += loss.item()
			tk.set_postfix({"Loss": "%6f" % float(total_loss / (t + 1))})
			if self.args.dry_run:
				break

		return total_loss / len(self.val_dataloader)

	def train(self):
		best_valid_loss = np.inf
		best_train_loss = np.inf
		for i in range(self.epoch):
			train_loss = self.train_fn(i)
			val_loss = self.eval_fn(i)

			if val_loss < best_valid_loss:
				torch.save(self.model.state_dict(), "best-weights.pt")
				print("Saved Best Weights")
				best_valid_loss = val_loss
				best_train_loss = train_loss
		print(f"Training Loss : {best_train_loss}")
		print(f"Valid Loss : {best_valid_loss}")



def main():
	parser = argparse.ArgumentParser(description='Vision Transformer in PyTorch')
	parser.add_argument('--no-cuda', action='store_true', default=False,
						help='disables CUDA training')
	parser.add_argument('--patch-size', type=int, default=16,
						help='patch size for images (default : 16)')
	parser.add_argument('--latent-size', type=int, default=768,
						help='latent size (default : 768)')
	parser.add_argument('--n-channels', type=int, default=3,
						help='number of channels in images (default : 3 for RGB)')
	parser.add_argument('--num-heads', type=int, default=12,
						help='(default : 12)')
	parser.add_argument('--num-encoders', type=int, default=12,
						help='number of encoders (default : 12)')
	parser.add_argument('--dropout', type=int, default=0.1,
						help='dropout value (default : 0.1)')
	parser.add_argument('--img-size', type=int, default=224,
						help='image size to be reshaped to (default : 224')
	parser.add_argument('--num-classes', type=int, default=10,
						help='number of classes in dataset (default : 10 for CIFAR10)')
	parser.add_argument('--epochs', type=int, default=10,
						help='number of epochs (default : 10)')
	parser.add_argument('--lr', type=float, default=1e-2,
						help='base learning rate (default : 0.01)')
	parser.add_argument('--weight-decay', type=int, default=3e-2,
						help='weight decay value (default : 0.03)')
	parser.add_argument('--batch-size', type=int, default=4,
						help='batch size (default : 4)')
	parser.add_argument('--dry-run', action='store_true', default=False,
						help='quickly check a single pass')
	args = parser.parse_args()

	use_cuda = not args.no_cuda and torch.cuda.is_available()
	device = torch.device("cuda" if use_cuda else "cpu")

	transforms = Compose([
		Resize((args.img_size, args.img_size)),
		ToTensor()
	])
	train_data = torchvision.datasets.CIFAR10(root='./dataset', train=True, download=True, transform=transforms)
	valid_data = torchvision.datasets.CIFAR10(root='./dataset', train=False, download=True, transform=transforms)
	train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)
	valid_loader = DataLoader(valid_data, batch_size=args.batch_size, shuffle=True)

	model = ViT(args).to(device)

	optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
	criterion = nn.CrossEntropyLoss()

	TrainEval(args, model, train_loader, valid_loader, optimizer, criterion, device).train()


if __name__ == "__main__":
	main()


import argparse
import gym
import numpy as np
from itertools import count
from collections import deque
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical


parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
					help='discount factor (default: 0.99)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
					help='random seed (default: 543)')
parser.add_argument('--render', action='store_true',
					help='render the environment')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='interval between training status logs (default: 10)')
args = parser.parse_args()


env = gym.make('CartPole-v1')
env.reset(seed=args.seed)
torch.manual_seed(args.seed)


class Policy(nn.Module):
	def __init__(self):
		super(Policy, self).__init__()
		self.affine1 = nn.Linear(4, 128)
		self.dropout = nn.Dropout(p=0.6)
		self.affine2 = nn.Linear(128, 2)

		self.saved_log_probs = []
		self.rewards = []

	def forward(self, x):
		x = self.affine1(x)
		x = self.dropout(x)
		x = F.relu(x)
		action_scores = self.affine2(x)
		return F.softmax(action_scores, dim=1)


policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
eps = np.finfo(np.float32).eps.item()


def select_action(state):
	state = torch.from_numpy(state).float().unsqueeze(0)
	probs = policy(state)
	m = Categorical(probs)
	action = m.sample()
	policy.saved_log_probs.append(m.log_prob(action))
	return action.item()


def finish_episode():
	R = 0
	policy_loss = []
	returns = deque()
	for r in policy.rewards[::-1]:
		R = r + args.gamma * R
		returns.appendleft(R)
	returns = torch.tensor(returns)
	returns = (returns - returns.mean()) / (returns.std() + eps)
	for log_prob, R in zip(policy.saved_log_probs, returns):
		policy_loss.append(-log_prob * R)
	optimizer.zero_grad()
	policy_loss = torch.cat(policy_loss).sum()
	policy_loss.backward()
	optimizer.step()
	del policy.rewards[:]
	del policy.saved_log_probs[:]


def main():
	running_reward = 10
	for i_episode in count(1):
		state, _ = env.reset()
		ep_reward = 0
		for t in range(1, 10000):  # Don't infinite loop while learning
			action = select_action(state)
			state, reward, done, _, _ = env.step(action)
			if args.render:
				env.render()
			policy.rewards.append(reward)
			ep_reward += reward
			if done:
				break

		running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward
		finish_episode()
		if i_episode % args.log_interval == 0:
			print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
				  i_episode, ep_reward, running_reward))
		if running_reward > env.spec.reward_threshold:
			print("Solved! Running reward is now {} and "
				  "the last episode runs to {} time steps!".format(running_reward, t))
			break


if __name__ == '__main__':
	main()

from typing import Dict, List


from spacy.tokens import Token as SpacyToken
import torch
import numpy

from allennlp.common.util import pad_sequence_to_length
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.tokenizers import Token
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList


@TokenIndexer.register("spacy")
class SpacyTokenIndexer(TokenIndexer):

	def __init__(self, hidden_dim: int = 96, token_min_padding_length: int = 0) -> None:
		self._hidden_dim = hidden_dim
		super().__init__(token_min_padding_length)

	def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
		pass

	def tokens_to_indices(
		self, tokens: List[SpacyToken], vocabulary: Vocabulary
	) -> Dict[str, List[numpy.ndarray]]:
		if not all(isinstance(x, SpacyToken) for x in tokens):
			raise ValueError(
				"The spacy indexer requires you to use a Tokenizer which produces SpacyTokens."
			)
		indices: List[numpy.ndarray] = [token.vector for token in tokens]
		return {"tokens": indices}

	def as_padded_tensor_dict(
		self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]
	) -> Dict[str, torch.Tensor]:
		def padding_token():
			return numpy.zeros(self._hidden_dim, dtype=numpy.float32)

		tensor = torch.FloatTensor(
			pad_sequence_to_length(
				tokens["tokens"], padding_lengths["tokens"], default_value=padding_token
			)
		)
		return {"tokens": tensor}

import logging
import torch

logging.basicConfig(
	format="%(asctime)s %(message)s", datefmt="%m/%d/%Y %I:%M:%S %p", level=logging.INFO
)

def get_logger():
	return logging.getLogger(__name__)


def rank_log(_rank, logger, msg):
	if _rank == 0:
		logger.info(f" {msg}")


def verify_min_gpu_count(min_gpus: int = 2) -> bool:
	has_cuda = torch.cuda.is_available()
	gpu_count = torch.cuda.device_count()
	return has_cuda and gpu_count >= min_gpus

from typing import List, Dict, Any, Optional, TYPE_CHECKING
import torch

from allennlp.common import Registrable
from allennlp.data import TensorDict


if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


class TrainerCallback(Registrable):

	def __init__(self, serialization_dir: str) -> None:
		self.serialization_dir = serialization_dir
		self.trainer: Optional["GradientDescentTrainer"] = None

	def on_start(
		self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
	) -> None:
		self.trainer = trainer

	def on_backward(
		self,
		trainer: "GradientDescentTrainer",
		batch_outputs: Dict[str, torch.Tensor],
		backward_called: bool,
		**kwargs,
	) -> bool:
		return False

	def on_batch(
		self,
		trainer: "GradientDescentTrainer",
		batch_inputs: List[TensorDict],
		batch_outputs: List[Dict[str, Any]],
		batch_metrics: Dict[str, Any],
		epoch: int,
		batch_number: int,
		is_training: bool,
		is_primary: bool = True,
		batch_grad_norm: Optional[float] = None,
		**kwargs,
	) -> None:
		pass

	def on_epoch(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any],
		epoch: int,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		pass

	def on_end(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any] = None,
		epoch: int = None,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		pass

	def state_dict(self) -> Dict[str, Any]:
		return {}

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		pass


TrainerCallback.register("null")(TrainerCallback)

from typing import Dict, Any, TYPE_CHECKING, Optional

from allennlp.training.callbacks.callback import TrainerCallback

if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


@TrainerCallback.register("should_validate_callback")
class ShouldValidateCallback(TrainerCallback):

	def __init__(
		self,
		serialization_dir: str,
		validation_start: Optional[int] = None,
		validation_interval: Optional[int] = None,
	) -> None:
		super().__init__(serialization_dir)
		self._validation_start = validation_start
		self._validation_interval = validation_interval

	def on_start(
		self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
	) -> None:
		trainer._should_validate_this_epoch = self._should_validate(epoch=0)

	def on_epoch(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any],
		epoch: int,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		trainer._should_validate_this_epoch = self._should_validate(epoch=epoch + 1)

	def on_end(
		self,
		trainer: "GradientDescentTrainer",
		metrics: Dict[str, Any] = None,
		epoch: int = None,
		is_primary: bool = True,
		**kwargs,
	) -> None:
		epoch = epoch + 1 if epoch is not None else trainer._epochs_completed
		trainer._should_validate_this_epoch = self._should_validate(epoch=epoch)

	def _should_validate(self, epoch: int) -> bool:
		should_validate = True
		if self._validation_start is not None and epoch < self._validation_start:
			should_validate = False
		elif self._validation_interval is not None and epoch % self._validation_interval != 0:
			should_validate = False
		return should_validate


import logging
import shutil
import tarfile
import tempfile
import zipfile
from os import PathLike
from pathlib import Path
from typing import Optional, Union

from huggingface_hub import HfApi, HfFolder, Repository

from allennlp.common.file_utils import cached_path

logger = logging.getLogger(__name__)



def _create_model_card(repo_dir: Path):
	readme_path = repo_dir / "README.md"
	prev_readme = ""
	if readme_path.exists():
		with readme_path.open("r", encoding="utf8") as f:
			prev_readme = f.read()
	with readme_path.open("w", encoding="utf-8") as f:
		f.write(README_TEMPLATE)
		f.write(prev_readme)


_ALLOWLIST_PATHS = ["vocabulary", "config.json", "weights.th", "best.th", "metrics.json", "log"]


def _copy_allowed_file(filepath: Path, dst_directory: Path):
	if filepath.name not in _ALLOWLIST_PATHS:
		return

	dst = dst_directory / filepath.name
	if dst.is_dir():
		shutil.rmtree(str(dst))
	elif dst.is_file():
		dst.unlink()
	if filepath.is_dir():
		shutil.copytree(filepath, dst)
	elif filepath.is_file():
		if filepath.name in ["best.th", "weights.th"]:
			dst = dst_directory / "weights.th"
		shutil.copy(str(filepath), str(dst))


def push_to_hf(
	repo_name: str,
	serialization_dir: Optional[Union[str, PathLike]] = None,
	archive_path: Optional[Union[str, PathLike]] = None,
	organization: Optional[str] = None,
	commit_message: str = "Update repository",
	local_repo_path: Union[str, PathLike] = "hub",
	use_auth_token: Union[bool, str] = True,
) -> str:

	if serialization_dir is not None:
		working_dir = Path(serialization_dir)
		if archive_path is not None:
			raise ValueError(
				"serialization_dir and archive_path are mutually exclusive, please just use one."
			)
		if not working_dir.exists() or not working_dir.is_dir():
			raise ValueError(
				f"Can't find path: {serialization_dir}, please point"
				"to a directory with the serialized model."
			)
	elif archive_path is not None:
		working_dir = Path(archive_path)
		if (
			not working_dir.exists()
			or not zipfile.is_zipfile(working_dir)
			and not tarfile.is_tarfile(working_dir)
		):
			raise ValueError(
				f"Can't find path: {archive_path}, please point to a .tar.gz archive"
				"or to a directory with the serialized model."
			)
		else:
			logging.info(
				"Using the archive_path is discouraged. Using the serialization_dir"
				"will also upload metrics and TensorBoard traces to the Hugging Face Hub."
			)
	else:
		raise ValueError("please specify either serialization_dir or archive_path")

	info_msg = f"Preparing repository '{use_auth_token}'"
	if isinstance(use_auth_token, str):
		huggingface_token = use_auth_token
	elif use_auth_token:
		huggingface_token = HfFolder.get_token()

	api = HfApi()
	repo_url = api.create_repo(
		name=repo_name,
		token=huggingface_token,
		organization=organization,
		private=False,
		exist_ok=True,
	)

	repo_local_path = Path(local_repo_path) / repo_name
	repo = Repository(repo_local_path, clone_from=repo_url, use_auth_token=use_auth_token)
	repo.git_pull(rebase=True)

	repo.lfs_track(["*.th"])
	info_msg = f"Preparing repository '{repo_name}'"
	if organization is not None:
		info_msg += f" ({organization})"
	logging.info(info_msg)

	if serialization_dir is not None:
		for filename in working_dir.iterdir():
			_copy_allowed_file(Path(filename), repo_local_path)
	else:
		with tempfile.TemporaryDirectory() as temp_dir:
			extracted_dir = Path(cached_path(working_dir, temp_dir, extract_archive=True))
			for filename in extracted_dir.iterdir():
				_copy_allowed_file(Path(filename), repo_local_path)

	_create_model_card(repo_local_path)

	logging.info(f"Pushing repo {repo_name} to the Hugging Face Hub")
	repo.push_to_hub(commit_message=commit_message)

	logging.info(f"View your model in {repo_url}")
	return repo_url

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import DataLoader

from ..model import BERTLM, BERT
from .optim_schedule import ScheduledOptim

import tqdm


class BERTTrainer:

	def __init__(self, bert: BERT, vocab_size: int,
				 train_dataloader: DataLoader, test_dataloader: DataLoader = None,
				 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000,
				 with_cuda: bool = True, cuda_devices=None, log_freq: int = 10):

		cuda_condition = torch.cuda.is_available() and with_cuda
		self.device = torch.device("cuda:0" if cuda_condition else "cpu")

		self.bert = bert
		self.model = BERTLM(bert, vocab_size).to(self.device)

		if with_cuda and torch.cuda.device_count() > 1:
			print("Using %d GPUS for BERT" % torch.cuda.device_count())
			self.model = nn.DataParallel(self.model, device_ids=cuda_devices)

		self.train_data = train_dataloader
		self.test_data = test_dataloader

		self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)
		self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)

		self.criterion = nn.NLLLoss(ignore_index=0)

		self.log_freq = log_freq

		print("Total Parameters:", sum([p.nelement() for p in self.model.parameters()]))

	def train(self, epoch):
		self.iteration(epoch, self.train_data)

	def test(self, epoch):
		self.iteration(epoch, self.test_data, train=False)

	def iteration(self, epoch, data_loader, train=True):
		str_code = "train" if train else "test"

		data_iter = tqdm.tqdm(enumerate(data_loader),
							  desc="EP_%s:%d" % (str_code, epoch),
							  total=len(data_loader),
							  bar_format="{l_bar}{r_bar}")

		avg_loss = 0.0
		total_correct = 0
		total_element = 0

		for i, data in data_iter:
			data = {key: value.to(self.device) for key, value in data.items()}

			next_sent_output, mask_lm_output = self.model.forward(data["bert_input"], data["segment_label"])

			next_loss = self.criterion(next_sent_output, data["is_next"])

			mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data["bert_label"])

			loss = next_loss + mask_loss

			if train:
				self.optim_schedule.zero_grad()
				loss.backward()
				self.optim_schedule.step_and_update_lr()

			correct = next_sent_output.argmax(dim=-1).eq(data["is_next"]).sum().item()
			avg_loss += loss.item()
			total_correct += correct
			total_element += data["is_next"].nelement()

			post_fix = {
				"epoch": epoch,
				"iter": i,
				"avg_loss": avg_loss / (i + 1),
				"avg_acc": total_correct / total_element * 100,
				"loss": loss.item()
			}

			if i % self.log_freq == 0:
				data_iter.write(str(post_fix))

		print("EP%d_%s, avg_loss=" % (epoch, str_code), avg_loss / len(data_iter), "total_acc=",
			  total_correct * 100.0 / total_element)

	def save(self, epoch, file_path="output/bert_trained.model"):
		output_path = file_path + ".ep%d" % epoch
		torch.save(self.bert.cpu(), output_path)
		self.bert.to(self.device)
		print("EP:%d Model Saved on:" % epoch, output_path)
		return output_path

import os
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from transformers import AutoTokenizer, GPT2TokenizerFast
from transformers import T5Tokenizer, T5ForConditionalGeneration
import functools
from torch.optim.lr_scheduler import StepLR
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from transformers.models.t5.modeling_t5 import T5Block

from torch.distributed.fsdp import (
	FullyShardedDataParallel as FSDP,
	CPUOffload,
	MixedPrecision,
	BackwardPrefetch,
	ShardingStrategy,
	FullStateDictConfig,
	StateDictType,
)

from functools import partial
from torch.utils.data import DataLoader
from pathlib import Path
from summarization_dataset import *
import policies
import model_checkpointing
from configs import fsdp_config, train_config
from utils import (bfloat_support, setup,
				   cleanup, get_date_of_run,
				   format_metrics_to_gb,
				   train,validation,setup_model)
from transformers.models.t5.modeling_t5 import T5Block
from typing import Type
import time
import tqdm
from datetime import datetime


def get_policies(cfg, rank):


	mixed_precision_policy = None
	wrapping_policy = None

	if cfg.mixed_precision:
		bfloat_available = bfloat_support()
		if bfloat_available and not cfg.use_fp16:
			mixed_precision_policy = policies.bfSixteen
			if rank == 0:
				print(f"bFloat16 enabled for mixed precision - using bfSixteen policy")
		elif cfg.use_fp16:
			mixed_precision_policy = policies.fpSixteen
			if rank == 0:
				print(f"FP16 enabled. ")
		else:
			print(
				f"bFloat16 support not present. Will use FP32, and not mixed precision"
			)

	wrapping_policy = policies.get_t5_wrapper()

	return mixed_precision_policy, wrapping_policy


def fsdp_main(args):

	model, tokenizer = setup_model(train_config.model_name)

	local_rank = int(os.environ['LOCAL_RANK'])
	rank = int(os.environ['RANK'])
	world_size = int(os.environ['WORLD_SIZE'])


	dataset = load_dataset('wikihow', 'all', data_dir='data/')
	print(dataset.keys())
	print("Size of train dataset: ", dataset['train'].shape)
	print("Size of Validation dataset: ", dataset['validation'].shape)

   
	train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False) 
	val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)
 
	sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)
	sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)

	setup()


	train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}
	test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}
	cuda_kwargs = {'num_workers': 2,
					'pin_memory': True,
					'shuffle': False}
	train_kwargs.update(cuda_kwargs)
	test_kwargs.update(cuda_kwargs)

	train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)
	val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)
 
	torch.cuda.set_device(local_rank)
	
	mixed_precision_policy, t5_auto_wrap_policy = get_policies(train_config, rank)
	
	model = FSDP(model,
		auto_wrap_policy=t5_auto_wrap_policy,
		mixed_precision=mixed_precision_policy,
		sharding_strategy=fsdp_config.sharding_strategy,
		device_id=torch.cuda.current_device(),
		limit_all_gathers=fsdp_config.limit_all_gathers)
	
	if fsdp_config.fsdp_activation_checkpointing:
		policies.apply_fsdp_checkpointing(model)

	optimizer = optim.AdamW(model.parameters(), lr=train_config.lr)

	scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)
	best_val_loss = float("inf")
	curr_val_loss = float("inf")
	file_save_name = "T5-model-"

	if rank == 0:
		time_of_run = get_date_of_run()
		dur = []
		train_acc_tracking = []
		val_acc_tracking = []
		training_start_time = time.time()

	if rank == 0 and args.track_memory:
		mem_alloc_tracker = []
		mem_reserved_tracker = []

	for epoch in range(1, args.epochs + 1):
		t0 = time.time()
		train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)
		if args.run_validation:
			curr_val_loss = validation(model, rank, world_size, val_loader)
		scheduler.step()
		
		if rank == 0:

			print(f"--> epoch {epoch} completed...entering save and stats zone")

			dur.append(time.time() - t0)
			train_acc_tracking.append(train_accuracy.item())

			if args.run_validation:
				val_acc_tracking.append(curr_val_loss.item())

			if args.track_memory:
				mem_alloc_tracker.append(
					format_metrics_to_gb(torch.cuda.memory_allocated())
				)
				mem_reserved_tracker.append(
					format_metrics_to_gb(torch.cuda.memory_reserved())
				)

		if train_config.save_model and curr_val_loss < best_val_loss:
			
			if fsdp_config.checkpoint_type == StateDictType.FULL_STATE_DICT:
				model_checkpointing.save_model_checkpoint(
					model, optimizer, rank, fsdp_config, epoch=1
				)
			elif fsdp_config.checkpoint_type == StateDictType.SHARDED_STATE_DICT:
				model_checkpointing.save_model_and_optimizer_sharded(model, rank, fsdp_config)
				if fsdp_config.save_optimizer:
					model_checkpointing.save_model_and_optimizer_sharded(model, rank, fsdp_config, optim=optimizer)

			if fsdp_config.save_optimizer:
				model_checkpointing.save_optimizer_checkpoint(
					model, optimizer, rank, fsdp_config, epoch=1
				)		   
		if curr_val_loss < best_val_loss:

			best_val_loss = curr_val_loss
			if rank==0:
				print(f"-->>>> New Val Loss Record: {best_val_loss}")

	dist.barrier()
	cleanup()


if __name__ == '__main__':
	parser = argparse.ArgumentParser(description='PyTorch T5 FSDP Example')
	parser.add_argument('--batch-size', type=int, default=4, metavar='N',
						help='input batch size for training (default: 64)')
	parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',
						help='input batch size for testing (default: 1000)')
	parser.add_argument('--epochs', type=int, default=2, metavar='N',
						help='number of epochs to train (default: 3)')
	parser.add_argument('--seed', type=int, default=1, metavar='S',
						help='random seed (default: 1)')
	parser.add_argument('--track_memory', action='store_false', default=True,
						help='track the gpu memory')
	parser.add_argument('--run_validation', action='store_false', default=True,
						help='running the validation')
	args = parser.parse_args()

	torch.manual_seed(args.seed)
	
	fsdp_main(args)

from os.path import exists, join, basename
from os import makedirs, remove
from six.moves import urllib
import tarfile
from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize

from dataset import DatasetFromFolder


def download_bsd300(dest="dataset"):
	output_image_dir = join(dest, "BSDS300/images")

	if not exists(output_image_dir):
		makedirs(dest)
		url = "http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz"
		print("downloading url ", url)

		data = urllib.request.urlopen(url)

		file_path = join(dest, basename(url))
		with open(file_path, 'wb') as f:
			f.write(data.read())

		print("Extracting data")
		with tarfile.open(file_path) as tar:
			for item in tar:
				tar.extract(item, dest)

		remove(file_path)

	return output_image_dir


def calculate_valid_crop_size(crop_size, upscale_factor):
	return crop_size - (crop_size % upscale_factor)


def input_transform(crop_size, upscale_factor):
	return Compose([
		CenterCrop(crop_size),
		Resize(crop_size // upscale_factor),
		ToTensor(),
	])


def target_transform(crop_size):
	return Compose([
		CenterCrop(crop_size),
		ToTensor(),
	])


def get_training_set(upscale_factor):
	root_dir = download_bsd300()
	train_dir = join(root_dir, "train")
	crop_size = calculate_valid_crop_size(256, upscale_factor)

	return DatasetFromFolder(train_dir,
							 input_transform=input_transform(crop_size, upscale_factor),
							 target_transform=target_transform(crop_size))


def get_test_set(upscale_factor):
	root_dir = download_bsd300()
	test_dir = join(root_dir, "test")
	crop_size = calculate_valid_crop_size(256, upscale_factor)

	return DatasetFromFolder(test_dir,
							 input_transform=input_transform(crop_size, upscale_factor),
							 target_transform=target_transform(crop_size))

import os
from typing import Any, Dict, Optional

import torch

from allennlp.models import Model
from allennlp.training.checkpointer import Checkpointer
from allennlp.training.trainer import Trainer, TrainerCheckpoint


@Trainer.register("no_op")
class NoOpTrainer(Trainer):

	def __init__(self, serialization_dir: str, model: Model) -> None:

		super().__init__(serialization_dir, cuda_device=-1)
		self.model = model
		self._best_model_filename: Optional[str] = None

	def train(self) -> Dict[str, Any]:
		assert self._serialization_dir is not None
		self.model.vocab.save_to_files(os.path.join(self._serialization_dir, "vocabulary"))
		checkpointer = Checkpointer(self._serialization_dir)
		checkpointer.save_checkpoint(self)

		best_model_filename = os.path.join(self._serialization_dir, "best.th")
		torch.save(self.model.state_dict(), best_model_filename)
		self._best_model_filename = best_model_filename

		return {}

	def get_checkpoint_state(self) -> TrainerCheckpoint:
		return TrainerCheckpoint(
			self.model.state_dict(), {"epochs_completed": 0, "batches_in_epoch_completed": 0}
		)

	def get_best_weights_path(self) -> Optional[str]:
		return self._best_model_filename

from typing import List, Dict, Any


from allennlp.data.tokenizers.token_class import Token
from allennlp.data.tokenizers.tokenizer import Tokenizer


@Tokenizer.register("whitespace")
@Tokenizer.register("just_spaces")
class WhitespaceTokenizer(Tokenizer):

	def tokenize(self, text: str) -> List[Token]:
		return [Token(t) for t in text.split()]

	def _to_params(self) -> Dict[str, Any]:
		return {"type": "whitespace"}

import torch.nn as nn

from .bert import BERT


class BERTLM(nn.Module):

	def __init__(self, bert: BERT, vocab_size):

		super().__init__()
		self.bert = bert
		self.next_sentence = NextSentencePrediction(self.bert.hidden)
		self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)

	def forward(self, x, segment_label):
		x = self.bert(x, segment_label)
		return self.next_sentence(x), self.mask_lm(x)


class NextSentencePrediction(nn.Module):

	def __init__(self, hidden):
		super().__init__()
		self.linear = nn.Linear(hidden, 2)
		self.softmax = nn.LogSoftmax(dim=-1)

	def forward(self, x):
		return self.softmax(self.linear(x[:, 0]))


class MaskedLanguageModel(nn.Module):

	def __init__(self, hidden, vocab_size):
		super().__init__()
		self.linear = nn.Linear(hidden, vocab_size)
		self.softmax = nn.LogSoftmax(dim=-1)

	def forward(self, x):
		return self.softmax(self.linear(x))

import os

from allennlp.common.file_utils import CACHE_DIRECTORY
from allennlp.common.file_utils import filename_to_url


def main():
	print(f"Looking for datasets in {CACHE_DIRECTORY}...")
	if not os.path.exists(CACHE_DIRECTORY):
		print("Directory does not exist.")
		print("No cached datasets found.")

	cached_files = os.listdir(CACHE_DIRECTORY)

	if not cached_files:
		print("Directory is empty.")
		print("No cached datasets found.")

	for filename in cached_files:
		if not filename.endswith("json"):
			url, etag = filename_to_url(filename)
			print("Filename: %s" % filename)
			print("Url: %s" % url)
			print("ETag: %s" % etag)
			print()


if __name__ == "__main__":
	main()

from typing import List, Iterator, Optional
import argparse
import sys
import json


from allennlp.commands.subcommand import Subcommand
from allennlp.common import logging as common_logging
from allennlp.common.checks import check_for_gpu, ConfigurationError
from allennlp.common.file_utils import cached_path, open_compressed
from allennlp.common.util import lazy_groups_of
from allennlp.data.dataset_readers import MultiTaskDatasetReader
from allennlp.models.archival import load_archive
from allennlp.predictors.predictor import Predictor, JsonDict
from allennlp.data import Instance


@Subcommand.register("predict")
class Predict(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:

		description = """Run the specified model against a JSON-lines input file."""
		subparser = parser.add_parser(
			self.name, description=description, help="Use a trained model to make predictions."
		)

		subparser.add_argument(
			"archive_file", type=str, help="the archived model to make predictions with"
		)
		subparser.add_argument("input_file", type=str, help="path to or url of the input file")

		subparser.add_argument("--output-file", type=str, help="path to output file")
		subparser.add_argument(
			"--weights-file", type=str, help="a path that overrides which weights file to use"
		)

		batch_size = subparser.add_mutually_exclusive_group(required=False)
		batch_size.add_argument(
			"--batch-size", type=int, default=1, help="The batch size to use for processing"
		)

		subparser.add_argument(
			"--silent", action="store_true", help="do not print output to stdout"
		)

		cuda_device = subparser.add_mutually_exclusive_group(required=False)
		cuda_device.add_argument(
			"--cuda-device", type=int, default=-1, help="id of GPU to use (if any)"
		)

		subparser.add_argument(
			"--use-dataset-reader",
			action="store_true",
			help="Whether to use the dataset reader of the original model to load Instances. "
			"The validation dataset reader will be used if it exists, otherwise it will "
			"fall back to the train dataset reader. This behavior can be overridden "
			"with the --dataset-reader-choice flag.",
		)

		subparser.add_argument(
			"--dataset-reader-choice",
			type=str,
			choices=["train", "validation"],
			default="validation",
			help="Indicates which model dataset reader to use if the --use-dataset-reader "
			"flag is set.",
		)

		subparser.add_argument(
			"--multitask-head",
			type=str,
			default=None,
			help="If you are using a dataset reader to make predictions, and the model is a"
			"multitask model, you have to specify the name of the model head to use here.",
		)

		subparser.add_argument(
			"-o",
			"--overrides",
			type=str,
			default="",
			help=(
				"a json(net) structure used to override the experiment configuration, e.g., "
				"'{\"iterator.batch_size\": 16}'.  Nested parameters can be specified either"
				" with nested dictionaries or with dot syntax."
			),
		)

		subparser.add_argument(
			"--predictor", type=str, help="optionally specify a specific predictor to use"
		)

		subparser.add_argument(
			"--predictor-args",
			type=str,
			default="",
			help=(
				"an optional JSON structure used to provide additional parameters to the predictor"
			),
		)

		subparser.add_argument(
			"--file-friendly-logging",
			action="store_true",
			default=False,
			help="outputs tqdm status on separate lines and slows tqdm refresh rate",
		)

		subparser.set_defaults(func=_predict)

		return subparser


def _get_predictor(args: argparse.Namespace) -> Predictor:
	check_for_gpu(args.cuda_device)
	archive = load_archive(
		args.archive_file,
		weights_file=args.weights_file,
		cuda_device=args.cuda_device,
		overrides=args.overrides,
	)

	predictor_args = args.predictor_args.strip()
	if len(predictor_args) <= 0:
		predictor_args = {}
	else:
		import json

		predictor_args = json.loads(predictor_args)

	return Predictor.from_archive(
		archive,
		args.predictor,
		dataset_reader_to_load=args.dataset_reader_choice,
		extra_args=predictor_args,
	)


class _PredictManager:
	def __init__(
		self,
		predictor: Predictor,
		input_file: str,
		output_file: Optional[str],
		batch_size: int,
		print_to_console: bool,
		has_dataset_reader: bool,
		multitask_head: Optional[str] = None,
	) -> None:
		self._predictor = predictor
		self._input_file = input_file
		self._output_file = None if output_file is None else open(output_file, "w")
		self._batch_size = batch_size
		self._print_to_console = print_to_console
		self._dataset_reader = None if not has_dataset_reader else predictor._dataset_reader
		self._multitask_head = multitask_head
		if self._multitask_head is not None:
			if self._dataset_reader is None:
				raise ConfigurationError(
					"You must use a dataset reader when using --multitask-head."
				)
			if not isinstance(self._dataset_reader, MultiTaskDatasetReader):
				raise ConfigurationError(
					"--multitask-head only works with a multitask dataset reader."
				)
		if (
			isinstance(self._dataset_reader, MultiTaskDatasetReader)
			and self._multitask_head is None
		):
			raise ConfigurationError(
				"You must specify --multitask-head when using a multitask dataset reader."
			)

	def _predict_json(self, batch_data: List[JsonDict]) -> Iterator[str]:
		if len(batch_data) == 1:
			results = [self._predictor.predict_json(batch_data[0])]
		else:
			results = self._predictor.predict_batch_json(batch_data)
		for output in results:
			yield self._predictor.dump_line(output)

	def _predict_instances(self, batch_data: List[Instance]) -> Iterator[str]:
		if len(batch_data) == 1:
			results = [self._predictor.predict_instance(batch_data[0])]
		else:
			results = self._predictor.predict_batch_instance(batch_data)
		for output in results:
			yield self._predictor.dump_line(output)

	def _maybe_print_to_console_and_file(
		self, index: int, prediction: str, model_input: str = None
	) -> None:
		if self._print_to_console:
			if model_input is not None:
				print(f"input {index}: ", model_input)
			print("prediction: ", prediction)
		if self._output_file is not None:
			self._output_file.write(prediction)

	def _get_json_data(self) -> Iterator[JsonDict]:
		if self._input_file == "-":
			for line in sys.stdin:
				if not line.isspace():
					yield self._predictor.load_line(line)
		else:
			input_file = cached_path(self._input_file)
			with open_compressed(input_file) as file_input:
				for line in file_input:
					if not line.isspace():
						yield self._predictor.load_line(line)

	def _get_instance_data(self) -> Iterator[Instance]:
		if self._input_file == "-":
			raise ConfigurationError("stdin is not an option when using a DatasetReader.")
		elif self._dataset_reader is None:
			raise ConfigurationError("To generate instances directly, pass a DatasetReader.")
		else:
			if isinstance(self._dataset_reader, MultiTaskDatasetReader):
				assert (
					self._multitask_head is not None
				)  # This is properly checked by the constructor.
				yield from self._dataset_reader.read(
					self._input_file, force_task=self._multitask_head
				)
			else:
				yield from self._dataset_reader.read(self._input_file)

	def run(self) -> None:
		has_reader = self._dataset_reader is not None
		index = 0
		if has_reader:
			for batch in lazy_groups_of(self._get_instance_data(), self._batch_size):
				for model_input_instance, result in zip(batch, self._predict_instances(batch)):
					self._maybe_print_to_console_and_file(index, result, str(model_input_instance))
					index = index + 1
		else:
			for batch_json in lazy_groups_of(self._get_json_data(), self._batch_size):
				for model_input_json, result in zip(batch_json, self._predict_json(batch_json)):
					self._maybe_print_to_console_and_file(
						index, result, json.dumps(model_input_json)
					)
					index = index + 1

		if self._output_file is not None:
			self._output_file.close()


def _predict(args: argparse.Namespace) -> None:
	common_logging.FILE_FRIENDLY_LOGGING = args.file_friendly_logging

	predictor = _get_predictor(args)

	if args.silent and not args.output_file:
		print("--silent specified without --output-file.")
		print("Exiting early because no output will be created.")
		sys.exit(0)

	manager = _PredictManager(
		predictor,
		args.input_file,
		args.output_file,
		args.batch_size,
		not args.silent,
		args.use_dataset_reader,
		args.multitask_head,
	)
	manager.run()

from allennlp.training.momentum_schedulers.momentum_scheduler import MomentumScheduler
from allennlp.training.momentum_schedulers.inverted_triangular import InvertedTriangular

import torch
from typing import Union, Optional
from os import PathLike

from allennlp.fairness.bias_direction import (
	BiasDirection,
	PCABiasDirection,
	PairedPCABiasDirection,
	TwoMeansBiasDirection,
	ClassificationNormalBiasDirection,
)
from allennlp.fairness.bias_utils import load_word_pairs, load_words

from allennlp.common import Registrable
from allennlp.data.tokenizers.tokenizer import Tokenizer
from allennlp.data import Vocabulary


class BiasDirectionWrapper(Registrable):

	def __init__(self):
		self.direction: BiasDirection = None
		self.noise: float = None

	def __call__(self, module):
		raise NotImplementedError

	def train(self, mode: bool = True):
		self.direction.requires_grad = mode

	def add_noise(self, t: torch.Tensor):
		return t + self.noise * torch.randn(t.size(), device=t.device)


@BiasDirectionWrapper.register("pca")
class PCABiasDirectionWrapper(BiasDirectionWrapper):

	def __init__(
		self,
		seed_words_file: Union[PathLike, str],
		tokenizer: Tokenizer,
		direction_vocab: Optional[Vocabulary] = None,
		namespace: str = "tokens",
		requires_grad: bool = False,
		noise: float = 1e-10,
	):
		self.ids = load_words(seed_words_file, tokenizer, direction_vocab, namespace)
		self.direction = PCABiasDirection(requires_grad=requires_grad)
		self.noise = noise

	def __call__(self, module):
		ids_embeddings = []
		for i in self.ids:
			i = i.to(module.weight.device)
			ids_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids_embeddings = torch.cat(ids_embeddings)

		ids_embeddings = self.add_noise(ids_embeddings)

		return self.direction(ids_embeddings)


@BiasDirectionWrapper.register("paired_pca")
class PairedPCABiasDirectionWrapper(BiasDirectionWrapper):

	def __init__(
		self,
		seed_word_pairs_file: Union[PathLike, str],
		tokenizer: Tokenizer,
		direction_vocab: Optional[Vocabulary] = None,
		namespace: str = "tokens",
		requires_grad: bool = False,
		noise: float = 1e-10,
	):
		self.ids1, self.ids2 = load_word_pairs(
			seed_word_pairs_file, tokenizer, direction_vocab, namespace
		)
		self.direction = PairedPCABiasDirection(requires_grad=requires_grad)
		self.noise = noise

	def __call__(self, module):
		ids1_embeddings = []
		for i in self.ids1:
			i = i.to(module.weight.device)
			ids1_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids2_embeddings = []
		for i in self.ids2:
			i = i.to(module.weight.device)
			ids2_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids1_embeddings = torch.cat(ids1_embeddings)
		ids2_embeddings = torch.cat(ids2_embeddings)

		ids1_embeddings = self.add_noise(ids1_embeddings)
		ids2_embeddings = self.add_noise(ids2_embeddings)

		return self.direction(ids1_embeddings, ids2_embeddings)


@BiasDirectionWrapper.register("two_means")
class TwoMeansBiasDirectionWrapper(BiasDirectionWrapper):

	def __init__(
		self,
		seed_word_pairs_file: Union[PathLike, str],
		tokenizer: Tokenizer,
		direction_vocab: Optional[Vocabulary] = None,
		namespace: str = "tokens",
		requires_grad: bool = False,
		noise: float = 1e-10,
	):
		self.ids1, self.ids2 = load_word_pairs(
			seed_word_pairs_file, tokenizer, direction_vocab, namespace
		)
		self.direction = TwoMeansBiasDirection(requires_grad=requires_grad)
		self.noise = noise

	def __call__(self, module):
		ids1_embeddings = []
		for i in self.ids1:
			i = i.to(module.weight.device)
			ids1_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids2_embeddings = []
		for i in self.ids2:
			i = i.to(module.weight.device)
			ids2_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids1_embeddings = torch.cat(ids1_embeddings)
		ids2_embeddings = torch.cat(ids2_embeddings)

		ids1_embeddings = self.add_noise(ids1_embeddings)
		ids2_embeddings = self.add_noise(ids2_embeddings)

		return self.direction(ids1_embeddings, ids2_embeddings)


@BiasDirectionWrapper.register("classification_normal")
class ClassificationNormalBiasDirectionWrapper(BiasDirectionWrapper):

	def __init__(
		self,
		seed_word_pairs_file: Union[PathLike, str],
		tokenizer: Tokenizer,
		direction_vocab: Optional[Vocabulary] = None,
		namespace: str = "tokens",
		noise: float = 1e-10,
	):
		self.ids1, self.ids2 = load_word_pairs(
			seed_word_pairs_file, tokenizer, direction_vocab, namespace
		)
		self.direction = ClassificationNormalBiasDirection()
		self.noise = noise

	def __call__(self, module):
		ids1_embeddings = []
		for i in self.ids1:
			i = i.to(module.weight.device)
			ids1_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids2_embeddings = []
		for i in self.ids2:
			i = i.to(module.weight.device)
			ids2_embeddings.append(torch.mean(module.forward(i), dim=0, keepdim=True))
		ids1_embeddings = torch.cat(ids1_embeddings)
		ids2_embeddings = torch.cat(ids2_embeddings)

		ids1_embeddings = self.add_noise(ids1_embeddings)
		ids2_embeddings = self.add_noise(ids2_embeddings)

		return self.direction(ids1_embeddings, ids2_embeddings)


import argparse
import atexit
import logging
import os
import shutil
import subprocess
import tempfile
import tarfile

from allennlp.common.file_utils import cached_path
from allennlp.models.archival import CONFIG_NAME

logger = logging.getLogger(__name__)


def main():
	parser = argparse.ArgumentParser(
		description="Perform surgery on a model.tar.gz archive",
		formatter_class=argparse.ArgumentDefaultsHelpFormatter,
	)

	parser.add_argument("--input-file", required=True, help="path to input file")
	parser.add_argument(
		"--editor",
		default=os.environ.get("EDITOR"),
		help="editor to launch, whose default value is `$EDITOR` the environment variable",
	)
	output = parser.add_mutually_exclusive_group()
	output.add_argument("--output-file", help="path to output file")
	output.add_argument(
		"--inplace",
		action="store_true",
		help="overwrite the input file with the modified configuration",
	)
	parser.add_argument(
		"-f", "--force", action="store_true", help="overwrite the output file if it exists"
	)

	args = parser.parse_args()

	if args.editor is None:
		raise RuntimeError("please specify an editor or set the $EDITOR environment variable")

	if not args.inplace and os.path.exists(args.output_file) and not args.force:
		raise ValueError("output file already exists, use --force to override")

	archive_file = cached_path(args.input_file)
	if not os.path.exists(archive_file):
		raise ValueError("input file doesn't exist")
	if args.inplace:
		output_file = archive_file
	else:
		output_file = args.output_file

	tempdir = tempfile.mkdtemp()
	with tarfile.open(archive_file, "r:gz") as archive:
		archive.extractall(tempdir)
	atexit.register(lambda: shutil.rmtree(tempdir))

	config_path = os.path.join(tempdir, CONFIG_NAME)
	subprocess.run([args.editor, config_path], check=False)

	with tarfile.open(output_file, "w:gz") as tar:
		tar.add(tempdir, arcname=os.path.sep)


if __name__ == "__main__":
	logging.basicConfig(level=logging.ERROR)
	main()

import torch

from allennlp.common import Registrable


class TokenEmbedder(torch.nn.Module, Registrable):

	default_implementation = "embedding"

	def get_output_dim(self) -> int:
		raise NotImplementedError

import numpy as np
import torch

np.random.seed(2)

T = 20
L = 1000
N = 100

x = np.empty((N, L), 'int64')
x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)
data = np.sin(x / 1.0 / T).astype('float64')
torch.save(data, open('traindata.pt', 'wb'))

import torch
from torch.nn.parameter import Parameter

from allennlp.common.checks import ConfigurationError
from allennlp.modules.span_extractors.span_extractor import SpanExtractor
from allennlp.modules.span_extractors.span_extractor_with_span_width_embedding import (
	SpanExtractorWithSpanWidthEmbedding,
)
from allennlp.nn import util


@SpanExtractor.register("bidirectional_endpoint")
class BidirectionalEndpointSpanExtractor(SpanExtractorWithSpanWidthEmbedding):

	def __init__(
		self,
		input_dim: int,
		forward_combination: str = "y-x",
		backward_combination: str = "x-y",
		num_width_embeddings: int = None,
		span_width_embedding_dim: int = None,
		bucket_widths: bool = False,
		use_sentinels: bool = True,
	) -> None:
		super().__init__(
			input_dim=input_dim,
			num_width_embeddings=num_width_embeddings,
			span_width_embedding_dim=span_width_embedding_dim,
			bucket_widths=bucket_widths,
		)
		self._forward_combination = forward_combination
		self._backward_combination = backward_combination

		if self._input_dim % 2 != 0:
			raise ConfigurationError(
				"The input dimension is not divisible by 2, but the "
				"BidirectionalEndpointSpanExtractor assumes the embedded representation "
				"is bidirectional (and hence divisible by 2)."
			)

		self._use_sentinels = use_sentinels
		if use_sentinels:
			self._start_sentinel = Parameter(torch.randn([1, 1, int(input_dim / 2)]))
			self._end_sentinel = Parameter(torch.randn([1, 1, int(input_dim / 2)]))

	def get_output_dim(self) -> int:
		unidirectional_dim = int(self._input_dim / 2)
		forward_combined_dim = util.get_combined_dim(
			self._forward_combination, [unidirectional_dim, unidirectional_dim]
		)
		backward_combined_dim = util.get_combined_dim(
			self._backward_combination, [unidirectional_dim, unidirectional_dim]
		)
		if self._span_width_embedding is not None:
			return (
				forward_combined_dim
				+ backward_combined_dim
				+ self._span_width_embedding.get_output_dim()
			)
		return forward_combined_dim + backward_combined_dim

	def _embed_spans(
		self,
		sequence_tensor: torch.FloatTensor,
		span_indices: torch.LongTensor,
		sequence_mask: torch.BoolTensor = None,
		span_indices_mask: torch.BoolTensor = None,
	) -> torch.FloatTensor:

		forward_sequence, backward_sequence = sequence_tensor.split(
			int(self._input_dim / 2), dim=-1
		)
		forward_sequence = forward_sequence.contiguous()
		backward_sequence = backward_sequence.contiguous()

		span_starts, span_ends = [index.squeeze(-1) for index in span_indices.split(1, dim=-1)]

		if span_indices_mask is not None:
			span_starts = span_starts * span_indices_mask
			span_ends = span_ends * span_indices_mask
		exclusive_span_starts = span_starts - 1
		start_sentinel_mask = (exclusive_span_starts == -1).unsqueeze(-1)

		exclusive_span_ends = span_ends + 1

		if sequence_mask is not None:
			sequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)
		else:
			sequence_lengths = torch.ones_like(
				sequence_tensor[:, 0, 0], dtype=torch.long
			) * sequence_tensor.size(1)

		end_sentinel_mask = (exclusive_span_ends >= sequence_lengths.unsqueeze(-1)).unsqueeze(-1)

		exclusive_span_ends = exclusive_span_ends * ~end_sentinel_mask.squeeze(-1)
		exclusive_span_starts = exclusive_span_starts * ~start_sentinel_mask.squeeze(-1)

		if (exclusive_span_starts < 0).any() or (
			exclusive_span_ends > sequence_lengths.unsqueeze(-1)
		).any():
			raise ValueError(
				f"Adjusted span indices must lie inside the length of the sequence tensor, "
				f"but found: exclusive_span_starts: {exclusive_span_starts}, "
				f"exclusive_span_ends: {exclusive_span_ends} for a sequence tensor with lengths "
				f"{sequence_lengths}."
			)

		forward_start_embeddings = util.batched_index_select(
			forward_sequence, exclusive_span_starts
		)
		forward_end_embeddings = util.batched_index_select(forward_sequence, span_ends)

		backward_start_embeddings = util.batched_index_select(
			backward_sequence, exclusive_span_ends
		)
		backward_end_embeddings = util.batched_index_select(backward_sequence, span_starts)

		if self._use_sentinels:
			forward_start_embeddings = (
				forward_start_embeddings * ~start_sentinel_mask
				+ start_sentinel_mask * self._start_sentinel
			)
			backward_start_embeddings = (
				backward_start_embeddings * ~end_sentinel_mask
				+ end_sentinel_mask * self._end_sentinel
			)

		forward_spans = util.combine_tensors(
			self._forward_combination, [forward_start_embeddings, forward_end_embeddings]
		)
		backward_spans = util.combine_tensors(
			self._backward_combination, [backward_start_embeddings, backward_end_embeddings]
		)
		span_embeddings = torch.cat([forward_spans, backward_spans], -1)

		return span_embeddings


import logging
import math
import os
import sys
import warnings
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional

import datasets
import evaluate
from datasets import load_dataset

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_FOR_MASKED_LM_MAPPING,
	AutoConfig,
	AutoModelForMaskedLM,
	AutoTokenizer,
	DataCollatorForLanguageModeling,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	is_torch_tpu_available,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/language-modeling/requirements.txt")

logger = logging.getLogger(__name__)
MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:

	model_name_or_path: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch."
			)
		},
	)
	model_type: Optional[str] = field(
		default=None,
		metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
	)
	config_overrides: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"Override some existing default config settings when a model is trained from scratch. Example: "
				"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
			)
		},
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	low_cpu_mem_usage: bool = field(
		default=False,
		metadata={
			"help": (
				"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. "
				"set True will benefit LLM loading time and RAM consumption."
			)
		},
	)

	def __post_init__(self):
		if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
			raise ValueError(
				"--config_overrides can't be used in combination with --config_name or --model_name_or_path"
			)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	validation_split_percentage: Optional[int] = field(
		default=5,
		metadata={
			"help": "The percentage of the train set used as validation set in case there's no validation split"
		},
	)
	max_seq_length: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated."
			)
		},
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	mlm_probability: float = field(
		default=0.15, metadata={"help": "Ratio of tokens to mask for masked language modeling loss"}
	)
	line_by_line: bool = field(
		default=False,
		metadata={"help": "Whether distinct lines of text in the dataset are to be handled as distinct sequences."},
	)
	pad_to_max_length: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})

	def __post_init__(self):
		if self.streaming:
			require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")

		if self.dataset_name is None and self.train_file is None and self.validation_file is None:
			raise ValueError("Need either a dataset name or a training/validation file.")
		else:
			if self.train_file is not None:
				extension = self.train_file.split(".")[-1]
				if extension not in ["csv", "json", "txt"]:
					raise ValueError("`train_file` should be a csv, a json or a txt file.")
			if self.validation_file is not None:
				extension = self.validation_file.split(".")[-1]
				if extension not in ["csv", "json", "txt"]:
					raise ValueError("`validation_file` should be a csv, a json or a txt file.")


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_mlm", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
			streaming=data_args.streaming,
		)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				data_args.dataset_name,
				data_args.dataset_config_name,
				split=f"train[:{data_args.validation_split_percentage}%]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
				streaming=data_args.streaming,
			)
			raw_datasets["train"] = load_dataset(
				data_args.dataset_name,
				data_args.dataset_config_name,
				split=f"train[{data_args.validation_split_percentage}%:]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
				streaming=data_args.streaming,
			)
	else:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
			extension = data_args.train_file.split(".")[-1]
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
			extension = data_args.validation_file.split(".")[-1]
		if extension == "txt":
			extension = "text"
		raw_datasets = load_dataset(
			extension,
			data_files=data_files,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)

		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[:{data_args.validation_split_percentage}%]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
			raw_datasets["train"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[{data_args.validation_split_percentage}%:]",
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)


	config_kwargs = {
		"cache_dir": model_args.cache_dir,
		"revision": model_args.model_revision,
		"token": model_args.token,
		"trust_remote_code": model_args.trust_remote_code,
	}
	if model_args.config_name:
		config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
	elif model_args.model_name_or_path:
		config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
	else:
		config = CONFIG_MAPPING[model_args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")
		if model_args.config_overrides is not None:
			logger.info(f"Overriding config: {model_args.config_overrides}")
			config.update_from_string(model_args.config_overrides)
			logger.info(f"New config: {config}")

	tokenizer_kwargs = {
		"cache_dir": model_args.cache_dir,
		"use_fast": model_args.use_fast_tokenizer,
		"revision": model_args.model_revision,
		"token": model_args.token,
		"trust_remote_code": model_args.trust_remote_code,
	}
	if model_args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
	elif model_args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if model_args.model_name_or_path:
		model = AutoModelForMaskedLM.from_pretrained(
			model_args.model_name_or_path,
			from_tf=bool(".ckpt" in model_args.model_name_or_path),
			config=config,
			cache_dir=model_args.cache_dir,
			revision=model_args.model_revision,
			token=model_args.token,
			trust_remote_code=model_args.trust_remote_code,
			low_cpu_mem_usage=model_args.low_cpu_mem_usage,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForMaskedLM.from_config(config, trust_remote_code=model_args.trust_remote_code)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if training_args.do_train:
		column_names = list(raw_datasets["train"].features)
	else:
		column_names = list(raw_datasets["validation"].features)
	text_column_name = "text" if "text" in column_names else column_names[0]

	if data_args.max_seq_length is None:
		max_seq_length = tokenizer.model_max_length
		if max_seq_length > 1024:
			logger.warning(
				"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
				" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
				" override this default with `--block_size xxx`."
			)
			max_seq_length = 1024
	else:
		if data_args.max_seq_length > tokenizer.model_max_length:
			logger.warning(
				f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
				f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
			)
		max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	if data_args.line_by_line:
		padding = "max_length" if data_args.pad_to_max_length else False

		def tokenize_function(examples):
			examples[text_column_name] = [
				line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()
			]
			return tokenizer(
				examples[text_column_name],
				padding=padding,
				truncation=True,
				max_length=max_seq_length,
				return_special_tokens_mask=True,
			)

		with training_args.main_process_first(desc="dataset map tokenization"):
			if not data_args.streaming:
				tokenized_datasets = raw_datasets.map(
					tokenize_function,
					batched=True,
					num_proc=data_args.preprocessing_num_workers,
					remove_columns=[text_column_name],
					load_from_cache_file=not data_args.overwrite_cache,
					desc="Running tokenizer on dataset line_by_line",
				)
			else:
				tokenized_datasets = raw_datasets.map(
					tokenize_function,
					batched=True,
					remove_columns=[text_column_name],
				)
	else:
		def tokenize_function(examples):
			return tokenizer(examples[text_column_name], return_special_tokens_mask=True)

		with training_args.main_process_first(desc="dataset map tokenization"):
			if not data_args.streaming:
				tokenized_datasets = raw_datasets.map(
					tokenize_function,
					batched=True,
					num_proc=data_args.preprocessing_num_workers,
					remove_columns=column_names,
					load_from_cache_file=not data_args.overwrite_cache,
					desc="Running tokenizer on every text in dataset",
				)
			else:
				tokenized_datasets = raw_datasets.map(
					tokenize_function,
					batched=True,
					remove_columns=column_names,
				)

		def group_texts(examples):
			concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
			total_length = len(concatenated_examples[list(examples.keys())[0]])
			total_length = (total_length // max_seq_length) * max_seq_length
			result = {
				k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]
				for k, t in concatenated_examples.items()
			}
			return result


		with training_args.main_process_first(desc="grouping texts together"):
			if not data_args.streaming:
				tokenized_datasets = tokenized_datasets.map(
					group_texts,
					batched=True,
					num_proc=data_args.preprocessing_num_workers,
					load_from_cache_file=not data_args.overwrite_cache,
					desc=f"Grouping texts in chunks of {max_seq_length}",
				)
			else:
				tokenized_datasets = tokenized_datasets.map(
					group_texts,
					batched=True,
				)

	if training_args.do_train:
		if "train" not in tokenized_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = tokenized_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	if training_args.do_eval:
		if "validation" not in tokenized_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_dataset = tokenized_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

		def preprocess_logits_for_metrics(logits, labels):
			if isinstance(logits, tuple):
				logits = logits[0]
			return logits.argmax(dim=-1)

		metric = evaluate.load("accuracy", cache_dir=model_args.cache_dir)

		def compute_metrics(eval_preds):
			preds, labels = eval_preds
			labels = labels.reshape(-1)
			preds = preds.reshape(-1)
			mask = labels != -100
			labels = labels[mask]
			preds = preds[mask]
			return metric.compute(predictions=preds, references=labels)

	pad_to_multiple_of_8 = data_args.line_by_line and training_args.fp16 and not data_args.pad_to_max_length
	data_collator = DataCollatorForLanguageModeling(
		tokenizer=tokenizer,
		mlm_probability=data_args.mlm_probability,
		pad_to_multiple_of=8 if pad_to_multiple_of_8 else None,
	)

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
		preprocess_logits_for_metrics=preprocess_logits_for_metrics
		if training_args.do_eval and not is_torch_tpu_available()
		else None,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload
		metrics = train_result.metrics

		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")

		metrics = trainer.evaluate()

		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
		try:
			perplexity = math.exp(metrics["eval_loss"])
		except OverflowError:
			perplexity = float("inf")
		metrics["perplexity"] = perplexity

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "fill-mask"}
	if data_args.dataset_name is not None:
		kwargs["dataset_tags"] = data_args.dataset_name
		if data_args.dataset_config_name is not None:
			kwargs["dataset_args"] = data_args.dataset_config_name
			kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
		else:
			kwargs["dataset"] = data_args.dataset_name

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from __future__ import print_function
import argparse, random, copy
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision import transforms as T
from torch.optim.lr_scheduler import StepLR


class SiameseNetwork(nn.Module):
	def __init__(self):
		super(SiameseNetwork, self).__init__()
		self.resnet = torchvision.models.resnet18(weights=None)

		self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
		self.fc_in_features = self.resnet.fc.in_features
		
		self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))

		self.fc = nn.Sequential(
			nn.Linear(self.fc_in_features * 2, 256),
			nn.ReLU(inplace=True),
			nn.Linear(256, 1),
		)

		self.sigmoid = nn.Sigmoid()

		self.resnet.apply(self.init_weights)
		self.fc.apply(self.init_weights)
		
	def init_weights(self, m):
		if isinstance(m, nn.Linear):
			torch.nn.init.xavier_uniform_(m.weight)
			m.bias.data.fill_(0.01)

	def forward_once(self, x):
		output = self.resnet(x)
		output = output.view(output.size()[0], -1)
		return output

	def forward(self, input1, input2):
		output1 = self.forward_once(input1)
		output2 = self.forward_once(input2)

		output = torch.cat((output1, output2), 1)

		output = self.fc(output)

		output = self.sigmoid(output)
		
		return output

class APP_MATCHER(Dataset):
	def __init__(self, root, train, download=False):
		super(APP_MATCHER, self).__init__()

		self.dataset = datasets.MNIST(root, train=train, download=download)
		
		self.data = self.dataset.data.unsqueeze(1).clone()

		self.group_examples()

	def group_examples(self):

		np_arr = np.array(self.dataset.targets.clone())
		
		self.grouped_examples = {}
		for i in range(0,10):
			self.grouped_examples[i] = np.where((np_arr==i))[0]
	
	def __len__(self):
		return self.data.shape[0]
	
	def __getitem__(self, index):

		selected_class = random.randint(0, 9)

		random_index_1 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)
		
		index_1 = self.grouped_examples[selected_class][random_index_1]

		image_1 = self.data[index_1].clone().float()

		if index % 2 == 0:
			random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)
			
			while random_index_2 == random_index_1:
				random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)
			
			index_2 = self.grouped_examples[selected_class][random_index_2]

			image_2 = self.data[index_2].clone().float()

			target = torch.tensor(1, dtype=torch.float)
		
		else:
			other_selected_class = random.randint(0, 9)

			while other_selected_class == selected_class:
				other_selected_class = random.randint(0, 9)

			
			random_index_2 = random.randint(0, self.grouped_examples[other_selected_class].shape[0]-1)

			index_2 = self.grouped_examples[other_selected_class][random_index_2]

			image_2 = self.data[index_2].clone().float()

			target = torch.tensor(0, dtype=torch.float)

		return image_1, image_2, target


def train(args, model, device, train_loader, optimizer, epoch):
	model.train()

	criterion = nn.BCELoss()

	for batch_idx, (images_1, images_2, targets) in enumerate(train_loader):
		images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)
		optimizer.zero_grad()
		outputs = model(images_1, images_2).squeeze()
		loss = criterion(outputs, targets)
		loss.backward()
		optimizer.step()
		if batch_idx % args.log_interval == 0:
			print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
				epoch, batch_idx * len(images_1), len(train_loader.dataset),
				100. * batch_idx / len(train_loader), loss.item()))
			if args.dry_run:
				break


def test(model, device, test_loader):
	model.eval()
	test_loss = 0
	correct = 0

	criterion = nn.BCELoss()

	with torch.no_grad():
		for (images_1, images_2, targets) in test_loader:
			images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)
			outputs = model(images_1, images_2).squeeze()
			test_loss += criterion(outputs, targets).sum().item()  # sum up batch loss
			pred = torch.where(outputs > 0.5, 1, 0)  # get the index of the max log-probability
			correct += pred.eq(targets.view_as(pred)).sum().item()

	test_loss /= len(test_loader.dataset)

	print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
		test_loss, correct, len(test_loader.dataset),
		100. * correct / len(test_loader.dataset)))


def main():
	parser = argparse.ArgumentParser(description='PyTorch Siamese network Example')
	parser.add_argument('--batch-size', type=int, default=64, metavar='N',
						help='input batch size for training (default: 64)')
	parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
						help='input batch size for testing (default: 1000)')
	parser.add_argument('--epochs', type=int, default=14, metavar='N',
						help='number of epochs to train (default: 14)')
	parser.add_argument('--lr', type=float, default=1.0, metavar='LR',
						help='learning rate (default: 1.0)')
	parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
						help='Learning rate step gamma (default: 0.7)')
	parser.add_argument('--no-cuda', action='store_true', default=False,
						help='disables CUDA training')
	parser.add_argument('--no-mps', action='store_true', default=False,
						help='disables macOS GPU training')
	parser.add_argument('--dry-run', action='store_true', default=False,
						help='quickly check a single pass')
	parser.add_argument('--seed', type=int, default=1, metavar='S',
						help='random seed (default: 1)')
	parser.add_argument('--log-interval', type=int, default=10, metavar='N',
						help='how many batches to wait before logging training status')
	parser.add_argument('--save-model', action='store_true', default=False,
						help='For Saving the current Model')
	args = parser.parse_args()
	
	use_cuda = not args.no_cuda and torch.cuda.is_available()
	use_mps = not args.no_mps and torch.backends.mps.is_available()

	torch.manual_seed(args.seed)

	if use_cuda:
		device = torch.device("cuda")
	elif use_mps:
		device = torch.device("mps")
	else:
		device = torch.device("cpu")

	train_kwargs = {'batch_size': args.batch_size}
	test_kwargs = {'batch_size': args.test_batch_size}
	if use_cuda:
		cuda_kwargs = {'num_workers': 1,
					   'pin_memory': True,
					   'shuffle': True}
		train_kwargs.update(cuda_kwargs)
		test_kwargs.update(cuda_kwargs)

	train_dataset = APP_MATCHER('../data', train=True, download=True)
	test_dataset = APP_MATCHER('../data', train=False)
	train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)
	test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)

	model = SiameseNetwork().to(device)
	optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

	scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
	for epoch in range(1, args.epochs + 1):
		train(args, model, device, train_loader, optimizer, epoch)
		test(model, device, test_loader)
		scheduler.step()

	if args.save_model:
		torch.save(model.state_dict(), "siamese_network.pt")


if __name__ == '__main__':
	main()

from typing import Dict, Optional


import torch

from allennlp.data import Vocabulary
from allennlp.models.heads.head import Head
from allennlp.modules import FeedForward, Seq2VecEncoder
from allennlp.training.metrics import CategoricalAccuracy


@Head.register("classifier")
class ClassifierHead(Head):

	def __init__(
		self,
		vocab: Vocabulary,
		seq2vec_encoder: Seq2VecEncoder,
		feedforward: Optional[FeedForward] = None,
		input_dim: int = None,
		dropout: float = None,
		num_labels: int = None,
		label_namespace: str = "labels",
	) -> None:

		super().__init__(vocab)
		self._seq2vec_encoder = seq2vec_encoder
		self._feedforward = feedforward
		if self._feedforward is not None:
			self._classifier_input_dim = self._feedforward.get_output_dim()
		else:
			self._classifier_input_dim = self._seq2vec_encoder.get_output_dim() or input_dim

		if self._classifier_input_dim is None:
			raise ValueError("No input dimension given!")

		if dropout:
			self._dropout = torch.nn.Dropout(dropout)
		else:
			self._dropout = None
		self._label_namespace = label_namespace

		if num_labels:
			self._num_labels = num_labels
		else:
			self._num_labels = vocab.get_vocab_size(namespace=self._label_namespace)
		self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)
		self._accuracy = CategoricalAccuracy()
		self._loss = torch.nn.CrossEntropyLoss()

	def forward(  # type: ignore
		self,
		encoded_text: torch.FloatTensor,
		encoded_text_mask: torch.BoolTensor,
		label: torch.IntTensor = None,
	) -> Dict[str, torch.Tensor]:
		encoding = self._seq2vec_encoder(encoded_text, mask=encoded_text_mask)

		if self._dropout:
			encoding = self._dropout(encoding)

		if self._feedforward is not None:
			encoding = self._feedforward(encoding)

		logits = self._classification_layer(encoding)
		probs = torch.nn.functional.softmax(logits, dim=-1)

		output_dict = {"logits": logits, "probs": probs}
		if label is not None:
			loss = self._loss(logits, label.long().view(-1))
			output_dict["loss"] = loss
			self._accuracy(logits, label)

		return output_dict

	def make_output_human_readable(
		self, output_dict: Dict[str, torch.Tensor]
	) -> Dict[str, torch.Tensor]:
		if "probs" in output_dict:
			predictions = output_dict["probs"]
			if predictions.dim() == 2:
				predictions_list = [predictions[i] for i in range(predictions.shape[0])]
			else:
				predictions_list = [predictions]
			classes = []
			for prediction in predictions_list:
				label_idx = prediction.argmax(dim=-1).item()
				label_str = self.vocab.get_index_to_token_vocabulary(self._label_namespace).get(
					label_idx, str(label_idx)
				)
				classes.append(label_str)
			output_dict["label"] = classes
		return output_dict

	def get_metrics(self, reset: bool = False) -> Dict[str, float]:
		metrics = {"accuracy": self._accuracy.get_metric(reset)}
		return metrics


import logging
from typing import Optional, Tuple, List, Union, Dict, TYPE_CHECKING, NamedTuple, Callable

import torch
from torch import nn
import torch.nn.functional as F
from torch.nn import CrossEntropyLoss

from allennlp.common import FromParams, Params, Lazy, Registrable
from allennlp.common.checks import ConfigurationError
from allennlp.modules.transformer.transformer_module import TransformerModule
from allennlp.modules.transformer.attention_module import (
	T5Attention,
	AttentionOutput,
)
from allennlp.modules.transformer.util import (
	get_extended_attention_mask,
	FloatT,
	IntT,
	BoolT,
)
from allennlp.nn.beam_search import BeamSearch
from allennlp.nn.parallel import DdpAccelerator
from allennlp.nn.checkpoint import CheckpointWrapper

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig

logger = logging.getLogger(__name__)


class T5LayerNorm(TransformerModule, FromParams):

	def __init__(self, hidden_size: int = 512, eps: float = 1e-6):
		super().__init__()
		self.weight = nn.Parameter(torch.ones(hidden_size))
		self.variance_epsilon = eps

	def forward(self, hidden_states) -> FloatT:
		variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
		hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)

		if self.weight.dtype == torch.float16:
			hidden_states = hidden_states.to(torch.float16)
		return self.weight * hidden_states


class T5FeedForwardProjection(TransformerModule, Registrable):
	def forward(self, hidden_states) -> FloatT:
		raise NotImplementedError


@T5FeedForwardProjection.register("relu")
class T5DenseReluDense(TransformerModule, FromParams):
	def __init__(self, hidden_size: int = 512, ff_size: int = 2048, dropout: float = 0.1):
		super().__init__()
		self.wi = nn.Linear(hidden_size, ff_size, bias=False)
		self.wi.weight.data.normal_(mean=0.0, std=hidden_size**-0.5)
		self.wo = nn.Linear(ff_size, hidden_size, bias=False)
		self.wo.weight.data.normal_(mean=0.0, std=ff_size**-0.5)
		self.dropout = nn.Dropout(dropout)

	def forward(self, hidden_states) -> FloatT:
		hidden_states = self.wi(hidden_states)
		hidden_states = F.relu(hidden_states)
		hidden_states = self.dropout(hidden_states)
		hidden_states = self.wo(hidden_states)
		return hidden_states


@T5FeedForwardProjection.register("gated-gelu")
class T5DenseGatedGeluDense(TransformerModule, FromParams):
	def __init__(self, hidden_size: int = 512, ff_size: int = 2048, dropout: float = 0.1):
		super().__init__()
		self.wi_0 = nn.Linear(hidden_size, ff_size, bias=False)
		self.wi_0.weight.data.normal_(mean=0.0, std=hidden_size**-0.5)
		self.wi_1 = nn.Linear(hidden_size, ff_size, bias=False)
		self.wi_1.weight.data.normal_(mean=0.0, std=hidden_size**-0.5)
		self.wo = nn.Linear(ff_size, hidden_size, bias=False)
		self.wo.weight.data.normal_(mean=0.0, std=ff_size**-0.5)
		self.dropout = nn.Dropout(dropout)
		from allennlp.nn import Activation

		self.gelu_act = Activation.by_name("gelu_new")()

	def forward(self, hidden_states) -> FloatT:
		hidden_gelu = self.gelu_act(self.wi_0(hidden_states))
		hidden_linear = self.wi_1(hidden_states)
		hidden_states = hidden_gelu * hidden_linear
		hidden_states = self.dropout(hidden_states)
		hidden_states = self.wo(hidden_states)
		return hidden_states


class T5LayerFF(TransformerModule, FromParams):
	_pretrained_mapping = {"DenseReluDense": "ff_proj"}

	def __init__(
		self,
		ff_proj: Optional[T5FeedForwardProjection] = None,
		layer_norm: Optional[T5LayerNorm] = None,
		dropout: float = 0.1,
	):
		super().__init__()
		self.ff_proj = ff_proj or T5DenseReluDense()
		self.layer_norm = layer_norm or T5LayerNorm()
		self.dropout = nn.Dropout(dropout)

	def forward(self, hidden_states) -> FloatT:
		forwarded_states = self.layer_norm(hidden_states)
		forwarded_states = self.ff_proj(forwarded_states)
		hidden_states = hidden_states + self.dropout(forwarded_states)
		return hidden_states


class T5LayerSelfAttentionOutput(NamedTuple):
	hidden_states: FloatT
	attn_key_value_state: Optional[Tuple[FloatT, FloatT]]
	attn_position_bias: FloatT
	attn_weights: Optional[FloatT] = None


class T5LayerSelfAttention(TransformerModule, FromParams):
	_pretrained_mapping = {"SelfAttention": "self_attention"}

	def __init__(
		self,
		self_attention: Optional[T5Attention] = None,
		layer_norm: Optional[T5LayerNorm] = None,
		dropout: float = 0.1,
		has_relative_attention_bias: bool = False,
	):
		super().__init__()
		self.self_attention = self_attention or T5Attention(
			has_relative_attention_bias=has_relative_attention_bias
		)
		self.layer_norm = layer_norm or T5LayerNorm(hidden_size=self.self_attention.hidden_size)
		self.dropout = nn.Dropout(dropout)

	@property
	def hidden_size(self) -> int:
		return self.self_attention.hidden_size

	def forward(
		self,
		hidden_states: FloatT,
		attention_mask: Optional[torch.BoolTensor] = None,
		position_bias: Optional[torch.Tensor] = None,
		layer_head_mask: Optional[torch.BoolTensor] = None,
		past_key_value: Optional[Tuple[FloatT]] = None,
		use_cache: bool = False,
		output_attentions: bool = False,
	) -> T5LayerSelfAttentionOutput:

		normed_hidden_states = self.layer_norm(hidden_states)

		attention_output: AttentionOutput = self.self_attention(
			normed_hidden_states,
			mask=attention_mask,
			position_bias=position_bias,
			layer_head_mask=layer_head_mask,
			past_key_value=past_key_value,
			use_cache=use_cache,
			output_attentions=output_attentions,
		)

		hidden_states = hidden_states + self.dropout(attention_output.hidden_states)

		return T5LayerSelfAttentionOutput(
			hidden_states,
			attention_output.key_value_state,
			attention_output.position_bias,
			attention_output.attention_probs,
		)


class T5LayerCrossAttentionOutput(NamedTuple):
	hidden_states: FloatT
	attn_key_value_state: Optional[Tuple[FloatT, FloatT]]
	attn_position_bias: FloatT
	attn_weights: Optional[FloatT] = None


class T5LayerCrossAttention(TransformerModule, FromParams):
	_pretrained_mapping = {"EncDecAttention": "enc_dec_attention"}

	def __init__(
		self,
		enc_dec_attention: Optional[T5Attention] = None,
		layer_norm: Optional[T5LayerNorm] = None,
		dropout: float = 0.1,
	):
		super().__init__()
		self.enc_dec_attention = enc_dec_attention or T5Attention(
			is_decoder=True,
			has_relative_attention_bias=False,
			is_cross_attention=True,
		)
		self.layer_norm = layer_norm or T5LayerNorm(hidden_size=self.enc_dec_attention.hidden_size)
		self.dropout = nn.Dropout(dropout)

	def forward(
		self,
		hidden_states: FloatT,
		key_value_states: Optional[FloatT],
		attention_mask: Optional[torch.BoolTensor] = None,
		position_bias: Optional[FloatT] = None,
		layer_head_mask: Optional[torch.BoolTensor] = None,
		past_key_value: Optional[Tuple[Tuple[FloatT]]] = None,
		use_cache: bool = False,
		query_length: int = None,
		output_attentions: bool = False,
	) -> T5LayerCrossAttentionOutput:
		normed_hidden_states = self.layer_norm(hidden_states)
		attention_output: AttentionOutput = self.enc_dec_attention(
			normed_hidden_states,
			mask=attention_mask,
			key_value_states=key_value_states,
			position_bias=position_bias,
			layer_head_mask=layer_head_mask,
			past_key_value=past_key_value,
			use_cache=use_cache,
			query_length=query_length,
			output_attentions=output_attentions,
		)
		layer_output = hidden_states + self.dropout(attention_output.hidden_states)

		return T5LayerCrossAttentionOutput(
			layer_output,
			attention_output.key_value_state,
			attention_output.position_bias,
			attention_output.attention_probs,
		)


KeyValueStates = Union[
	Tuple[FloatT, FloatT],  # without cross attention
	Tuple[FloatT, FloatT, FloatT, FloatT],  # with cross attention
]


class T5BlockOutput(NamedTuple):
	hidden_states: FloatT
	present_key_value_states: Optional[KeyValueStates]
	self_attn_weights: Optional[FloatT]
	self_attn_position_bias: Optional[FloatT]
	cross_attn_weights: Optional[FloatT] = None
	cross_attn_position_bias: Optional[FloatT] = None


class T5Block(TransformerModule, FromParams):
	def __init__(
		self,
		attention: Optional[T5LayerSelfAttention] = None,
		cross_attention: Optional[T5LayerCrossAttention] = None,
		ff: Optional[T5LayerFF] = None,
	):
		super().__init__()
		self.layer = nn.ModuleList()
		self.layer.append(attention or T5LayerSelfAttention())
		if cross_attention is None:
			self.is_decoder = False
		else:
			self.layer.append(cross_attention)
			self.is_decoder = True
		self.layer.append(ff or T5LayerFF())

	@property
	def hidden_size(self) -> int:
		return self.layer[0].hidden_size

	def forward(
		self,
		hidden_states: FloatT,
		attention_mask: Optional[torch.BoolTensor] = None,
		position_bias: Optional[FloatT] = None,
		encoder_hidden_states: Optional[FloatT] = None,
		encoder_attention_mask: Optional[torch.BoolTensor] = None,
		encoder_decoder_position_bias: Optional[FloatT] = None,
		layer_head_mask: Optional[torch.BoolTensor] = None,
		encoder_layer_head_mask: Optional[torch.BoolTensor] = None,
		past_key_value: Optional[KeyValueStates] = None,
		use_cache: bool = False,
		output_attentions: bool = False,
	) -> T5BlockOutput:
		if past_key_value is not None:
			assert self.is_decoder, "Only decoder can use `past_key_values`"
			expected_num_past_key_values = 2 if encoder_hidden_states is None else 4

			error_message = f"There should be {expected_num_past_key_values} past states. "
			error_message += "2 (past / key) for self attention. "
			if expected_num_past_key_values == 4:
				error_message += "2 (past / key) for cross attention. "
			error_message += f"Got {len(past_key_value)} past key / value states"
			assert len(past_key_value) == expected_num_past_key_values, error_message

		self_attention_outputs: T5LayerSelfAttentionOutput = self.layer[0](
			hidden_states,
			attention_mask=attention_mask,
			position_bias=position_bias,
			layer_head_mask=layer_head_mask,
			past_key_value=None if past_key_value is None else past_key_value[:2],
			use_cache=use_cache,
			output_attentions=output_attentions,
		)
		hidden_states = self_attention_outputs.hidden_states
		present_key_value_state: Optional[
			Tuple[FloatT, FloatT]
		] = self_attention_outputs.attn_key_value_state

		if torch.isinf(hidden_states).any():
			clamp_value = torch.finfo(hidden_states.dtype).max - 1000
			hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

		do_cross_attention = self.is_decoder and encoder_hidden_states is not None
		if do_cross_attention:
			if present_key_value_state is not None:
				query_length = present_key_value_state[0].shape[2]
			else:
				query_length = None

			cross_attention_outputs: T5LayerCrossAttentionOutput = self.layer[1](
				hidden_states,
				key_value_states=encoder_hidden_states,
				attention_mask=encoder_attention_mask,
				position_bias=encoder_decoder_position_bias,
				layer_head_mask=encoder_layer_head_mask,
				past_key_value=None if past_key_value is None else past_key_value[2:],
				query_length=query_length,
				use_cache=use_cache,
				output_attentions=output_attentions,
			)
			hidden_states = cross_attention_outputs.hidden_states
			if torch.isinf(hidden_states).any():
				clamp_value = torch.finfo(hidden_states.dtype).max - 1000
				hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

			if (
				present_key_value_state is not None
				and cross_attention_outputs.attn_key_value_state is not None
			):
				present_key_value_state: KeyValueStates = (  # type: ignore[no-redef]
					present_key_value_state + cross_attention_outputs.attn_key_value_state
				)

		hidden_states = self.layer[-1](hidden_states)
		if torch.isinf(hidden_states).any():
			clamp_value = torch.finfo(hidden_states.dtype).max - 1000
			hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

		output = T5BlockOutput(
			hidden_states,
			present_key_value_state,
			self_attention_outputs.attn_weights,
			self_attention_outputs.attn_position_bias,
			cross_attn_weights=(
				None if not do_cross_attention else cross_attention_outputs.attn_weights
			),
			cross_attn_position_bias=(
				None if not do_cross_attention else cross_attention_outputs.attn_position_bias
			),
		)
		return output


class T5StackOutput(NamedTuple):
	last_hidden_state: FloatT
	past_key_values: Optional[List[KeyValueStates]] = None
	all_hidden_states: Optional[List[FloatT]] = None
	attentions: Optional[List[FloatT]] = None
	cross_attentions: Optional[List[FloatT]] = None


class T5Stack(TransformerModule, FromParams):
	_pretrained_mapping = {"embed_tokens": "token_embeddings", "block": "blocks"}

	def __init__(
		self,
		token_embeddings: nn.Embedding,
		blocks: List[T5Block],
		final_layer_norm: Optional[T5LayerNorm] = None,
		dropout: float = 0.1,
	):
		super().__init__()
		self.is_decoder = blocks[0].is_decoder
		if not all(b.is_decoder == self.is_decoder for b in blocks):
			raise ConfigurationError("Found mismatched blocks in stack.")
		self.blocks = nn.ModuleList(blocks)
		self.token_embeddings = token_embeddings
		self.final_layer_norm = final_layer_norm or T5LayerNorm(hidden_size=self.hidden_size)
		self.dropout = nn.Dropout(dropout)

	@property
	def num_blocks(self) -> int:
		return len(self.blocks)

	@property
	def hidden_size(self) -> int:
		return self.blocks[0].hidden_size

	@staticmethod
	def get_head_mask(head_mask: Optional[torch.BoolTensor], num_hidden_layers: int) -> BoolT:
		if head_mask is not None:
			if head_mask.dim() == 1:
				head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)
				head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)
			elif head_mask.dim() == 2:
				head_mask = (
					head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)
				)  # We can specify head_mask for each layer
			assert head_mask.dim() == 5, f"head_mask.dim != 5, instead {head_mask.dim()}"
		else:
			head_mask = [None] * num_hidden_layers
		return head_mask

	def resize_token_embeddings(
		self, new_size: int, *, init_fn: Callable = torch.nn.init.normal_
	) -> None:
		old_size, embedding_dim = tuple(self.token_embeddings.weight.shape)
		if old_size == new_size:
			return
		if old_size > new_size:
			logger.warning(
				"Shrinking vocabulary from size %d to size %d. This is probably not what you want?",
				old_size,
				new_size,
			)

		result = torch.nn.Embedding(
			new_size,
			embedding_dim,
			self.token_embeddings.padding_idx,
			self.token_embeddings.max_norm,
			self.token_embeddings.norm_type,
			self.token_embeddings.scale_grad_by_freq,
			self.token_embeddings.sparse,
			device=self.token_embeddings.weight.device,
			dtype=self.token_embeddings.weight.dtype,
		)
		copy_size = min(old_size, new_size)
		result.weight.data[:copy_size, ...] = self.token_embeddings.weight.data[:copy_size, ...]
		if new_size > old_size:
			init_fn(result.weight.data[copy_size:, ...])
		self.token_embeddings = result

	def forward(
		self,
		input_ids: Optional[torch.IntTensor] = None,
		attention_mask: Optional[torch.BoolTensor] = None,
		encoder_hidden_states: Optional[FloatT] = None,
		encoder_attention_mask: Optional[torch.BoolTensor] = None,
		inputs_embeds: Optional[FloatT] = None,
		head_mask: Optional[torch.BoolTensor] = None,
		encoder_head_mask: Optional[torch.BoolTensor] = None,
		past_key_values: Optional[KeyValueStates] = None,
		use_cache: bool = False,
		output_attentions: bool = False,
		output_all_hidden_states: bool = False,
	) -> T5StackOutput:
		if input_ids is not None and inputs_embeds is not None:
			err_msg_prefix = "decoder_" if self.is_decoder else ""
			raise ValueError(
				f"You cannot specify both {err_msg_prefix}inputs "
				f"and {err_msg_prefix}inputs_embeds at the same time"
			)
		elif input_ids is not None:
			input_shape = input_ids.size()
			input_ids = input_ids.view(-1, input_shape[-1])
		elif inputs_embeds is not None:
			input_shape = inputs_embeds.size()[:-1]
		else:
			err_msg_prefix = "decoder_" if self.is_decoder else ""
			raise ValueError(
				f"You have to specify either {err_msg_prefix}inputs or {err_msg_prefix}inputs_embeds"
			)

		if inputs_embeds is None:
			assert (
				self.token_embeddings is not None
			), "You have to initialize the model with valid token embeddings"
			inputs_embeds = self.token_embeddings(input_ids)

		batch_size, seq_length = input_shape

		mask_seq_length = (
			seq_length if past_key_values is None else past_key_values[0][0].shape[2] + seq_length
		)

		if use_cache is True:
			assert (
				self.is_decoder
			), ":obj:`use_cache` can only be set to `True` if {} is used as a decoder".format(self)

		if attention_mask is None:
			attention_mask = torch.ones(
				batch_size, mask_seq_length, dtype=torch.bool, device=inputs_embeds.device
			)
		if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:
			encoder_seq_length = encoder_hidden_states.shape[1]
			encoder_attention_mask = torch.ones(
				batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.bool
			)

		extended_attention_mask = get_extended_attention_mask(
			attention_mask, input_shape, inputs_embeds.dtype, is_decoder=self.is_decoder
		)

		head_mask = self.get_head_mask(head_mask, self.num_blocks)
		encoder_head_mask = self.get_head_mask(encoder_head_mask, self.num_blocks)
		present_key_value_states: Optional[List[KeyValueStates]] = [] if use_cache else None
		all_hidden_states: Optional[List[FloatT]] = [] if output_all_hidden_states else None
		all_attentions: Optional[List[FloatT]] = [] if output_attentions else None
		all_cross_attentions: Optional[List[FloatT]] = (
			[] if (output_attentions and self.is_decoder) else None
		)
		position_bias: Optional[FloatT] = None
		encoder_decoder_position_bias: Optional[FloatT] = None

		hidden_states = self.dropout(inputs_embeds)

		for i, (layer_module, past_key_value) in enumerate(
			zip(self.blocks, past_key_values or [None] * self.num_blocks)
		):
			layer_head_mask = head_mask[i]
			encoder_layer_head_mask = encoder_head_mask[i]
			if output_all_hidden_states:
				all_hidden_states.append(hidden_states)  # type: ignore[union-attr]

			layer_outputs: T5BlockOutput = layer_module(
				hidden_states,
				attention_mask=extended_attention_mask,
				position_bias=position_bias,
				encoder_hidden_states=encoder_hidden_states,
				encoder_attention_mask=encoder_attention_mask,
				encoder_decoder_position_bias=encoder_decoder_position_bias,
				layer_head_mask=layer_head_mask,
				encoder_layer_head_mask=encoder_layer_head_mask,
				past_key_value=past_key_value,
				use_cache=use_cache,
				output_attentions=output_attentions,
			)
			if not isinstance(layer_outputs, T5BlockOutput):
				layer_outputs = T5BlockOutput(*layer_outputs)
			hidden_states = layer_outputs.hidden_states

			position_bias = layer_outputs.self_attn_position_bias
			if self.is_decoder and encoder_hidden_states is not None:
				encoder_decoder_position_bias = layer_outputs.cross_attn_position_bias
			if use_cache:
				present_key_value_states.append(layer_outputs.present_key_value_states)  # type: ignore
			if output_attentions:
				all_attentions.append(layer_outputs.self_attn_weights)  # type: ignore[union-attr]
				if self.is_decoder:
					all_cross_attentions.append(layer_outputs.cross_attn_weights)  # type: ignore[union-attr]

		hidden_states = self.final_layer_norm(hidden_states)
		hidden_states = self.dropout(hidden_states)

		if output_all_hidden_states:
			all_hidden_states.append(hidden_states)  # type: ignore[union-attr]

		return T5StackOutput(
			last_hidden_state=hidden_states,
			past_key_values=present_key_value_states,
			all_hidden_states=all_hidden_states,
			attentions=all_attentions,
			cross_attentions=all_cross_attentions,
		)


class T5EncoderStack(T5Stack, FromParams):
	def __init__(
		self,
		token_embeddings: nn.Embedding,
		blocks: List[T5Block],
		final_layer_norm: Optional[T5LayerNorm] = None,
		dropout: float = 0.1,
	):
		if any(b.is_decoder for b in blocks):
			raise ConfigurationError("Found a decoder block in an encoder stack. This won't work.")

		super().__init__(
			token_embeddings,
			blocks,
			final_layer_norm=final_layer_norm,
			dropout=dropout,
		)

	@classmethod
	def basic_encoder(
		cls,
		token_embeddings: nn.Embedding,
		num_blocks: int = 6,
		block_self_attention: Lazy[T5Attention] = Lazy(T5Attention),
		final_layer_norm: Optional[T5LayerNorm] = None,
		block_ff: Lazy[T5LayerFF] = Lazy(T5LayerFF),
		dropout: float = 0.1,
		ddp_accelerator: Optional[DdpAccelerator] = None,
		checkpoint_wrapper: Optional[CheckpointWrapper] = None,
	) -> "T5EncoderStack":
		if ddp_accelerator is not None:
			logger.info("Initializing T5 encoder with DdpAccelerator %s", ddp_accelerator)
		blocks: List[T5Block] = []
		for i in range(num_blocks):
			block = T5Block(
				attention=T5LayerSelfAttention(
					self_attention=block_self_attention.construct(
						is_decoder=False, has_relative_attention_bias=(i == 0)
					)
				),
				cross_attention=None,
				ff=block_ff.construct(),
			)
			if checkpoint_wrapper is not None:
				block = checkpoint_wrapper.wrap_module(block)
			if ddp_accelerator is not None:
				block = ddp_accelerator.wrap_module(block)
			blocks.append(block)
		return cls(token_embeddings, blocks, final_layer_norm=final_layer_norm, dropout=dropout)


class T5DecoderStack(T5Stack, FromParams):
	def __init__(
		self,
		token_embeddings: nn.Embedding,
		blocks: List[T5Block],
		final_layer_norm: Optional[T5LayerNorm] = None,
		dropout: float = 0.1,
	):
		if not all(b.is_decoder for b in blocks):
			raise ConfigurationError("Found an encoder block in a decoder stack. This won't work.")

		super().__init__(
			token_embeddings,
			blocks,
			final_layer_norm=final_layer_norm,
			dropout=dropout,
		)

	@classmethod
	def basic_decoder(
		cls,
		token_embeddings: nn.Embedding,
		num_blocks: int = 6,
		block_self_attention: Lazy[T5Attention] = Lazy(T5Attention),
		block_cross_attention: Lazy[T5Attention] = Lazy(T5Attention),
		final_layer_norm: Optional[T5LayerNorm] = None,
		block_ff: Lazy[T5LayerFF] = Lazy(T5LayerFF),
		dropout: float = 0.1,
		ddp_accelerator: Optional[DdpAccelerator] = None,
		checkpoint_wrapper: Optional[CheckpointWrapper] = None,
	) -> "T5DecoderStack":
		if ddp_accelerator is not None:
			logger.info("Initializing T5 decoder with DdpAccelerator %s", ddp_accelerator)
		blocks: List[T5Block] = []
		for i in range(num_blocks):
			block = T5Block(
				attention=T5LayerSelfAttention(
					self_attention=block_self_attention.construct(
						is_decoder=True, has_relative_attention_bias=(i == 0)
					)
				),
				cross_attention=T5LayerCrossAttention(
					enc_dec_attention=block_cross_attention.construct(
						is_decoder=True,
						has_relative_attention_bias=False,
					)
				),
				ff=block_ff.construct(),
			)
			if checkpoint_wrapper is not None:
				block = checkpoint_wrapper.wrap_module(block)
			if ddp_accelerator is not None:
				block = ddp_accelerator.wrap_module(block)
			blocks.append(block)
		return cls(token_embeddings, blocks, final_layer_norm=final_layer_norm, dropout=dropout)


class T5Output(NamedTuple):

	encoder_last_hidden_state: FloatT

	encoder_all_hidden_states: Optional[List[FloatT]] = None

	decoder_last_hidden_state: Optional[FloatT] = None

	decoder_all_hidden_states: Optional[List[FloatT]] = None

	encoder_attentions: Optional[List[FloatT]] = None

	decoder_attentions: Optional[List[FloatT]] = None

	cross_attentions: Optional[List[FloatT]] = None

	loss: Optional[FloatT] = None

	logits: Optional[FloatT] = None

	predictions: Optional[IntT] = None

	predicted_log_probs: Optional[FloatT] = None


class T5(TransformerModule, Registrable):

	_pretrained_mapping = {"shared": "token_embeddings"}

	_pretrained_ignore = [
		r"^decoder\.block\.0\.layer\.1\.EncDecAttention\.relative_attention_bias\.weight$"
	]

	default_implementation = "default"

	def __init__(
		self,
		token_embeddings: Optional[nn.Embedding] = None,
		encoder: Lazy[T5EncoderStack] = Lazy(T5EncoderStack.basic_encoder),
		decoder: Lazy[T5DecoderStack] = Lazy(T5DecoderStack.basic_decoder),
		decoder_start_token_id: int = 0,
		pad_token_id: int = 0,  # These are both 0 in t5-(small|base|large). Go figure.
		eos_token_id: int = 1,
		vocab_size: int = 32128,
		model_dim: int = 512,
		output_attentions: bool = False,
		output_all_hidden_states: bool = False,
		beam_search: Lazy[BeamSearch] = Lazy(BeamSearch, beam_size=3, max_steps=100),
		ddp_accelerator: Optional[DdpAccelerator] = None,
		checkpoint_wrapper: Optional[CheckpointWrapper] = None,
		tie_word_embeddings: bool = True,
	):
		super().__init__()
		self._tie_word_embeddings = tie_word_embeddings

		self.model_dim = model_dim
		self.token_embeddings = token_embeddings or nn.Embedding(vocab_size, model_dim)
		if token_embeddings is None:
			self.token_embeddings.weight.data.normal_(mean=0.0, std=1.0)
		self.encoder: T5EncoderStack = encoder.construct(
			token_embeddings=self.token_embeddings,
			ddp_accelerator=ddp_accelerator,
			checkpoint_wrapper=checkpoint_wrapper,
		)
		self.decoder: T5DecoderStack = decoder.construct(
			token_embeddings=self.token_embeddings,
			ddp_accelerator=ddp_accelerator,
			checkpoint_wrapper=checkpoint_wrapper,
		)
		self.lm_head = nn.Linear(
			self.decoder.hidden_size, self.token_embeddings.num_embeddings, bias=False
		)
		if self._tie_word_embeddings:
			self.lm_head.weight = self.token_embeddings.weight

		self.loss_fct = CrossEntropyLoss(ignore_index=-100)

		self.decoder_start_token_id = decoder_start_token_id
		self.pad_token_id = pad_token_id
		self.eos_token_id = eos_token_id
		self.output_attentions = output_attentions
		self.output_all_hidden_states = output_all_hidden_states

		self.beam_search = beam_search.construct(end_index=self.eos_token_id)

	def resize_token_embeddings(
		self, new_size: int, *, init_fn: Callable = torch.nn.init.normal_
	) -> None:
		self.encoder.resize_token_embeddings(new_size, init_fn=init_fn)
		self.decoder.resize_token_embeddings(new_size, init_fn=init_fn)

		old_size = self.lm_head.out_features
		if old_size == new_size:
			return
		new_lm_head = torch.nn.Linear(
			self.lm_head.in_features,
			new_size,
			self.lm_head.bias,
			self.lm_head.weight.device,
			self.lm_head.weight.dtype,
		)
		copy_size = min(old_size, new_size)
		new_lm_head.weight.data[:copy_size, ...] = self.lm_head.weight.data[:copy_size, ...]
		if self.lm_head.bias and new_lm_head.bias:
			new_lm_head.bias.data[:copy_size, ...] = self.lm_head.bias[:copy_size, ...]
		if new_size > old_size:
			init_fn(new_lm_head.weight.data[copy_size:, ...])
			if new_lm_head.bias:
				init_fn(new_lm_head.bias[copy_size:, ...])

		self.lm_head = new_lm_head

	def _post_load_state_dict(
		self, missing_keys: List[str], unexpected_keys: List[str]
	) -> Tuple[List[str], List[str]]:
		missing_keys_to_ignore = [
			"encoder.token_embeddings.weight",
			"decoder.token_embeddings.weight",
		]
		if self._tie_word_embeddings:
			missing_keys_to_ignore.append("lm_head.weight")
		for key in missing_keys_to_ignore:
			if key in missing_keys:
				missing_keys.remove(key)
		return missing_keys, unexpected_keys

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		attention_kwargs = {
			"hidden_size": config.d_model,
			"key_value_proj_dim": config.d_kv,
			"num_heads": config.num_heads,
			"relative_attention_num_buckets": config.relative_attention_num_buckets,
			"dropout": config.dropout_rate,
		}
		layer_norm_kwargs = {
			"hidden_size": config.d_model,
			"eps": config.layer_norm_epsilon,
		}
		block_ff = Lazy(
			T5LayerFF,
			params=Params(
				{
					"ff_proj": {
						"type": config.feed_forward_proj,
						"hidden_size": config.d_model,
						"ff_size": config.d_ff,
						"dropout": config.dropout_rate,
					},
					"layer_norm": layer_norm_kwargs,
					"dropout": config.dropout_rate,
				}
			),
		)
		return cls(
			encoder=Lazy(
				T5EncoderStack.basic_encoder,
				constructor_extras={
					"num_blocks": config.num_layers,
					"block_self_attention": Lazy(T5Attention, constructor_extras=attention_kwargs),
					"final_layer_norm": T5LayerNorm(**layer_norm_kwargs),
					"block_ff": block_ff,
					"dropout": config.dropout_rate,
				},
			),
			decoder=Lazy(
				T5DecoderStack.basic_decoder,
				constructor_extras={
					"num_blocks": config.num_decoder_layers,
					"block_self_attention": Lazy(T5Attention, constructor_extras=attention_kwargs),
					"block_cross_attention": Lazy(T5Attention, constructor_extras=attention_kwargs),
					"final_layer_norm": T5LayerNorm(**layer_norm_kwargs),
					"block_ff": block_ff,
					"dropout": config.dropout_rate,
				},
			),
			decoder_start_token_id=config.decoder_start_token_id,
			pad_token_id=config.pad_token_id,
			eos_token_id=config.eos_token_id,
			vocab_size=config.vocab_size,
			model_dim=config.d_model,
			tie_word_embeddings=kwargs.pop("tie_word_embeddings", config.tie_word_embeddings),
			**kwargs,
		)

	def _shift_right(self, input_ids, start_value: int):
		shifted_input_ids = input_ids.new_zeros(input_ids.shape)
		shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()
		shifted_input_ids[..., 0] = start_value

		return shifted_input_ids

	def _get_lm_logits(self, decoder_last_hidden_state: FloatT) -> FloatT:
		sequence_output = decoder_last_hidden_state
		sequence_output = sequence_output * (self.model_dim**-0.5)
		logits = self.lm_head(sequence_output)
		return logits

	def forward(
		self,
		input_ids: IntT,
		attention_mask: Optional[BoolT] = None,
		labels: Optional[IntT] = None,
		decoder_attention_mask: Optional[BoolT] = None,
	) -> T5Output:
		if attention_mask is None:
			attention_mask = ~(input_ids == self.pad_token_id)

		encoder_outputs: T5StackOutput = self.encoder(
			input_ids=input_ids,
			attention_mask=attention_mask,
			output_attentions=self.output_attentions,
			output_all_hidden_states=self.output_all_hidden_states,
		)

		logits: Optional[FloatT] = None
		loss: Optional[FloatT] = None
		decoder_outputs: Optional[T5StackOutput] = None
		predictions: Optional[IntT] = None
		predicted_log_probs: Optional[FloatT] = None

		if labels is not None:

			if decoder_attention_mask is None:
				decoder_attention_mask = ~(labels == self.pad_token_id)

			decoder_input_ids = self._shift_right(labels, self.decoder_start_token_id)

			decoder_input_ids.masked_fill_(decoder_input_ids == -100, self.pad_token_id)

			decoder_outputs = self.decoder(
				input_ids=decoder_input_ids,
				attention_mask=decoder_attention_mask,
				encoder_hidden_states=encoder_outputs.last_hidden_state,
				encoder_attention_mask=attention_mask,
				output_attentions=self.output_attentions,
				output_all_hidden_states=self.output_all_hidden_states,
			)

			logits = self._get_lm_logits(decoder_outputs.last_hidden_state)  # type: ignore[union-attr]

			loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.to(torch.long).view(-1))
		elif self.training:
			raise ValueError("'labels' required during training")

		if not self.training:

			initial_decoder_ids = torch.tensor(
				[[self.decoder_start_token_id]],
				dtype=input_ids.dtype,
				device=input_ids.device,
			).repeat(input_ids.shape[0], 1)

			initial_state = {
				"input_ids": input_ids,
				"encoder_hidden_states": encoder_outputs.last_hidden_state,
				"encoder_attention_mask": attention_mask,
			}

			predictions, predicted_log_probs = self.beam_search.search(
				initial_decoder_ids, initial_state, self.take_search_step
			)

		return T5Output(
			encoder_last_hidden_state=encoder_outputs.last_hidden_state,
			encoder_all_hidden_states=encoder_outputs.all_hidden_states,
			decoder_last_hidden_state=(
				None if decoder_outputs is None else decoder_outputs.last_hidden_state
			),
			decoder_all_hidden_states=(
				None if decoder_outputs is None else decoder_outputs.all_hidden_states
			),
			encoder_attentions=encoder_outputs.attentions,
			decoder_attentions=None if decoder_outputs is None else decoder_outputs.attentions,
			cross_attentions=None if decoder_outputs is None else decoder_outputs.cross_attentions,
			loss=loss,
			logits=logits,
			predictions=predictions,
			predicted_log_probs=predicted_log_probs,
		)

	def take_search_step(
		self, last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], step: int
	) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
		decoder_cache: Optional[List[KeyValueStates]] = None
		decoder_cache_dict = {
			k: state[k].contiguous() for k in state if k.startswith("decoder_cache_")
		}
		if decoder_cache_dict:
			decoder_cache = self._dict_to_decoder_cache(decoder_cache_dict)

		if len(last_predictions.shape) == 1:
			last_predictions = last_predictions.unsqueeze(-1)

		decoder_outputs: T5StackOutput = self.decoder(
			input_ids=last_predictions,
			past_key_values=decoder_cache,
			encoder_hidden_states=state["encoder_hidden_states"],
			encoder_attention_mask=state["encoder_attention_mask"],
			use_cache=True,
		)

		lm_logits = self._get_lm_logits(decoder_outputs.last_hidden_state)

		logits = lm_logits[:, -1, :]

		log_probabilities = F.log_softmax(logits, dim=-1)

		decoder_cache = decoder_outputs.past_key_values
		assert decoder_cache is not None
		decoder_cache_dict = self._decoder_cache_to_dict(decoder_cache)
		state.update(decoder_cache_dict)

		return log_probabilities, state

	@staticmethod
	def _decoder_cache_to_dict(decoder_cache: List[KeyValueStates]) -> Dict[str, torch.Tensor]:
		cache_dict = {}
		for layer_index, layer_cache in enumerate(decoder_cache):
			assert len(layer_cache) == 4
			for tensor_index, tensor in enumerate(layer_cache):
				key = f"decoder_cache_{layer_index}_{tensor_index}"
				cache_dict[key] = tensor
		return cache_dict

	def _dict_to_decoder_cache(self, cache_dict: Dict[str, torch.Tensor]) -> List[KeyValueStates]:
		decoder_cache: List[KeyValueStates] = []
		for block_index in range(self.decoder.num_blocks):
			base_key = f"decoder_cache_{block_index}_"
			layer_cache = (
				cache_dict[base_key + "0"].contiguous(),
				cache_dict[base_key + "1"].contiguous(),
				cache_dict[base_key + "2"].contiguous(),
				cache_dict[base_key + "3"].contiguous(),
			)
			decoder_cache.append(layer_cache)
		return decoder_cache


T5.register("default")(T5)
T5.register("from_pretrained", constructor="from_pretrained_module")(T5)


import argparse
import json
import logging
import math
import os
import random
from pathlib import Path

import datasets
import evaluate
import numpy as np
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import ClassLabel, load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_MAPPING,
	AutoConfig,
	AutoModelForTokenClassification,
	AutoTokenizer,
	DataCollatorForTokenClassification,
	PretrainedConfig,
	SchedulerType,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)
require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/token-classification/requirements.txt")

MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def parse_args():
	parser = argparse.ArgumentParser(
		description="Finetune a transformers model on a text classification task (NER) with accelerate library"
	)
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--text_column_name",
		type=str,
		default=None,
		help="The column name of text to input in the file (a csv or JSON file).",
	)
	parser.add_argument(
		"--label_column_name",
		type=str,
		default=None,
		help="The column name of label to input in the file (a csv or JSON file).",
	)
	parser.add_argument(
		"--max_length",
		type=int,
		default=128,
		help=(
			"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
			" sequences shorter will be padded if `--pad_to_max_length` is passed."
		),
	)
	parser.add_argument(
		"--pad_to_max_length",
		action="store_true",
		help="If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=False,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--tokenizer_name",
		type=str,
		default=None,
		help="Pretrained tokenizer name or path if not the same as model_name",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="Model type to use if training from scratch.",
		choices=MODEL_TYPES,
	)
	parser.add_argument(
		"--label_all_tokens",
		action="store_true",
		help="Setting labels of all special tokens to -100 and thus PyTorch will ignore them.",
	)
	parser.add_argument(
		"--return_entity_level_metrics",
		action="store_true",
		help="Indication whether entity level metrics are to be returner.",
	)
	parser.add_argument(
		"--task_name",
		type=str,
		default="ner",
		choices=["ner", "pos", "chunk"],
		help="The name of the task.",
	)
	parser.add_argument(
		"--debug",
		action="store_true",
		help="Activate debug mode and run training only with a subset of data.",
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	parser.add_argument(
		"--ignore_mismatched_sizes",
		action="store_true",
		help="Whether or not to enable to load a pretrained model whose head dimensions are different.",
	)
	args = parser.parse_args()

	if args.task_name is None and args.train_file is None and args.validation_file is None:
		raise ValueError("Need either a task name or a training/validation file.")
	else:
		if args.train_file is not None:
			extension = args.train_file.split(".")[-1]
			assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
		if args.validation_file is not None:
			extension = args.validation_file.split(".")[-1]
			assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_ner_no_trainer", args)

	accelerator = (
		Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()
	)
	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		extension = args.train_file.split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files)
	if args.debug:
		for split in raw_datasets.keys():
			raw_datasets[split] = raw_datasets[split].select(range(100))

	if raw_datasets["train"] is not None:
		column_names = raw_datasets["train"].column_names
		features = raw_datasets["train"].features
	else:
		column_names = raw_datasets["validation"].column_names
		features = raw_datasets["validation"].features

	if args.text_column_name is not None:
		text_column_name = args.text_column_name
	elif "tokens" in column_names:
		text_column_name = "tokens"
	else:
		text_column_name = column_names[0]

	if args.label_column_name is not None:
		label_column_name = args.label_column_name
	elif f"{args.task_name}_tags" in column_names:
		label_column_name = f"{args.task_name}_tags"
	else:
		label_column_name = column_names[1]

	def get_label_list(labels):
		unique_labels = set()
		for label in labels:
			unique_labels = unique_labels | set(label)
		label_list = list(unique_labels)
		label_list.sort()
		return label_list

	labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)
	if labels_are_int:
		label_list = features[label_column_name].feature.names
		label_to_id = {i: i for i in range(len(label_list))}
	else:
		label_list = get_label_list(raw_datasets["train"][label_column_name])
		label_to_id = {l: i for i, l in enumerate(label_list)}

	num_labels = len(label_list)

	if args.config_name:
		config = AutoConfig.from_pretrained(
			args.config_name, num_labels=num_labels, trust_remote_code=args.trust_remote_code
		)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(
			args.model_name_or_path, num_labels=num_labels, trust_remote_code=args.trust_remote_code
		)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")

	tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path
	if not tokenizer_name_or_path:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if config.model_type in {"bloom", "gpt2", "roberta"}:
		tokenizer = AutoTokenizer.from_pretrained(
			tokenizer_name_or_path, use_fast=True, add_prefix_space=True, trust_remote_code=args.trust_remote_code
		)
	else:
		tokenizer = AutoTokenizer.from_pretrained(
			tokenizer_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code
		)

	if args.model_name_or_path:
		model = AutoModelForTokenClassification.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			ignore_mismatched_sizes=args.ignore_mismatched_sizes,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForTokenClassification.from_config(config, trust_remote_code=args.trust_remote_code)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:
		if sorted(model.config.label2id.keys()) == sorted(label_list):
			if labels_are_int:
				label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}
				label_list = [model.config.id2label[i] for i in range(num_labels)]
			else:
				label_list = [model.config.id2label[i] for i in range(num_labels)]
				label_to_id = {l: i for i, l in enumerate(label_list)}
		else:
			logger.warning(
				"Your model seems to have been trained with labels, but they don't match the dataset: ",
				f"model labels: {sorted(model.config.label2id.keys())}, dataset labels:"
				f" {sorted(label_list)}.\nIgnoring the model labels as a result.",
			)

	model.config.label2id = {l: i for i, l in enumerate(label_list)}
	model.config.id2label = dict(enumerate(label_list))

	b_to_i_label = []
	for idx, label in enumerate(label_list):
		if label.startswith("B-") and label.replace("B-", "I-") in label_list:
			b_to_i_label.append(label_list.index(label.replace("B-", "I-")))
		else:
			b_to_i_label.append(idx)

	padding = "max_length" if args.pad_to_max_length else False


	def tokenize_and_align_labels(examples):
		tokenized_inputs = tokenizer(
			examples[text_column_name],
			max_length=args.max_length,
			padding=padding,
			truncation=True,
			is_split_into_words=True,
		)

		labels = []
		for i, label in enumerate(examples[label_column_name]):
			word_ids = tokenized_inputs.word_ids(batch_index=i)
			previous_word_idx = None
			label_ids = []
			for word_idx in word_ids:
				if word_idx is None:
					label_ids.append(-100)
				elif word_idx != previous_word_idx:
					label_ids.append(label_to_id[label[word_idx]])
				else:
					if args.label_all_tokens:
						label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])
					else:
						label_ids.append(-100)
				previous_word_idx = word_idx

			labels.append(label_ids)
		tokenized_inputs["labels"] = labels
		return tokenized_inputs

	with accelerator.main_process_first():
		processed_raw_datasets = raw_datasets.map(
			tokenize_and_align_labels,
			batched=True,
			remove_columns=raw_datasets["train"].column_names,
			desc="Running tokenizer on dataset",
		)

	train_dataset = processed_raw_datasets["train"]
	eval_dataset = processed_raw_datasets["validation"]

	for index in random.sample(range(len(train_dataset)), 3):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if args.pad_to_max_length:
		data_collator = default_data_collator
	else:
		data_collator = DataCollatorForTokenClassification(
			tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)
		)

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	device = accelerator.device
	model.to(device)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps,
		num_training_steps=args.max_train_steps,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("ner_no_trainer", experiment_config)

	metric = evaluate.load("seqeval")

	def get_labels(predictions, references):
		if device.type == "cpu":
			y_pred = predictions.detach().clone().numpy()
			y_true = references.detach().clone().numpy()
		else:
			y_pred = predictions.detach().cpu().clone().numpy()
			y_true = references.detach().cpu().clone().numpy()

		true_predictions = [
			[label_list[p] for (p, l) in zip(pred, gold_label) if l != -100]
			for pred, gold_label in zip(y_pred, y_true)
		]
		true_labels = [
			[label_list[l] for (p, l) in zip(pred, gold_label) if l != -100]
			for pred, gold_label in zip(y_pred, y_true)
		]
		return true_predictions, true_labels

	def compute_metrics():
		results = metric.compute()
		if args.return_entity_level_metrics:
			final_results = {}
			for key, value in results.items():
				if isinstance(value, dict):
					for n, v in value.items():
						final_results[f"{key}_{n}"] = v
				else:
					final_results[key] = value
			return final_results
		else:
			return {
				"precision": results["overall_precision"],
				"recall": results["overall_recall"],
				"f1": results["overall_f1"],
				"accuracy": results["overall_accuracy"],
			}

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0
	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			outputs = model(**batch)
			loss = outputs.loss
			if args.with_tracking:
				total_loss += loss.detach().float()
			loss = loss / args.gradient_accumulation_steps
			accelerator.backward(loss)
			if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()
		samples_seen = 0
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				outputs = model(**batch)
			predictions = outputs.logits.argmax(dim=-1)
			labels = batch["labels"]
			if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered
				predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
				labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)
			predictions_gathered, labels_gathered = accelerator.gather((predictions, labels))
			if accelerator.num_processes > 1:
				if step == len(eval_dataloader) - 1:
					predictions_gathered = predictions_gathered[: len(eval_dataloader.dataset) - samples_seen]
					labels_gathered = labels_gathered[: len(eval_dataloader.dataset) - samples_seen]
				else:
					samples_seen += labels_gathered.shape[0]
			preds, refs = get_labels(predictions_gathered, labels_gathered)
			metric.add_batch(
				predictions=preds,
				references=refs,
			)  # predictions and preferences are expected to be a nested list of labels, not label_ids

		eval_metric = compute_metrics()
		accelerator.print(f"epoch {epoch}:", eval_metric)
		if args.with_tracking:
			accelerator.log(
				{
					"seqeval": eval_metric,
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			all_results = {f"eval_{k}": v for k, v in eval_metric.items()}
			if args.with_tracking:
				all_results.update({"train_loss": total_loss.item() / len(train_dataloader)})
			with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
				for key, value in all_results.items():
					if isinstance(value, np.float64):
						all_results[key] = float(value)
					elif isinstance(value, np.int64):
						all_results[key] = int(value)
				json.dump(all_results, f)


if __name__ == "__main__":
	main()

from typing import List, Optional

import torch


from allennlp.training.metrics import FBetaMeasure
from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("fbeta_multi_label")
class FBetaMultiLabelMeasure(FBetaMeasure):

	def __init__(
		self,
		beta: float = 1.0,
		average: str = None,
		labels: List[int] = None,
		threshold: float = 0.5,
	) -> None:
		super().__init__(beta, average, labels)
		self._threshold = threshold

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		num_classes = predictions.size(-1)

		if self._true_positive_sum is None:
			self._true_positive_sum = torch.zeros(num_classes, device=predictions.device)
			self._true_sum = torch.zeros(num_classes, device=predictions.device)
			self._pred_sum = torch.zeros(num_classes, device=predictions.device)
			self._total_sum = torch.zeros(num_classes, device=predictions.device)

		if mask is None:
			mask = torch.ones_like(gold_labels, dtype=torch.bool)
		gold_labels = gold_labels.float()

		pred_mask = (predictions.sum(dim=-1) != 0).unsqueeze(-1)
		threshold_predictions = (predictions >= self._threshold).float()

		class_indices = torch.arange(num_classes, device=predictions.device).repeat(
			gold_labels.shape[:-1] + (1,)
		)
		true_positives = (gold_labels * threshold_predictions).bool() & mask & pred_mask
		true_positives_bins = class_indices[true_positives]

		if true_positives_bins.shape[0] == 0:
			true_positive_sum = torch.zeros(num_classes, device=predictions.device)
		else:
			true_positive_sum = torch.bincount(
				true_positives_bins.long(), minlength=num_classes
			).float()

		pred_bins = class_indices[threshold_predictions.bool() & mask & pred_mask]
		if pred_bins.shape[0] != 0:
			pred_sum = torch.bincount(pred_bins, minlength=num_classes).float()
		else:
			pred_sum = torch.zeros(num_classes, device=predictions.device)

		gold_labels_bins = class_indices[gold_labels.bool() & mask]
		if gold_labels_bins.shape[0] != 0:
			true_sum = torch.bincount(gold_labels_bins, minlength=num_classes).float()
		else:
			true_sum = torch.zeros(num_classes, device=predictions.device)

		self._total_sum += mask.expand_as(gold_labels).sum().to(torch.float)

		self._true_positive_sum += dist_reduce_sum(true_positive_sum)
		self._pred_sum += dist_reduce_sum(pred_sum)
		self._true_sum += dist_reduce_sum(true_sum)

	@property
	def _true_negative_sum(self):
		if self._total_sum is None:
			return None
		else:
			true_negative_sum = (
				self._total_sum[0] / self._true_positive_sum.size(0)
				- self._pred_sum
				- self._true_sum
				+ self._true_positive_sum
			)
			return true_negative_sum


@Metric.register("f1_multi_label")
class F1MultiLabelMeasure(FBetaMultiLabelMeasure):
	def __init__(
		self, average: str = None, labels: List[int] = None, threshold: float = 0.5
	) -> None:
		super().__init__(1.0, average, labels, threshold)

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.data.vocabulary import Vocabulary
from allennlp.modules.token_embedders.token_embedder import TokenEmbedder
from allennlp.nn.util import get_text_field_mask


@TokenEmbedder.register("bag_of_word_counts")
class BagOfWordCountsTokenEmbedder(TokenEmbedder):

	def __init__(
		self,
		vocab: Vocabulary,
		vocab_namespace: str = "tokens",
		projection_dim: int = None,
		ignore_oov: bool = False,
	) -> None:
		super().__init__()
		self.vocab = vocab
		self.vocab_size = vocab.get_vocab_size(vocab_namespace)
		if projection_dim:
			self._projection = torch.nn.Linear(self.vocab_size, projection_dim)
		else:
			self._projection = None
		self._ignore_oov = ignore_oov
		oov_token = vocab._oov_token
		self._oov_idx = vocab.get_token_to_index_vocabulary(vocab_namespace).get(oov_token)
		if self._oov_idx is None:
			raise ConfigurationError(
				"OOV token does not exist in vocabulary namespace {}".format(vocab_namespace)
			)
		self.output_dim = projection_dim or self.vocab_size

	def get_output_dim(self):
		return self.output_dim

	def forward(self, inputs: torch.Tensor) -> torch.Tensor:
		bag_of_words_vectors = []

		mask = get_text_field_mask({"tokens": {"tokens": inputs}})
		if self._ignore_oov:
			mask &= inputs != self._oov_idx
		for document, doc_mask in zip(inputs, mask):
			document = torch.masked_select(document, doc_mask)
			vec = torch.bincount(document, minlength=self.vocab_size).float()
			vec = vec.view(1, -1)
			bag_of_words_vectors.append(vec)
		bag_of_words_output = torch.cat(bag_of_words_vectors, 0)

		if self._projection:
			projection = self._projection
			bag_of_words_output = projection(bag_of_words_output)
		return bag_of_words_output

from dataclasses import dataclass
from typing import ClassVar


@dataclass
class train_config:
	model_name: str="t5-base"
	run_validation: bool=True
	batch_size_training: int=4
	num_workers_dataloader: int=2
	lr: float=0.002
	weight_decay: float=0.0
	gamma: float= 0.85
	use_fp16: bool=False
	mixed_precision: bool=True
	save_model: bool=False
	
	
	
from typing import List, Dict

from allennlp.common.util import nan_safe_tensor_divide
from allennlp.training.metrics.metric import Metric
from allennlp.training.metrics.fbeta_measure import FBetaMeasure


@Metric.register("fbeta_verbose")
class FBetaVerboseMeasure(FBetaMeasure):

	def __init__(
		self,
		beta: float = 1.0,
		labels: List[int] = None,
		index_to_label: Dict[int, str] = None,
	) -> None:
		super().__init__(beta=beta, average=None, labels=labels)
		self._index_to_label = index_to_label

	def get_metric(self, reset: bool = False):
		if self._true_positive_sum is None or self._pred_sum is None or self._true_sum is None:
			raise RuntimeError("You have never called this metric before.")

		tp_sum = self._true_positive_sum
		pred_sum = self._pred_sum
		true_sum = self._true_sum

		if self._labels is not None:
			tp_sum = tp_sum[self._labels]
			pred_sum = pred_sum[self._labels]  # type: ignore
			true_sum = true_sum[self._labels]  # type: ignore

		beta2 = self._beta**2

		precision = nan_safe_tensor_divide(tp_sum, pred_sum)
		recall = nan_safe_tensor_divide(tp_sum, true_sum)
		fscore = nan_safe_tensor_divide(
			(1 + beta2) * precision * recall, beta2 * precision + recall
		)

		all_metrics = {}
		for c, (p, r, f) in enumerate(zip(precision.tolist(), recall.tolist(), fscore.tolist())):
			label = str(c)
			if self._index_to_label:
				label = self._index_to_label[c]
			all_metrics[f"{label}-precision"] = p
			all_metrics[f"{label}-recall"] = r
			all_metrics[f"{label}-fscore"] = f

		all_metrics["macro-precision"] = precision.mean().item()
		all_metrics["macro-recall"] = recall.mean().item()
		all_metrics["macro-fscore"] = fscore.mean().item()

		weights = true_sum
		weights_sum = true_sum.sum()  # type: ignore
		all_metrics["weighted-precision"] = nan_safe_tensor_divide(
			(weights * precision).sum(), weights_sum
		).item()
		all_metrics["weighted-recall"] = nan_safe_tensor_divide(
			(weights * recall).sum(), weights_sum
		).item()
		all_metrics["weighted-fscore"] = nan_safe_tensor_divide(
			(weights * fscore).sum(), weights_sum
		).item()

		micro_precision = nan_safe_tensor_divide(tp_sum.sum(), pred_sum.sum())
		micro_recall = nan_safe_tensor_divide(tp_sum.sum(), true_sum.sum())
		all_metrics["micro-precision"] = micro_precision.item()
		all_metrics["micro-recall"] = micro_recall.item()
		all_metrics["micro-fscore"] = nan_safe_tensor_divide(
			(1 + beta2) * micro_precision * micro_recall, beta2 * micro_precision + micro_recall
		).item()

		if reset:
			self.reset()

		return all_metrics

import torch
import torch.nn.functional as F
from torch.autograd import Variable

def _inflate(tensor, times, dim):
		repeat_dims = [1] * tensor.dim()
		repeat_dims[dim] = times
		return tensor.repeat(*repeat_dims)

class TopKDecoder(torch.nn.Module):

	def __init__(self, decoder_rnn, k):
		super(TopKDecoder, self).__init__()
		self.rnn = decoder_rnn
		self.k = k
		self.hidden_size = self.rnn.hidden_size
		self.V = self.rnn.output_size
		self.SOS = self.rnn.sos_id
		self.EOS = self.rnn.eos_id

	def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None, function=F.log_softmax,
					teacher_forcing_ratio=0, retain_output_probs=True):

		inputs, batch_size, max_length = self.rnn._validate_args(inputs, encoder_hidden, encoder_outputs,
																 function, teacher_forcing_ratio)

		self.pos_index = Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1)

		encoder_hidden = self.rnn._init_state(encoder_hidden)
		if encoder_hidden is None:
			hidden = None
		else:
			if isinstance(encoder_hidden, tuple):
				hidden = tuple([_inflate(h, self.k, 1) for h in encoder_hidden])
			else:
				hidden = _inflate(encoder_hidden, self.k, 1)

		if self.rnn.use_attention:
			inflated_encoder_outputs = _inflate(encoder_outputs, self.k, 0)
		else:
			inflated_encoder_outputs = None

		sequence_scores = torch.Tensor(batch_size * self.k, 1)
		sequence_scores.fill_(-float('Inf'))
		sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)
		sequence_scores = Variable(sequence_scores)

		input_var = Variable(torch.transpose(torch.LongTensor([[self.SOS] * batch_size * self.k]), 0, 1))

		stored_outputs = list()
		stored_scores = list()
		stored_predecessors = list()
		stored_emitted_symbols = list()
		stored_hidden = list()

		for _ in range(0, max_length):

			log_softmax_output, hidden, _ = self.rnn.forward_step(input_var, hidden,
																  inflated_encoder_outputs, function=function)

			if retain_output_probs:
				stored_outputs.append(log_softmax_output)

			sequence_scores = _inflate(sequence_scores, self.V, 1)
			sequence_scores += log_softmax_output.squeeze(1)
			scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)

			input_var = (candidates % self.V).view(batch_size * self.k, 1)
			sequence_scores = scores.view(batch_size * self.k, 1)

			predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)
			if isinstance(hidden, tuple):
				hidden = tuple([h.index_select(1, predecessors.squeeze()) for h in hidden])
			else:
				hidden = hidden.index_select(1, predecessors.squeeze())

			stored_scores.append(sequence_scores.clone())
			eos_indices = input_var.data.eq(self.EOS)
			if eos_indices.nonzero().dim() > 0:
				sequence_scores.data.masked_fill_(eos_indices, -float('inf'))

			stored_predecessors.append(predecessors)
			stored_emitted_symbols.append(input_var)
			stored_hidden.append(hidden)

		output, h_t, h_n, s, l, p = self._backtrack(stored_outputs, stored_hidden,
													stored_predecessors, stored_emitted_symbols,
													stored_scores, batch_size, self.hidden_size)

		decoder_outputs = [step[:, 0, :] for step in output]
		if isinstance(h_n, tuple):
			decoder_hidden = tuple([h[:, :, 0, :] for h in h_n])
		else:
			decoder_hidden = h_n[:, :, 0, :]
		metadata = {}
		metadata['inputs'] = inputs
		metadata['output'] = output
		metadata['h_t'] = h_t
		metadata['score'] = s
		metadata['topk_length'] = l
		metadata['topk_sequence'] = p
		metadata['length'] = [seq_len[0] for seq_len in l]
		metadata['sequence'] = [seq[0] for seq in p]
		return decoder_outputs, decoder_hidden, metadata

	def _backtrack(self, nw_output, nw_hidden, predecessors, symbols, scores, b, hidden_size):

		lstm = isinstance(nw_hidden[0], tuple)

		output = list()
		h_t = list()
		p = list()
		if lstm:
			state_size = nw_hidden[0][0].size()
			h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])
		else:
			h_n = torch.zeros(nw_hidden[0].size())
		l = [[self.rnn.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences

		sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)
		s = sorted_score.clone()

		batch_eos_found = [0] * b   # the number of EOS found

		t = self.rnn.max_length - 1
		t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)
		while t >= 0:
			current_output = nw_output[t].index_select(0, t_predecessors)
			if lstm:
				current_hidden = tuple([h.index_select(1, t_predecessors) for h in nw_hidden[t]])
			else:
				current_hidden = nw_hidden[t].index_select(1, t_predecessors)
			current_symbol = symbols[t].index_select(0, t_predecessors)
			t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()

			eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()
			if eos_indices.dim() > 0:
				for i in range(eos_indices.size(0)-1, -1, -1):
					idx = eos_indices[i]
					b_idx = int(idx[0] / self.k)
					res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1
					batch_eos_found[b_idx] += 1
					res_idx = b_idx * self.k + res_k_idx

					t_predecessors[res_idx] = predecessors[t][idx[0]]
					current_output[res_idx, :] = nw_output[t][idx[0], :]
					if lstm:
						current_hidden[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :]
						current_hidden[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :]
						h_n[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :].data
						h_n[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :].data
					else:
						current_hidden[:, res_idx, :] = nw_hidden[t][:, idx[0], :]
						h_n[:, res_idx, :] = nw_hidden[t][:, idx[0], :].data
					current_symbol[res_idx, :] = symbols[t][idx[0]]
					s[b_idx, res_k_idx] = scores[t][idx[0]].data[0]
					l[b_idx][res_k_idx] = t + 1

			output.append(current_output)
			h_t.append(current_hidden)
			p.append(current_symbol)

			t -= 1

		s, re_sorted_idx = s.topk(self.k)
		for b_idx in range(b):
			l[b_idx] = [l[b_idx][k_idx.item()] for k_idx in re_sorted_idx[b_idx,:]]

		re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)

		output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]
		p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]
		if lstm:
			h_t = [tuple([h.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for h in step]) for step in reversed(h_t)]
			h_n = tuple([h.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size) for h in h_n])
		else:
			h_t = [step.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for step in reversed(h_t)]
			h_n = h_n.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size)
		s = s.data

		return output, h_t, h_n, s, l, p

	def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):
			score[idx] = masking_score

	def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):
		if len(idx.size()) > 0:
			indices = idx[:, 0]
			tensor.index_fill_(dim, indices, masking_score)



from typing import Dict, List, Iterator, Sequence, Any


from allennlp.data.fields.field import DataArray, Field
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.fields.sequence_field import SequenceField
from allennlp.common.util import pad_sequence_to_length


class ListField(SequenceField[DataArray]):

	__slots__ = ["field_list"]

	def __init__(self, field_list: Sequence[Field]) -> None:
		field_class_set = {field.__class__ for field in field_list}
		assert (
			len(field_class_set) == 1
		), "ListFields must contain a single field type, found " + str(field_class_set)
		self.field_list = field_list

	def __iter__(self) -> Iterator[Field]:
		return iter(self.field_list)

	def __getitem__(self, idx: int) -> Field:
		return self.field_list[idx]

	def __len__(self) -> int:
		return len(self.field_list)

	def count_vocab_items(self, counter: Dict[str, Dict[str, int]]):
		for field in self.field_list:
			field.count_vocab_items(counter)

	def index(self, vocab: Vocabulary):
		for field in self.field_list:
			field.index(vocab)

	def get_padding_lengths(self) -> Dict[str, int]:
		field_lengths = [field.get_padding_lengths() for field in self.field_list]
		padding_lengths = {"num_fields": len(self.field_list)}

		possible_padding_keys = [
			key for field_length in field_lengths for key in list(field_length.keys())
		]

		for key in set(possible_padding_keys):
			padding_lengths["list_" + key] = max(x[key] if key in x else 0 for x in field_lengths)

		for padding_key in padding_lengths:
			padding_lengths[padding_key] = max(padding_lengths[padding_key], 1)

		return padding_lengths

	def sequence_length(self) -> int:
		return len(self.field_list)

	def as_tensor(self, padding_lengths: Dict[str, int]) -> DataArray:
		padded_field_list = pad_sequence_to_length(
			self.field_list, padding_lengths["num_fields"], self.field_list[0].empty_field
		)
		child_padding_lengths = {
			key.replace("list_", "", 1): value
			for key, value in padding_lengths.items()
			if key.startswith("list_")
		}
		padded_fields = [field.as_tensor(child_padding_lengths) for field in padded_field_list]
		return self.field_list[0].batch_tensors(padded_fields)

	def empty_field(self):
		return ListField([self.field_list[0].empty_field()])

	def batch_tensors(self, tensor_list: List[DataArray]) -> DataArray:
		return self.field_list[0].batch_tensors(tensor_list)

	def __str__(self) -> str:
		field_class = self.field_list[0].__class__.__name__
		base_string = f"ListField of {len(self.field_list)} {field_class}s : \n"
		return " ".join([base_string] + [f"\t {field} \n" for field in self.field_list])

	def human_readable_repr(self) -> List[Any]:
		return [f.human_readable_repr() for f in self.field_list]

import argparse
import os
import sys
import time
import re

import numpy as np
import torch
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms
import torch.onnx

import utils
from transformer_net import TransformerNet
from vgg import Vgg16


def check_paths(args):
	try:
		if not os.path.exists(args.save_model_dir):
			os.makedirs(args.save_model_dir)
		if args.checkpoint_model_dir is not None and not (os.path.exists(args.checkpoint_model_dir)):
			os.makedirs(args.checkpoint_model_dir)
	except OSError as e:
		print(e)
		sys.exit(1)


def train(args):
	if args.cuda:
		device = torch.device("cuda")
	elif args.mps:
		device = torch.device("mps")
	else:
		device = torch.device("cpu")

	np.random.seed(args.seed)
	torch.manual_seed(args.seed)

	transform = transforms.Compose([
		transforms.Resize(args.image_size),
		transforms.CenterCrop(args.image_size),
		transforms.ToTensor(),
		transforms.Lambda(lambda x: x.mul(255))
	])
	train_dataset = datasets.ImageFolder(args.dataset, transform)
	train_loader = DataLoader(train_dataset, batch_size=args.batch_size)

	transformer = TransformerNet().to(device)
	optimizer = Adam(transformer.parameters(), args.lr)
	mse_loss = torch.nn.MSELoss()

	vgg = Vgg16(requires_grad=False).to(device)
	style_transform = transforms.Compose([
		transforms.ToTensor(),
		transforms.Lambda(lambda x: x.mul(255))
	])
	style = utils.load_image(args.style_image, size=args.style_size)
	style = style_transform(style)
	style = style.repeat(args.batch_size, 1, 1, 1).to(device)

	features_style = vgg(utils.normalize_batch(style))
	gram_style = [utils.gram_matrix(y) for y in features_style]

	for e in range(args.epochs):
		transformer.train()
		agg_content_loss = 0.
		agg_style_loss = 0.
		count = 0
		for batch_id, (x, _) in enumerate(train_loader):
			n_batch = len(x)
			count += n_batch
			optimizer.zero_grad()

			x = x.to(device)
			y = transformer(x)

			y = utils.normalize_batch(y)
			x = utils.normalize_batch(x)

			features_y = vgg(y)
			features_x = vgg(x)

			content_loss = args.content_weight * mse_loss(features_y.relu2_2, features_x.relu2_2)

			style_loss = 0.
			for ft_y, gm_s in zip(features_y, gram_style):
				gm_y = utils.gram_matrix(ft_y)
				style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])
			style_loss *= args.style_weight

			total_loss = content_loss + style_loss
			total_loss.backward()
			optimizer.step()

			agg_content_loss += content_loss.item()
			agg_style_loss += style_loss.item()

			if (batch_id + 1) % args.log_interval == 0:
				mesg = "{}\tEpoch {}:\t[{}/{}]\tcontent: {:.6f}\tstyle: {:.6f}\ttotal: {:.6f}".format(
					time.ctime(), e + 1, count, len(train_dataset),
								  agg_content_loss / (batch_id + 1),
								  agg_style_loss / (batch_id + 1),
								  (agg_content_loss + agg_style_loss) / (batch_id + 1)
				)
				print(mesg)

			if args.checkpoint_model_dir is not None and (batch_id + 1) % args.checkpoint_interval == 0:
				transformer.eval().cpu()
				ckpt_model_filename = "ckpt_epoch_" + str(e) + "_batch_id_" + str(batch_id + 1) + ".pth"
				ckpt_model_path = os.path.join(args.checkpoint_model_dir, ckpt_model_filename)
				torch.save(transformer.state_dict(), ckpt_model_path)
				transformer.to(device).train()

	transformer.eval().cpu()
	save_model_filename = "epoch_" + str(args.epochs) + "_" + str(time.ctime()).replace(' ', '_') + "_" + str(
		args.content_weight) + "_" + str(args.style_weight) + ".model"
	save_model_path = os.path.join(args.save_model_dir, save_model_filename)
	torch.save(transformer.state_dict(), save_model_path)

	print("\nDone, trained model saved at", save_model_path)


def stylize(args):
	device = torch.device("cuda" if args.cuda else "cpu")

	content_image = utils.load_image(args.content_image, scale=args.content_scale)
	content_transform = transforms.Compose([
		transforms.ToTensor(),
		transforms.Lambda(lambda x: x.mul(255))
	])
	content_image = content_transform(content_image)
	content_image = content_image.unsqueeze(0).to(device)

	if args.model.endswith(".onnx"):
		output = stylize_onnx(content_image, args)
	else:
		with torch.no_grad():
			style_model = TransformerNet()
			state_dict = torch.load(args.model)
			for k in list(state_dict.keys()):
				if re.search(r'in\d+\.running_(mean|var)$', k):
					del state_dict[k]
			style_model.load_state_dict(state_dict)
			style_model.to(device)
			style_model.eval()
			if args.export_onnx:
				assert args.export_onnx.endswith(".onnx"), "Export model file should end with .onnx"
				output = torch.onnx._export(
					style_model, content_image, args.export_onnx, opset_version=11,
				).cpu()			
			else:
				output = style_model(content_image).cpu()
	utils.save_image(args.output_image, output[0])


def stylize_onnx(content_image, args):

	assert not args.export_onnx

	import onnxruntime

	ort_session = onnxruntime.InferenceSession(args.model)

	def to_numpy(tensor):
		return (
			tensor.detach().cpu().numpy()
			if tensor.requires_grad
			else tensor.cpu().numpy()
		)

	ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(content_image)}
	ort_outs = ort_session.run(None, ort_inputs)
	img_out_y = ort_outs[0]

	return torch.from_numpy(img_out_y)


def main():
	main_arg_parser = argparse.ArgumentParser(description="parser for fast-neural-style")
	subparsers = main_arg_parser.add_subparsers(title="subcommands", dest="subcommand")

	train_arg_parser = subparsers.add_parser("train", help="parser for training arguments")
	train_arg_parser.add_argument("--epochs", type=int, default=2,
								  help="number of training epochs, default is 2")
	train_arg_parser.add_argument("--batch-size", type=int, default=4,
								  help="batch size for training, default is 4")
	train_arg_parser.add_argument("--dataset", type=str, required=True,
								  help="path to training dataset, the path should point to a folder "
									   "containing another folder with all the training images")
	train_arg_parser.add_argument("--style-image", type=str, default="images/style-images/mosaic.jpg",
								  help="path to style-image")
	train_arg_parser.add_argument("--save-model-dir", type=str, required=True,
								  help="path to folder where trained model will be saved.")
	train_arg_parser.add_argument("--checkpoint-model-dir", type=str, default=None,
								  help="path to folder where checkpoints of trained models will be saved")
	train_arg_parser.add_argument("--image-size", type=int, default=256,
								  help="size of training images, default is 256 X 256")
	train_arg_parser.add_argument("--style-size", type=int, default=None,
								  help="size of style-image, default is the original size of style image")
	train_arg_parser.add_argument("--cuda", type=int, required=True,
								  help="set it to 1 for running on GPU, 0 for CPU")
	train_arg_parser.add_argument("--seed", type=int, default=42,
								  help="random seed for training")
	train_arg_parser.add_argument("--content-weight", type=float, default=1e5,
								  help="weight for content-loss, default is 1e5")
	train_arg_parser.add_argument("--style-weight", type=float, default=1e10,
								  help="weight for style-loss, default is 1e10")
	train_arg_parser.add_argument("--lr", type=float, default=1e-3,
								  help="learning rate, default is 1e-3")
	train_arg_parser.add_argument("--log-interval", type=int, default=500,
								  help="number of images after which the training loss is logged, default is 500")
	train_arg_parser.add_argument("--checkpoint-interval", type=int, default=2000,
								  help="number of batches after which a checkpoint of the trained model will be created")

	eval_arg_parser = subparsers.add_parser("eval", help="parser for evaluation/stylizing arguments")
	eval_arg_parser.add_argument("--content-image", type=str, required=True,
								 help="path to content image you want to stylize")
	eval_arg_parser.add_argument("--content-scale", type=float, default=None,
								 help="factor for scaling down the content image")
	eval_arg_parser.add_argument("--output-image", type=str, required=True,
								 help="path for saving the output image")
	eval_arg_parser.add_argument("--model", type=str, required=True,
								 help="saved model to be used for stylizing the image. If file ends in .pth - PyTorch path is used, if in .onnx - Caffe2 path")
	eval_arg_parser.add_argument("--cuda", type=int, default=False,
								 help="set it to 1 for running on cuda, 0 for CPU")
	eval_arg_parser.add_argument("--export_onnx", type=str,
								 help="export ONNX model to a given file")
	eval_arg_parser.add_argument('--mps', action='store_true', default=False, help='enable macOS GPU training')

	args = main_arg_parser.parse_args()

	if args.subcommand is None:
		print("ERROR: specify either train or eval")
		sys.exit(1)
	if args.cuda and not torch.cuda.is_available():
		print("ERROR: cuda is not available, try running on CPU")
		sys.exit(1)
	if not args.mps and torch.backends.mps.is_available():
		print("WARNING: mps is available, run with --mps to enable macOS GPU")

	if args.subcommand == "train":
		check_paths(args)
		train(args)
	else:
		stylize(args)


if __name__ == "__main__":
	main()

from allennlp.data.fields.tensor_field import TensorField

ArrayField = TensorField

import torch

from allennlp.common.registrable import Registrable
from allennlp.data.fields.text_field import TextFieldTensors


class TextFieldEmbedder(torch.nn.Module, Registrable):

	default_implementation = "basic"

	def forward(
		self, text_field_input: TextFieldTensors, num_wrapping_dims: int = 0, **kwargs
	) -> torch.Tensor:
		raise NotImplementedError

	def get_output_dim(self) -> int:
		raise NotImplementedError


import logging
import os
import sys
import warnings
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, Union

import datasets
import numpy as np
import torch
from datasets import load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoModelForMultipleChoice,
	AutoTokenizer,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	default_data_collator,
	set_seed,
)
from transformers.tokenization_utils_base import PreTrainedTokenizerBase
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import PaddingStrategy, check_min_version, send_example_telemetry


check_min_version("4.38.0.dev0")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)


@dataclass
class DataTrainingArguments:

	train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
	validation_file: Optional[str] = field(
		default=None,
		metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
	)
	preprocessing_num_workers: Optional[int] = field(
		default=None,
		metadata={"help": "The number of processes to use for the preprocessing."},
	)
	max_seq_length: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. If passed, sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	pad_to_max_length: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether to pad all samples to the maximum sentence length. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
				"efficient on GPU but very bad for TPU."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)

	def __post_init__(self):
		if self.train_file is not None:
			extension = self.train_file.split(".")[-1]
			assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
		if self.validation_file is not None:
			extension = self.validation_file.split(".")[-1]
			assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."


@dataclass
class DataCollatorForMultipleChoice:

	tokenizer: PreTrainedTokenizerBase
	padding: Union[bool, str, PaddingStrategy] = True
	max_length: Optional[int] = None
	pad_to_multiple_of: Optional[int] = None

	def __call__(self, features):
		label_name = "label" if "label" in features[0].keys() else "labels"
		labels = [feature.pop(label_name) for feature in features]
		batch_size = len(features)
		num_choices = len(features[0]["input_ids"])
		flattened_features = [
			[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features
		]
		flattened_features = list(chain(*flattened_features))

		batch = self.tokenizer.pad(
			flattened_features,
			padding=self.padding,
			max_length=self.max_length,
			pad_to_multiple_of=self.pad_to_multiple_of,
			return_tensors="pt",
		)

		batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
		batch["labels"] = torch.tensor(labels, dtype=torch.int64)
		return batch


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_swag", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)



	if data_args.train_file is not None or data_args.validation_file is not None:
		data_files = {}
		if data_args.train_file is not None:
			data_files["train"] = data_args.train_file
		if data_args.validation_file is not None:
			data_files["validation"] = data_args.validation_file
		extension = data_args.train_file.split(".")[-1]
		raw_datasets = load_dataset(
			extension,
			data_files=data_files,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		raw_datasets = load_dataset(
			"swag",
			"regular",
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)


	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForMultipleChoice.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	ending_names = [f"ending{i}" for i in range(4)]
	context_name = "sent1"
	question_header_name = "sent2"

	if data_args.max_seq_length is None:
		max_seq_length = tokenizer.model_max_length
		if max_seq_length > 1024:
			logger.warning(
				"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
				" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
				" override this default with `--block_size xxx`."
			)
			max_seq_length = 1024
	else:
		if data_args.max_seq_length > tokenizer.model_max_length:
			logger.warning(
				f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
				f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
			)
		max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	def preprocess_function(examples):
		first_sentences = [[context] * 4 for context in examples[context_name]]
		question_headers = examples[question_header_name]
		second_sentences = [
			[f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
		]

		first_sentences = list(chain(*first_sentences))
		second_sentences = list(chain(*second_sentences))

		tokenized_examples = tokenizer(
			first_sentences,
			second_sentences,
			truncation=True,
			max_length=max_seq_length,
			padding="max_length" if data_args.pad_to_max_length else False,
		)
		return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))
		with training_args.main_process_first(desc="train dataset map pre-processing"):
			train_dataset = train_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				load_from_cache_file=not data_args.overwrite_cache,
			)

	if training_args.do_eval:
		if "validation" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_dataset = raw_datasets["validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))
		with training_args.main_process_first(desc="validation dataset map pre-processing"):
			eval_dataset = eval_dataset.map(
				preprocess_function,
				batched=True,
				num_proc=data_args.preprocessing_num_workers,
				load_from_cache_file=not data_args.overwrite_cache,
			)

	data_collator = (
		default_data_collator
		if data_args.pad_to_max_length
		else DataCollatorForMultipleChoice(tokenizer=tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)
	)

	def compute_metrics(eval_predictions):
		predictions, label_ids = eval_predictions
		preds = np.argmax(predictions, axis=1)
		return {"accuracy": (preds == label_ids).astype(np.float32).mean().item()}

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		tokenizer=tokenizer,
		data_collator=data_collator,
		compute_metrics=compute_metrics,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		trainer.save_model()  # Saves the tokenizer too for easy upload
		metrics = train_result.metrics

		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")

		metrics = trainer.evaluate()
		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	kwargs = {
		"finetuned_from": model_args.model_name_or_path,
		"tasks": "multiple-choice",
		"dataset_tags": "swag",
		"dataset_args": "regular",
		"dataset": "SWAG",
		"language": "en",
	}

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from allennlp.confidence_checks.verification_base import VerificationBase
from allennlp.confidence_checks.normalization_bias_verification import NormalizationBiasVerification

import warnings

warnings.warn(
	"Module 'sanity_checks' is deprecated, please use 'confidence_checks' instead.",
	DeprecationWarning,
)

import torch
import torch.fx
import operator

torch.classes.load_library('build/libinterpreter.so')

def lower_to_elementwise_interpreter(orig_mod : torch.nn.Module) -> torch.nn.Module:
	mod = torch.fx.symbolic_trace(orig_mod)

	instructions = []
	constant_idx = 0
	constants = {}
	fn_input_names = []

	target_to_name = {
		operator.add : "add",
		operator.mul : "mul"
	}

	output_node : Optional[torch.fx.Node] = None
	for n in mod.graph.nodes:
		target, args, out_name = n.target, n.args, n.name
		assert len(n.kwargs) == 0, "kwargs currently not supported"

		if n.op == 'placeholder':
			fn_input_names.append(target)
		elif n.op == 'call_function':
			assert target in target_to_name, "Unsupported call target " + target
			arg_names = []
			for arg in args:
				if not isinstance(arg, torch.fx.Node):
					arg_name = f'constant_{constant_idx}'
					constants[arg_name] = torch.Tensor(
						[arg] if isinstance(arg, numbers.Number) else arg)
					arg_names.append(arg_name)
					constant_idx += 1
				else:
					arg_names.append(arg.name)
			instructions.append((target_to_name[target], arg_names, out_name))
		elif n.op == 'output':
			if output_node is not None:
				raise RuntimeError('Multiple output nodes!')
			output_node = n
		else:
			raise RuntimeError('Unsupported opcode ' + n.op)

	interpreter = torch.classes.NativeInterpretation.ElementwiseInterpreter()
	for k, v in constants.items():
		interpreter.add_constant(k, v)
	interpreter.set_input_names(fn_input_names)
	interpreter.set_instructions(instructions)
	assert isinstance(output_node.args[0], torch.fx.Node)
	interpreter.set_output_name(output_node.args[0].name)

	class WrapperModule(torch.nn.Module):
		def __init__(self, interpreter):
			super().__init__()
			self.interpreter = interpreter

	wrapper = WrapperModule(interpreter)


	graph = torch.fx.Graph()
	placeholder_nodes = []
	for name in fn_input_names:
		placeholder_nodes.append(graph.create_node('placeholder', name))

	interpreter_node = graph.create_node('get_attr', 'interpreter')

	output_node = graph.create_node(
		op='call_method', target='__call__', args=(interpreter_node, placeholder_nodes))

	graph.output(output_node)

	graph.lint(wrapper)

	return torch.fx.GraphModule(wrapper, graph)

class MyElementwiseModule(torch.nn.Module):
	def forward(self, x, y):
		return x * y + y

mem = MyElementwiseModule()
lowered = lower_to_elementwise_interpreter(mem)
print(lowered.code)
scripted = torch.jit.script(lowered)
print(scripted.graph)

for _ in range(50):
	x, y = torch.randn(10, 20, 30), torch.randn(10, 20, 30)
	torch.testing.assert_allclose(lowered(x, y), mem(x, y))
	torch.testing.assert_allclose(scripted(x, y), mem(x, y))


from allennlp.modules.seq2seq_encoders.compose_encoder import ComposeEncoder
from allennlp.modules.seq2seq_encoders.feedforward_encoder import FeedForwardEncoder
from allennlp.modules.seq2seq_encoders.gated_cnn_encoder import GatedCnnEncoder
from allennlp.modules.seq2seq_encoders.pass_through_encoder import PassThroughEncoder
from allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import (
	AugmentedLstmSeq2SeqEncoder,
	GruSeq2SeqEncoder,
	LstmSeq2SeqEncoder,
	PytorchSeq2SeqWrapper,
	RnnSeq2SeqEncoder,
	StackedAlternatingLstmSeq2SeqEncoder,
	StackedBidirectionalLstmSeq2SeqEncoder,
)
from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder
from allennlp.modules.seq2seq_encoders.pytorch_transformer_wrapper import PytorchTransformer

from pathlib import Path
from datetime import datetime
import torch
import time

from torch.distributed.fsdp import (
	FullyShardedDataParallel as FSDP,
	StateDictType,
	FullStateDictConfig,  # general model non-sharded, non-flattened params
	LocalStateDictConfig,  # flattened params, usable only by FSDP
)

from torch.distributed._shard.checkpoint import (
	FileSystemReader,
	FileSystemWriter,
	save_state_dict,
	load_state_dict,
)
from torch.distributed.checkpoint.default_planner import (
	DefaultSavePlanner,
	DefaultLoadPlanner,
)


from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType
import torch.distributed._shard.checkpoint as dist_cp
import torch.distributed as dist


def get_date_of_run():
	date_of_run = datetime.now().strftime("%Y-%m-%d-%I:%M:%S_%p")
	print(f"--> current date and time of run = {date_of_run}")
	return date_of_run


fullstate_save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)


def load_model_sharded(model, rank, cfg, verbose=True):
	folder_name = (
		cfg.dist_checkpoint_root_folder
		+ "/"
		+ cfg.dist_checkpoint_folder
		+ "-"
		+ cfg.model_name
	)

	load_dir = Path.cwd() / folder_name

	if not load_dir.exists():
		if rank == 0:
			print(f"No sharded_state_dict checkpoint directory found...skipping")
		return

	reader = FileSystemReader(load_dir)

	with FSDP.state_dict_type(model, StateDictType.SHARDED_STATE_DICT):
		checkpoint = model.state_dict()
		if rank == 0:
			ck = checkpoint.keys()
			print(f" checkpoint key len = {len(ck)} and \n keys =  {ck}")
	  
		dist_cp.load_state_dict(
			state_dict=checkpoint,
			storage_reader=reader,
		)
		if rank == 0:
			print(f"checkpoint after load_state_dict()")
			ck = checkpoint.keys()
			print(f" checkpoint key len = {len(ck)} and \n keys =  {ck}")
		model.load_state_dict(checkpoint)
	if rank == 0:
		print(f"Sharded state checkpoint loaded from {load_dir}")


def save_model_and_optimizer_sharded(model, rank, cfg,optim=None, verbose=True):
	folder_name = (
		cfg.dist_checkpoint_root_folder
		+ "/"
		+ cfg.dist_checkpoint_folder
		+ "-"
		+ cfg.model_name
	)

	save_dir = Path.cwd() / folder_name
	if rank == 0:
		print(f"Saving model to {save_dir}")

	distributed_writer = dist_cp.FileSystemWriter(
		save_dir,
	)
	t0 = time.perf_counter()

	with FSDP.state_dict_type(model, StateDictType.SHARDED_STATE_DICT):

		state_dict = {"model": model.state_dict()}
		if optim is not None:
			state_dict["optim"] = FSDP.optim_state_dict(model, optim)

		dist_cp.save_state_dict(
			state_dict=state_dict,
			storage_writer=distributed_writer,
			planner=DefaultSavePlanner(),
			
		)
	dist.barrier()
	t1 = time.perf_counter()
	if rank == 0:
		print(f"Sharded state checkpoint saved to {save_dir}")
		print(
			f"Checkpoint Time = {t1-t0:.4f}\n using {cfg.save_using_num_threads=} total threads"
		)
		
def save_model_checkpoint(
	model,
	optimizer,
	rank,
	cfg,
	epoch=1,
):

	if not cfg.checkpoint_type == StateDictType.FULL_STATE_DICT:
		print(f" unable to handle checkpoint type {cfg.checkpoint_type}, aborting")

	with FSDP.state_dict_type(
		model, StateDictType.FULL_STATE_DICT, fullstate_save_policy
	):
		cpu_state = model.state_dict()

	if cfg.verbose:
		print(f"saving process: rank {rank}  done w model state_dict\n")
   

	if rank == 0:
		print(f"--> saving model ...")
		save_dir = Path.cwd() / cfg.checkpoint_folder
		save_dir.mkdir(parents=True, exist_ok=True)
		save_name = cfg.model_save_name + "-" + str(epoch) + ".pt"
		save_full_path = str(save_dir) + "/" + save_name

		torch.save(cpu_state, save_full_path)

		if cfg.verbose:
			print(f"model checkpoint saved for epoch {epoch} at {save_full_path}\n")
	  


def load_model_checkpoint(model, rank, cfg, verbose=True):

	if rank != 0:
		return

	full_state_dict_model_path = (
		Path.cwd() / cfg.checkpoint_folder / cfg.checkpoint_model_filename
	)
	if not full_state_dict_model_path.is_file():
		print(
			f"model checkpoint {full_state_dict_model_path} not present. Returning..."
		)
		return


	model_checkpoint = torch.load(full_state_dict_model_path)
	model.load_state_dict(model_checkpoint)

	if cfg.verbose:
		print(f"model checkpoint loaded to rank0 cpu")


def save_optimizer_checkpoint(model, optimizer, rank, cfg, epoch=1):

	if cfg.verbose:
		print(f"--> optim state call on rank {rank}\n")


	optim_state = FSDP.full_optim_state_dict(model, optimizer)

	if cfg.verbose:
		print(f"optim state dict ready on {rank} and len of {len(optim_state)}\n")

	if rank == 0:
		save_dir = Path.cwd() / cfg.checkpoint_folder
		save_dir.mkdir(parents=True, exist_ok=True)

		opt_save_name = (
			cfg.optimizer_name + "-" + cfg.model_save_name + "-" + str(epoch) + ".pt"
		)
		opt_save_full_path = save_dir / opt_save_name

		print(f"--> saving optimizer state...")

		torch.save(optim_state, opt_save_full_path)

		print(f"--> saved {opt_save_full_path} to disk")


def load_optimizer_checkpoint(model, optimizer, rank, cfg):

	opt_file_path = Path.cwd() / cfg.checkpoint_folder / cfg.optimizer_checkpoint_file

	if not opt_file_path.is_file():
		print(
			f"warning - optimizer checkpoint not present {opt_file_path}. Returning. "
		)
		return

	full_osd = None

	if rank == 0:
		full_osd = torch.load(opt_file_path)

		if cfg.verbose:
			print(f"loaded full osd on rank 0")

	sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, model)

	if cfg.verbose:
		print(f"optimizer shard loaded on rank {rank}")



def load_distributed_model_checkpoint(model, rank, cfg):
	if cfg.checkpoint_type == StateDictType.LOCAL_STATE_DICT:
		print(f"loading distributed checkpoint, rank {rank}...")
		folder_name = (
			cfg.dist_checkpoint_root_folder
			+ "/"
			+ cfg.dist_checkpoint_folder
			+ "-"
			+ cfg.model_name
		)

		checkdir = Path.cwd() / folder_name

		if not checkdir.exists():
			if rank == 0:
				print(f"No checkpoint directory found...skipping")
			return


		reader = FileSystemReader(checkdir)

		with FSDP.state_dict_type(
			model,
			StateDictType.LOCAL_STATE_DICT,
		):
			state_dict = model.state_dict()
			load_state_dict(state_dict, reader)
			model.load_state_dict(state_dict)

		print(f"--> local state loaded on rank {rank}")

		return


def save_distributed_model_checkpoint(model, rank, cfg, epoch=1):

	if cfg.checkpoint_type == StateDictType.LOCAL_STATE_DICT:
		folder_name = (
			cfg.dist_checkpoint_root_folder
			+ "/"
			+ cfg.dist_checkpoint_folder
			+ "-"
			+ cfg.model_name
		)
		save_dir = Path.cwd() / folder_name

		writer = FileSystemWriter(
			save_dir,
		)

		with FSDP.state_dict_type(
			model,
			StateDictType.LOCAL_STATE_DICT,
		):
			state_dict = model.state_dict()
	   

		save_state_dict(state_dict, writer)

		return


import torch
import numpy as np
import scipy
import sklearn

from allennlp.common.checks import ConfigurationError


class BiasMitigator:

	def __init__(self, requires_grad: bool = False):
		self.requires_grad = requires_grad

	def _proj(self, u: torch.Tensor, v: torch.Tensor, normalize: bool = False):
		proj = torch.matmul(u, v.reshape(-1, 1)) * v
		if normalize:
			return proj / torch.dot(v, v)
		return proj

	def _remove_component(
		self, embeddings: torch.Tensor, bias_direction: torch.Tensor, normalize: bool = False
	):
		return embeddings - self._proj(embeddings, bias_direction, normalize)


class HardBiasMitigator(BiasMitigator):

	def __call__(
		self,
		evaluation_embeddings: torch.Tensor,
		bias_direction: torch.Tensor,
		equalize_embeddings1: torch.Tensor,
		equalize_embeddings2: torch.Tensor,
	):

		if equalize_embeddings1.size() != equalize_embeddings2.size():
			raise ConfigurationError(
				"equalize_embeddings1 and equalize_embeddings2 must be the same size."
			)
		if equalize_embeddings1.ndim < 2:
			raise ConfigurationError(
				"equalize_embeddings1 and equalize_embeddings2 must have at least two dimensions."
			)
		if evaluation_embeddings.ndim < 2:
			raise ConfigurationError("evaluation_embeddings must have at least two dimensions.")
		if evaluation_embeddings.size()[1:] != equalize_embeddings1.size()[1:]:
			raise ConfigurationError(
				"evaluation_embeddings, equalize_embeddings1, and equalize_embeddings2 must have same size \
				except for 0th dim (i.e. batch dimension)."
			)
		if bias_direction.ndim != 1:
			raise ConfigurationError("bias_direction must be one-dimensional.")
		if evaluation_embeddings.size(-1) != bias_direction.size(-1):
			raise ConfigurationError(
				"All embeddings and bias_direction must have the same dimensionality."
			)

		with torch.set_grad_enabled(self.requires_grad):
			bias_direction = bias_direction / torch.linalg.norm(bias_direction)

			bias_mitigated_embeddings = self._remove_component(
				evaluation_embeddings, bias_direction, normalize=True
			)

			mean_equalize_embeddings = (equalize_embeddings1 + equalize_embeddings2) / 2
			y = self._remove_component(mean_equalize_embeddings, bias_direction, normalize=True)
			z = torch.sqrt(1 - torch.square(torch.linalg.norm(y, dim=-1, keepdim=True)))
			z = torch.where(
				torch.matmul(
					equalize_embeddings1 - equalize_embeddings2, bias_direction.reshape(-1, 1)
				)
				< 0,
				-z,
				z,
			)
			return torch.cat(
				[bias_mitigated_embeddings, z * bias_direction + y, -z * bias_direction + y]
			)


class LinearBiasMitigator(BiasMitigator):

	def __call__(self, evaluation_embeddings: torch.Tensor, bias_direction: torch.Tensor):
		if evaluation_embeddings.ndim < 2:
			raise ConfigurationError("evaluation_embeddings must have at least two dimensions.")
		if bias_direction.ndim != 1:
			raise ConfigurationError("bias_direction must be one-dimensional.")
		if evaluation_embeddings.size(-1) != bias_direction.size(-1):
			raise ConfigurationError(
				"All embeddings and bias_direction must have the same dimensionality."
			)

		with torch.set_grad_enabled(self.requires_grad):
			bias_direction = bias_direction / torch.linalg.norm(bias_direction)
			return self._remove_component(evaluation_embeddings, bias_direction)


class INLPBiasMitigator(BiasMitigator):

	def __init__(self):
		super().__init__()

	def __call__(
		self,
		evaluation_embeddings: torch.Tensor,
		seed_embeddings1: torch.Tensor,
		seed_embeddings2: torch.Tensor,
		num_iters: int = 35,
	):
		if seed_embeddings1.ndim < 2 or seed_embeddings2.ndim < 2:
			raise ConfigurationError(
				"seed_embeddings1 and seed_embeddings2 must have at least two dimensions."
			)
		if seed_embeddings1.size(-1) != seed_embeddings2.size(-1):
			raise ConfigurationError("All seed embeddings must have same dimensionality.")
		if evaluation_embeddings.ndim < 2:
			raise ConfigurationError("evaluation_embeddings must have at least two dimensions.")
		if evaluation_embeddings.size(-1) != seed_embeddings1.size(
			-1
		) or evaluation_embeddings.size(-1) != seed_embeddings2.size(-1):
			raise ConfigurationError(
				"evaluation_embeddings, seed_embeddings1, and seed_embeddings2 must have the same dimensionality."
			)

		device = seed_embeddings1.device
		seed_embeddings1 = seed_embeddings1.flatten(end_dim=-2).detach().cpu().numpy()
		seed_embeddings2 = seed_embeddings2.flatten(end_dim=-2).detach().cpu().numpy()
		X = np.vstack([seed_embeddings1, seed_embeddings2])
		Y = np.concatenate([[0] * seed_embeddings1.shape[0], [1] * seed_embeddings2.shape[0]])

		rowspace_projs = []
		for iter_idx in range(num_iters):
			classifier = sklearn.svm.SVC(kernel="linear").fit(X, Y)
			weights = np.expand_dims(classifier.coef_[0], 0)

			if (np.linalg.norm(weights) < 1e-10 or classifier.score(X, Y) < 0.55) and iter_idx > 1:
				break

			rowspace_projs.append(self._get_rowspace_proj(weights))
			nullspace_proj = np.eye(seed_embeddings1.shape[1]) - self._get_rowspace_proj(
				np.sum(rowspace_projs, axis=0)
			)
			evaluation_embeddings = torch.matmul(
				evaluation_embeddings, torch.from_numpy(nullspace_proj).float().t().to(device)
			)
			X = nullspace_proj.dot(X.T).T

		return evaluation_embeddings

	def _get_rowspace_proj(self, weights: np.ndarray):
		if np.allclose(weights, 0):
			weights_basis = np.zeros_like(weights.T)
		else:
			weights_basis = scipy.linalg.orth(weights.T)
		return weights_basis.dot(weights_basis.T)


class OSCaRBiasMitigator(BiasMitigator):

	def __call__(
		self,
		evaluation_embeddings: torch.Tensor,
		bias_direction1: torch.Tensor,
		bias_direction2: torch.Tensor,
	):
		if evaluation_embeddings.ndim < 2:
			raise ConfigurationError("evaluation_embeddings must have at least two dimensions.")
		if bias_direction1.ndim != 1 or bias_direction2.ndim != 1:
			raise ConfigurationError("bias_direction1 and bias_direction2 must be one-dimensional.")
		if evaluation_embeddings.size(-1) != bias_direction1.size(-1) or evaluation_embeddings.size(
			-1
		) != bias_direction2.size(-1):
			raise ConfigurationError(
				"All embeddings, bias_direction1, and bias_direction2 must have the same dimensionality."
			)
		if bias_direction1.size(-1) < 2:
			raise ConfigurationError(
				"Dimensionality of all embeddings, bias_direction1, and bias_direction2 must \
				be >= 2."
			)

		with torch.set_grad_enabled(self.requires_grad):
			bias_direction1 = bias_direction1 / torch.linalg.norm(bias_direction1)
			bias_direction2 = bias_direction2 / torch.linalg.norm(bias_direction2)

			bias_direction2_orth = self._remove_component(
				bias_direction2.reshape(1, -1), bias_direction1
			)[0]
			bias_direction2_orth = bias_direction2_orth / torch.linalg.norm(bias_direction2_orth)

			init_orth_matrix = torch.eye(
				bias_direction1.size(0),
				device=evaluation_embeddings.device,
				requires_grad=self.requires_grad,
			)
			rotation_matrix = torch.zeros(
				(bias_direction1.size(0), bias_direction1.size(0)),
				device=evaluation_embeddings.device,
				requires_grad=self.requires_grad,
			)
			rotation_matrix = torch.cat(
				[
					bias_direction1.reshape(1, -1),
					bias_direction2_orth.reshape(1, -1),
					rotation_matrix[2:],
				]
			)
			for i in range(len(rotation_matrix) - 2):
				subspace_proj = torch.sum(
					self._proj(
						rotation_matrix[: i + 2].clone(), init_orth_matrix[i], normalize=True
					),
					dim=0,
				)
				rotation_matrix[i + 2] = (init_orth_matrix[i] - subspace_proj) / torch.linalg.norm(
					init_orth_matrix[i] - subspace_proj
				)

			mask = ~(evaluation_embeddings == 0).all(dim=-1)
			rotated_evaluation_embeddings = torch.matmul(
				evaluation_embeddings[mask], rotation_matrix.t()
			)
			fixed_rotated_evaluation_embeddings = rotated_evaluation_embeddings[..., 2:]
			restricted_rotated_evaluation_embeddings = torch.cat(
				[
					torch.matmul(rotated_evaluation_embeddings, bias_direction1.reshape(-1, 1)),
					torch.matmul(
						rotated_evaluation_embeddings, bias_direction2_orth.reshape(-1, 1)
					),
				],
				dim=-1,
			)

			restricted_bias_direction1 = torch.tensor(
				[1.0, 0.0], device=evaluation_embeddings.device, requires_grad=self.requires_grad
			)
			bias_direction_inner_prod = torch.dot(bias_direction1, bias_direction2)
			restricted_bias_direction2 = torch.tensor(
				[
					bias_direction_inner_prod,
					torch.sqrt(1 - torch.square(bias_direction_inner_prod)),
				],
				device=evaluation_embeddings.device,
				requires_grad=self.requires_grad,
			)
			restricted_bias_direction2_orth = torch.tensor(
				[0.0, 1.0], device=evaluation_embeddings.device, requires_grad=self.requires_grad
			)

			restricted_bias_direction_inner_prod = torch.dot(
				restricted_bias_direction1, restricted_bias_direction2
			)
			theta = torch.abs(torch.arccos(restricted_bias_direction_inner_prod))
			theta_proj = np.pi / 2 - theta
			phi = torch.arccos(
				torch.matmul(
					restricted_rotated_evaluation_embeddings
					/ torch.linalg.norm(
						restricted_rotated_evaluation_embeddings, dim=-1, keepdim=True
					),
					restricted_bias_direction1,
				)
			)
			d = torch.matmul(
				restricted_rotated_evaluation_embeddings
				/ torch.linalg.norm(restricted_rotated_evaluation_embeddings, dim=-1, keepdim=True),
				restricted_bias_direction2_orth,
			)

			theta_x = torch.zeros_like(phi, requires_grad=self.requires_grad)
			theta_x = torch.where(
				(d > 0) & (phi < theta_proj),
				theta * (phi / (theta_proj + 1e-10)),
				theta_x,
			)
			theta_x = torch.where(
				(d > 0) & (phi > theta_proj),
				theta * ((np.pi - phi) / (np.pi - theta_proj + 1e-10)),
				theta_x,
			)
			theta_x = torch.where(
				(d < 0) & (phi >= np.pi - theta_proj),
				theta * ((np.pi - phi) / (theta_proj + 1e-10)),
				theta_x,
			)
			theta_x = torch.where(
				(d < 0) & (phi < np.pi - theta_proj),
				theta * (phi / (np.pi - theta_proj + 1e-10)),
				theta_x,
			)

			f_matrix = torch.cat(
				[
					torch.cos(theta_x).unsqueeze(-1),
					-torch.sin(theta_x).unsqueeze(-1),
					torch.sin(theta_x).unsqueeze(-1),
					torch.cos(theta_x).unsqueeze(-1),
				],
				dim=-1,
			)
			f_matrix = f_matrix.reshape(f_matrix.size()[:-1] + (2, 2))

			evaluation_embeddings_clone = evaluation_embeddings.clone()
			evaluation_embeddings_clone[mask] = torch.cat(
				[
					torch.bmm(
						f_matrix,
						restricted_rotated_evaluation_embeddings.unsqueeze(-1),
					).squeeze(-1),
					fixed_rotated_evaluation_embeddings,
				],
				dim=-1,
			)
			return torch.matmul(evaluation_embeddings_clone, rotation_matrix)

from typing import Dict, Optional, List, Any, Union


import torch
import torch.nn.functional

from allennlp.data.fields.field import Field
from allennlp.nn import util


def _tensorize(x: Union[torch.Tensor, List[int]]) -> torch.Tensor:
	if not isinstance(x, torch.Tensor):
		return torch.tensor(x)
	return x


class TransformerTextField(Field[torch.Tensor]):

	__slots__ = [
		"input_ids",
		"token_type_ids",
		"attention_mask",
		"special_tokens_mask",
		"offsets_mapping",
		"padding_token_id",
	]

	def __init__(
		self,
		input_ids: Union[torch.Tensor, List[int]],
		token_type_ids: Optional[Union[torch.Tensor, List[int]]] = None,
		attention_mask: Optional[Union[torch.Tensor, List[int]]] = None,
		special_tokens_mask: Optional[Union[torch.Tensor, List[int]]] = None,
		offsets_mapping: Optional[Union[torch.Tensor, List[int]]] = None,
		padding_token_id: int = 0,
	) -> None:
		self.input_ids = _tensorize(input_ids)
		self.token_type_ids = None if token_type_ids is None else _tensorize(token_type_ids)
		self.attention_mask = None if attention_mask is None else _tensorize(attention_mask)
		self.special_tokens_mask = (
			None if special_tokens_mask is None else _tensorize(special_tokens_mask)
		)
		self.offsets_mapping = None if offsets_mapping is None else _tensorize(offsets_mapping)
		self.padding_token_id = padding_token_id

	def get_padding_lengths(self) -> Dict[str, int]:
		return {
			name: getattr(self, name).shape[-1]
			for name in self.__slots__
			if isinstance(getattr(self, name), torch.Tensor)
		}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:
		result = {}
		for name, padding_length in padding_lengths.items():
			tensor = getattr(self, name)
			if len(tensor.shape) > 1:
				tensor = tensor.squeeze(0)
			result[name] = torch.nn.functional.pad(
				tensor,
				(0, padding_length - tensor.shape[-1]),
				value=self.padding_token_id if name == "input_ids" else 0,
			)
		if "attention_mask" not in result:
			result["attention_mask"] = torch.tensor(
				[True] * self.input_ids.shape[-1]
				+ [False] * (padding_lengths["input_ids"] - self.input_ids.shape[-1]),
				dtype=torch.bool,
			)
		return result

	def empty_field(self):
		return TransformerTextField(torch.LongTensor(), padding_token_id=self.padding_token_id)

	def batch_tensors(self, tensor_list: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
		result: Dict[str, torch.Tensor] = util.batch_tensor_dicts(tensor_list)
		result = {
			name: t.to(torch.int64) if t.dtype == torch.int32 else t for name, t in result.items()
		}
		return result

	def human_readable_repr(self) -> Dict[str, Any]:
		def format_item(x) -> str:
			return str(x.item())

		def readable_tensor(t: torch.Tensor) -> str:
			if t.shape[-1] <= 16:
				return "[" + ", ".join(map(format_item, t)) + "]"
			else:
				return (
					"["
					+ ", ".join(map(format_item, t[:8]))
					+ ", ..., "
					+ ", ".join(map(format_item, t[-8:]))
					+ "]"
				)

		return {
			name: readable_tensor(getattr(self, name))
			for name in self.__slots__
			if isinstance(getattr(self, name), torch.Tensor)
		}

	def __len__(self):
		return len(self.input_ids)


from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import (
	LearningRateScheduler,
	ConstantLearningRateScheduler,
	ConstantWithWarmupLearningRateScheduler,
	CosineWithWarmupLearningRateScheduler,
	CosineHardRestartsWithWarmupLearningRateScheduler,
)
from allennlp.training.learning_rate_schedulers.pytorch_lr_schedulers import (
	StepLearningRateScheduler,
	MultiStepLearningRateScheduler,
	ExponentialLearningRateScheduler,
	ReduceOnPlateauLearningRateScheduler,
)
from allennlp.training.learning_rate_schedulers.combined import CombinedLearningRateScheduler
from allennlp.training.learning_rate_schedulers.cosine import CosineWithRestarts
from allennlp.training.learning_rate_schedulers.noam import NoamLR
from allennlp.training.learning_rate_schedulers.slanted_triangular import SlantedTriangular
from allennlp.training.learning_rate_schedulers.polynomial_decay import PolynomialDecay
from allennlp.training.learning_rate_schedulers.linear_with_warmup import LinearWithWarmup

import math
from typing import Dict, Any

import numpy
import torch

from allennlp.common.util import JsonDict, sanitize
from allennlp.data import Instance
from allennlp.interpret.saliency_interpreters.saliency_interpreter import SaliencyInterpreter
from allennlp.predictors import Predictor


@SaliencyInterpreter.register("smooth-gradient")
class SmoothGradient(SaliencyInterpreter):

	def __init__(self, predictor: Predictor) -> None:
		super().__init__(predictor)
		self.stdev = 0.01
		self.num_samples = 10

	def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:
		labeled_instances = self.predictor.json_to_labeled_instances(inputs)

		instances_with_grads = dict()
		for idx, instance in enumerate(labeled_instances):
			grads = self._smooth_grads(instance)

			for key, grad in grads.items():

				embedding_grad = numpy.sum(grad[0], axis=1)
				norm = numpy.linalg.norm(embedding_grad, ord=1)
				normalized_grad = [math.fabs(e) / norm for e in embedding_grad]
				grads[key] = normalized_grad

			instances_with_grads["instance_" + str(idx + 1)] = grads

		return sanitize(instances_with_grads)

	def _register_forward_hook(self, stdev: float):

		def forward_hook(module, inputs, output):
			scale = output.detach().max() - output.detach().min()
			noise = torch.randn(output.shape, device=output.device) * stdev * scale

			output.add_(noise)

		embedding_layer = self.predictor.get_interpretable_layer()
		handle = embedding_layer.register_forward_hook(forward_hook)
		return handle

	def _smooth_grads(self, instance: Instance) -> Dict[str, numpy.ndarray]:
		total_gradients: Dict[str, Any] = {}
		for _ in range(self.num_samples):
			handle = self._register_forward_hook(self.stdev)
			try:
				grads = self.predictor.get_gradients([instance])[0]
			finally:
				handle.remove()

			if total_gradients == {}:
				total_gradients = grads
			else:
				for key in grads.keys():
					total_gradients[key] += grads[key]

		for key in total_gradients.keys():
			total_gradients[key] /= self.num_samples

		return total_gradients


import argparse
import json
import logging
import math
import os
import random
from dataclasses import dataclass
from itertools import chain
from pathlib import Path
from typing import Optional, Union

import datasets
import evaluate
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_MAPPING,
	AutoConfig,
	AutoModelForMultipleChoice,
	AutoTokenizer,
	PreTrainedTokenizerBase,
	SchedulerType,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import PaddingStrategy, check_min_version, send_example_telemetry


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)
MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a multiple choice task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--max_seq_length",
		type=int,
		default=128,
		help=(
			"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
			" sequences shorter will be padded if `--pad_to_max_lengh` is passed."
		),
	)
	parser.add_argument(
		"--pad_to_max_length",
		action="store_true",
		help="If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=False,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--tokenizer_name",
		type=str,
		default=None,
		help="Pretrained tokenizer name or path if not the same as model_name",
	)
	parser.add_argument(
		"--use_slow_tokenizer",
		action="store_true",
		help="If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="Model type to use if training from scratch.",
		choices=MODEL_TYPES,
	)
	parser.add_argument(
		"--debug",
		action="store_true",
		help="Activate debug mode and run training only with a subset of data.",
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	args = parser.parse_args()

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


@dataclass
class DataCollatorForMultipleChoice:

	tokenizer: PreTrainedTokenizerBase
	padding: Union[bool, str, PaddingStrategy] = True
	max_length: Optional[int] = None
	pad_to_multiple_of: Optional[int] = None

	def __call__(self, features):
		label_name = "label" if "label" in features[0].keys() else "labels"
		labels = [feature.pop(label_name) for feature in features]
		batch_size = len(features)
		num_choices = len(features[0]["input_ids"])
		flattened_features = [
			[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features
		]
		flattened_features = list(chain(*flattened_features))

		batch = self.tokenizer.pad(
			flattened_features,
			padding=self.padding,
			max_length=self.max_length,
			pad_to_multiple_of=self.pad_to_multiple_of,
			return_tensors="pt",
		)

		batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
		batch["labels"] = torch.tensor(labels, dtype=torch.int64)
		return batch


def main():
	args = parse_args()

	send_example_telemetry("run_swag_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		extension = args.train_file.split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files)
	if args.debug:
		for split in raw_datasets.keys():
			raw_datasets[split] = raw_datasets[split].select(range(100))

	if raw_datasets["train"] is not None:
		column_names = raw_datasets["train"].column_names
	else:
		column_names = raw_datasets["validation"].column_names

	ending_names = [f"ending{i}" for i in range(4)]
	context_name = "sent1"
	question_header_name = "sent2"
	label_column_name = "label" if "label" in column_names else "labels"

	if args.config_name:
		config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")

	if args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(
			args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	elif args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(
			args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if args.model_name_or_path:
		model = AutoModelForMultipleChoice.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForMultipleChoice.from_config(config, trust_remote_code=args.trust_remote_code)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	padding = "max_length" if args.pad_to_max_length else False

	def preprocess_function(examples):
		first_sentences = [[context] * 4 for context in examples[context_name]]
		question_headers = examples[question_header_name]
		second_sentences = [
			[f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
		]
		labels = examples[label_column_name]

		first_sentences = list(chain(*first_sentences))
		second_sentences = list(chain(*second_sentences))

		tokenized_examples = tokenizer(
			first_sentences,
			second_sentences,
			max_length=args.max_seq_length,
			padding=padding,
			truncation=True,
		)
		tokenized_inputs = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}
		tokenized_inputs["labels"] = labels
		return tokenized_inputs

	with accelerator.main_process_first():
		processed_datasets = raw_datasets.map(
			preprocess_function, batched=True, remove_columns=raw_datasets["train"].column_names
		)

	train_dataset = processed_datasets["train"]
	eval_dataset = processed_datasets["validation"]

	for index in random.sample(range(len(train_dataset)), 3):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if args.pad_to_max_length:
		data_collator = default_data_collator
	else:
		data_collator = DataCollatorForMultipleChoice(
			tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)
		)

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	device = accelerator.device
	model.to(device)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("swag_no_trainer", experiment_config)

	metric = evaluate.load("accuracy")

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()
				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				outputs = model(**batch)
			predictions = outputs.logits.argmax(dim=-1)
			predictions, references = accelerator.gather_for_metrics((predictions, batch["labels"]))
			metric.add_batch(
				predictions=predictions,
				references=references,
			)

		eval_metric = metric.compute()
		accelerator.print(f"epoch {epoch}: {eval_metric}")

		if args.with_tracking:
			accelerator.log(
				{
					"accuracy": eval_metric,
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			all_results = {f"eval_{k}": v for k, v in eval_metric.items()}
			with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
				json.dump(all_results, f)


if __name__ == "__main__":
	main()


import argparse
import json
import logging
from json import JSONDecodeError
from pathlib import Path
from os import PathLike
from typing import Union, Dict, Any, Optional
from copy import deepcopy

from allennlp.commands.subcommand import Subcommand
from allennlp.common import logging as common_logging
from allennlp.common.util import prepare_environment
from allennlp.data import DataLoader
from allennlp.models.archival import load_archive
from allennlp.evaluation import Evaluator

logger = logging.getLogger(__name__)


@Subcommand.register("evaluate")
class Evaluate(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		description = """Evaluate the specified model + dataset(s)"""
		subparser = parser.add_parser(
			self.name, description=description, help="Evaluate the specified model + dataset(s)."
		)

		subparser.add_argument("archive_file", type=str, help="path to an archived trained model")

		subparser.add_argument(
			"input_file",
			type=str,
			help="path to the file containing the evaluation data (for multiple "
			"files, put between filenames e.g., input1.txt,input2.txt)",
		)

		subparser.add_argument(
			"--output-file",
			type=str,
			help="optional path to write the metrics to as JSON (for multiple "
			"files, put  between filenames e.g., output1.txt,output2.txt)",
		)

		subparser.add_argument(
			"--predictions-output-file",
			type=str,
			help="optional path to write the predictions to as JSON lines "
			"(for mutiple files, put  between filenames e.g., "
			"output1.jsonl,output2.jsonl)",
		)

		subparser.add_argument(
			"--weights-file", type=str, help="a path that overrides which weights file to use"
		)

		cuda_device = subparser.add_mutually_exclusive_group(required=False)
		cuda_device.add_argument(
			"--cuda-device", type=int, default=-1, help="id of GPU to use (if any)"
		)

		subparser.add_argument(
			"-o",
			"--overrides",
			type=str,
			default="",
			help=(
				"a json(net) structure used to override the experiment configuration, e.g., "
				"'{\"iterator.batch_size\": 16}'.  Nested parameters can be specified either"
				" with nested dictionaries or with dot syntax."
			),
		)

		subparser.add_argument(
			"--batch-size", type=int, help="If non-empty, the batch size to use during evaluation."
		)

		subparser.add_argument(
			"--batch-weight-key",
			type=str,
			default="",
			help="If non-empty, name of metric used to weight the loss on a per-batch basis.",
		)

		subparser.add_argument(
			"--extend-vocab",
			action="store_true",
			default=False,
			help="if specified, we will use the instances in your new dataset to "
			"extend your vocabulary. If pretrained-file was used to initialize "
			"embedding layers, you may also need to pass --embedding-sources-mapping.",
		)

		subparser.add_argument(
			"--embedding-sources-mapping",
			type=str,
			default="",
			help="a JSON dict defining mapping from embedding module path to embedding "
			"pretrained-file used during training. If not passed, and embedding needs to be "
			"extended, we will try to use the original file paths used during training. If "
			"they are not available we will use random vectors for embedding extension.",
		)
		subparser.add_argument(
			"--file-friendly-logging",
			action="store_true",
			default=False,
			help="outputs tqdm status on separate lines and slows tqdm refresh rate",
		)

		subparser.add_argument(
			"--auto-names",
			default="NONE",
			help="Automatically create output names for each evaluation file. "
			"`NONE` will not automatically generate a file name for the "
			"neither the metrics nor the predictions. In this case you will"
			" need to pas in both `metrics_output_file` and `predictions_output_file`. "
			"`METRICS` will only automatically create a file name for the"
			" metrics file. `PREDS` will only automatically create a file"
			" name for the predictions outputs. `ALL` will create a "
			"filename for both the metrics and the predictions.",
			choices=["NONE", "METRICS", "PREDS", "ALL"],
		)

		subparser.set_defaults(func=evaluate_from_args)

		return subparser


def evaluate_from_args(args: argparse.Namespace) -> Dict[str, Any]:
	return evaluate_from_archive(
		archive_file=args.archive_file,
		input_file=args.input_file,
		metrics_output_file=args.output_file,
		predictions_output_file=args.predictions_output_file,
		batch_size=args.batch_size,
		cmd_overrides=args.overrides,
		cuda_device=args.cuda_device,
		embedding_sources_mapping=args.embedding_sources_mapping,
		extend_vocab=args.extend_vocab,
		weights_file=args.weights_file,
		file_friendly_logging=args.file_friendly_logging,
		batch_weight_key=args.batch_weight_key,
		auto_names=args.auto_names,
	)


def evaluate_from_archive(
	archive_file: Union[str, PathLike],
	input_file: str,
	metrics_output_file: Optional[str] = None,
	predictions_output_file: Optional[str] = None,
	batch_size: Optional[int] = None,
	cmd_overrides: Union[str, Dict[str, Any]] = "",
	cuda_device: int = -1,
	embedding_sources_mapping: str = None,
	extend_vocab: bool = False,
	weights_file: str = None,
	file_friendly_logging: bool = False,
	batch_weight_key: str = None,
	auto_names: str = "NONE",
) -> Dict[str, Any]:
	common_logging.FILE_FRIENDLY_LOGGING = file_friendly_logging

	logging.getLogger("allennlp.common.params").disabled = True
	logging.getLogger("allennlp.nn.initializers").disabled = True
	logging.getLogger("allennlp.modules.token_embedders.embedding").setLevel(logging.INFO)

	archive = load_archive(
		archive_file,
		weights_file=weights_file,
		cuda_device=cuda_device,
		overrides=cmd_overrides,
	)
	config = deepcopy(archive.config)
	prepare_environment(config)
	model = archive.model
	model.eval()

	evaluator_params = config.pop("evaluation", {})
	evaluator_params["cuda_device"] = cuda_device
	evaluator = Evaluator.from_params(evaluator_params)

	dataset_reader = archive.validation_dataset_reader

	try:
		evaluation_data_path_list = json.loads(f"[{input_file}]")
	except JSONDecodeError:
		evaluation_data_path_list = input_file.split(",")

	if metrics_output_file is not None:
		if auto_names == "METRICS" or auto_names == "ALL":
			logger.warning(
				f"Passed output_files will be ignored, auto_names is set to {auto_names}"
			)

			assert all(isinstance(p, str) for p in evaluation_data_path_list), (
				"When specifying JSON blobs as input, the output files must be explicitly named with "
				"--output-file."
			)
			output_file_list = [
				p.parent.joinpath(f"{p.stem}.outputs") for p in map(Path, evaluation_data_path_list)
			]
		else:
			output_file_list = metrics_output_file.split(",")  # type: ignore
			assert len(output_file_list) == len(evaluation_data_path_list), (
				"The number of `metrics_output_file` paths must be equal to the number "
				"of datasets being evaluated."
			)
	if predictions_output_file is not None:
		if auto_names == "PREDS" or auto_names == "ALL":
			logger.warning(
				f"Passed predictions files will be ignored, auto_names is" f" set to {auto_names}"
			)

			assert all(isinstance(p, str) for p in evaluation_data_path_list), (
				"When specifying JSON blobs as input, the predictions output files must be explicitly named with "
				"--predictions-output-file."
			)
			predictions_output_file_list = [
				p.parent.joinpath(f"{p.stem}.preds") for p in map(Path, evaluation_data_path_list)
			]
		else:
			predictions_output_file_list = predictions_output_file.split(",")  # type: ignore
			assert len(predictions_output_file_list) == len(evaluation_data_path_list), (
				"The number of `predictions_output_file` paths must be equal"
				+ "to the number of datasets being evaluated. "
			)

	output_file_path = None
	predictions_output_file_path = None

	if extend_vocab:
		logger.info("Vocabulary is being extended with embedding sources.")
		embedding_sources = (
			json.loads(embedding_sources_mapping) if embedding_sources_mapping else {}
		)

	all_metrics = {}
	for index, evaluation_data_path in enumerate(evaluation_data_path_list):
		config = deepcopy(archive.config)

		if isinstance(evaluation_data_path, str):
			eval_file_name = Path(evaluation_data_path).stem
		else:
			eval_file_name = str(index)

		if metrics_output_file is not None:
			output_file_path = output_file_list[index]

		if predictions_output_file is not None:
			predictions_output_file_path = predictions_output_file_list[index]

		logger.info("Reading evaluation data from %s", eval_file_name)
		data_loader_params = config.get("validation_data_loader", None)
		if data_loader_params is None:
			data_loader_params = config.get("data_loader")
		if batch_size:
			data_loader_params["batch_size"] = batch_size
		data_loader = DataLoader.from_params(
			params=data_loader_params, reader=dataset_reader, data_path=evaluation_data_path
		)

		if extend_vocab:
			logger.info("Vocabulary is being extended with test instances.")
			model.vocab.extend_from_instances(instances=data_loader.iter_instances())
			model.extend_embedder_vocab(embedding_sources)

		data_loader.index_with(model.vocab)

		metrics = evaluator(
			model,
			data_loader,
			batch_weight_key,
			metrics_output_file=output_file_path,
			predictions_output_file=predictions_output_file_path,
		)

		for name, value in metrics.items():
			if len(evaluation_data_path_list) > 1:
				key = f"{eval_file_name}_"
			else:
				key = ""
			all_metrics[f"{key}{name}"] = value

	logger.info("Finished evaluating.")

	return all_metrics

from typing import Set, Tuple

import numpy as np

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.nn import util


def _choice(num_words: int, num_samples: int) -> Tuple[np.ndarray, int]:
	num_tries = 0
	num_chosen = 0

	def get_buffer() -> np.ndarray:
		log_samples = np.random.rand(num_samples) * np.log(num_words + 1)
		samples = np.exp(log_samples).astype("int64") - 1
		return np.clip(samples, a_min=0, a_max=num_words - 1)

	sample_buffer = get_buffer()
	buffer_index = 0
	samples: Set[int] = set()

	while num_chosen < num_samples:
		num_tries += 1
		sample_id = sample_buffer[buffer_index]
		if sample_id not in samples:
			samples.add(sample_id)
			num_chosen += 1

		buffer_index += 1
		if buffer_index == num_samples:
			sample_buffer = get_buffer()
			buffer_index = 0

	return np.array(list(samples)), num_tries


class SampledSoftmaxLoss(torch.nn.Module):

	def __init__(
		self,
		num_words: int,
		embedding_dim: int,
		num_samples: int,
		sparse: bool = False,
		unk_id: int = None,
		use_character_inputs: bool = True,
		use_fast_sampler: bool = False,
	) -> None:
		super().__init__()

		self.tie_embeddings = False

		assert num_samples < num_words

		if use_fast_sampler:
			raise ConfigurationError("fast sampler is not implemented")
		else:
			self.choice_func = _choice

		if sparse:
			self.softmax_w = torch.nn.Embedding(
				num_embeddings=num_words, embedding_dim=embedding_dim, sparse=True
			)
			self.softmax_w.weight.data.normal_(mean=0.0, std=1.0 / np.sqrt(embedding_dim))
			self.softmax_b = torch.nn.Embedding(
				num_embeddings=num_words, embedding_dim=1, sparse=True
			)
			self.softmax_b.weight.data.fill_(0.0)
		else:
			self.softmax_w = torch.nn.Parameter(
				torch.randn(num_words, embedding_dim) / np.sqrt(embedding_dim)
			)
			self.softmax_b = torch.nn.Parameter(torch.zeros(num_words))

		self.sparse = sparse
		self.use_character_inputs = use_character_inputs

		if use_character_inputs:
			self._unk_id = unk_id

		self._num_samples = num_samples
		self._embedding_dim = embedding_dim
		self._num_words = num_words
		self.initialize_num_words()

	def initialize_num_words(self):
		if self.sparse:
			num_words = self.softmax_w.weight.size(0)
		else:
			num_words = self.softmax_w.size(0)

		self._num_words = num_words
		self._log_num_words_p1 = np.log(num_words + 1)

		self._probs = (
			np.log(np.arange(num_words) + 2) - np.log(np.arange(num_words) + 1)
		) / self._log_num_words_p1

	def forward(
		self,
		embeddings: torch.Tensor,
		targets: torch.Tensor,
		target_token_embedding: torch.Tensor = None,
	) -> torch.Tensor:

		if embeddings.shape[0] == 0:
			return torch.tensor(0.0, device=embeddings.device)

		if not self.training:
			return self._forward_eval(embeddings, targets)
		else:
			return self._forward_train(embeddings, targets, target_token_embedding)

	def _forward_train(
		self, embeddings: torch.Tensor, targets: torch.Tensor, target_token_embedding: torch.Tensor
	) -> torch.Tensor:




		(
			sampled_ids,
			target_expected_count,
			sampled_expected_count,
		) = self.log_uniform_candidate_sampler(targets, choice_func=self.choice_func)

		long_targets = targets.long()
		long_targets.requires_grad_(False)

		all_ids = torch.cat([long_targets, sampled_ids], dim=0)

		if self.sparse:
			all_ids_1 = all_ids.unsqueeze(1)
			all_w = self.softmax_w(all_ids_1).squeeze(1)
			all_b = self.softmax_b(all_ids_1).squeeze(2).squeeze(1)
		else:
			all_w = torch.nn.functional.embedding(all_ids, self.softmax_w)
			all_b = torch.nn.functional.embedding(all_ids, self.softmax_b.unsqueeze(1)).squeeze(1)

		batch_size = long_targets.size(0)
		true_w = all_w[:batch_size, :]
		sampled_w = all_w[batch_size:, :]
		true_b = all_b[:batch_size]
		sampled_b = all_b[batch_size:]

		true_logits = (
			(true_w * embeddings).sum(dim=1)
			+ true_b
			- torch.log(
				target_expected_count + util.tiny_value_of_dtype(target_expected_count.dtype)
			)
		)
		sampled_logits = (
			torch.matmul(embeddings, sampled_w.t())
			+ sampled_b
			- torch.log(
				sampled_expected_count + util.tiny_value_of_dtype(sampled_expected_count.dtype)
			)
		)

		true_in_sample_mask = sampled_ids == long_targets.unsqueeze(1)
		masked_sampled_logits = sampled_logits.masked_fill(true_in_sample_mask, -10000.0)
		logits = torch.cat([true_logits.unsqueeze(1), masked_sampled_logits], dim=1)

		log_softmax = torch.nn.functional.log_softmax(logits, dim=1)
		nll_loss = -1.0 * log_softmax[:, 0].sum()
		return nll_loss

	def _forward_eval(self, embeddings: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:

		if self.sparse:
			w = self.softmax_w.weight
			b = self.softmax_b.weight.squeeze(1)
		else:
			w = self.softmax_w
			b = self.softmax_b

		log_softmax = torch.nn.functional.log_softmax(torch.matmul(embeddings, w.t()) + b, dim=-1)
		if self.tie_embeddings and not self.use_character_inputs:
			targets_ = targets + 1
		else:
			targets_ = targets
		return torch.nn.functional.nll_loss(log_softmax, targets_.long(), reduction="sum")

	def log_uniform_candidate_sampler(self, targets, choice_func=_choice):



		np_sampled_ids, num_tries = choice_func(self._num_words, self._num_samples)

		sampled_ids = torch.from_numpy(np_sampled_ids).to(targets.device)

		target_probs = (
			torch.log((targets.float() + 2.0) / (targets.float() + 1.0)) / self._log_num_words_p1
		)
		target_expected_count = -1.0 * (torch.exp(num_tries * torch.log1p(-target_probs)) - 1.0)
		sampled_probs = (
			torch.log((sampled_ids.float() + 2.0) / (sampled_ids.float() + 1.0))
			/ self._log_num_words_p1
		)
		sampled_expected_count = -1.0 * (torch.exp(num_tries * torch.log1p(-sampled_probs)) - 1.0)

		sampled_ids.requires_grad_(False)
		target_expected_count.requires_grad_(False)
		sampled_expected_count.requires_grad_(False)

		return sampled_ids, target_expected_count, sampled_expected_count

from typing import Dict, List, Optional, Any
import itertools


from allennlp.data.vocabulary import Vocabulary
from allennlp.data.tokenizers import Token
from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList


_DEFAULT_VALUE = "THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING"


@TokenIndexer.register("single_id")
class SingleIdTokenIndexer(TokenIndexer):

	def __init__(
		self,
		namespace: Optional[str] = "tokens",
		lowercase_tokens: bool = False,
		start_tokens: List[str] = None,
		end_tokens: List[str] = None,
		feature_name: str = "text",
		default_value: str = _DEFAULT_VALUE,
		token_min_padding_length: int = 0,
	) -> None:
		super().__init__(token_min_padding_length)
		self.namespace = namespace
		self.lowercase_tokens = lowercase_tokens

		self._start_tokens = [Token(st) for st in (start_tokens or [])]
		self._end_tokens = [Token(et) for et in (end_tokens or [])]
		self._feature_name = feature_name
		self._default_value = default_value

	def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
		if self.namespace is not None:
			text = self._get_feature_value(token)
			if self.lowercase_tokens:
				text = text.lower()
			counter[self.namespace][text] += 1

	def tokens_to_indices(
		self, tokens: List[Token], vocabulary: Vocabulary
	) -> Dict[str, List[int]]:
		indices: List[int] = []

		for token in itertools.chain(self._start_tokens, tokens, self._end_tokens):
			text = self._get_feature_value(token)
			if self.namespace is None:
				indices.append(text)  # type: ignore
			else:
				if self.lowercase_tokens:
					text = text.lower()
				indices.append(vocabulary.get_token_index(text, self.namespace))

		return {"tokens": indices}

	def get_empty_token_list(self) -> IndexedTokenList:
		return {"tokens": []}

	def _get_feature_value(self, token: Token) -> str:
		text = getattr(token, self._feature_name)
		if text is None:
			if self._default_value is not _DEFAULT_VALUE:
				text = self._default_value
			else:
				raise ValueError(
					f"{token} did not have attribute {self._feature_name}. If you "
					"want to ignore this kind of error, give a default value in the "
					"constructor of this indexer."
				)
		return text

	def _to_params(self) -> Dict[str, Any]:
		return {
			"namespace": self.namespace,
			"lowercase_tokens": self.lowercase_tokens,
			"start_tokens": [t.text for t in self._start_tokens],
			"end_tokens": [t.text for t in self._end_tokens],
			"feature_name": self._feature_name,
			"default_value": self._default_value,
			"token_min_padding_length": self._token_min_padding_length,
		}

from torch.utils.data import Dataset
import tqdm
import torch
import random


class BERTDataset(Dataset):
	def __init__(self, corpus_path, vocab, seq_len, encoding="utf-8", corpus_lines=None, on_memory=True):
		self.vocab = vocab
		self.seq_len = seq_len

		self.on_memory = on_memory
		self.corpus_lines = corpus_lines
		self.corpus_path = corpus_path
		self.encoding = encoding

		with open(corpus_path, "r", encoding=encoding) as f:
			if self.corpus_lines is None and not on_memory:
				for _ in tqdm.tqdm(f, desc="Loading Dataset", total=corpus_lines):
					self.corpus_lines += 1

			if on_memory:
				self.lines = [line[:-1].split("\t")
							  for line in tqdm.tqdm(f, desc="Loading Dataset", total=corpus_lines)]
				self.corpus_lines = len(self.lines)

		if not on_memory:
			self.file = open(corpus_path, "r", encoding=encoding)
			self.random_file = open(corpus_path, "r", encoding=encoding)

			for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):
				self.random_file.__next__()

	def __len__(self):
		return self.corpus_lines

	def __getitem__(self, item):
		t1, t2, is_next_label = self.random_sent(item)
		t1_random, t1_label = self.random_word(t1)
		t2_random, t2_label = self.random_word(t2)

		t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]
		t2 = t2_random + [self.vocab.eos_index]

		t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]
		t2_label = t2_label + [self.vocab.pad_index]

		segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]
		bert_input = (t1 + t2)[:self.seq_len]
		bert_label = (t1_label + t2_label)[:self.seq_len]

		padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]
		bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)

		output = {"bert_input": bert_input,
				  "bert_label": bert_label,
				  "segment_label": segment_label,
				  "is_next": is_next_label}

		return {key: torch.tensor(value) for key, value in output.items()}

	def random_word(self, sentence):
		tokens = sentence.split()
		output_label = []

		for i, token in enumerate(tokens):
			prob = random.random()
			if prob < 0.15:
				prob /= 0.15

				if prob < 0.8:
					tokens[i] = self.vocab.mask_index

				elif prob < 0.9:
					tokens[i] = random.randrange(len(self.vocab))

				else:
					tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)

				output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))

			else:
				tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)
				output_label.append(0)

		return tokens, output_label

	def random_sent(self, index):
		t1, t2 = self.get_corpus_line(index)

		if random.random() > 0.5:
			return t1, t2, 1
		else:
			return t1, self.get_random_line(), 0

	def get_corpus_line(self, item):
		if self.on_memory:
			return self.lines[item][0], self.lines[item][1]
		else:
			line = self.file.__next__()
			if line is None:
				self.file.close()
				self.file = open(self.corpus_path, "r", encoding=self.encoding)
				line = self.file.__next__()

			t1, t2 = line[:-1].split("\t")
			return t1, t2

	def get_random_line(self):
		if self.on_memory:
			return self.lines[random.randrange(len(self.lines))][1]

		line = self.file.__next__()
		if line is None:
			self.file.close()
			self.file = open(self.corpus_path, "r", encoding=self.encoding)
			for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):
				self.random_file.__next__()
			line = self.random_file.__next__()
		return line[:-1].split("\t")[1]

import torch.nn

from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder
from allennlp.nn.util import get_final_encoder_states


@Seq2VecEncoder.register("cls_pooler")
class ClsPooler(Seq2VecEncoder):

	def __init__(self, embedding_dim: int, cls_is_last_token: bool = False):
		super().__init__()
		self._embedding_dim = embedding_dim
		self._cls_is_last_token = cls_is_last_token

	def get_input_dim(self) -> int:
		return self._embedding_dim

	def get_output_dim(self) -> int:
		return self._embedding_dim

	def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None):
		if not self._cls_is_last_token:
			return tokens[:, 0, :]
		else:  # [CLS] at the end
			if mask is None:
				raise ValueError("Must provide mask for transformer models with [CLS] at the end.")
			return get_final_encoder_states(tokens, mask)

import os
import zipfile

try:
	from torch.utils.model_zoo import _download_url_to_file
except ImportError:
	try:
		from torch.hub import download_url_to_file as _download_url_to_file
	except ImportError:
		from torch.hub import _download_url_to_file


def unzip(source_filename, dest_dir):
	with zipfile.ZipFile(source_filename) as zf:
		zf.extractall(path=dest_dir)


if __name__ == '__main__':
	_download_url_to_file('https://www.dropbox.com/s/lrvwfehqdcxoza8/saved_models.zip?dl=1', 'saved_models.zip', None, True)
	unzip('saved_models.zip', '.')

from .bert import BERT
from .language_model import BERTLM


import argparse
import json
import logging
import math
import os
import random
from itertools import chain
from pathlib import Path

import datasets
import torch
from accelerate import Accelerator, DistributedType
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_MAPPING,
	AutoConfig,
	AutoModelForMaskedLM,
	AutoTokenizer,
	DataCollatorForLanguageModeling,
	SchedulerType,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)
require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/language-modeling/requirements.txt")
MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a Masked Language Modeling task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--validation_split_percentage",
		default=5,
		help="The percentage of the train set used as validation set in case there's no validation split",
	)
	parser.add_argument(
		"--pad_to_max_length",
		action="store_true",
		help="If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=False,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--tokenizer_name",
		type=str,
		default=None,
		help="Pretrained tokenizer name or path if not the same as model_name",
	)
	parser.add_argument(
		"--use_slow_tokenizer",
		action="store_true",
		help="If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="Model type to use if training from scratch.",
		choices=MODEL_TYPES,
	)
	parser.add_argument(
		"--max_seq_length",
		type=int,
		default=None,
		help=(
			"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated."
		),
	)
	parser.add_argument(
		"--line_by_line",
		type=bool,
		default=False,
		help="Whether distinct lines of text in the dataset are to be handled as distinct sequences.",
	)
	parser.add_argument(
		"--preprocessing_num_workers",
		type=int,
		default=None,
		help="The number of processes to use for the preprocessing.",
	)
	parser.add_argument(
		"--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
	)
	parser.add_argument(
		"--mlm_probability", type=float, default=0.15, help="Ratio of tokens to mask for masked language modeling loss"
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	parser.add_argument(
		"--low_cpu_mem_usage",
		action="store_true",
		help=(
			"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. "
			"If passed, LLM loading time and RAM consumption will be benefited."
		),
	)
	args = parser.parse_args()

	if args.dataset_name is None and args.train_file is None and args.validation_file is None:
		raise ValueError("Need either a dataset name or a training/validation file.")
	else:
		if args.train_file is not None:
			extension = args.train_file.split(".")[-1]
			if extension not in ["csv", "json", "txt"]:
				raise ValueError("`train_file` should be a csv, json or txt file.")
		if args.validation_file is not None:
			extension = args.validation_file.split(".")[-1]
			if extension not in ["csv", "json", "txt"]:
				raise ValueError("`validation_file` should be a csv, json or txt file.")

	if args.push_to_hub:
		if args.output_dir is None:
			raise ValueError("Need an `output_dir` to create a repo when `--push_to_hub` is passed.")

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_mlm_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				args.dataset_name,
				args.dataset_config_name,
				split=f"train[:{args.validation_split_percentage}%]",
			)
			raw_datasets["train"] = load_dataset(
				args.dataset_name,
				args.dataset_config_name,
				split=f"train[{args.validation_split_percentage}%:]",
			)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		extension = args.train_file.split(".")[-1]
		if extension == "txt":
			extension = "text"
		raw_datasets = load_dataset(extension, data_files=data_files)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[:{args.validation_split_percentage}%]",
			)
			raw_datasets["train"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[{args.validation_split_percentage}%:]",
			)


	if args.config_name:
		config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")

	if args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(
			args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	elif args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(
			args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if args.model_name_or_path:
		model = AutoModelForMaskedLM.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			low_cpu_mem_usage=args.low_cpu_mem_usage,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForMaskedLM.from_config(config, trust_remote_code=args.trust_remote_code)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	column_names = raw_datasets["train"].column_names
	text_column_name = "text" if "text" in column_names else column_names[0]

	if args.max_seq_length is None:
		max_seq_length = tokenizer.model_max_length
		if max_seq_length > 1024:
			logger.warning(
				"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
				" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
				" override this default with `--block_size xxx`."
			)
			max_seq_length = 1024
	else:
		if args.max_seq_length > tokenizer.model_max_length:
			logger.warning(
				f"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the "
				f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
			)
		max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)

	if args.line_by_line:
		padding = "max_length" if args.pad_to_max_length else False

		def tokenize_function(examples):
			examples[text_column_name] = [
				line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()
			]
			return tokenizer(
				examples[text_column_name],
				padding=padding,
				truncation=True,
				max_length=max_seq_length,
				return_special_tokens_mask=True,
			)

		with accelerator.main_process_first():
			tokenized_datasets = raw_datasets.map(
				tokenize_function,
				batched=True,
				num_proc=args.preprocessing_num_workers,
				remove_columns=[text_column_name],
				load_from_cache_file=not args.overwrite_cache,
				desc="Running tokenizer on dataset line_by_line",
			)
	else:
		def tokenize_function(examples):
			return tokenizer(examples[text_column_name], return_special_tokens_mask=True)

		with accelerator.main_process_first():
			tokenized_datasets = raw_datasets.map(
				tokenize_function,
				batched=True,
				num_proc=args.preprocessing_num_workers,
				remove_columns=column_names,
				load_from_cache_file=not args.overwrite_cache,
				desc="Running tokenizer on every text in dataset",
			)

		def group_texts(examples):
			concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
			total_length = len(concatenated_examples[list(examples.keys())[0]])
			total_length = (total_length // max_seq_length) * max_seq_length
			result = {
				k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]
				for k, t in concatenated_examples.items()
			}
			return result


		with accelerator.main_process_first():
			tokenized_datasets = tokenized_datasets.map(
				group_texts,
				batched=True,
				num_proc=args.preprocessing_num_workers,
				load_from_cache_file=not args.overwrite_cache,
				desc=f"Grouping texts in chunks of {max_seq_length}",
			)

	train_dataset = tokenized_datasets["train"]
	eval_dataset = tokenized_datasets["validation"]

	if len(train_dataset) > 3:
		for index in random.sample(range(len(train_dataset)), 3):
			logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)


	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	if accelerator.distributed_type == DistributedType.TPU:
		model.tie_weights()

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("mlm_no_trainer", experiment_config)

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()
				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()
		losses = []
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				outputs = model(**batch)

			loss = outputs.loss
			losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

		losses = torch.cat(losses)
		try:
			eval_loss = torch.mean(losses)
			perplexity = math.exp(eval_loss)
		except OverflowError:
			perplexity = float("inf")

		logger.info(f"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}")

		if args.with_tracking:
			accelerator.log(
				{
					"perplexity": perplexity,
					"eval_loss": eval_loss,
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
				json.dump({"perplexity": perplexity}, f)


if __name__ == "__main__":
	main()

from allennlp.confidence_checks.task_checklists.task_suite import TaskSuite
from allennlp.confidence_checks.task_checklists.sentiment_analysis_suite import (
	SentimentAnalysisSuite,
)
from allennlp.confidence_checks.task_checklists.question_answering_suite import (
	QuestionAnsweringSuite,
)
from allennlp.confidence_checks.task_checklists.textual_entailment_suite import (
	TextualEntailmentSuite,
)


from allennlp.data.token_indexers.single_id_token_indexer import SingleIdTokenIndexer
from allennlp.data.token_indexers.token_characters_indexer import TokenCharactersIndexer
from allennlp.data.token_indexers.token_indexer import TokenIndexer
from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer
from allennlp.data.token_indexers.spacy_indexer import SpacyTokenIndexer
from allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer
from allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer import (
	PretrainedTransformerMismatchedIndexer,
)


import os
import logging
from dataclasses import dataclass
from typing import Optional, Union, Dict, Any, Callable
from allennlp.common.from_params import FromParams

from allennlp.models import Model
from allennlp.common.checks import ConfigurationError

logger = logging.getLogger(__name__)


def get_description(model_class):
	return model_class.__doc__.split("# Parameters")[0].strip()


class ModelCardInfo(FromParams):
	def to_dict(self):
		info = {}
		for key, val in self.__dict__.items():
			if val:
				info[key] = val
		return info

	def __str__(self):
		display = ""
		for key, val in self.to_dict().items():
			display += "\n" + key.replace("_", " ").capitalize() + ": "
			display += "\n\t" + str(val).replace("\n", "\n\t") + "\n"
		if not display:
			display = super(ModelCardInfo, self).__str__()
		return display.strip()


@dataclass(frozen=True)
class Paper(ModelCardInfo):

	title: Optional[str] = None
	url: Optional[str] = None
	citation: Optional[str] = None


class ModelDetails(ModelCardInfo):

	def __init__(
		self,
		description: Optional[str] = None,
		short_description: Optional[str] = None,
		developed_by: Optional[str] = None,
		contributed_by: Optional[str] = None,
		date: Optional[str] = None,
		version: Optional[str] = None,
		model_type: Optional[str] = None,
		paper: Optional[Union[str, Dict, Paper]] = None,
		license: Optional[str] = None,
		contact: Optional[str] = None,
	):
		self.description = description
		self.short_description = short_description
		self.developed_by = developed_by
		self.contributed_by = contributed_by
		self.date = date
		self.version = version
		self.model_type = model_type
		if isinstance(paper, Paper):
			self.paper = paper
		elif isinstance(paper, Dict):
			self.paper = Paper(**paper)
		else:
			self.paper = Paper(title=paper)
		self.license = license
		self.contact = contact


@dataclass(frozen=True)
class IntendedUse(ModelCardInfo):

	primary_uses: Optional[str] = None
	primary_users: Optional[str] = None
	out_of_scope_use_cases: Optional[str] = None


@dataclass(frozen=True)
class Factors(ModelCardInfo):

	relevant_factors: Optional[str] = None
	evaluation_factors: Optional[str] = None


@dataclass(frozen=True)
class Metrics(ModelCardInfo):

	model_performance_measures: Optional[str] = None
	decision_thresholds: Optional[str] = None
	variation_approaches: Optional[str] = None


@dataclass(frozen=True)
class Dataset(ModelCardInfo):

	name: Optional[str] = None
	url: Optional[str] = None
	processed_url: Optional[str] = None
	notes: Optional[str] = None


class EvaluationData(ModelCardInfo):

	def __init__(
		self,
		dataset: Optional[Union[str, Dict, Dataset]] = None,
		motivation: Optional[str] = None,
		preprocessing: Optional[str] = None,
	):
		if isinstance(dataset, Dataset):
			self.dataset = dataset
		elif isinstance(dataset, Dict):
			self.dataset = Dataset(**dataset)
		else:
			self.dataset = Dataset(name=dataset)
		self.motivation = motivation
		self.preprocessing = preprocessing

	def to_dict(self):
		info = {}
		for key, val in self.__dict__.items():
			if val:
				info["evaluation_" + key] = val
		return info


class TrainingData(ModelCardInfo):

	def __init__(
		self,
		dataset: Optional[Union[str, Dict, Dataset]] = None,
		motivation: Optional[str] = None,
		preprocessing: Optional[str] = None,
	):
		if isinstance(dataset, Dataset):
			self.dataset = dataset
		elif isinstance(dataset, Dict):
			self.dataset = Dataset(**dataset)
		else:
			self.dataset = Dataset(name=dataset)
		self.motivation = motivation
		self.preprocessing = preprocessing

	def to_dict(self):
		info = {}
		for key, val in self.__dict__.items():
			if val:
				info["training_" + key] = val
		return info


@dataclass(frozen=True)
class QuantitativeAnalyses(ModelCardInfo):

	unitary_results: Optional[str] = None
	intersectional_results: Optional[str] = None


@dataclass(frozen=True)
class ModelEthicalConsiderations(ModelCardInfo):

	ethical_considerations: Optional[str] = None


@dataclass(frozen=True)
class ModelCaveatsAndRecommendations(ModelCardInfo):

	caveats_and_recommendations: Optional[str] = None


class ModelUsage(ModelCardInfo):

	_storage_location = "https://storage.googleapis.com/allennlp-public-models/"
	_config_location = (
		"https://raw.githubusercontent.com/allenai/allennlp-models/main/training_config"
	)

	def __init__(
		self,
		archive_file: Optional[str] = None,
		training_config: Optional[str] = None,
		install_instructions: Optional[str] = None,
		overrides: Optional[Dict] = None,
	):

		if archive_file and not archive_file.startswith("https:"):
			archive_file = os.path.join(self._storage_location, archive_file)

		if training_config and not training_config.startswith("https:"):
			training_config = os.path.join(self._config_location, training_config)

		self.archive_file = archive_file
		self.training_config = training_config
		self.install_instructions = install_instructions
		self.overrides = overrides


class ModelCard(ModelCardInfo):

	def __init__(
		self,
		id: str,
		registered_model_name: Optional[str] = None,
		model_class: Optional[Callable[..., Model]] = None,
		registered_predictor_name: Optional[str] = None,
		display_name: Optional[str] = None,
		task_id: Optional[str] = None,
		model_usage: Optional[Union[str, ModelUsage]] = None,
		model_details: Optional[Union[str, ModelDetails]] = None,
		intended_use: Optional[Union[str, IntendedUse]] = None,
		factors: Optional[Union[str, Factors]] = None,
		metrics: Optional[Union[str, Metrics]] = None,
		evaluation_data: Optional[Union[str, EvaluationData]] = None,
		training_data: Optional[Union[str, TrainingData]] = None,
		quantitative_analyses: Optional[Union[str, QuantitativeAnalyses]] = None,
		model_ethical_considerations: Optional[Union[str, ModelEthicalConsiderations]] = None,
		model_caveats_and_recommendations: Optional[
			Union[str, ModelCaveatsAndRecommendations]
		] = None,
	):

		assert id
		if not model_class and registered_model_name:
			try:
				model_class = Model.by_name(registered_model_name)
			except ConfigurationError:
				logger.warning("{} is not a registered model.".format(registered_model_name))

		if model_class:
			display_name = display_name or model_class.__name__
			model_details = model_details or get_description(model_class)
			if not registered_predictor_name:
				registered_predictor_name = model_class.default_predictor  # type: ignore

		if isinstance(model_usage, str):
			model_usage = ModelUsage(archive_file=model_usage)
		if isinstance(model_details, str):
			model_details = ModelDetails(description=model_details)
		if isinstance(intended_use, str):
			intended_use = IntendedUse(primary_uses=intended_use)
		if isinstance(factors, str):
			factors = Factors(relevant_factors=factors)
		if isinstance(metrics, str):
			metrics = Metrics(model_performance_measures=metrics)
		if isinstance(evaluation_data, str):
			evaluation_data = EvaluationData(dataset=evaluation_data)
		if isinstance(training_data, str):
			training_data = TrainingData(dataset=training_data)
		if isinstance(quantitative_analyses, str):
			quantitative_analyses = QuantitativeAnalyses(unitary_results=quantitative_analyses)
		if isinstance(model_ethical_considerations, str):
			model_ethical_considerations = ModelEthicalConsiderations(model_ethical_considerations)
		if isinstance(model_caveats_and_recommendations, str):
			model_caveats_and_recommendations = ModelCaveatsAndRecommendations(
				model_caveats_and_recommendations
			)

		self.id = id
		self.registered_model_name = registered_model_name
		self.registered_predictor_name = registered_predictor_name
		self.display_name = display_name
		self.task_id = task_id
		self.model_usage = model_usage
		self.model_details = model_details
		self.intended_use = intended_use
		self.factors = factors
		self.metrics = metrics
		self.evaluation_data = evaluation_data
		self.training_data = training_data
		self.quantitative_analyses = quantitative_analyses
		self.model_ethical_considerations = model_ethical_considerations
		self.model_caveats_and_recommendations = model_caveats_and_recommendations

	def to_dict(self) -> Dict[str, Any]:
		info = {}
		for key, val in self.__dict__.items():
			if key != "id":
				if isinstance(val, ModelCardInfo):
					info.update(val.to_dict())
				else:
					if val is not None:
						info[key] = val
		return info

import torch
from PIL import Image


def load_image(filename, size=None, scale=None):
	img = Image.open(filename).convert('RGB')
	if size is not None:
		img = img.resize((size, size), Image.ANTIALIAS)
	elif scale is not None:
		img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)
	return img


def save_image(filename, data):
	img = data.clone().clamp(0, 255).numpy()
	img = img.transpose(1, 2, 0).astype("uint8")
	img = Image.fromarray(img)
	img.save(filename)


def gram_matrix(y):
	(b, ch, h, w) = y.size()
	features = y.view(b, ch, w * h)
	features_t = features.transpose(1, 2)
	gram = features.bmm(features_t) / (ch * h * w)
	return gram


def normalize_batch(batch):
	mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)
	std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)
	batch = batch.div_(255.0)
	return (batch - mean) / std

import copy
import logging
import re
import math
from typing import Any, Dict, List, Tuple, Union, Optional


import torch
import transformers

from allennlp.common import Params, Registrable, Lazy
from allennlp.common.checks import ConfigurationError

logger = logging.getLogger(__name__)


ParameterGroupsType = List[Tuple[List[str], Dict[str, Any]]]


def make_parameter_groups(
	model_parameters: List[Tuple[str, torch.nn.Parameter]],
	groups: Optional[ParameterGroupsType] = None,
) -> Union[List[Dict[str, Any]], List[torch.nn.Parameter]]:
	if groups:
		parameter_groups: Union[List[Dict[str, Any]], List[torch.nn.Parameter]] = [
			{"params": []} for _ in range(len(groups) + 1)
		]
		for k in range(len(groups)):
			parameter_groups[k].update(groups[k][1])

		regex_use_counts: Dict[str, int] = {}
		parameter_group_names: List[set] = [set() for _ in range(len(groups) + 1)]
		for name, param in model_parameters:
			group_index = None
			for k, group_regexes in enumerate(groups):
				for regex in group_regexes[0]:
					if regex not in regex_use_counts:
						regex_use_counts[regex] = 0
					if re.search(regex, name):
						if group_index is not None and group_index != k:
							raise ValueError(
								"{} was specified in two separate parameter groups".format(name)
							)
						group_index = k
						regex_use_counts[regex] += 1

			if group_index is not None:
				parameter_groups[group_index]["params"].append(param)
				parameter_group_names[group_index].add(name)
			else:
				parameter_groups[-1]["params"].append(param)
				parameter_group_names[-1].add(name)

		no_grad_group_indices: List[int] = []
		for k, (names, group) in enumerate(zip(parameter_group_names, parameter_groups)):
			if group.get("requires_grad") is False:
				no_grad_group_indices.append(k)
				logger.info("Disabling gradient for the following parameters: %s", names)
				for param in group["params"]:
					param.requires_grad_(False)

				unused_options = {
					key: val for key, val in group.items() if key not in ("params", "requires_grad")
				}
				if unused_options:
					logger.warning("Ignoring unused options %s for %s", unused_options, names)
		parameter_group_names = [
			names
			for (k, names) in enumerate(parameter_group_names)
			if k not in no_grad_group_indices
		]
		parameter_groups = [
			group for (k, group) in enumerate(parameter_groups) if k not in no_grad_group_indices
		]

		logger.info("Done constructing parameter groups.")
		for k in range(len(parameter_groups)):
			group_options = {
				key: val for key, val in parameter_groups[k].items() if key != "params"
			}
			logger.info("Group %s: %s, %s", k, list(parameter_group_names[k]), group_options)

		for regex, count in regex_use_counts.items():
			if count == 0:
				logger.warning(
					"When constructing parameter groups, %s does not match any parameter name",
					regex,
				)
	else:
		parameter_groups = [param for name, param in model_parameters]

	num_parameters = 0
	for parameter_group in parameter_groups:
		if isinstance(parameter_group, dict):
			num_parameters += sum(parameter.numel() for parameter in parameter_group["params"])
		else:
			num_parameters += parameter_group.numel()  # type: ignore
	logger.info("Number of trainable parameters: %s", num_parameters)

	return parameter_groups


class Optimizer(torch.optim.Optimizer, Registrable):

	default_implementation = "adam"

	@staticmethod
	def default(model_parameters: List) -> "Optimizer":
		return Optimizer.from_params(model_parameters=model_parameters, params=Params({}))


@Optimizer.register("multi")
class MultiOptimizer(Optimizer):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		optimizers: Dict[str, Lazy[Optimizer]],
		parameter_groups: ParameterGroupsType,
	):
		if "default" not in optimizers:
			raise ConfigurationError(
				"No optimizer was provided for the 'default' group."
				" Please provide an Optimizer under the name 'default'"
			)

		optimizer_name_to_parameter_groups: Dict[str, ParameterGroupsType] = {
			optimizer_name: [] for optimizer_name in optimizers.keys()
		}
		for parameter_group in parameter_groups:
			regexes, pg_overrides = parameter_group
			optimizer_name = pg_overrides.get("optimizer_name", "default")
			optimizer_name_to_parameter_groups[optimizer_name].append(parameter_group)

		optimizer_name_to_model_parameters: Dict[str, List[Tuple[str, torch.nn.Parameter]]] = {
			optimizer_name: [] for optimizer_name in optimizers.keys()
		}
		for model_parameter_tuple in model_parameters:
			parameter_name, parameter_tensor = model_parameter_tuple
			for regexes, pg_overrides in parameter_groups:
				if any(re.search(regex, parameter_name) for regex in regexes):
					optimizer_name = pg_overrides.get("optimizer_name", "default")
					optimizer_name_to_model_parameters[optimizer_name].append(model_parameter_tuple)
					break
			else:
				optimizer_name_to_model_parameters["default"].append(model_parameter_tuple)

		for optimizer_name, optimizer_parameters in optimizer_name_to_model_parameters.items():
			if optimizer_name != "default" and len(optimizer_parameters) == 0:
				raise ConfigurationError(
					f"Optimizer '{optimizer_name}' did not receive any parameters."
					" If you are using `parameter_groups`, please make sure that the regexes you have provided"
					" match the desired model parameters, or that the `name` value of this optimizer "
					" matches that of the parameter group you are trying to assign to it."
					" Alternatively, you can remove this optimizer from the provided `optimizers`"
					" if it is not relevant to a particular parameter group."
				)
		if len(optimizer_name_to_model_parameters["default"]) == 0:
			del optimizers["default"]
			del optimizer_name_to_model_parameters["default"]
			del optimizer_name_to_parameter_groups["default"]

		self.optimizers = {
			optimizer_name: lazy_optimizer.construct(
				model_parameters=optimizer_name_to_model_parameters[optimizer_name],
				parameter_groups=optimizer_name_to_parameter_groups[optimizer_name],
			)
			for optimizer_name, lazy_optimizer in optimizers.items()
		}

		parameter_groups = copy.deepcopy(parameter_groups)
		for parameter_group in parameter_groups:
			regexes, pg_overrides = parameter_group
			optimizer_name = pg_overrides.get("optimizer_name", "default")
			optimizer = self.optimizers[optimizer_name]
			for key, value in optimizer.defaults.items():
				if key not in pg_overrides:
					pg_overrides[key] = value

		made_parameter_groups = make_parameter_groups(model_parameters, parameter_groups)
		if "default" in self.optimizers:
			for key, value in self.optimizers["default"].defaults.items():
				made_parameter_groups[-1][key] = value

		super().__init__(made_parameter_groups, {})

	def step(self):
		for optimizer in self.optimizers.values():
			optimizer.step()

	def state_dict(self):
		optimizer_state_dict = {
			f"{optimizer_key}_optimizer": optimizer.state_dict()
			for optimizer_key, optimizer in self.optimizers.items()
		}

		return optimizer_state_dict

	def load_state_dict(self, training_state: Dict[str, Any]):
		for optimizer_key, optimizer in self.optimizers.items():
			optimizer.load_state_dict(training_state[f"{optimizer_key}_optimizer"])

	def zero_grad(self, set_to_none: bool = False):
		for optimizer in self.optimizers.values():
			optimizer.zero_grad(set_to_none)


@Optimizer.register("adam")
class AdamOptimizer(Optimizer, torch.optim.Adam):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 0.001,
		betas: Tuple[float, float] = (0.9, 0.999),
		eps: float = 1e-08,
		weight_decay: float = 0.0,
		amsgrad: bool = False,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			betas=betas,
			eps=eps,
			weight_decay=weight_decay,
			amsgrad=amsgrad,
		)


@Optimizer.register("sparse_adam")
class SparseAdamOptimizer(Optimizer, torch.optim.SparseAdam):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 0.001,
		betas: Tuple[float, float] = (0.9, 0.999),
		eps: float = 1e-08,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			betas=betas,
			eps=eps,
		)


@Optimizer.register("adamax")
class AdamaxOptimizer(Optimizer, torch.optim.Adamax):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 0.002,
		betas: Tuple[float, float] = (0.9, 0.999),
		eps: float = 1e-08,
		weight_decay: float = 0.0,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			betas=betas,
			eps=eps,
			weight_decay=weight_decay,
		)


@Optimizer.register("adamw")
class AdamWOptimizer(Optimizer, torch.optim.AdamW):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 0.001,
		betas: Tuple[float, float] = (0.9, 0.999),
		eps: float = 1e-08,
		weight_decay: float = 0.01,
		amsgrad: bool = False,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			betas=betas,
			eps=eps,
			weight_decay=weight_decay,
			amsgrad=amsgrad,
		)


@Optimizer.register("huggingface_adamw")
class HuggingfaceAdamWOptimizer(Optimizer, transformers.AdamW):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 1e-5,
		betas: Tuple[float, float] = (0.9, 0.999),
		eps: float = 1e-08,
		weight_decay: float = 0.0,
		correct_bias: bool = True,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			betas=betas,
			eps=eps,
			weight_decay=weight_decay,
			correct_bias=correct_bias,
		)


@Optimizer.register("huggingface_adafactor")
class HuggingfaceAdafactor(Optimizer, transformers.Adafactor):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: Optional[float] = None,
		eps: Tuple[float, float] = (1e-30, 1e-3),
		clip_threshold: float = 1.0,
		decay_rate: float = -0.8,
		beta1: Optional[float] = None,
		weight_decay: float = 0.0,
		scale_parameter: bool = True,
		relative_step: bool = True,
		warmup_init: bool = False,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			eps=eps,
			clip_threshold=clip_threshold,
			decay_rate=decay_rate,
			beta1=beta1,
			weight_decay=weight_decay,
			scale_parameter=scale_parameter,
			relative_step=relative_step,
			warmup_init=warmup_init,
		)


@Optimizer.register("adagrad")
class AdagradOptimizer(Optimizer, torch.optim.Adagrad):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 0.01,
		lr_decay: float = 0.0,
		weight_decay: float = 0.0,
		initial_accumulator_value: float = 0.0,
		eps: float = 1e-10,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			lr_decay=lr_decay,
			weight_decay=weight_decay,
			initial_accumulator_value=initial_accumulator_value,
			eps=eps,
		)


@Optimizer.register("adadelta")
class AdadeltaOptimizer(Optimizer, torch.optim.Adadelta):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 1.0,
		rho: float = 0.9,
		eps: float = 1e-06,
		weight_decay: float = 0.0,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			rho=rho,
			eps=eps,
			weight_decay=weight_decay,
		)


@Optimizer.register("sgd")
class SgdOptimizer(Optimizer, torch.optim.SGD):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		lr: float,
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		momentum: float = 0.0,
		dampening: float = 0,
		weight_decay: float = 0.0,
		nesterov: bool = False,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			momentum=momentum,
			dampening=dampening,
			weight_decay=weight_decay,
			nesterov=nesterov,
		)


@Optimizer.register("rmsprop")
class RmsPropOptimizer(Optimizer, torch.optim.RMSprop):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 0.01,
		alpha: float = 0.99,
		eps: float = 1e-08,
		weight_decay: float = 0.0,
		momentum: float = 0.0,
		centered: bool = False,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			alpha=alpha,
			eps=eps,
			weight_decay=weight_decay,
			momentum=momentum,
			centered=centered,
		)


@Optimizer.register("averaged_sgd")
class AveragedSgdOptimizer(Optimizer, torch.optim.ASGD):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr: float = 0.01,
		lambd: float = 0.0001,
		alpha: float = 0.75,
		t0: float = 1000000.0,
		weight_decay: float = 0.0,
	):
		super().__init__(
			params=make_parameter_groups(model_parameters, parameter_groups),
			lr=lr,
			lambd=lambd,
			alpha=alpha,
			t0=t0,
			weight_decay=weight_decay,
		)


@Optimizer.register("dense_sparse_adam")
class DenseSparseAdam(Optimizer, torch.optim.Optimizer):

	def __init__(
		self,
		model_parameters: List[Tuple[str, torch.nn.Parameter]],
		parameter_groups: List[Tuple[List[str], Dict[str, Any]]] = None,
		lr=1e-3,
		betas=(0.9, 0.999),
		eps=1e-8,
	):
		if not 0.0 <= lr:
			raise ValueError("Invalid learning rate: {}".format(lr))
		if not 0.0 <= eps:
			raise ValueError("Invalid epsilon value: {}".format(eps))
		if not 0.0 <= betas[0] < 1.0:
			raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
		if not 0.0 <= betas[1] < 1.0:
			raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
		defaults = dict(lr=lr, betas=betas, eps=eps)
		super().__init__(make_parameter_groups(model_parameters, parameter_groups), defaults)

	def step(self, closure=None):
		loss = None
		if closure is not None:
			loss = closure()

		for group in self.param_groups:
			for p in group["params"]:
				if p.grad is None:
					continue
				grad = p.grad.data

				state = self.state[p]

				if len(state) == 0:
					state["step"] = 0
					state["exp_avg"] = torch.zeros_like(p.data)
					state["exp_avg_sq"] = torch.zeros_like(p.data)

				state["step"] += 1

				exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
				beta1, beta2 = group["betas"]

				if grad.is_sparse:
					grad = grad.coalesce()  # the update is non-linear so indices must be unique
					grad_indices = grad._indices()
					grad_values = grad._values()
					size = grad.size()

					def make_sparse(values):
						constructor = grad.new
						if grad_indices.dim() == 0 or values.dim() == 0:
							return constructor().resize_as_(grad)
						return constructor(grad_indices, values, size)

					old_exp_avg_values = exp_avg.sparse_mask(grad)._values()
					exp_avg_update_values = grad_values.sub(old_exp_avg_values).mul_(1 - beta1)
					exp_avg.add_(make_sparse(exp_avg_update_values))
					old_exp_avg_sq_values = exp_avg_sq.sparse_mask(grad)._values()
					exp_avg_sq_update_values = (
						grad_values.pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2)
					)
					exp_avg_sq.add_(make_sparse(exp_avg_sq_update_values))

					numer = exp_avg_update_values.add_(old_exp_avg_values)
					exp_avg_sq_update_values.add_(old_exp_avg_sq_values)
					denom = exp_avg_sq_update_values.sqrt_().add_(group["eps"])
					del exp_avg_update_values, exp_avg_sq_update_values

					bias_correction1 = 1 - beta1 ** state["step"]
					bias_correction2 = 1 - beta2 ** state["step"]
					step_size = group["lr"] * math.sqrt(bias_correction2) / bias_correction1

					p.data.add_(make_sparse(-step_size * numer.div_(denom)))

				else:
					exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
					exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
					denom = exp_avg_sq.sqrt().add_(group["eps"])

					bias_correction1 = 1 - beta1 ** state["step"]
					bias_correction2 = 1 - beta2 ** state["step"]
					step_size = group["lr"] * math.sqrt(bias_correction2) / bias_correction1

					p.data.addcdiv_(exp_avg, denom, value=-step_size)

		return loss

import numpy as np


class ScheduledOptim():

	def __init__(self, optimizer, d_model, n_warmup_steps):
		self._optimizer = optimizer
		self.n_warmup_steps = n_warmup_steps
		self.n_current_steps = 0
		self.init_lr = np.power(d_model, -0.5)

	def step_and_update_lr(self):
		"Step with the inner optimizer"
		self._update_learning_rate()
		self._optimizer.step()

	def zero_grad(self):
		"Zero out the gradients by the inner optimizer"
		self._optimizer.zero_grad()

	def _get_lr_scale(self):
		return np.min([
			np.power(self.n_current_steps, -0.5),
			np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])

	def _update_learning_rate(self):

		self.n_current_steps += 1
		lr = self.init_lr * self._get_lr_scale()

		for param_group in self._optimizer.param_groups:
			param_group['lr'] = lr

import math

import torch
from torch.nn import Parameter

from allennlp.modules.attention.attention import Attention
from allennlp.nn import util
from allennlp.nn.activations import Activation


@Attention.register("linear")
class LinearAttention(Attention):

	def __init__(
		self,
		tensor_1_dim: int,
		tensor_2_dim: int,
		combination: str = "x,y",
		activation: Activation = None,
		normalize: bool = True,
	) -> None:
		super().__init__(normalize)
		self._combination = combination
		combined_dim = util.get_combined_dim(combination, [tensor_1_dim, tensor_2_dim])
		self._weight_vector = Parameter(torch.Tensor(combined_dim))
		self._bias = Parameter(torch.Tensor(1))
		self._activation = activation or Activation.by_name("linear")()
		self.reset_parameters()

	def reset_parameters(self):
		std = math.sqrt(6 / (self._weight_vector.size(0) + 1))
		self._weight_vector.data.uniform_(-std, std)
		self._bias.data.fill_(0)

	def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:
		combined_tensors = util.combine_tensors_and_multiply(
			self._combination, [vector.unsqueeze(1), matrix], self._weight_vector
		)
		return self._activation(combined_tensors.squeeze(1) + self._bias)

import os
import tempfile
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim

from torch.nn.parallel import DistributedDataParallel as DDP


def setup(rank, world_size):
	os.environ['MASTER_ADDR'] = 'localhost'
	os.environ['MASTER_PORT'] = '12355'

	dist.init_process_group("gloo", rank=rank, world_size=world_size)


def cleanup():
	dist.destroy_process_group()


class ToyModel(nn.Module):
	def __init__(self):
		super(ToyModel, self).__init__()
		self.net1 = nn.Linear(10, 10)
		self.relu = nn.ReLU()
		self.net2 = nn.Linear(10, 5)

	def forward(self, x):
		return self.net2(self.relu(self.net1(x)))


def demo_basic(rank, world_size):
	print(f"Running basic DDP example on rank {rank}.")
	setup(rank, world_size)

	model = ToyModel().to(rank)
	ddp_model = DDP(model, device_ids=[rank])

	loss_fn = nn.MSELoss()
	optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

	optimizer.zero_grad()
	outputs = ddp_model(torch.randn(20, 10))
	labels = torch.randn(20, 5).to(rank)
	loss_fn(outputs, labels).backward()
	optimizer.step()

	cleanup()


def run_demo(demo_fn, world_size):
	mp.spawn(demo_fn,
			 args=(world_size,),
			 nprocs=world_size,
			 join=True)


def demo_checkpoint(rank, world_size):
	print(f"Running DDP checkpoint example on rank {rank}.")
	setup(rank, world_size)

	model = ToyModel().to(rank)
	ddp_model = DDP(model, device_ids=[rank])

	loss_fn = nn.MSELoss()
	optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

	CHECKPOINT_PATH = tempfile.gettempdir() + "/model.checkpoint"
	if rank == 0:
		torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)

	dist.barrier()
	map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
	ddp_model.load_state_dict(
		torch.load(CHECKPOINT_PATH, map_location=map_location))

	optimizer.zero_grad()
	outputs = ddp_model(torch.randn(20, 10))
	labels = torch.randn(20, 5).to(rank)
	loss_fn = nn.MSELoss()
	loss_fn(outputs, labels).backward()
	optimizer.step()

	dist.barrier()

	if rank == 0:
		os.remove(CHECKPOINT_PATH)

	cleanup()


class ToyMpModel(nn.Module):
	def __init__(self, dev0, dev1):
		super(ToyMpModel, self).__init__()
		self.dev0 = dev0
		self.dev1 = dev1
		self.net1 = torch.nn.Linear(10, 10).to(dev0)
		self.relu = torch.nn.ReLU()
		self.net2 = torch.nn.Linear(10, 5).to(dev1)

	def forward(self, x):
		x = x.to(self.dev0)
		x = self.relu(self.net1(x))
		x = x.to(self.dev1)
		return self.net2(x)


def demo_model_parallel(rank, world_size):
	print(f"Running DDP with model parallel example on rank {rank}.")
	setup(rank, world_size)

	dev0 = rank * 2
	dev1 = rank * 2 + 1
	mp_model = ToyMpModel(dev0, dev1)
	ddp_mp_model = DDP(mp_model)

	loss_fn = nn.MSELoss()
	optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)

	optimizer.zero_grad()
	outputs = ddp_mp_model(torch.randn(20, 10))
	labels = torch.randn(20, 5).to(dev1)
	loss_fn(outputs, labels).backward()
	optimizer.step()

	cleanup()


if __name__ == "__main__":
	n_gpus = torch.cuda.device_count()
	if n_gpus < 8:
		print(f"Requires at least 8 GPUs to run, but got {n_gpus}.")
	else:
		run_demo(demo_basic, 8)
		run_demo(demo_checkpoint, 8)
		run_demo(demo_model_parallel, 4)

from typing import List, Dict


import numpy

from allennlp.common.util import JsonDict
from allennlp.data import Instance
from allennlp.predictors.predictor import Predictor
from allennlp.data.fields import LabelField
from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer


@Predictor.register("text_classifier")
class TextClassifierPredictor(Predictor):

	def predict(self, sentence: str) -> JsonDict:
		return self.predict_json({"sentence": sentence})

	def _json_to_instance(self, json_dict: JsonDict) -> Instance:
		sentence = json_dict["sentence"]
		reader_has_tokenizer = (
			getattr(self._dataset_reader, "tokenizer", None) is not None
			or getattr(self._dataset_reader, "_tokenizer", None) is not None
		)
		if not reader_has_tokenizer:
			tokenizer = SpacyTokenizer()
			sentence = tokenizer.tokenize(sentence)
		return self._dataset_reader.text_to_instance(sentence)

	def predictions_to_labeled_instances(
		self, instance: Instance, outputs: Dict[str, numpy.ndarray]
	) -> List[Instance]:
		new_instance = instance.duplicate()
		label = numpy.argmax(outputs["probs"])
		new_instance.add_field("label", LabelField(int(label), skip_indexing=True))
		return [new_instance]

import string
from typing import Dict, Callable, List, Union
import numpy as np
import spacy
from checklist.editor import Editor


def add_common_lexicons(editor: Editor):
	profession = [
		"journalist",
		"historian",
		"secretary",
		"nurse",
		"waitress",
		"accountant",
		"engineer",
		"attorney",
		"artist",
		"editor",
		"architect",
		"model",
		"interpreter",
		"analyst",
		"actor",
		"actress",
		"assistant",
		"intern",
		"economist",
		"organizer",
		"author",
		"investigator",
		"agent",
		"administrator",
		"executive",
		"educator",
		"investor",
		"DJ",
		"entrepreneur",
		"auditor",
		"advisor",
		"instructor",
		"activist",
		"consultant",
		"apprentice",
		"reporter",
		"expert",
		"psychologist",
		"examiner",
		"painter",
		"manager",
		"contractor",
		"therapist",
		"programmer",
		"musician",
		"producer",
		"associate",
		"intermediary",
		"designer",
		"cook",
		"salesperson",
		"dentist",
		"attorney",
		"detective",
		"banker",
		"researcher",
		"cop",
		"driver",
		"counselor",
		"clerk",
		"professor",
		"tutor",
		"coach",
		"chemist",
		"scientist",
		"veterinarian",
		"firefighter",
		"baker",
		"psychiatrist",
		"prosecutor",
		"director",
		"technician",
	]

	editor.add_lexicon("profession", profession, overwrite=True)


def spacy_wrap(fn: Callable, language: str = "en_core_web_sm", **kwargs) -> Callable:
	from allennlp.common.util import get_spacy_model

	def new_fn(data: Union[spacy.tokens.doc.Doc, Dict, str]):
		if not isinstance(data, spacy.tokens.doc.Doc):
			model = get_spacy_model(language, **kwargs)
			if isinstance(data, Dict):
				for key, val in data.items():
					if isinstance(val, str):
						data[key] = model(val)
			elif isinstance(data, tuple):
				data = tuple(model(tup) if isinstance(tup, str) else tup for tup in data)
			elif isinstance(data, str):
				data = model(data)
			else:
				pass
		return fn(data)

	return new_fn


def strip_punctuation(data: Union[str, spacy.tokens.doc.Doc]) -> str:
	if isinstance(data, str):
		return data.rstrip(string.punctuation)
	elif isinstance(data, spacy.tokens.doc.Doc):
		while len(data) and data[-1].is_punct:
			data = data[:-1]
	else:
		pass
	return str(data)


def toggle_punctuation(data: str) -> List[str]:
	s = strip_punctuation(data)
	ret = []
	if s != data:
		ret.append(s)
	if s + "." != data:
		ret.append(s + ".")
	return ret


def random_string(n: int) -> str:
	return "".join(np.random.choice([x for x in string.ascii_letters + string.digits], n))


def random_url(n: int = 6) -> str:
	return "https://t.co/%s" % random_string(n)


def random_handle(n: int = 6) -> str:
	return "@%s" % random_string(n)


def add_random_strings(data: str) -> List[str]:
	urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]
	rets = ["%s %s" % (x, data) for x in urls_and_handles]
	rets += ["%s %s" % (data, x) for x in urls_and_handles]
	return rets

from typing import Optional, Dict, Any


import torch

from allennlp.common.checks import ConfigurationError
from allennlp.modules.token_embedders import PretrainedTransformerEmbedder, TokenEmbedder
from allennlp.nn import util


@TokenEmbedder.register("pretrained_transformer_mismatched")
class PretrainedTransformerMismatchedEmbedder(TokenEmbedder):

	def __init__(
		self,
		model_name: str,
		max_length: int = None,
		sub_module: str = None,
		train_parameters: bool = True,
		last_layer_only: bool = True,
		override_weights_file: Optional[str] = None,
		override_weights_strip_prefix: Optional[str] = None,
		load_weights: bool = True,
		gradient_checkpointing: Optional[bool] = None,
		tokenizer_kwargs: Optional[Dict[str, Any]] = None,
		transformer_kwargs: Optional[Dict[str, Any]] = None,
		sub_token_mode: Optional[str] = "avg",
	) -> None:
		super().__init__()
		self._matched_embedder = PretrainedTransformerEmbedder(
			model_name,
			max_length=max_length,
			sub_module=sub_module,
			train_parameters=train_parameters,
			last_layer_only=last_layer_only,
			override_weights_file=override_weights_file,
			override_weights_strip_prefix=override_weights_strip_prefix,
			load_weights=load_weights,
			gradient_checkpointing=gradient_checkpointing,
			tokenizer_kwargs=tokenizer_kwargs,
			transformer_kwargs=transformer_kwargs,
		)
		self.sub_token_mode = sub_token_mode

	def get_output_dim(self):
		return self._matched_embedder.get_output_dim()

	def forward(
		self,
		token_ids: torch.LongTensor,
		mask: torch.BoolTensor,
		offsets: torch.LongTensor,
		wordpiece_mask: torch.BoolTensor,
		type_ids: Optional[torch.LongTensor] = None,
		segment_concat_mask: Optional[torch.BoolTensor] = None,
	) -> torch.Tensor:  # type: ignore
		embeddings = self._matched_embedder(
			token_ids, wordpiece_mask, type_ids=type_ids, segment_concat_mask=segment_concat_mask
		)

		span_embeddings, span_mask = util.batched_span_select(embeddings.contiguous(), offsets)

		span_mask = span_mask.unsqueeze(-1)

		span_embeddings *= span_mask  # zero out paddings

		if self.sub_token_mode == "first":
			orig_embeddings = span_embeddings[:, :, 0, :]

		elif self.sub_token_mode == "avg":
			span_embeddings_sum = span_embeddings.sum(2)

			span_embeddings_len = span_mask.sum(2)

			orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)

			orig_embeddings[(span_embeddings_len == 0).expand(orig_embeddings.shape)] = 0

		else:
			raise ConfigurationError(f"Do not recognise 'sub_token_mode' {self.sub_token_mode}")

		return orig_embeddings

from typing import List, Tuple

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.modules.conditional_random_field.conditional_random_field import (
	ConditionalRandomField,
)


class ConditionalRandomFieldWeightLannoy(ConditionalRandomField):

	def __init__(
		self,
		num_tags: int,
		label_weights: List[float],
		constraints: List[Tuple[int, int]] = None,
		include_start_end_transitions: bool = True,
	) -> None:
		super().__init__(num_tags, constraints, include_start_end_transitions)

		if label_weights is None:
			raise ConfigurationError("label_weights must be given")

		self.register_buffer("label_weights", torch.Tensor(label_weights))

	def forward(
		self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor = None
	) -> torch.Tensor:
		if mask is None:
			mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
		else:
			mask = mask.to(torch.bool)

		log_denominator = self._input_likelihood_lannoy(inputs, tags, mask)
		log_numerator = self._joint_likelihood_lannoy(inputs, tags, mask)

		return torch.sum(log_numerator - log_denominator)

	def _input_likelihood_lannoy(
		self, logits: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor
	) -> torch.Tensor:
		batch_size, sequence_length, num_tags = logits.size()

		mask = mask.transpose(0, 1).contiguous()
		logits = logits.transpose(0, 1).contiguous()
		tags = tags.transpose(0, 1).contiguous()

		label_weights = self.label_weights.view(num_tags, 1)

		emit_scores = logits[0]

		if self.include_start_end_transitions:
			alpha = torch.exp(self.start_transitions.view(1, num_tags) + emit_scores)
		else:
			alpha = torch.exp(emit_scores)

		z = alpha.sum(dim=1, keepdim=True)
		alpha = alpha / z
		sum_log_z = torch.log(z) * label_weights[tags[0]]

		for i in range(1, sequence_length):
			emit_scores = logits[i]

			emit_scores = emit_scores.view(batch_size, 1, num_tags)
			transition_scores = self.transitions.view(1, num_tags, num_tags)
			broadcast_alpha = alpha.view(batch_size, num_tags, 1)

			inner = broadcast_alpha * torch.exp(emit_scores + transition_scores)

			alpha = inner.sum(dim=1) * mask[i].view(batch_size, 1) + alpha * (~mask[i]).view(
				batch_size, 1
			)

			z = alpha.sum(dim=1, keepdim=True)
			alpha = alpha / z
			sum_log_z += torch.log(z) * label_weights[tags[i]]

		if self.include_start_end_transitions:
			alpha = alpha * torch.exp(self.end_transitions.view(1, num_tags))
			z = alpha.sum(dim=1, keepdim=True)
			sum_log_z += torch.log(z)

		return sum_log_z.squeeze(1)

	def _joint_likelihood_lannoy(
		self, logits: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor
	) -> torch.Tensor:
		batch_size, sequence_length, _ = logits.data.shape

		logits = logits.transpose(0, 1).contiguous()
		mask = mask.transpose(0, 1).contiguous()
		tags = tags.transpose(0, 1).contiguous()

		if self.include_start_end_transitions:
			score = self.start_transitions.index_select(0, tags[0])
		else:
			score = 0.0

		label_weights = self.label_weights

		transitions = self.transitions * label_weights.view(-1, 1)

		for i in range(sequence_length - 1):
			current_tag, next_tag = tags[i], tags[i + 1]

			transition_score = transitions[current_tag.view(-1), next_tag.view(-1)]

			emit_score = logits[i].gather(1, current_tag.view(batch_size, 1)).squeeze(1)

			emit_score *= label_weights[current_tag.view(-1)]

			score = score + transition_score * mask[i + 1] + emit_score * mask[i]

		last_tag_index = mask.sum(0).long() - 1
		last_tags = tags.gather(0, last_tag_index.view(1, batch_size)).squeeze(0)

		if self.include_start_end_transitions:
			last_transition_score = self.end_transitions.index_select(0, last_tags)
		else:
			last_transition_score = 0.0

		last_inputs = logits[-1]  # (batch_size, num_tags)
		last_input_score = last_inputs.gather(1, last_tags.view(-1, 1))  # (batch_size, 1)
		last_input_score = last_input_score.squeeze()  # (batch_size,)

		last_input_score = last_input_score * label_weights[last_tags.view(-1)]

		score = score + last_transition_score + last_input_score * mask[-1]

		return score


import logging
import os
import random
import sys
import warnings
from dataclasses import dataclass, field
from typing import Optional

import datasets
import evaluate
import numpy as np
from datasets import load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoModelForSequenceClassification,
	AutoTokenizer,
	DataCollatorWithPadding,
	EvalPrediction,
	HfArgumentParser,
	PretrainedConfig,
	Trainer,
	TrainingArguments,
	default_data_collator,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/text-classification/requirements.txt")

task_to_keys = {
	"cola": ("sentence", None),
	"mnli": ("premise", "hypothesis"),
	"mrpc": ("sentence1", "sentence2"),
	"qnli": ("question", "sentence"),
	"qqp": ("question1", "question2"),
	"rte": ("sentence1", "sentence2"),
	"sst2": ("sentence", None),
	"stsb": ("sentence1", "sentence2"),
	"wnli": ("sentence1", "sentence2"),
}

logger = logging.getLogger(__name__)


@dataclass
class DataTrainingArguments:

	task_name: Optional[str] = field(
		default=None,
		metadata={"help": "The name of the task to train on: " + ", ".join(task_to_keys.keys())},
	)
	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	max_seq_length: int = field(
		default=128,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
	)
	pad_to_max_length: bool = field(
		default=True,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch."
			)
		},
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	train_file: Optional[str] = field(
		default=None, metadata={"help": "A csv or a json file containing the training data."}
	)
	validation_file: Optional[str] = field(
		default=None, metadata={"help": "A csv or a json file containing the validation data."}
	)
	test_file: Optional[str] = field(default=None, metadata={"help": "A csv or a json file containing the test data."})

	def __post_init__(self):
		if self.task_name is not None:
			self.task_name = self.task_name.lower()
			if self.task_name not in task_to_keys.keys():
				raise ValueError("Unknown task, you should pick one in " + ",".join(task_to_keys.keys()))
		elif self.dataset_name is not None:
			pass
		elif self.train_file is None or self.validation_file is None:
			raise ValueError("Need either a GLUE task, a training/validation file or a dataset name.")
		else:
			train_extension = self.train_file.split(".")[-1]
			assert train_extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			validation_extension = self.validation_file.split(".")[-1]
			assert (
				validation_extension == train_extension
			), "`validation_file` should have the same extension (csv or json) as `train_file`."


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	ignore_mismatched_sizes: bool = field(
		default=False,
		metadata={"help": "Will enable to load a pretrained model whose head dimensions are different."},
	)


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_glue", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.task_name is not None:
		raw_datasets = load_dataset(
			"glue",
			data_args.task_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	elif data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
	else:
		data_files = {"train": data_args.train_file, "validation": data_args.validation_file}

		if training_args.do_predict:
			if data_args.test_file is not None:
				train_extension = data_args.train_file.split(".")[-1]
				test_extension = data_args.test_file.split(".")[-1]
				assert (
					test_extension == train_extension
				), "`test_file` should have the same extension (csv or json) as `train_file`."
				data_files["test"] = data_args.test_file
			else:
				raise ValueError("Need either a GLUE task or a test file for `do_predict`.")

		for key in data_files.keys():
			logger.info(f"load a local file for {key}: {data_files[key]}")

		if data_args.train_file.endswith(".csv"):
			raw_datasets = load_dataset(
				"csv",
				data_files=data_files,
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
		else:
			raw_datasets = load_dataset(
				"json",
				data_files=data_files,
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)

	if data_args.task_name is not None:
		is_regression = data_args.task_name == "stsb"
		if not is_regression:
			label_list = raw_datasets["train"].features["label"].names
			num_labels = len(label_list)
		else:
			num_labels = 1
	else:
		is_regression = raw_datasets["train"].features["label"].dtype in ["float32", "float64"]
		if is_regression:
			num_labels = 1
		else:
			label_list = raw_datasets["train"].unique("label")
			label_list.sort()  # Let's sort it for determinism
			num_labels = len(label_list)

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		num_labels=num_labels,
		finetuning_task=data_args.task_name,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSequenceClassification.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
		ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
	)

	if data_args.task_name is not None:
		sentence1_key, sentence2_key = task_to_keys[data_args.task_name]
	else:
		non_label_column_names = [name for name in raw_datasets["train"].column_names if name != "label"]
		if "sentence1" in non_label_column_names and "sentence2" in non_label_column_names:
			sentence1_key, sentence2_key = "sentence1", "sentence2"
		else:
			if len(non_label_column_names) >= 2:
				sentence1_key, sentence2_key = non_label_column_names[:2]
			else:
				sentence1_key, sentence2_key = non_label_column_names[0], None

	if data_args.pad_to_max_length:
		padding = "max_length"
	else:
		padding = False

	label_to_id = None
	if (
		model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
		and data_args.task_name is not None
		and not is_regression
	):
		label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
		if sorted(label_name_to_id.keys()) == sorted(label_list):
			label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}
		else:
			logger.warning(
				"Your model seems to have been trained with labels, but they don't match the dataset: ",
				f"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}."
				"\nIgnoring the model labels as a result.",
			)
	elif data_args.task_name is None and not is_regression:
		label_to_id = {v: i for i, v in enumerate(label_list)}

	if label_to_id is not None:
		model.config.label2id = label_to_id
		model.config.id2label = {id: label for label, id in config.label2id.items()}
	elif data_args.task_name is not None and not is_regression:
		model.config.label2id = {l: i for i, l in enumerate(label_list)}
		model.config.id2label = {id: label for label, id in config.label2id.items()}

	if data_args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)
	max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	def preprocess_function(examples):
		args = (
			(examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])
		)
		result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)

		if label_to_id is not None and "label" in examples:
			result["label"] = [(label_to_id[l] if l != -1 else -1) for l in examples["label"]]
		return result

	with training_args.main_process_first(desc="dataset map pre-processing"):
		raw_datasets = raw_datasets.map(
			preprocess_function,
			batched=True,
			load_from_cache_file=not data_args.overwrite_cache,
			desc="Running tokenizer on dataset",
		)
	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset")
		train_dataset = raw_datasets["train"]
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	if training_args.do_eval:
		if "validation" not in raw_datasets and "validation_matched" not in raw_datasets:
			raise ValueError("--do_eval requires a validation dataset")
		eval_dataset = raw_datasets["validation_matched" if data_args.task_name == "mnli" else "validation"]
		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

	if training_args.do_predict or data_args.task_name is not None or data_args.test_file is not None:
		if "test" not in raw_datasets and "test_matched" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_dataset = raw_datasets["test_matched" if data_args.task_name == "mnli" else "test"]
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))

	if training_args.do_train:
		for index in random.sample(range(len(train_dataset)), 3):
			logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if data_args.task_name is not None:
		metric = evaluate.load("glue", data_args.task_name, cache_dir=model_args.cache_dir)
	elif is_regression:
		metric = evaluate.load("mse", cache_dir=model_args.cache_dir)
	else:
		metric = evaluate.load("accuracy", cache_dir=model_args.cache_dir)

	def compute_metrics(p: EvalPrediction):
		preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
		preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)
		result = metric.compute(predictions=preds, references=p.label_ids)
		if len(result) > 1:
			result["combined_score"] = np.mean(list(result.values())).item()
		return result

	if data_args.pad_to_max_length:
		data_collator = default_data_collator
	elif training_args.fp16:
		data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
	else:
		data_collator = None

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		compute_metrics=compute_metrics,
		tokenizer=tokenizer,
		data_collator=data_collator,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))

		trainer.save_model()  # Saves the tokenizer too for easy upload

		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")

		tasks = [data_args.task_name]
		eval_datasets = [eval_dataset]
		if data_args.task_name == "mnli":
			tasks.append("mnli-mm")
			valid_mm_dataset = raw_datasets["validation_mismatched"]
			if data_args.max_eval_samples is not None:
				max_eval_samples = min(len(valid_mm_dataset), data_args.max_eval_samples)
				valid_mm_dataset = valid_mm_dataset.select(range(max_eval_samples))
			eval_datasets.append(valid_mm_dataset)
			combined = {}

		for eval_dataset, task in zip(eval_datasets, tasks):
			metrics = trainer.evaluate(eval_dataset=eval_dataset)

			max_eval_samples = (
				data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
			)
			metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

			if task == "mnli-mm":
				metrics = {k + "_mm": v for k, v in metrics.items()}
			if task is not None and "mnli" in task:
				combined.update(metrics)

			trainer.log_metrics("eval", metrics)
			trainer.save_metrics("eval", combined if task is not None and "mnli" in task else metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")

		tasks = [data_args.task_name]
		predict_datasets = [predict_dataset]
		if data_args.task_name == "mnli":
			tasks.append("mnli-mm")
			predict_datasets.append(raw_datasets["test_mismatched"])

		for predict_dataset, task in zip(predict_datasets, tasks):
			predict_dataset = predict_dataset.remove_columns("label")
			predictions = trainer.predict(predict_dataset, metric_key_prefix="predict").predictions
			predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)

			output_predict_file = os.path.join(training_args.output_dir, f"predict_results_{task}.txt")
			if trainer.is_world_process_zero():
				with open(output_predict_file, "w") as writer:
					logger.info(f"***** Predict results {task} *****")
					writer.write("index\tprediction\n")
					for index, item in enumerate(predictions):
						if is_regression:
							writer.write(f"{index}\t{item:3.3f}\n")
						else:
							item = label_list[item]
							writer.write(f"{index}\t{item}\n")

	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "text-classification"}
	if data_args.task_name is not None:
		kwargs["language"] = "en"
		kwargs["dataset_tags"] = "glue"
		kwargs["dataset_args"] = data_args.task_name
		kwargs["dataset"] = f"GLUE {data_args.task_name.upper()}"

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from typing import Iterable, Tuple, Optional, Any, Dict

import torch

from allennlp.common.registrable import Registrable

NamedParameter = Tuple[str, torch.Tensor]


class MovingAverage(Registrable):

	default_implementation = "exponential"

	def __init__(self, parameters: Iterable[NamedParameter]) -> None:
		self._parameters = list(parameters)
		self._shadows = {name: parameter.data.clone() for name, parameter in self._parameters}
		self._backups = {name: parameter.data.clone() for name, parameter in self._parameters}

	def apply(self, num_updates: Optional[int] = None):
		raise NotImplementedError

	def assign_average_value(self) -> None:
		for name, parameter in self._parameters:
			self._backups[name].copy_(parameter.data)
			parameter.data.copy_(self._shadows[name])

	def restore(self) -> None:
		for name, parameter in self._parameters:
			parameter.data.copy_(self._backups[name])

	def state_dict(self) -> Dict[str, Any]:
		return {"parameters": self._parameters, "shadows": self._shadows, "backups": self._backups}

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		self._parameters = state_dict["parameters"]
		self._shadows = state_dict["shadows"]
		self._backups = state_dict["backups"]


@MovingAverage.register("exponential")
class ExponentialMovingAverage(MovingAverage):

	def __init__(
		self,
		parameters: Iterable[NamedParameter],
		decay: float = 0.9999,
		numerator: float = 1.0,
		denominator: float = 10.0,
	) -> None:
		super().__init__(parameters)
		self._decay = decay
		self._numerator = numerator
		self._denominator = denominator

	def apply(self, num_updates: Optional[int] = None) -> None:
		if num_updates is not None:
			decay = min(
				self._decay, (self._numerator + num_updates) / (self._denominator + num_updates)
			)
		else:
			decay = self._decay

		for name, parameter in self._parameters:
			self._shadows[name].mul_(decay).add_((1 - decay) * parameter.data)


from dataclasses import dataclass, asdict
from collections import OrderedDict
from typing import Optional, Any, Dict
import os

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

import boto3
from urllib.parse import urlparse
import fsspec
import io

@dataclass
class TrainerConfig:
	max_epochs: int = None
	batch_size: int = None
	data_loader_workers: int = None
	grad_norm_clip: float = None
	snapshot_path: Optional[str] = None
	save_every: int = None
	use_amp: bool = None

@dataclass
class Snapshot:
	model_state: 'OrderedDict[str, torch.Tensor]'
	optimizer_state: Dict[str, Any]
	finished_epoch: int

def upload_to_s3(obj, dst):
	buffer = io.BytesIO()
	torch.save(obj, buffer)
	buffer.seek(0)
	dst = urlparse(dst, allow_fragments=False)
	boto3.client('s3').upload_fileobj(buffer, dst.netloc, dst.path.lstrip('/'))

class Trainer:

	def __init__(self, trainer_config: TrainerConfig, model, optimizer, train_dataset, test_dataset=None):
		self.config = trainer_config
		self.local_rank = int(os.environ["LOCAL_RANK"])
		self.global_rank = int(os.environ["RANK"])  
		self.train_dataset = train_dataset
		self.train_loader = self._prepare_dataloader(train_dataset)
		self.test_loader = self._prepare_dataloader(test_dataset) if test_dataset else None
		self.epochs_run = 0
		self.model = model.to(self.local_rank)
		self.optimizer = optimizer		
		self.save_every = self.config.save_every
		if self.config.use_amp:
			self.scaler = torch.cuda.amp.GradScaler()
		if self.config.snapshot_path is None:
			self.config.snapshot_path = "snapshot.pt"
		self._load_snapshot()
		self.model = DDP(self.model, device_ids=[self.local_rank])
		
	def _prepare_dataloader(self, dataset: Dataset):
		return DataLoader(
			dataset,
			batch_size=self.config.batch_size,
			pin_memory=True,
			shuffle=False,
			num_workers=self.config.data_loader_workers,
			sampler=DistributedSampler(dataset)
		)

	def _load_snapshot(self):
		try:
			snapshot = fsspec.open(self.config.snapshot_path)
			with snapshot as f:
				snapshot_data = torch.load(f, map_location="cpu")
		except FileNotFoundError:
			print("Snapshot not found. Training model from scratch")
			return 

		snapshot = Snapshot(**snapshot_data)
		self.model.load_state_dict(snapshot.model_state)
		self.optimizer.load_state_dict(snapshot.optimizer_state)
		self.epochs_run = snapshot.finished_epoch
		print(f"Resuming training from snapshot at Epoch {self.epochs_run}")


	def _run_batch(self, source, targets, train: bool = True) -> float:
		with torch.set_grad_enabled(train), torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=(self.config.use_amp)):
			_, loss = self.model(source, targets)
		
		if train:
			self.optimizer.zero_grad(set_to_none=True)
			if self.config.use_amp: 
				self.scaler.scale(loss).backward()
				torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm_clip)
				self.scaler.step(self.optimizer)
				self.scaler.update()
			else:
				loss.backward()
				torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm_clip)
				self.optimizer.step()
		
		return loss.item()

	def _run_epoch(self, epoch: int, dataloader: DataLoader, train: bool = True):
		dataloader.sampler.set_epoch(epoch)
		for iter, (source, targets) in enumerate(dataloader):
			step_type = "Train" if train else "Eval"
			source = source.to(self.local_rank)
			targets = targets.to(self.local_rank)
			batch_loss = self._run_batch(source, targets, train)
			if iter % 100 == 0:
				print(f"[GPU{self.global_rank}] Epoch {epoch} | Iter {iter} | {step_type} Loss {batch_loss:.5f}")

	def _save_snapshot(self, epoch):
		model = self.model
		raw_model = model.module if hasattr(model, "module") else model
		snapshot = Snapshot(
			model_state=raw_model.state_dict(),
			optimizer_state=self.optimizer.state_dict(),
			finished_epoch=epoch
		)
		snapshot = asdict(snapshot)
		if self.config.snapshot_path.startswith("s3://"):
			upload_to_s3(snapshot, self.config.snapshot_path)
		else:
			torch.save(snapshot, self.config.snapshot_path)
			
		print(f"Snapshot saved at epoch {epoch}")

	def train(self):
		for epoch in range(self.epochs_run, self.config.max_epochs):
			epoch += 1
			self._run_epoch(epoch, self.train_loader, train=True)
			if self.local_rank == 0 and epoch % self.save_every == 0:
				self._save_snapshot(epoch)
			if self.test_loader:
				self._run_epoch(epoch, self.test_loader, train=False)

from allennlp.interpret.influence_interpreters.influence_interpreter import InfluenceInterpreter
from allennlp.interpret.influence_interpreters.simple_influence import SimpleInfluence

from typing import Dict, Tuple


import torch

from allennlp.data.fields.field import Field
from allennlp.data.fields.sequence_field import SequenceField


class SpanField(Field[torch.Tensor]):

	__slots__ = ["span_start", "span_end", "sequence_field"]

	def __init__(self, span_start: int, span_end: int, sequence_field: SequenceField) -> None:
		self.span_start = span_start
		self.span_end = span_end
		self.sequence_field = sequence_field

		if not isinstance(span_start, int) or not isinstance(span_end, int):
			raise TypeError(
				f"SpanFields must be passed integer indices. Found span indices: "
				f"({span_start}, {span_end}) with types "
				f"({type(span_start)} {type(span_end)})"
			)
		if span_start > span_end:
			raise ValueError(
				f"span_start must be less than span_end, " f"but found ({span_start}, {span_end})."
			)

		if span_end > self.sequence_field.sequence_length() - 1:
			raise ValueError(
				f"span_end must be <= len(sequence_length) - 1, but found "
				f"{span_end} and {self.sequence_field.sequence_length() - 1} respectively."
			)

	def get_padding_lengths(self) -> Dict[str, int]:

		return {}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:

		tensor = torch.LongTensor([self.span_start, self.span_end])
		return tensor

	def empty_field(self):
		return SpanField(-1, -1, self.sequence_field.empty_field())

	def __str__(self) -> str:
		return f"SpanField with spans: ({self.span_start}, {self.span_end})."

	def __eq__(self, other) -> bool:
		if isinstance(other, tuple) and len(other) == 2:
			return other == (self.span_start, self.span_end)
		return super().__eq__(other)

	def __len__(self):
		return 2

	def human_readable_repr(self) -> Tuple[int, int]:
		return self.span_start, self.span_end


from allennlp.nn.regularizers.regularizer import Regularizer
from allennlp.nn.regularizers.regularizers import L1Regularizer
from allennlp.nn.regularizers.regularizers import L2Regularizer
from allennlp.nn.regularizers.regularizer_applicator import RegularizerApplicator

from typing import Dict, Any

import torch


class Scheduler:

	def __init__(
		self, optimizer: torch.optim.Optimizer, param_group_field: str, last_epoch: int = -1
	) -> None:
		self.optimizer = optimizer
		self.param_group_field = param_group_field
		self._initial_param_group_field = f"initial_{param_group_field}"
		if last_epoch == -1:
			for i, group in enumerate(self.optimizer.param_groups):
				if param_group_field not in group:
					raise KeyError(f"{param_group_field} missing from param_groups[{i}]")
				group.setdefault(self._initial_param_group_field, group[param_group_field])
		else:
			for i, group in enumerate(self.optimizer.param_groups):
				if self._initial_param_group_field not in group:
					raise KeyError(
						f"{self._initial_param_group_field} missing from param_groups[{i}]"
					)
		self.base_values = [
			group[self._initial_param_group_field] for group in self.optimizer.param_groups
		]
		self.last_epoch = last_epoch

	def state_dict(self) -> Dict[str, Any]:
		return {key: value for key, value in self.__dict__.items() if key != "optimizer"}

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		self.__dict__.update(state_dict)

	def get_values(self):
		raise NotImplementedError

	def step(self, metric: float = None) -> None:
		self.last_epoch += 1
		self.metric = metric
		for param_group, value in zip(self.optimizer.param_groups, self.get_values()):
			param_group[self.param_group_field] = value

	def step_batch(self, batch_num_total: int = None) -> None:
		return

import torch
from torch.fx import symbolic_trace, Tracer, Graph, GraphModule, Node
from typing import Any, Callable, Dict, Optional, Tuple, Union







class M1(torch.nn.Module):
	def __init__(self):
		super().__init__()
		self.relu = torch.nn.ReLU()

	def forward(self, x):
		return self.relu(x)

default_traced: GraphModule = symbolic_trace(M1())
default_traced.graph.print_tabular()

class LowerReluTracer(Tracer):
	def is_leaf_module(self, m : torch.nn.Module, qualname : str):
		if isinstance(m, torch.nn.ReLU):
			return False
		return super().is_leaf_module(m, qualname)

lower_relu_tracer = LowerReluTracer()
custom_traced_graph: Graph = lower_relu_tracer.trace(M1())
custom_traced_graph.print_tabular()




class M2(torch.nn.Module):
	def forward(self, a, b):
		return a + b

class TaggingTracer(Tracer):
	def create_node(self, kind : str, target : Union[str, Callable],
					args : Tuple[Any], kwargs : Dict[str, Any], name : Optional[str] = None,
					type_expr : Optional[Any] = None) -> Node:
		n = super().create_node(kind, target, args, kwargs, name)
		n.tag = "foo"
		return n

custom_traced_graph: Graph = TaggingTracer().trace(M2())

def assert_all_nodes_have_tags(g: Graph) -> bool:
	for n in g.nodes:
		if not hasattr(n, "tag") or not n.tag == "foo":
			return False
	return True

print(assert_all_nodes_have_tags(custom_traced_graph))


from typing import Dict, Any, Union, Optional

import torch
import numpy as np


from allennlp.data.fields.field import Field
from allennlp.common.util import JsonDict


class TensorField(Field[torch.Tensor]):

	__slots__ = ["tensor", "padding_value"]

	def __init__(
		self,
		tensor: Union[torch.Tensor, np.ndarray],
		padding_value: Any = 0.0,
		dtype: Optional[Union[np.dtype, torch.dtype]] = None,
	) -> None:
		if dtype is not None:
			if isinstance(tensor, np.ndarray):
				tensor = tensor.astype(dtype)
			elif isinstance(tensor, torch.Tensor):
				tensor = tensor.to(dtype)
			else:
				raise ValueError("Did not recognize the type of `tensor`.")
		if isinstance(tensor, np.ndarray):
			tensor = torch.from_numpy(tensor)

		self.tensor = tensor.cpu()
		self.padding_value = padding_value

	def get_padding_lengths(self) -> Dict[str, int]:
		return {"dimension_" + str(i): shape for i, shape in enumerate(self.tensor.size())}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:
		tensor = self.tensor
		while len(tensor.size()) < len(padding_lengths):
			tensor = tensor.unsqueeze(-1)
		pad = [
			padding
			for i, dimension_size in reversed(list(enumerate(tensor.size())))
			for padding in [0, padding_lengths["dimension_" + str(i)] - dimension_size]
		]
		return torch.nn.functional.pad(tensor, pad, value=self.padding_value)

	def empty_field(self):
		return TensorField(
			torch.tensor([], dtype=self.tensor.dtype), padding_value=self.padding_value
		)

	def __str__(self) -> str:
		return f"TensorField with shape: {self.tensor.size()} and dtype: {self.tensor.dtype}."

	def __len__(self):
		return 1 if len(self.tensor.size()) <= 0 else self.tensor.size(0)

	def __eq__(self, other) -> bool:
		if isinstance(self, other.__class__):
			return (
				torch.equal(self.tensor, other.tensor) and self.padding_value == other.padding_value
			)
		return NotImplemented

	@property
	def array(self):
		return self.tensor.numpy()

	def human_readable_repr(self) -> JsonDict:
		shape = list(self.tensor.shape)
		std = torch.std(self.tensor.float()).item()
		mean = torch.mean(self.tensor.float()).item()
		return {
			"shape": shape,
			"element_std": std,
			"element_mean": mean,
			"type": str(self.tensor.dtype).replace("torch.", ""),
		}

import argparse
import os
import sys
import tempfile
from urllib.parse import urlparse

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim

from torch.nn.parallel import DistributedDataParallel as DDP

class ToyModel(nn.Module):
	def __init__(self):
		super(ToyModel, self).__init__()
		self.net1 = nn.Linear(10, 10)
		self.relu = nn.ReLU()
		self.net2 = nn.Linear(10, 5)

	def forward(self, x):
		return self.net2(self.relu(self.net1(x)))


def demo_basic(local_world_size, local_rank):

	n = torch.cuda.device_count() // local_world_size
	device_ids = list(range(local_rank * n, (local_rank + 1) * n))

	print(
		f"[{os.getpid()}] rank = {dist.get_rank()}, "
		+ f"world_size = {dist.get_world_size()}, n = {n}, device_ids = {device_ids} \n", end=''
	)

	model = ToyModel().cuda(device_ids[0])
	ddp_model = DDP(model, device_ids)

	loss_fn = nn.MSELoss()
	optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

	optimizer.zero_grad()
	outputs = ddp_model(torch.randn(20, 10))
	labels = torch.randn(20, 5).to(device_ids[0])
	loss_fn(outputs, labels).backward()
	optimizer.step()


def spmd_main(local_world_size, local_rank):
	env_dict = {
		key: os.environ[key]
		for key in ("MASTER_ADDR", "MASTER_PORT", "RANK", "WORLD_SIZE")
	}
	
	if sys.platform == "win32":
		if "INIT_METHOD" in os.environ.keys():
			print(f"init_method is {os.environ['INIT_METHOD']}")
			url_obj = urlparse(os.environ["INIT_METHOD"])
			if url_obj.scheme.lower() != "file":
				raise ValueError("Windows only supports FileStore")
			else:
				init_method = os.environ["INIT_METHOD"]
		else:
			temp_dir = tempfile.gettempdir()
			init_method = f"file:///{os.path.join(temp_dir, 'ddp_example')}"
		dist.init_process_group(backend="gloo", init_method=init_method, rank=int(env_dict["RANK"]), world_size=int(env_dict["WORLD_SIZE"]))
	else:
		print(f"[{os.getpid()}] Initializing process group with: {env_dict}")  
		dist.init_process_group(backend="nccl")

	print(
		f"[{os.getpid()}]: world_size = {dist.get_world_size()}, "
		+ f"rank = {dist.get_rank()}, backend={dist.get_backend()} \n", end=''
	)

	demo_basic(local_world_size, local_rank)

	dist.destroy_process_group()


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--local_rank", type=int, default=0)
	parser.add_argument("--local_world_size", type=int, default=1)
	args = parser.parse_args()
	spmd_main(args.local_world_size, args.local_rank)


import codecs
import copy
import logging
import os
import re
from collections import defaultdict
from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Union, TYPE_CHECKING

from transformers import PreTrainedTokenizer

from allennlp.common import Registrable
from allennlp.common.file_utils import cached_path, FileLock
from allennlp.common.checks import ConfigurationError
from allennlp.common.tqdm import Tqdm
from allennlp.common.util import namespace_match

if TYPE_CHECKING:
	from allennlp.data import instance as adi  # noqa

logger = logging.getLogger(__name__)

DEFAULT_NON_PADDED_NAMESPACES = ("*tags", "*labels")
DEFAULT_PADDING_TOKEN = "@@PADDING@@"
DEFAULT_OOV_TOKEN = "@@UNKNOWN@@"
NAMESPACE_PADDING_FILE = "non_padded_namespaces.txt"
_NEW_LINE_REGEX = re.compile(r"\n|\r\n")


class _NamespaceDependentDefaultDict(defaultdict):

	def __init__(
		self,
		non_padded_namespaces: Iterable[str],
		padded_function: Callable[[], Any],
		non_padded_function: Callable[[], Any],
	) -> None:
		self._non_padded_namespaces = set(non_padded_namespaces)
		self._padded_function = padded_function
		self._non_padded_function = non_padded_function
		super().__init__()

	def __missing__(self, key: str):
		if any(namespace_match(pattern, key) for pattern in self._non_padded_namespaces):
			value = self._non_padded_function()
		else:
			value = self._padded_function()
		dict.__setitem__(self, key, value)
		return value

	def add_non_padded_namespaces(self, non_padded_namespaces: Set[str]):
		self._non_padded_namespaces.update(non_padded_namespaces)


class _TokenToIndexDefaultDict(_NamespaceDependentDefaultDict):
	def __init__(self, non_padded_namespaces: Set[str], padding_token: str, oov_token: str) -> None:
		super().__init__(
			non_padded_namespaces, lambda: {padding_token: 0, oov_token: 1}, lambda: {}
		)


class _IndexToTokenDefaultDict(_NamespaceDependentDefaultDict):
	def __init__(self, non_padded_namespaces: Set[str], padding_token: str, oov_token: str) -> None:
		super().__init__(
			non_padded_namespaces, lambda: {0: padding_token, 1: oov_token}, lambda: {}
		)


def _read_pretrained_tokens(embeddings_file_uri: str) -> List[str]:
	from allennlp.modules.token_embedders.embedding import EmbeddingsTextFile

	logger.info("Reading pretrained tokens from: %s", embeddings_file_uri)
	tokens: List[str] = []
	with EmbeddingsTextFile(embeddings_file_uri) as embeddings_file:
		for line_number, line in enumerate(Tqdm.tqdm(embeddings_file), start=1):
			token_end = line.find(" ")
			if token_end >= 0:
				token = line[:token_end]
				tokens.append(token)
			else:
				line_begin = line[:20] + "..." if len(line) > 20 else line
				logger.warning("Skipping line number %d: %s", line_number, line_begin)
	return tokens


class Vocabulary(Registrable):

	default_implementation = "from_instances"

	def __init__(
		self,
		counter: Dict[str, Dict[str, int]] = None,
		min_count: Dict[str, int] = None,
		max_vocab_size: Union[int, Dict[str, int]] = None,
		non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,
		pretrained_files: Optional[Dict[str, str]] = None,
		only_include_pretrained_words: bool = False,
		tokens_to_add: Dict[str, List[str]] = None,
		min_pretrained_embeddings: Dict[str, int] = None,
		padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,
		oov_token: Optional[str] = DEFAULT_OOV_TOKEN,
	) -> None:
		self._padding_token = padding_token if padding_token is not None else DEFAULT_PADDING_TOKEN
		self._oov_token = oov_token if oov_token is not None else DEFAULT_OOV_TOKEN

		self._non_padded_namespaces = set(non_padded_namespaces)

		self._token_to_index = _TokenToIndexDefaultDict(
			self._non_padded_namespaces, self._padding_token, self._oov_token
		)
		self._index_to_token = _IndexToTokenDefaultDict(
			self._non_padded_namespaces, self._padding_token, self._oov_token
		)

		self._retained_counter: Optional[Dict[str, Dict[str, int]]] = None

		self._extend(
			counter,
			min_count,
			max_vocab_size,
			non_padded_namespaces,
			pretrained_files,
			only_include_pretrained_words,
			tokens_to_add,
			min_pretrained_embeddings,
		)

	@classmethod
	def from_pretrained_transformer(
		cls, model_name: str, namespace: str = "tokens", oov_token: Optional[str] = None
	) -> "Vocabulary":
		from allennlp.common import cached_transformers

		tokenizer = cached_transformers.get_tokenizer(model_name)
		if oov_token is None:
			if hasattr(tokenizer, "_unk_token"):
				oov_token = tokenizer._unk_token
			elif hasattr(tokenizer, "special_tokens_map"):
				oov_token = tokenizer.special_tokens_map.get("unk_token")
		vocab = cls(non_padded_namespaces=[namespace], oov_token=oov_token)
		vocab.add_transformer_vocab(tokenizer, namespace)
		return vocab

	@classmethod
	def from_instances(
		cls,
		instances: Iterable["adi.Instance"],
		min_count: Dict[str, int] = None,
		max_vocab_size: Union[int, Dict[str, int]] = None,
		non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,
		pretrained_files: Optional[Dict[str, str]] = None,
		only_include_pretrained_words: bool = False,
		tokens_to_add: Dict[str, List[str]] = None,
		min_pretrained_embeddings: Dict[str, int] = None,
		padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,
		oov_token: Optional[str] = DEFAULT_OOV_TOKEN,
	) -> "Vocabulary":
		logger.info("Fitting token dictionary from dataset.")
		padding_token = padding_token if padding_token is not None else DEFAULT_PADDING_TOKEN
		oov_token = oov_token if oov_token is not None else DEFAULT_OOV_TOKEN
		namespace_token_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
		for instance in Tqdm.tqdm(instances, desc="building vocab"):
			instance.count_vocab_items(namespace_token_counts)

		return cls(
			counter=namespace_token_counts,
			min_count=min_count,
			max_vocab_size=max_vocab_size,
			non_padded_namespaces=non_padded_namespaces,
			pretrained_files=pretrained_files,
			only_include_pretrained_words=only_include_pretrained_words,
			tokens_to_add=tokens_to_add,
			min_pretrained_embeddings=min_pretrained_embeddings,
			padding_token=padding_token,
			oov_token=oov_token,
		)

	@classmethod
	def from_files(
		cls,
		directory: Union[str, os.PathLike],
		padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,
		oov_token: Optional[str] = DEFAULT_OOV_TOKEN,
	) -> "Vocabulary":
		logger.info("Loading token dictionary from %s.", directory)
		padding_token = padding_token if padding_token is not None else DEFAULT_PADDING_TOKEN
		oov_token = oov_token if oov_token is not None else DEFAULT_OOV_TOKEN

		if not os.path.isdir(directory):
			base_directory = cached_path(directory, extract_archive=True)
			vocab_subdir = os.path.join(base_directory, "vocabulary")
			if os.path.isdir(vocab_subdir):
				directory = vocab_subdir
			elif os.path.isdir(base_directory):
				directory = base_directory
			else:
				raise ConfigurationError(f"{directory} is neither a directory nor an archive")

		with FileLock(os.path.join(directory, ".lock"), read_only_ok=True):
			with codecs.open(
				os.path.join(directory, NAMESPACE_PADDING_FILE), "r", "utf-8"
			) as namespace_file:
				non_padded_namespaces = [namespace_str.strip() for namespace_str in namespace_file]

			vocab = cls(
				non_padded_namespaces=non_padded_namespaces,
				padding_token=padding_token,
				oov_token=oov_token,
			)

			for namespace_filename in os.listdir(directory):
				if namespace_filename == NAMESPACE_PADDING_FILE:
					continue
				if namespace_filename.startswith("."):
					continue
				namespace = namespace_filename.replace(".txt", "")
				if any(namespace_match(pattern, namespace) for pattern in non_padded_namespaces):
					is_padded = False
				else:
					is_padded = True
				filename = os.path.join(directory, namespace_filename)
				vocab.set_from_file(filename, is_padded, namespace=namespace, oov_token=oov_token)

		return vocab

	@classmethod
	def from_files_and_instances(
		cls,
		instances: Iterable["adi.Instance"],
		directory: str,
		padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,
		oov_token: Optional[str] = DEFAULT_OOV_TOKEN,
		min_count: Dict[str, int] = None,
		max_vocab_size: Union[int, Dict[str, int]] = None,
		non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,
		pretrained_files: Optional[Dict[str, str]] = None,
		only_include_pretrained_words: bool = False,
		tokens_to_add: Dict[str, List[str]] = None,
		min_pretrained_embeddings: Dict[str, int] = None,
	) -> "Vocabulary":
		vocab = cls.from_files(directory, padding_token, oov_token)
		logger.info("Fitting token dictionary from dataset.")
		namespace_token_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
		for instance in Tqdm.tqdm(instances):
			instance.count_vocab_items(namespace_token_counts)
		vocab._extend(
			counter=namespace_token_counts,
			min_count=min_count,
			max_vocab_size=max_vocab_size,
			non_padded_namespaces=non_padded_namespaces,
			pretrained_files=pretrained_files,
			only_include_pretrained_words=only_include_pretrained_words,
			tokens_to_add=tokens_to_add,
			min_pretrained_embeddings=min_pretrained_embeddings,
		)
		return vocab

	@classmethod
	def from_pretrained_transformer_and_instances(
		cls,
		instances: Iterable["adi.Instance"],
		transformers: Dict[str, str],
		min_count: Dict[str, int] = None,
		max_vocab_size: Union[int, Dict[str, int]] = None,
		non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,
		pretrained_files: Optional[Dict[str, str]] = None,
		only_include_pretrained_words: bool = False,
		tokens_to_add: Dict[str, List[str]] = None,
		min_pretrained_embeddings: Dict[str, int] = None,
		padding_token: Optional[str] = DEFAULT_PADDING_TOKEN,
		oov_token: Optional[str] = DEFAULT_OOV_TOKEN,
	) -> "Vocabulary":
		vocab = cls.from_instances(
			instances=instances,
			min_count=min_count,
			max_vocab_size=max_vocab_size,
			non_padded_namespaces=non_padded_namespaces,
			pretrained_files=pretrained_files,
			only_include_pretrained_words=only_include_pretrained_words,
			tokens_to_add=tokens_to_add,
			min_pretrained_embeddings=min_pretrained_embeddings,
			padding_token=padding_token,
			oov_token=oov_token,
		)

		for namespace, model_name in transformers.items():
			transformer_vocab = cls.from_pretrained_transformer(
				model_name=model_name, namespace=namespace
			)
			vocab.extend_from_vocab(transformer_vocab)

		return vocab

	@classmethod
	def empty(cls) -> "Vocabulary":
		return cls()

	def add_transformer_vocab(
		self, tokenizer: PreTrainedTokenizer, namespace: str = "tokens"
	) -> None:
		try:
			vocab_items = tokenizer.get_vocab().items()
		except NotImplementedError:
			vocab_items = (
				(tokenizer.convert_ids_to_tokens(idx), idx) for idx in range(tokenizer.vocab_size)
			)

		for word, idx in vocab_items:
			self._token_to_index[namespace][word] = idx
			self._index_to_token[namespace][idx] = word

		self._non_padded_namespaces.add(namespace)

	def set_from_file(
		self,
		filename: str,
		is_padded: bool = True,
		oov_token: str = DEFAULT_OOV_TOKEN,
		namespace: str = "tokens",
	):
		if is_padded:
			self._token_to_index[namespace] = {self._padding_token: 0}
			self._index_to_token[namespace] = {0: self._padding_token}
		else:
			self._token_to_index[namespace] = {}
			self._index_to_token[namespace] = {}
		with codecs.open(filename, "r", "utf-8") as input_file:
			lines = _NEW_LINE_REGEX.split(input_file.read())
			if lines and lines[-1] == "":
				lines = lines[:-1]
			for i, line in enumerate(lines):
				index = i + 1 if is_padded else i
				token = line.replace("@@NEWLINE@@", "\n")
				if token == oov_token:
					token = self._oov_token
				self._token_to_index[namespace][token] = index
				self._index_to_token[namespace][index] = token
		if is_padded:
			assert self._oov_token in self._token_to_index[namespace], "OOV token not found!"

	def extend_from_instances(self, instances: Iterable["adi.Instance"]) -> None:
		logger.info("Fitting token dictionary from dataset.")
		namespace_token_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
		for instance in Tqdm.tqdm(instances):
			instance.count_vocab_items(namespace_token_counts)
		self._extend(counter=namespace_token_counts)

	def extend_from_vocab(self, vocab: "Vocabulary") -> None:
		self._non_padded_namespaces.update(vocab._non_padded_namespaces)
		self._token_to_index._non_padded_namespaces.update(vocab._non_padded_namespaces)
		self._index_to_token._non_padded_namespaces.update(vocab._non_padded_namespaces)
		for namespace in vocab.get_namespaces():
			for token in vocab.get_token_to_index_vocabulary(namespace):
				self.add_token_to_namespace(token, namespace)

	def _extend(
		self,
		counter: Dict[str, Dict[str, int]] = None,
		min_count: Dict[str, int] = None,
		max_vocab_size: Union[int, Dict[str, int]] = None,
		non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,
		pretrained_files: Optional[Dict[str, str]] = None,
		only_include_pretrained_words: bool = False,
		tokens_to_add: Dict[str, List[str]] = None,
		min_pretrained_embeddings: Dict[str, int] = None,
	) -> None:
		if min_count is not None:
			for key in min_count:
				if counter is not None and key not in counter or counter is None:
					raise ConfigurationError(
						f"The key '{key}' is present in min_count but not in counter"
					)

		if not isinstance(max_vocab_size, dict):
			int_max_vocab_size = max_vocab_size
			max_vocab_size = defaultdict(lambda: int_max_vocab_size)  # type: ignore
		min_count = min_count or {}
		pretrained_files = pretrained_files or {}
		min_pretrained_embeddings = min_pretrained_embeddings or {}
		non_padded_namespaces = set(non_padded_namespaces)
		counter = counter or {}
		tokens_to_add = tokens_to_add or {}

		self._retained_counter = counter
		current_namespaces = {*self._token_to_index}
		extension_namespaces = {*counter, *tokens_to_add}

		for namespace in current_namespaces & extension_namespaces:
			original_padded = not any(
				namespace_match(pattern, namespace) for pattern in self._non_padded_namespaces
			)
			extension_padded = not any(
				namespace_match(pattern, namespace) for pattern in non_padded_namespaces
			)
			if original_padded != extension_padded:
				raise ConfigurationError(
					"Common namespace {} has conflicting ".format(namespace)
					+ "setting of padded = True/False. "
					+ "Hence extension cannot be done."
				)

		self._token_to_index.add_non_padded_namespaces(non_padded_namespaces)
		self._index_to_token.add_non_padded_namespaces(non_padded_namespaces)
		self._non_padded_namespaces.update(non_padded_namespaces)

		for namespace in counter:
			pretrained_set: Optional[Set] = None
			if namespace in pretrained_files:
				pretrained_list = _read_pretrained_tokens(pretrained_files[namespace])
				min_embeddings = min_pretrained_embeddings.get(namespace, 0)
				if min_embeddings > 0 or min_embeddings == -1:
					tokens_old = tokens_to_add.get(namespace, [])
					tokens_new = (
						pretrained_list
						if min_embeddings == -1
						else pretrained_list[:min_embeddings]
					)
					tokens_to_add[namespace] = tokens_old + tokens_new
				pretrained_set = set(pretrained_list)
			token_counts = list(counter[namespace].items())
			token_counts.sort(key=lambda x: x[1], reverse=True)
			max_vocab: Optional[int]
			try:
				max_vocab = max_vocab_size[namespace]
			except KeyError:
				max_vocab = None
			if max_vocab:
				token_counts = token_counts[:max_vocab]
			for token, count in token_counts:
				if pretrained_set is not None:
					if only_include_pretrained_words:
						if token in pretrained_set and count >= min_count.get(namespace, 1):
							self.add_token_to_namespace(token, namespace)
					elif token in pretrained_set or count >= min_count.get(namespace, 1):
						self.add_token_to_namespace(token, namespace)
				elif count >= min_count.get(namespace, 1):
					self.add_token_to_namespace(token, namespace)

		for namespace, tokens in tokens_to_add.items():
			for token in tokens:
				self.add_token_to_namespace(token, namespace)

	def __getstate__(self):
		state = copy.copy(self.__dict__)
		state["_token_to_index"] = dict(state["_token_to_index"])
		state["_index_to_token"] = dict(state["_index_to_token"])

		if "_retained_counter" in state:
			state["_retained_counter"] = {
				key: dict(value) for key, value in state["_retained_counter"].items()
			}

		return state

	def __setstate__(self, state):

		self.__dict__ = copy.copy(state)
		self._token_to_index = _TokenToIndexDefaultDict(
			self._non_padded_namespaces, self._padding_token, self._oov_token
		)
		self._token_to_index.update(state["_token_to_index"])
		self._index_to_token = _IndexToTokenDefaultDict(
			self._non_padded_namespaces, self._padding_token, self._oov_token
		)
		self._index_to_token.update(state["_index_to_token"])

	def save_to_files(self, directory: str) -> None:
		os.makedirs(directory, exist_ok=True)
		if os.listdir(directory):
			logger.warning("vocabulary serialization directory %s is not empty", directory)

		with FileLock(os.path.join(directory, ".lock")):
			with codecs.open(
				os.path.join(directory, NAMESPACE_PADDING_FILE), "w", "utf-8"
			) as namespace_file:
				for namespace_str in self._non_padded_namespaces:
					print(namespace_str, file=namespace_file)

			for namespace, mapping in self._index_to_token.items():
				with codecs.open(
					os.path.join(directory, namespace + ".txt"), "w", "utf-8"
				) as token_file:
					num_tokens = len(mapping)
					start_index = 1 if mapping[0] == self._padding_token else 0
					for i in range(start_index, num_tokens):
						print(mapping[i].replace("\n", "@@NEWLINE@@"), file=token_file)

	def is_padded(self, namespace: str) -> bool:
		return self._index_to_token[namespace][0] == self._padding_token

	def add_token_to_namespace(self, token: str, namespace: str = "tokens") -> int:
		if not isinstance(token, str):
			raise ValueError(
				"Vocabulary tokens must be strings, or saving and loading will break."
				"  Got %s (with type %s)" % (repr(token), type(token))
			)
		if token not in self._token_to_index[namespace]:
			index = len(self._token_to_index[namespace])
			self._token_to_index[namespace][token] = index
			self._index_to_token[namespace][index] = token
			return index
		else:
			return self._token_to_index[namespace][token]

	def add_tokens_to_namespace(self, tokens: List[str], namespace: str = "tokens") -> List[int]:
		return [self.add_token_to_namespace(token, namespace) for token in tokens]

	def get_index_to_token_vocabulary(self, namespace: str = "tokens") -> Dict[int, str]:
		return self._index_to_token[namespace]

	def get_token_to_index_vocabulary(self, namespace: str = "tokens") -> Dict[str, int]:
		return self._token_to_index[namespace]

	def get_token_index(self, token: str, namespace: str = "tokens") -> int:
		try:
			return self._token_to_index[namespace][token]
		except KeyError:
			try:
				return self._token_to_index[namespace][self._oov_token]
			except KeyError:
				logger.error("Namespace: %s", namespace)
				logger.error("Token: %s", token)
				raise KeyError(
					f"'{token}' not found in vocab namespace '{namespace}', and namespace "
					f"does not contain the default OOV token ('{self._oov_token}')"
				)

	def get_token_from_index(self, index: int, namespace: str = "tokens") -> str:
		return self._index_to_token[namespace][index]

	def get_vocab_size(self, namespace: str = "tokens") -> int:
		return len(self._token_to_index[namespace])

	def get_namespaces(self) -> Set[str]:
		return set(self._index_to_token.keys())

	def __eq__(self, other):
		if isinstance(self, other.__class__):
			return self.__dict__ == other.__dict__
		return False

	def __str__(self) -> str:
		base_string = "Vocabulary with namespaces:\n"
		non_padded_namespaces = f"\tNon Padded Namespaces: {self._non_padded_namespaces}\n"
		namespaces = [
			f"\tNamespace: {name}, Size: {self.get_vocab_size(name)} \n"
			for name in self._index_to_token
		]
		return " ".join([base_string, non_padded_namespaces] + namespaces)

	def __repr__(self) -> str:
		base_string = "Vocabulary with namespaces: "
		namespaces = [
			f"{name}, Size: {self.get_vocab_size(name)} ||" for name in self._index_to_token
		]
		non_padded_namespaces = f"Non Padded Namespaces: {self._non_padded_namespaces}"
		return " ".join([base_string] + namespaces + [non_padded_namespaces])

	def print_statistics(self) -> None:
		if self._retained_counter:
			logger.info(
				"Printed vocabulary statistics are only for the part of the vocabulary generated "
				"from instances. If vocabulary is constructed by extending saved vocabulary with "
				"dataset instances, the directly loaded portion won't be considered here."
			)
			print("\n\n----Vocabulary Statistics----\n")
			for namespace in self._retained_counter:
				tokens_with_counts = list(self._retained_counter[namespace].items())
				tokens_with_counts.sort(key=lambda x: x[1], reverse=True)
				print(f"\nTop 10 most frequent tokens in namespace '{namespace}':")
				for token, freq in tokens_with_counts[:10]:
					print(f"\tToken: {token}\t\tFrequency: {freq}")
				tokens_with_counts.sort(key=lambda x: len(x[0]), reverse=True)

				print(f"\nTop 10 longest tokens in namespace '{namespace}':")
				for token, freq in tokens_with_counts[:10]:
					print(f"\tToken: {token}\t\tlength: {len(token)}\tFrequency: {freq}")

				print(f"\nTop 10 shortest tokens in namespace '{namespace}':")
				for token, freq in reversed(tokens_with_counts[-10:]):
					print(f"\tToken: {token}\t\tlength: {len(token)}\tFrequency: {freq}")
		else:
			logger.info(
				"Vocabulary statistics cannot be printed since "
				"dataset instances were not used for its construction."
			)


Vocabulary.register("from_pretrained_transformer", constructor="from_pretrained_transformer")(
	Vocabulary
)
Vocabulary.register(
	"from_pretrained_transformer_and_instances",
	constructor="from_pretrained_transformer_and_instances",
)(Vocabulary)
Vocabulary.register("from_instances", constructor="from_instances")(Vocabulary)
Vocabulary.register("from_files", constructor="from_files")(Vocabulary)
Vocabulary.register("extend", constructor="from_files_and_instances")(Vocabulary)
Vocabulary.register("empty", constructor="empty")(Vocabulary)

import sys
import logging
from typing import Type, Optional, Dict, Any, Callable, List, Iterable, Union, TextIO, Tuple

import numpy as np
from checklist.test_suite import TestSuite
from checklist.editor import Editor
from checklist.test_types import MFT, INV, DIR
from checklist.perturb import Perturb
from allennlp.common.registrable import Registrable
from allennlp.common.file_utils import cached_path
from allennlp.predictors.predictor import Predictor
from allennlp.confidence_checks.task_checklists import utils

logger = logging.getLogger(__name__)


class TaskSuite(Registrable):

	_capabilities: List[str] = [
		"Vocabulary",
		"Taxonomy",
		"Robustness",
		"NER",
		"Fairness",
		"Temporal",
		"Negation",
		"Coref",
		"SRL",
		"Logic",
	]

	def __init__(
		self,
		suite: Optional[TestSuite] = None,
		add_default_tests: bool = True,
		data: Optional[List[Any]] = None,
		num_test_cases: int = 100,
		**kwargs,
	):
		self.suite = suite or TestSuite()

		if add_default_tests:
			self._default_tests(data, num_test_cases)

	def _prediction_and_confidence_scores(self, predictor: Predictor) -> Callable:
		return NotImplementedError

	def describe(self):
		self._summary(overview_only=True)

	def summary(
		self, capabilities: Optional[List[str]] = None, file: TextIO = sys.stdout, **kwargs
	):
		old_stdout = sys.stdout
		try:
			sys.stdout = file
			self._summary(capabilities=capabilities, **kwargs)
		finally:
			sys.stdout = old_stdout

	def _summary(
		self, overview_only: bool = False, capabilities: Optional[List[str]] = None, **kwargs
	):

		def cap_order(x):
			return self._capabilities.index(x) if x in self._capabilities else 100

		capabilities = capabilities or sorted(
			set([x["capability"] for x in self.suite.info.values()]), key=cap_order
		)
		print(
			"\n\nThis suite contains {} tests across {} capabilities.".format(
				len(self.suite.tests), len(capabilities)
			)
		)
		print()
		for capability in capabilities:
			tests = [
				name for name, test in self.suite.info.items() if test["capability"] == capability
			]
			num_tests = len(tests)
			if num_tests > 0:
				print(f'\nCapability: "{capability}" ({num_tests} tests)\n')
				for test in tests:
					description = self.suite.info[test]["description"]
					num_test_cases = len(self.suite.tests[test].data)
					about_test = f"* Name: {test} ({num_test_cases} test cases)"
					if description:
						about_test += f"\n{description}"
					print(about_test)

					if not overview_only:
						if "format_example_fn" not in kwargs:
							kwargs["format_example_fn"] = self.suite.info[test].get(
								"format_example_fn", self._format_failing_examples
							)
						if "print_fn" not in kwargs:
							kwargs["print_fn"] = self.suite.info[test].get(
								"print_fn", self.suite.print_fn
							)
						print()
						self.suite.tests[test].summary(**kwargs)
						print()

	def _format_failing_examples(
		self,
		inputs: Tuple[Any],
		pred: Any,
		conf: Union[np.array, np.ndarray],
		*args,
		**kwargs,
	):
		if conf.shape[0] <= 4:
			confs = " ".join(["%.1f" % c for c in conf])
			ret = "%s %s" % (confs, str(inputs))
		else:
			conf = conf[pred]
			ret = "%s (%.1f) %s" % (pred, conf, str(inputs))
		return ret

	def run(
		self,
		predictor: Predictor,
		capabilities: Optional[List[str]] = None,
		max_examples: Optional[int] = None,
	):
		preds_and_confs_fn = self._prediction_and_confidence_scores(predictor)
		if preds_and_confs_fn is NotImplementedError:
			raise NotImplementedError(
				"The `_prediction_and_confidence_scores` function needs "
				"to be implemented for the class `{}`".format(self.__class__)
			)
		if not capabilities:
			self.suite.run(preds_and_confs_fn, overwrite=True, n=max_examples)
		else:
			for _, test in self.suite.tests.items():
				if test.capability in capabilities:
					test.run(preds_and_confs_fn, verbose=True, overwrite=True, n=max_examples)

	@classmethod
	def constructor(
		cls,
		name: Optional[str] = None,
		suite_file: Optional[str] = None,
		extra_args: Optional[Dict[str, Any]] = None,
	) -> "TaskSuite":
		suite_class: Type[TaskSuite] = (
			TaskSuite.by_name(name) if name is not None else cls  # type: ignore
		)

		if extra_args is None:
			extra_args = {}

		if suite_file is not None:
			return suite_class(TestSuite.from_file(cached_path(suite_file)), **extra_args)
		return suite_class(**extra_args)

	def save_suite(self, suite_file: str):
		self.suite.save(suite_file)

	def _default_tests(self, data: Optional[Iterable], num_test_cases: int = 100):
		if data:


			self._punctuation_test(data, num_test_cases)
			self._typo_test(data, num_test_cases)
			self._contraction_test(data, num_test_cases)

	@classmethod
	def contractions(cls) -> Callable:
		return Perturb.contractions

	@classmethod
	def typos(cls) -> Callable:
		return Perturb.add_typos

	@classmethod
	def punctuation(cls) -> Callable:
		return utils.toggle_punctuation

	def _punctuation_test(self, data: Iterable, num_test_cases: int):
		template = Perturb.perturb(data, self.punctuation(), nsamples=num_test_cases)
		test = INV(
			template.data,
			name="Punctuation",
			description="Strip punctuation and / or add '.'",
			capability="Robustness",
		)
		self.add_test(test)

	def _typo_test(self, data: Iterable, num_test_cases: int):
		template = Perturb.perturb(data, self.typos(), nsamples=num_test_cases, typos=1)
		test = INV(
			template.data,
			name="Typos",
			capability="Robustness",
			description="Add one typo to input by swapping two adjacent characters",
		)

		self.add_test(test)

		template = Perturb.perturb(data, self.typos(), nsamples=num_test_cases, typos=2)
		test = INV(
			template.data,
			name="2 Typos",
			capability="Robustness",
			description="Add two typos to input by swapping two adjacent characters twice",
		)
		self.add_test(test)

	def _contraction_test(self, data: Iterable, num_test_cases: int):
		template = Perturb.perturb(data, self.contractions(), nsamples=num_test_cases)
		test = INV(
			template.data,
			name="Contractions",
			capability="Robustness",
			description="Contract or expand contractions, e.g. What is <-> What's",
		)
		self.add_test(test)

	def _setup_editor(self):
		if not hasattr(self, "editor"):
			self.editor = Editor()
			utils.add_common_lexicons(self.editor)

	def add_test(self, test: Union[MFT, INV, DIR]):
		if test.data:  # test data should contain at least one example.
			self.suite.add(test)
		else:
			logger.warning("'{}' was not added, as it contains no examples.".format(test.name))

from typing import Dict, List, Optional, Sequence, Iterable
import itertools
import logging
import warnings


from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path
from allennlp.data.dataset_readers.dataset_reader import DatasetReader, PathOrStr
from allennlp.data.dataset_readers.dataset_utils import to_bioul
from allennlp.data.fields import TextField, SequenceLabelField, Field, MetadataField
from allennlp.data.instance import Instance
from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer
from allennlp.data.tokenizers import Token

logger = logging.getLogger(__name__)


def _is_divider(line: str) -> bool:
	empty_line = line.strip() == ""
	if empty_line:
		return True
	else:
		first_token = line.split()[0]
		if first_token == "-DOCSTART-":
			return True
		else:
			return False


@DatasetReader.register("conll2003")
class Conll2003DatasetReader(DatasetReader):

	_VALID_LABELS = {"ner", "pos", "chunk"}

	def __init__(
		self,
		token_indexers: Dict[str, TokenIndexer] = None,
		tag_label: str = "ner",
		feature_labels: Sequence[str] = (),
		convert_to_coding_scheme: Optional[str] = None,
		label_namespace: str = "labels",
		**kwargs,
	) -> None:

		if "coding_scheme" in kwargs:
			warnings.warn("`coding_scheme` is deprecated.", DeprecationWarning)
			coding_scheme = kwargs.pop("coding_scheme")

			if coding_scheme not in ("IOB1", "BIOUL"):
				raise ConfigurationError("unknown coding_scheme: {}".format(coding_scheme))

			if coding_scheme == "IOB1":
				convert_to_coding_scheme = None
			else:
				convert_to_coding_scheme = coding_scheme

		super().__init__(
			manual_distributed_sharding=True, manual_multiprocess_sharding=True, **kwargs
		)
		self._token_indexers = token_indexers or {"tokens": SingleIdTokenIndexer()}
		if tag_label is not None and tag_label not in self._VALID_LABELS:
			raise ConfigurationError("unknown tag label type: {}".format(tag_label))
		for label in feature_labels:
			if label not in self._VALID_LABELS:
				raise ConfigurationError("unknown feature label type: {}".format(label))
		if convert_to_coding_scheme not in (None, "BIOUL"):
			raise ConfigurationError(
				"unknown convert_to_coding_scheme: {}".format(convert_to_coding_scheme)
			)

		self.tag_label = tag_label
		self.feature_labels = set(feature_labels)
		self.convert_to_coding_scheme = convert_to_coding_scheme
		self.label_namespace = label_namespace
		self._original_coding_scheme = "IOB1"

	def _read(self, file_path: PathOrStr) -> Iterable[Instance]:
		file_path = cached_path(file_path)

		with open(file_path, "r") as data_file:
			logger.info("Reading instances from lines in file at: %s", file_path)

			line_chunks = (
				lines
				for is_divider, lines in itertools.groupby(data_file, _is_divider)
				if not is_divider
			)
			for lines in self.shard_iterable(line_chunks):
				fields = [line.strip().split() for line in lines]
				fields = [list(field) for field in zip(*fields)]
				tokens_, pos_tags, chunk_tags, ner_tags = fields
				tokens = [Token(token) for token in tokens_]

				yield self.text_to_instance(tokens, pos_tags, chunk_tags, ner_tags)

	def text_to_instance(  # type: ignore
		self,
		tokens: List[Token],
		pos_tags: List[str] = None,
		chunk_tags: List[str] = None,
		ner_tags: List[str] = None,
	) -> Instance:

		sequence = TextField(tokens)
		instance_fields: Dict[str, Field] = {"tokens": sequence}
		instance_fields["metadata"] = MetadataField({"words": [x.text for x in tokens]})

		if self.convert_to_coding_scheme == "BIOUL":
			coded_chunks = (
				to_bioul(chunk_tags, encoding=self._original_coding_scheme)
				if chunk_tags is not None
				else None
			)
			coded_ner = (
				to_bioul(ner_tags, encoding=self._original_coding_scheme)
				if ner_tags is not None
				else None
			)
		else:
			coded_chunks = chunk_tags
			coded_ner = ner_tags

		if "pos" in self.feature_labels:
			if pos_tags is None:
				raise ConfigurationError(
					"Dataset reader was specified to use pos_tags as "
					"features. Pass them to text_to_instance."
				)
			instance_fields["pos_tags"] = SequenceLabelField(pos_tags, sequence, "pos_tags")
		if "chunk" in self.feature_labels:
			if coded_chunks is None:
				raise ConfigurationError(
					"Dataset reader was specified to use chunk tags as "
					"features. Pass them to text_to_instance."
				)
			instance_fields["chunk_tags"] = SequenceLabelField(coded_chunks, sequence, "chunk_tags")
		if "ner" in self.feature_labels:
			if coded_ner is None:
				raise ConfigurationError(
					"Dataset reader was specified to use NER tags as "
					" features. Pass them to text_to_instance."
				)
			instance_fields["ner_tags"] = SequenceLabelField(coded_ner, sequence, "ner_tags")

		if self.tag_label == "ner" and coded_ner is not None:
			instance_fields["tags"] = SequenceLabelField(coded_ner, sequence, self.label_namespace)
		elif self.tag_label == "pos" and pos_tags is not None:
			instance_fields["tags"] = SequenceLabelField(pos_tags, sequence, self.label_namespace)
		elif self.tag_label == "chunk" and coded_chunks is not None:
			instance_fields["tags"] = SequenceLabelField(
				coded_chunks, sequence, self.label_namespace
			)

		return Instance(instance_fields)

	def apply_token_indexers(self, instance: Instance) -> None:
		instance.fields["tokens"]._token_indexers = self._token_indexers  # type: ignore

from typing import Union, TYPE_CHECKING

import torch

from allennlp.common import FromParams
from allennlp.modules.transformer.activation_layer import ActivationLayer

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig


class TransformerPooler(ActivationLayer, FromParams):

	_pretrained_relevant_module = ["pooler", "bert.pooler", "roberta.pooler"]

	def __init__(
		self,
		hidden_size: int,
		intermediate_size: int,
		activation: Union[str, torch.nn.Module] = "relu",
	):
		super().__init__(hidden_size, intermediate_size, activation, pool=True)

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		return cls(config.hidden_size, config.hidden_size, "tanh")  # BERT has this hardcoded

import torch
import torch.fx as fx


invert_mapping = {}
def add_inverse(a, b):
	invert_mapping[a] = b
	invert_mapping[b] = a
inverses = [
	(torch.sin, torch.arcsin),
	(torch.cos, torch.arccos),
	(torch.tan, torch.arctan),
	(torch.exp, torch.log),
]
for a, b in inverses:
	add_inverse(a, b)

def invert(model: torch.nn.Module) -> torch.nn.Module:
	fx_model = fx.symbolic_trace(model)
	new_graph = fx.Graph()  # As we're building up a new graph
	env = {}
	for node in reversed(fx_model.graph.nodes):
		if node.op == 'call_function':
			new_node = new_graph.call_function(invert_mapping[node.target], (env[node.name],))
			env[node.args[0].name] = new_node
		elif node.op == 'output':
			new_node = new_graph.placeholder(node.name)
			env[node.args[0].name] = new_node
		elif node.op == 'placeholder':
			new_graph.output(env[node.name])
		else:
			raise RuntimeError("Not implemented")

	new_graph.lint()
	return fx.GraphModule(fx_model, new_graph)


def f(x):
	return torch.exp(torch.tan(x))

res = invert(f)
print(res.code)
print(f(res((torch.arange(5) + 1))))  # [1., 2., 3., 4, 5.]

from allennlp.evaluation.evaluator import Evaluator, SimpleEvaluator
from allennlp.evaluation.serializers.serializers import Serializer

from allennlp.modules.attention.attention import Attention
from allennlp.modules.attention.bilinear_attention import BilinearAttention
from allennlp.modules.attention.additive_attention import AdditiveAttention
from allennlp.modules.attention.cosine_attention import CosineAttention
from allennlp.modules.attention.dot_product_attention import DotProductAttention
from allennlp.modules.attention.linear_attention import LinearAttention
from allennlp.modules.attention.scaled_dot_product_attention import ScaledDotProductAttention

import torch

from torch.distributed.fsdp import (
	MixedPrecision,
)

fpSixteen = MixedPrecision(
	param_dtype=torch.float16,
	reduce_dtype=torch.float16,
	buffer_dtype=torch.float16,
)

bfSixteen = MixedPrecision(
	param_dtype=torch.bfloat16,
	reduce_dtype=torch.bfloat16,
	buffer_dtype=torch.bfloat16,
)

bfSixteen_working = MixedPrecision(
	param_dtype=torch.float32,
	reduce_dtype=torch.bfloat16,
	buffer_dtype=torch.bfloat16,
)

fp32_policy = MixedPrecision(
	param_dtype=torch.float32,
	reduce_dtype=torch.float32,
	buffer_dtype=torch.float32,
)

from typing import Dict, Any, List, Tuple, Optional


import torch

from allennlp.common.lazy import Lazy
from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler


@LearningRateScheduler.register("combined")
class CombinedLearningRateScheduler(LearningRateScheduler):

	def __init__(
		self,
		optimizer: torch.optim.Optimizer,
		schedulers: List[Tuple[int, Lazy[LearningRateScheduler]]],
		num_steps_per_epoch: Optional[int] = None,
		last_epoch: int = -1,
	) -> None:
		super().__init__(optimizer, last_epoch=last_epoch)
		self.num_steps_per_epoch = num_steps_per_epoch
		self.schedulers = schedulers
		self._last_epoch_updated = -2
		self._current_scheduler: Optional[LearningRateScheduler] = None
		self._current_scheduler_first_epoch: Optional[int] = None
		self.current_scheduler

	@property
	def current_scheduler(self) -> Optional[LearningRateScheduler]:
		if self._last_epoch_updated != self.last_epoch:
			current_epoch = self.last_epoch + 1
			scheduler_first_epoch, scheduler_last_epoch = 0, -1
			for scheduler_epochs, lazy_scheduler in self.schedulers:
				scheduler_last_epoch += scheduler_epochs

				if current_epoch == scheduler_first_epoch or (
					self._current_scheduler_first_epoch != scheduler_first_epoch
					and scheduler_first_epoch <= current_epoch <= scheduler_last_epoch
				):
					for group in self.optimizer.param_groups:
						group[self._initial_param_group_field] = group[self.param_group_field]
					self._current_scheduler = lazy_scheduler.construct(
						optimizer=self.optimizer,
						num_epochs=scheduler_epochs,
						num_steps_per_epoch=self.num_steps_per_epoch,
					)
					self._current_scheduler_first_epoch = scheduler_first_epoch
					break

				scheduler_first_epoch = scheduler_last_epoch + 1
			else:
				if current_epoch > scheduler_last_epoch:
					self._current_scheduler = None
		self._last_epoch_updated = self.last_epoch
		return self._current_scheduler

	def state_dict(self) -> Dict[str, Any]:
		current_scheduler = self.current_scheduler
		return {
			"last_epoch": self.last_epoch,
			"num_steps_per_epoch": self.num_steps_per_epoch,
			"current_scheduler": None
			if current_scheduler is None
			else current_scheduler.state_dict(),
		}

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		self.last_epoch = state_dict["last_epoch"]
		self.num_steps_per_epoch = state_dict["num_steps_per_epoch"]
		if self.current_scheduler is not None:
			assert state_dict["current_scheduler"] is not None
			self.current_scheduler.load_state_dict(state_dict["current_scheduler"])

	def get_values(self):
		raise NotImplementedError

	def step_batch(self, batch_num_total: int = None) -> None:
		if self.current_scheduler is not None:
			self.current_scheduler.step_batch(batch_num_total)

	def step(self, metric: float = None) -> None:
		self.last_epoch += 1
		self.metric = metric
		if self.current_scheduler is not None:
			self.current_scheduler.step(metric)

import os
import sys
import torch
import torch.nn as nn

from torch.distributed.tensor.parallel import (
	parallelize_module,
	ColwiseParallel,
	RowwiseParallel,
)

from log_utils import rank_log, get_logger, verify_min_gpu_count

_min_gpu_count = 2

if not verify_min_gpu_count(min_gpus=_min_gpu_count):
	print(f"Unable to locate sufficient {_min_gpu_count} gpus to run this example. Exiting.")
	sys.exit()

from torch.distributed._tensor.device_mesh import init_device_mesh





class ToyModel(nn.Module):

	def __init__(self):
		super(ToyModel, self).__init__()
		self.in_proj = nn.Linear(10, 32)
		self.relu = nn.ReLU()
		self.out_proj = nn.Linear(32, 5)

	def forward(self, x):
		return self.out_proj(self.relu(self.in_proj(x)))


logger = get_logger()

_world_size = int(os.environ["WORLD_SIZE"])

device_mesh = init_device_mesh(device_type="cuda", mesh_shape=(_world_size,))
_rank = device_mesh.get_rank()


print(f"Starting PyTorch TP example on rank {_rank}.")
assert (
	_world_size % 2 == 0
), f"TP examples require even number of GPUs, but got {_world_size} gpus"

rank_log(_rank, logger, f"Device Mesh created: {device_mesh=}")

tp_model = ToyModel().to("cuda")

lr = 0.25
optimizer = torch.optim.AdamW(tp_model.parameters(), lr=lr, foreach=True)

tp_model = parallelize_module(
	module=tp_model,
	device_mesh=device_mesh,
	parallelize_plan={
		"in_proj": ColwiseParallel(),
		"out_proj": RowwiseParallel(),
	},
)
num_iters = 10
rank_log(_rank, logger, "Tensor Parallel training starting...")

for i in range(num_iters):
	torch.manual_seed(i)
	inp = torch.rand(20, 10, device="cuda")
	output = tp_model(inp)
	output.sum().backward()
	optimizer.step()
	rank_log(_rank, logger, f"Tensor Parallel iter {i} completed")

rank_log(_rank, logger, "Tensor Parallel training completed!")

import math
from typing import Optional, Tuple, TYPE_CHECKING
from dataclasses import dataclass
import torch
import torch.nn.functional as F

from allennlp.common import FromParams
from allennlp.common.checks import ConfigurationError
from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention
from allennlp.modules.transformer.transformer_module import TransformerModule
from allennlp.modules.transformer.util import apply_mask, FloatT, IntT, BoolT

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig


@dataclass
class AttentionOutput:

	hidden_states: FloatT
	key_value_state: Optional[Tuple[FloatT, FloatT]] = None
	position_bias: Optional[FloatT] = None
	attention_probs: Optional[FloatT] = None


class AttentionModule(TransformerModule, FromParams):

	def __init__(
		self,
		hidden_size: int = 512,
		attention_head_size: int = 64,
		num_attention_heads: int = 8,
		scoring_func: str = "scaled_dot_product",
		output_linear: bool = False,
		dropout: float = 0.0,
		bias: bool = True,
		normalize_weights: bool = False,
		is_decoder: bool = False,
		is_cross_attention: bool = False,
		relative_attention_num_buckets: Optional[int] = None,
	):

		super().__init__()

		if hidden_size % num_attention_heads != 0:
			raise ConfigurationError(
				"The hidden size (%d) is not a multiple of the number of attention "
				"heads (%d)" % (hidden_size, num_attention_heads)
			)

		if is_cross_attention and not is_decoder:
			raise ConfigurationError(
				"The attention layer can be a cross-attention layer only "
				"if it is within a decoder."
			)

		self.hidden_size = hidden_size
		self.num_attention_heads = num_attention_heads
		self.attention_head_size = attention_head_size
		self.all_head_size = self.num_attention_heads * self.attention_head_size

		self.query = torch.nn.Linear(hidden_size, self.all_head_size, bias=bias)
		self.key = torch.nn.Linear(hidden_size, self.all_head_size, bias=bias)
		self.value = torch.nn.Linear(hidden_size, self.all_head_size, bias=bias)

		if output_linear:
			self.output = torch.nn.Linear(self.all_head_size, hidden_size, bias=bias)

		self.scoring_func = scoring_func
		self.attn = MatrixAttention.by_name(self.scoring_func)()

		self.relative_attention_num_buckets = relative_attention_num_buckets

		if self.relative_attention_num_buckets is not None:
			self.relative_attention_bias = torch.nn.Embedding(
				self.relative_attention_num_buckets, self.num_attention_heads
			)

		self.dropout = dropout

		self.is_decoder = is_decoder
		self.is_cross_attention = is_cross_attention

		if normalize_weights:
			self._normalize()

	def _normalize(self) -> None:
		self.query.weight.data.normal_(
			mean=0.0, std=(self.hidden_size * self.attention_head_size) ** -0.5
		)
		self.key.weight.data.normal_(mean=0.0, std=self.hidden_size**-0.5)
		self.value.weight.data.normal_(mean=0.0, std=self.hidden_size**-0.5)

		if hasattr(self, "output"):
			self.output.weight.data.normal_(
				mean=0.0, std=(self.num_attention_heads * self.attention_head_size) ** -0.5
			)

		if hasattr(self, "relative_attention_bias"):
			self.relative_attention_bias.weight.data.normal_(mean=0.0, std=self.hidden_size**-0.5)

	def _transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:
		new_x_shape = x.size()[:-1] + (
			self.num_attention_heads,
			self.attention_head_size,
		)
		x = x.view(*new_x_shape)
		return x.permute(0, 2, 1, 3)

	def _query_layer(self, query_states: torch.Tensor) -> torch.Tensor:
		mixed_query_layer = self.query(query_states)
		query_layer = self._transpose_for_scores(mixed_query_layer)
		return query_layer

	def _project(
		self,
		hidden_states: torch.Tensor,
		layer: torch.nn.Linear,
		source_states: Optional[torch.Tensor] = None,
		past_key_or_value: Optional[torch.Tensor] = None,
	) -> torch.Tensor:
		if source_states is None:
			hidden_states = self._transpose_for_scores(layer(hidden_states))
		elif past_key_or_value is None:
			hidden_states = self._transpose_for_scores(layer(source_states))

		if past_key_or_value is not None:
			if source_states is None:
				hidden_states = torch.cat([past_key_or_value, hidden_states], dim=2)
			else:
				hidden_states = past_key_or_value
		return hidden_states

	def _position_bias(
		self,
		position_bias: Optional[torch.Tensor],
		seq_lengths: Tuple[int, int, int],
		past_key_states: Optional[torch.Tensor],
		attention_scores: torch.Tensor,
	) -> torch.Tensor:
		seq_length, real_seq_length, key_length = seq_lengths

		if position_bias is None:
			if self.relative_attention_num_buckets is not None:
				position_bias = self.compute_bias(real_seq_length, key_length)
			else:
				position_bias = torch.zeros(
					(1, self.num_attention_heads, real_seq_length, key_length),
					device=attention_scores.device,
					dtype=attention_scores.dtype,
				)

			if past_key_states is not None:
				position_bias = position_bias[:, :, -seq_length:, :]
		return position_bias

	def _get_attention_probs(
		self,
		query_layer: torch.Tensor,
		key_layer: torch.Tensor,
		attention_mask: torch.Tensor,
		head_mask: torch.Tensor,
		seq_lengths: Tuple[int, int, int],
		position_bias: Optional[torch.Tensor] = None,
		past_key_states: Optional[torch.Tensor] = None,
		**kwargs,
	):
		attention_scores = self.attn(query_layer, key_layer)

		position_bias = self._position_bias(
			position_bias, seq_lengths, past_key_states, attention_scores
		)

		if attention_mask is not None:
			position_bias = apply_mask(position_bias, attention_mask)
		attention_scores += position_bias

		attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)

		attention_probs = F.dropout(attention_probs, p=self.dropout, training=self.training)

		if head_mask is not None:
			attention_probs = attention_probs * head_mask

		return attention_probs, position_bias

	def _output_layer(self, attention_probs: torch.Tensor, value_layer: torch.Tensor):
		context_layer = torch.matmul(attention_probs, value_layer)
		context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
		new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
		context_layer = context_layer.view(*new_context_layer_shape)

		if hasattr(self, "output"):
			context_layer = self.output(context_layer)

		return context_layer

	def _get_lengths(
		self,
		query_states: torch.Tensor,
		past_key_states: Optional[torch.Tensor] = None,
		source_states: Optional[torch.Tensor] = None,
		query_length: Optional[int] = None,
	) -> Tuple[int, int, int]:

		seq_length = query_states.shape[1]
		effective_seq_len = seq_length

		if past_key_states is not None:
			effective_seq_len += past_key_states.shape[2] if query_length is None else query_length

		key_length = effective_seq_len if source_states is None else source_states.shape[1]

		return (seq_length, effective_seq_len, key_length)

	def forward(
		self,
		query_states: torch.Tensor,
		past_key_states: Optional[torch.Tensor] = None,
		past_value_states: Optional[torch.Tensor] = None,
		attention_mask: Optional[torch.BoolTensor] = None,
		source_states: Optional[torch.Tensor] = None,
		source_attention_mask: Optional[torch.BoolTensor] = None,
		head_mask: Optional[torch.Tensor] = None,
		position_bias: Optional[torch.Tensor] = None,
		output_attentions: bool = False,
		use_cache: bool = False,
		query_length: Optional[int] = None,
	):
		query_layer = self._query_layer(query_states)

		key_layer = self._project(
			query_states,
			self.key,
			source_states,
			past_key_states,
		)

		value_layer = self._project(
			query_states,
			self.value,
			source_states,
			past_value_states,
		)

		if self.is_cross_attention:
			attention_mask = source_attention_mask

		seq_lengths = self._get_lengths(query_states, past_key_states, source_states, query_length)

		attention_probs, position_bias = self._get_attention_probs(
			query_layer,
			key_layer,
			attention_mask,
			head_mask,
			seq_lengths,
			position_bias,
			past_key_states,
		)

		context_layer = self._output_layer(attention_probs, value_layer)

		present_key_value_state = (
			(key_layer, value_layer) if (self.is_decoder and use_cache) else None
		)

		if not output_attentions:
			attention_probs = None

		outputs = AttentionOutput(
			context_layer, present_key_value_state, position_bias, attention_probs
		)

		return outputs

	@staticmethod
	def _relative_position_bucket(
		relative_position: IntT,
		bidirectional: bool = True,
		num_buckets: int = 32,
		max_distance: int = 128,
	) -> IntT:
		relative_buckets = relative_position.new_zeros(relative_position.shape)
		if bidirectional:
			num_buckets //= 2
			relative_buckets += (relative_position > 0).to(torch.long) * num_buckets
			relative_position = torch.abs(relative_position)
		else:
			relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))

		max_exact = num_buckets // 2
		is_small = relative_position < max_exact

		relative_postion_if_large = max_exact + (
			torch.log(relative_position.float() / max_exact)
			/ math.log(max_distance / max_exact)
			* (num_buckets - max_exact)
		).to(torch.long)
		relative_postion_if_large = torch.min(
			relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)
		)

		relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)
		return relative_buckets

	def compute_bias(self, query_length: int, key_length: int) -> FloatT:
		context_position = torch.arange(query_length, dtype=torch.long)[:, None]
		memory_position = torch.arange(key_length, dtype=torch.long)[None, :]
		relative_position = memory_position - context_position  # shape (query_length, key_length)
		relative_position_bucket = self._relative_position_bucket(
			relative_position,  # shape (query_length, key_length)
			bidirectional=(not self.is_decoder),
			num_buckets=self.relative_attention_num_buckets,  # type: ignore
		)
		relative_position_bucket = relative_position_bucket.to(
			self.relative_attention_bias.weight.device
		)
		values = self.relative_attention_bias(
			relative_position_bucket
		)  # shape (query_length, key_length, num_heads)
		values = values.permute([2, 0, 1]).unsqueeze(
			0
		)  # shape (1, num_heads, query_length, key_length)
		return values


class T5Attention(AttentionModule):

	_pretrained_relevant_module = ["encoder.block.0.layer.0.SelfAttention"]
	_pretrained_mapping = {
		"q": "query",
		"k": "key",
		"v": "value",
		"o": "output",
	}

	def __init__(
		self,
		is_decoder: bool = False,
		hidden_size: int = 512,
		key_value_proj_dim: int = 64,
		num_heads: int = 8,
		has_relative_attention_bias: bool = False,
		relative_attention_num_buckets: int = 32,
		dropout: float = 0.1,
		normalize: bool = True,
		is_cross_attention: bool = False,
	):

		if not has_relative_attention_bias:
			relative_attention_num_buckets = None  # type: ignore

		super().__init__(
			hidden_size=hidden_size,
			attention_head_size=key_value_proj_dim,
			num_attention_heads=num_heads,
			output_linear=True,
			scoring_func="dot_product",
			dropout=dropout,
			bias=False,
			normalize_weights=normalize,
			is_decoder=is_decoder,
			is_cross_attention=is_cross_attention,
			relative_attention_num_buckets=relative_attention_num_buckets,
		)

	def forward(  # type: ignore
		self,
		hidden_states: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
		key_value_states: Optional[FloatT] = None,
		position_bias: Optional[FloatT] = None,
		past_key_value: Optional[
			Tuple[FloatT, FloatT]
		] = None,  # this is used when taking decoding steps.
		layer_head_mask: Optional[BoolT] = None,
		query_length: Optional[int] = None,  # only relevant in cross-attention.
		use_cache: bool = False,
		output_attentions: bool = False,
	) -> AttentionOutput:
		if past_key_value:
			past_key_states = past_key_value[0]
			past_value_states = past_key_value[1]
		else:
			past_key_states = None
			past_value_states = None

		outputs = super().forward(
			query_states=hidden_states,
			past_key_states=past_key_states,
			past_value_states=past_value_states,
			attention_mask=mask,
			source_states=key_value_states,
			source_attention_mask=None,  # TODO: is this a bug in current T5 code?
			head_mask=layer_head_mask,
			position_bias=position_bias,
			output_attentions=output_attentions,
			use_cache=use_cache,
			query_length=query_length,
		)

		return outputs

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		final_kwargs = {}
		final_kwargs["hidden_size"] = config.hidden_size
		final_kwargs["key_value_proj_dim"] = config.d_kv

		final_kwargs["is_decoder"] = getattr(config, "is_decoder", False)
		final_kwargs["has_relative_attention_bias"] = getattr(
			config, "has_relative_attention_bias", True
		)
		final_kwargs["normalize"] = getattr(config, "normalize", True)
		final_kwargs["is_cross_attention"] = getattr(config, "is_cross_attention", False)

		final_kwargs["relative_attention_num_buckets"] = config.relative_attention_num_buckets
		final_kwargs["num_heads"] = config.num_attention_heads

		final_kwargs["dropout"] = config.dropout_rate
		final_kwargs.update(**kwargs)
		return cls(**final_kwargs)


class SelfAttention(AttentionModule):

	_pretrained_relevant_module = ["encoder.layers.0.attention.self", "encoder.layers.0.attention"]
	_pretrained_mapping = {
		"layer": "layers",
		"q_lin": "query",
		"k_lin": "key",
		"v_lin": "value",
		"out_lin": "output",
		"transformer": "encoder",
	}

	def __init__(
		self,
		hidden_size: int,
		num_attention_heads: int,
		dropout: float = 0.0,
		scoring_func: str = "scaled_dot_product",
		output_linear: bool = False,
		is_decoder: bool = False,
		is_cross_attention: bool = False,
	):

		attention_head_size = int(hidden_size / num_attention_heads)

		super().__init__(
			hidden_size=hidden_size,
			attention_head_size=attention_head_size,
			num_attention_heads=num_attention_heads,
			scoring_func=scoring_func,
			output_linear=output_linear,
			dropout=dropout,
			bias=True,
			is_decoder=is_decoder,
			is_cross_attention=is_cross_attention,
		)

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		final_kwargs = {}
		final_kwargs["hidden_size"] = config.hidden_size
		final_kwargs["num_attention_heads"] = config.num_attention_heads
		final_kwargs["output_linear"] = hasattr(
			config, "n_heads"
		)  # This is the distilbert case; they have a linear layer as the output.
		if hasattr(config, "attention_dropout"):
			final_kwargs["dropout"] = config.attention_dropout
		else:
			final_kwargs["dropout"] = config.attention_probs_dropout_prob
		final_kwargs.update(**kwargs)
		return cls(**final_kwargs)


import argparse
import json
import logging
import math
import os
import random
from itertools import chain
from pathlib import Path

import datasets
import torch
from accelerate import Accelerator, DistributedType
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_MAPPING,
	AutoConfig,
	AutoModelForCausalLM,
	AutoTokenizer,
	SchedulerType,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/language-modeling/requirements.txt")

MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a causal language modeling task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv, txt or a json file containing the training data."
	)
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv, txt or a json file containing the validation data."
	)
	parser.add_argument(
		"--validation_split_percentage",
		default=5,
		help="The percentage of the train set used as validation set in case there's no validation split",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=False,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--tokenizer_name",
		type=str,
		default=None,
		help="Pretrained tokenizer name or path if not the same as model_name",
	)
	parser.add_argument(
		"--use_slow_tokenizer",
		action="store_true",
		help="If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="Model type to use if training from scratch.",
		choices=MODEL_TYPES,
	)
	parser.add_argument(
		"--block_size",
		type=int,
		default=None,
		help=(
			"Optional input sequence length after tokenization. The training dataset will be truncated in block of"
			" this size for training. Default to the model max input length for single sentence inputs (take into"
			" account special tokens)."
		),
	)
	parser.add_argument(
		"--preprocessing_num_workers",
		type=int,
		default=None,
		help="The number of processes to use for the preprocessing.",
	)
	parser.add_argument(
		"--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
	)
	parser.add_argument(
		"--no_keep_linebreaks", action="store_true", help="Do not keep line breaks when using TXT files."
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	parser.add_argument(
		"--low_cpu_mem_usage",
		action="store_true",
		help=(
			"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. "
			"If passed, LLM loading time and RAM consumption will be benefited."
		),
	)
	args = parser.parse_args()

	if args.dataset_name is None and args.train_file is None and args.validation_file is None:
		raise ValueError("Need either a dataset name or a training/validation file.")
	else:
		if args.train_file is not None:
			extension = args.train_file.split(".")[-1]
			if extension not in ["csv", "json", "txt"]:
				raise ValueError("`train_file` should be a csv, json or txt file.")
		if args.validation_file is not None:
			extension = args.validation_file.split(".")[-1]
			if extension not in ["csv", "json", "txt"]:
				raise ValueError("`validation_file` should be a csv, json or txt file.")

	if args.push_to_hub:
		if args.output_dir is None:
			raise ValueError("Need an `output_dir` to create a repo when `--push_to_hub` is passed.")

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_clm_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				args.dataset_name,
				args.dataset_config_name,
				split=f"train[:{args.validation_split_percentage}%]",
			)
			raw_datasets["train"] = load_dataset(
				args.dataset_name,
				args.dataset_config_name,
				split=f"train[{args.validation_split_percentage}%:]",
			)
	else:
		data_files = {}
		dataset_args = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		extension = args.train_file.split(".")[-1]
		if extension == "txt":
			extension = "text"
			dataset_args["keep_linebreaks"] = not args.no_keep_linebreaks
		raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)
		if "validation" not in raw_datasets.keys():
			raw_datasets["validation"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[:{args.validation_split_percentage}%]",
				**dataset_args,
			)
			raw_datasets["train"] = load_dataset(
				extension,
				data_files=data_files,
				split=f"train[{args.validation_split_percentage}%:]",
				**dataset_args,
			)


	if args.config_name:
		config = AutoConfig.from_pretrained(
			args.config_name,
			trust_remote_code=args.trust_remote_code,
		)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(
			args.model_name_or_path,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")

	if args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(
			args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	elif args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(
			args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if args.model_name_or_path:
		model = AutoModelForCausalLM.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			low_cpu_mem_usage=args.low_cpu_mem_usage,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForCausalLM.from_config(config, trust_remote_code=args.trust_remote_code)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	column_names = raw_datasets["train"].column_names
	text_column_name = "text" if "text" in column_names else column_names[0]

	def tokenize_function(examples):
		return tokenizer(examples[text_column_name])

	with accelerator.main_process_first():
		tokenized_datasets = raw_datasets.map(
			tokenize_function,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on dataset",
		)

	if args.block_size is None:
		block_size = tokenizer.model_max_length
		if block_size > config.max_position_embeddings:
			logger.warning(
				f"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). "
				f"Using block_size={min(1024, config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx."
			)
			block_size = min(1024, config.max_position_embeddings)
	else:
		if args.block_size > tokenizer.model_max_length:
			logger.warning(
				f"The block_size passed ({args.block_size}) is larger than the maximum length for the model "
				f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
			)
		block_size = min(args.block_size, tokenizer.model_max_length)

	def group_texts(examples):
		concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
		total_length = len(concatenated_examples[list(examples.keys())[0]])
		total_length = (total_length // block_size) * block_size
		result = {
			k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
			for k, t in concatenated_examples.items()
		}
		result["labels"] = result["input_ids"].copy()
		return result


	with accelerator.main_process_first():
		lm_datasets = tokenized_datasets.map(
			group_texts,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			load_from_cache_file=not args.overwrite_cache,
			desc=f"Grouping texts in chunks of {block_size}",
		)

	train_dataset = lm_datasets["train"]
	eval_dataset = lm_datasets["validation"]

	for index in random.sample(range(len(train_dataset)), 3):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(
		eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size
	)

	no_decay = ["bias", "layer_norm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	if accelerator.distributed_type == DistributedType.TPU:
		model.tie_weights()

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("clm_no_trainer", experiment_config)

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()
				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)
			if completed_steps >= args.max_train_steps:
				break

		model.eval()
		losses = []
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				outputs = model(**batch)

			loss = outputs.loss
			losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

		losses = torch.cat(losses)
		try:
			eval_loss = torch.mean(losses)
			perplexity = math.exp(eval_loss)
		except OverflowError:
			perplexity = float("inf")

		logger.info(f"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}")

		if args.with_tracking:
			accelerator.log(
				{
					"perplexity": perplexity,
					"eval_loss": eval_loss,
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
				json.dump({"perplexity": perplexity}, f)


if __name__ == "__main__":
	main()

import torch
from torch.nn.utils.rnn import pad_packed_sequence

from allennlp.common.checks import ConfigurationError
from allennlp.modules.augmented_lstm import AugmentedLstm
from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder
from allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm
from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm


class PytorchSeq2SeqWrapper(Seq2SeqEncoder):

	def __init__(self, module: torch.nn.Module, stateful: bool = False) -> None:
		super().__init__(stateful)
		self._module = module
		try:
			if not self._module.batch_first:
				raise ConfigurationError("Our encoder semantics assumes batch is always first!")
		except AttributeError:
			pass

		try:
			self._is_bidirectional = self._module.bidirectional
		except AttributeError:
			self._is_bidirectional = False
		if self._is_bidirectional:
			self._num_directions = 2
		else:
			self._num_directions = 1

	def get_input_dim(self) -> int:
		return self._module.input_size

	def get_output_dim(self) -> int:
		return self._module.hidden_size * self._num_directions

	def is_bidirectional(self) -> bool:
		return self._is_bidirectional

	def forward(
		self, inputs: torch.Tensor, mask: torch.BoolTensor, hidden_state: torch.Tensor = None
	) -> torch.Tensor:

		if self.stateful and mask is None:
			raise ValueError("Always pass a mask with stateful RNNs.")
		if self.stateful and hidden_state is not None:
			raise ValueError("Stateful RNNs provide their own initial hidden_state.")

		if mask is None:
			return self._module(inputs, hidden_state)[0]

		batch_size, total_sequence_length = mask.size()

		packed_sequence_output, final_states, restoration_indices = self.sort_and_run_forward(
			self._module, inputs, mask, hidden_state
		)

		unpacked_sequence_tensor, _ = pad_packed_sequence(packed_sequence_output, batch_first=True)

		num_valid = unpacked_sequence_tensor.size(0)
		if not isinstance(final_states, (list, tuple)) and self.stateful:
			final_states = [final_states]

		if num_valid < batch_size:
			_, length, output_dim = unpacked_sequence_tensor.size()
			zeros = unpacked_sequence_tensor.new_zeros(batch_size - num_valid, length, output_dim)
			unpacked_sequence_tensor = torch.cat([unpacked_sequence_tensor, zeros], 0)

			if self.stateful:
				new_states = []
				for state in final_states:
					num_layers, _, state_dim = state.size()
					zeros = state.new_zeros(num_layers, batch_size - num_valid, state_dim)
					new_states.append(torch.cat([state, zeros], 1))
				final_states = new_states

		sequence_length_difference = total_sequence_length - unpacked_sequence_tensor.size(1)
		if sequence_length_difference > 0:
			zeros = unpacked_sequence_tensor.new_zeros(
				batch_size, sequence_length_difference, unpacked_sequence_tensor.size(-1)
			)
			unpacked_sequence_tensor = torch.cat([unpacked_sequence_tensor, zeros], 1)

		if self.stateful:
			self._update_states(final_states, restoration_indices)

		return unpacked_sequence_tensor.index_select(0, restoration_indices)


@Seq2SeqEncoder.register("gru")
class GruSeq2SeqEncoder(PytorchSeq2SeqWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int = 1,
		bias: bool = True,
		dropout: float = 0.0,
		bidirectional: bool = False,
		stateful: bool = False,
	):
		module = torch.nn.GRU(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			bias=bias,
			batch_first=True,
			dropout=dropout,
			bidirectional=bidirectional,
		)
		super().__init__(module=module, stateful=stateful)


@Seq2SeqEncoder.register("lstm")
class LstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int = 1,
		bias: bool = True,
		dropout: float = 0.0,
		bidirectional: bool = False,
		stateful: bool = False,
	):
		module = torch.nn.LSTM(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			bias=bias,
			batch_first=True,
			dropout=dropout,
			bidirectional=bidirectional,
		)
		super().__init__(module=module, stateful=stateful)


@Seq2SeqEncoder.register("rnn")
class RnnSeq2SeqEncoder(PytorchSeq2SeqWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int = 1,
		nonlinearity: str = "tanh",
		bias: bool = True,
		dropout: float = 0.0,
		bidirectional: bool = False,
		stateful: bool = False,
	):
		module = torch.nn.RNN(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			nonlinearity=nonlinearity,
			bias=bias,
			batch_first=True,
			dropout=dropout,
			bidirectional=bidirectional,
		)
		super().__init__(module=module, stateful=stateful)


@Seq2SeqEncoder.register("augmented_lstm")
class AugmentedLstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		go_forward: bool = True,
		recurrent_dropout_probability: float = 0.0,
		use_highway: bool = True,
		use_input_projection_bias: bool = True,
		stateful: bool = False,
	) -> None:
		module = AugmentedLstm(
			input_size=input_size,
			hidden_size=hidden_size,
			go_forward=go_forward,
			recurrent_dropout_probability=recurrent_dropout_probability,
			use_highway=use_highway,
			use_input_projection_bias=use_input_projection_bias,
		)
		super().__init__(module=module, stateful=stateful)


@Seq2SeqEncoder.register("alternating_lstm")
class StackedAlternatingLstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int,
		recurrent_dropout_probability: float = 0.0,
		use_highway: bool = True,
		use_input_projection_bias: bool = True,
		stateful: bool = False,
	) -> None:
		module = StackedAlternatingLstm(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			recurrent_dropout_probability=recurrent_dropout_probability,
			use_highway=use_highway,
			use_input_projection_bias=use_input_projection_bias,
		)
		super().__init__(module=module, stateful=stateful)


@Seq2SeqEncoder.register("stacked_bidirectional_lstm")
class StackedBidirectionalLstmSeq2SeqEncoder(PytorchSeq2SeqWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int,
		recurrent_dropout_probability: float = 0.0,
		layer_dropout_probability: float = 0.0,
		use_highway: bool = True,
		stateful: bool = False,
	) -> None:
		module = StackedBidirectionalLstm(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			recurrent_dropout_probability=recurrent_dropout_probability,
			layer_dropout_probability=layer_dropout_probability,
			use_highway=use_highway,
		)
		super().__init__(module=module, stateful=stateful)

import re
from typing import List


from allennlp.data.tokenizers.token_class import Token
from allennlp.data.tokenizers.tokenizer import Tokenizer


@Tokenizer.register("letters_digits")
class LettersDigitsTokenizer(Tokenizer):

	def tokenize(self, text: str) -> List[Token]:
		tokens = [Token(m.group(), idx=m.start()) for m in re.finditer(r"[^\W\d_]+|\d+|\S", text)]
		return tokens

import argparse
import logging
import sys
from typing import Any, Optional, Tuple, Set
import warnings

from allennlp import __version__
from allennlp.commands.build_vocab import BuildVocab
from allennlp.commands.cached_path import CachedPath
from allennlp.commands.checklist import CheckList
from allennlp.commands.diff import Diff
from allennlp.commands.evaluate import Evaluate
from allennlp.commands.find_learning_rate import FindLearningRate
from allennlp.commands.predict import Predict
from allennlp.commands.print_results import PrintResults
from allennlp.commands.subcommand import Subcommand
from allennlp.commands.test_install import TestInstall
from allennlp.commands.train import Train
from allennlp.commands.push_to_hf import PushToHf
from allennlp.commands.count_instances import CountInstances
from allennlp.common.plugins import import_plugins
from allennlp.common.util import import_module_and_submodules

logger = logging.getLogger(__name__)


class ArgumentParserWithDefaults(argparse.ArgumentParser):

	_action_defaults_to_ignore = {"help", "store_true", "store_false", "store_const"}

	@staticmethod
	def _is_empty_default(default: Any) -> bool:
		if default is None:
			return True
		if isinstance(default, (str, list, tuple, set)):
			return not bool(default)
		return False

	def add_argument(self, *args, **kwargs):
		default = kwargs.get("default")
		if kwargs.get(
			"action"
		) not in self._action_defaults_to_ignore and not self._is_empty_default(default):
			description = kwargs.get("help", "")
			kwargs["help"] = f"{description} (default = {default})"
		super().add_argument(*args, **kwargs)


def parse_args(prog: Optional[str] = None) -> Tuple[argparse.ArgumentParser, argparse.Namespace]:
	parser = ArgumentParserWithDefaults(description="Run AllenNLP", prog=prog)
	parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

	subparsers = parser.add_subparsers(title="Commands", metavar="")

	subcommands: Set[str] = set()

	def add_subcommands():
		for subcommand_name in sorted(Subcommand.list_available()):
			if subcommand_name in subcommands:
				continue
			subcommands.add(subcommand_name)
			subcommand_class = Subcommand.by_name(subcommand_name)
			subcommand = subcommand_class()
			subparser = subcommand.add_subparser(subparsers)
			if subcommand_class.requires_plugins:
				subparser.add_argument(
					"--include-package",
					type=str,
					action="append",
					default=[],
					help="additional packages to include",
				)

	add_subcommands()

	argv = sys.argv[1:]
	plugins_imported: bool = False
	if not argv or argv == ["--help"] or argv[0] not in subcommands:
		import_plugins()
		plugins_imported = True
		add_subcommands()

	args = parser.parse_args()

	if not plugins_imported and Subcommand.by_name(argv[0]).requires_plugins:  # type: ignore
		import_plugins()

	return parser, args


def main(prog: Optional[str] = None) -> None:
	parser, args = parse_args(prog)

	if "func" in dir(args):
		for package_name in getattr(args, "include_package", []):
			import_module_and_submodules(package_name)
		args.func(args)
	else:
		parser.print_help()

from allennlp.modules.backbones.backbone import Backbone
from allennlp.modules.backbones.pretrained_transformer_backbone import PretrainedTransformerBackbone
from allennlp.modules.backbones.vilbert_backbone import VilbertBackbone

from typing import Union, Optional, TYPE_CHECKING
from dataclasses import dataclass

import torch

from allennlp.common import FromParams
from allennlp.modules.transformer.transformer_module import TransformerModule
from allennlp.modules.transformer.activation_layer import ActivationLayer
from allennlp.modules.transformer.attention_module import SelfAttention, AttentionOutput
from allennlp.modules.transformer.output_layer import OutputLayer
from allennlp.modules.transformer.util import FloatT

if TYPE_CHECKING:
	from transformers.configuration_utils import PretrainedConfig


class AttentionLayer(TransformerModule, FromParams):

	_pretrained_relevant_module = "encoder.layer.0.attention"
	_pretrained_mapping = {"layer": "layers"}

	def __init__(
		self,
		hidden_size: int,
		num_attention_heads: int,
		attention_dropout: float = 0.0,
		hidden_dropout: float = 0.0,
		is_cross_attention: bool = False,
		is_decoder: bool = False,
	):
		super().__init__()
		self.self = SelfAttention(
			hidden_size,
			num_attention_heads,
			attention_dropout,
			is_cross_attention=is_cross_attention,
			is_decoder=is_decoder,
		)
		self.output = OutputLayer(hidden_size, hidden_size, hidden_dropout)

	def forward(
		self,
		input_tensor: torch.Tensor,
		attention_mask: torch.BoolTensor,
		head_mask: Optional[torch.Tensor] = None,
		encoder_hidden_states: Optional[torch.Tensor] = None,
		encoder_attention_mask: Optional[torch.BoolTensor] = None,
		output_attentions: bool = False,
	):

		if encoder_hidden_states is not None:
			attention_mask = encoder_attention_mask

		self_output = self.self(
			input_tensor,
			source_states=encoder_hidden_states,
			attention_mask=attention_mask,
			head_mask=head_mask,
			output_attentions=output_attentions,
		)

		attention_output = self.output(self_output.hidden_states, input_tensor)
		outputs = AttentionOutput(
			attention_output,
			self_output.key_value_state,
			self_output.position_bias,
			self_output.attention_probs,
		)
		return outputs

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		final_kwargs = {}

		final_kwargs["hidden_size"] = config.hidden_size
		final_kwargs["num_attention_heads"] = config.num_attention_heads
		final_kwargs["attention_dropout"] = config.attention_probs_dropout_prob
		final_kwargs["hidden_dropout"] = config.hidden_dropout_prob

		final_kwargs.update(**kwargs)
		return cls(**final_kwargs)


@dataclass
class TransformerLayerOutput:

	hidden_states: FloatT
	self_attention_probs: Optional[FloatT] = None
	cross_attention_probs: Optional[FloatT] = None


class TransformerLayer(TransformerModule, FromParams):

	_pretrained_relevant_module = "encoder.layer.0"
	_pretrained_mapping = {
		"layer": "layers",
		"intermediate_act_fn": "act_fn",
		"crossattention": "cross_attention",
	}

	def __init__(
		self,
		hidden_size: int,
		intermediate_size: int,
		num_attention_heads: int,
		attention_dropout: float = 0.0,
		hidden_dropout: float = 0.0,
		activation: Union[str, torch.nn.Module] = "relu",
		add_cross_attention: bool = False,
	):
		super().__init__()

		self.attention = AttentionLayer(
			hidden_size=hidden_size,
			num_attention_heads=num_attention_heads,
			attention_dropout=attention_dropout,
			hidden_dropout=hidden_dropout,
		)

		if add_cross_attention:
			self.cross_attention = AttentionLayer(
				hidden_size=hidden_size,
				num_attention_heads=num_attention_heads,
				attention_dropout=attention_dropout,
				hidden_dropout=hidden_dropout,
				is_cross_attention=True,
				is_decoder=True,
			)

		self.intermediate = ActivationLayer(
			hidden_size=hidden_size, intermediate_size=intermediate_size, activation=activation
		)
		self.output = OutputLayer(
			input_size=intermediate_size, hidden_size=hidden_size, dropout=hidden_dropout
		)

	def get_output_dim(self) -> int:
		return self.output.get_output_dim()

	def forward(
		self,
		hidden_states: torch.Tensor,
		attention_mask: torch.Tensor,
		head_mask: Optional[torch.Tensor] = None,
		encoder_hidden_states: Optional[torch.Tensor] = None,
		encoder_attention_mask: Optional[torch.Tensor] = None,
		output_attentions: bool = False,
	) -> TransformerLayerOutput:
		attention_outputs = self.attention(
			hidden_states,
			attention_mask,
			head_mask,
			output_attentions=output_attentions,
		)
		attention_output = attention_outputs.hidden_states
		self_attention_probs = attention_outputs.attention_probs
		cross_attention_probs = None

		if encoder_hidden_states is not None:
			assert hasattr(
				self, "cross_attention"
			), f"If `encoder_hidden_states` are passed, {self} has to be instantiated "
			"with cross-attention layers by setting `config.add_cross_attention=True`"

			cross_attention_outputs = self.cross_attention(
				attention_output,
				attention_mask,
				head_mask,
				encoder_hidden_states,
				encoder_attention_mask,
				output_attentions,
			)
			attention_output = cross_attention_outputs.hidden_states
			cross_attention_probs = cross_attention_outputs.attention_probs

		intermediate_output = self.intermediate(attention_output)
		layer_output = self.output(intermediate_output, attention_output)

		outputs = TransformerLayerOutput(layer_output, self_attention_probs, cross_attention_probs)
		return outputs

	@classmethod
	def _from_config(cls, config: "PretrainedConfig", **kwargs):
		final_kwargs = {}
		final_kwargs["hidden_size"] = config.hidden_size
		final_kwargs["num_attention_heads"] = config.num_attention_heads
		final_kwargs["attention_dropout"] = config.attention_probs_dropout_prob
		final_kwargs["hidden_dropout"] = config.hidden_dropout_prob
		final_kwargs["intermediate_size"] = config.intermediate_size
		final_kwargs["activation"] = config.hidden_act
		final_kwargs["add_cross_attention"] = config.add_cross_attention
		final_kwargs.update(**kwargs)
		return cls(**final_kwargs)

import logging
import os
import sys


if os.environ.get("ALLENNLP_DEBUG"):
	LEVEL = logging.DEBUG
else:
	level_name = os.environ.get("ALLENNLP_LOG_LEVEL", "INFO")
	LEVEL = logging._nameToLevel.get(level_name, logging.INFO)

sys.path.insert(0, os.path.dirname(os.path.abspath(os.path.join(__file__, os.pardir))))
logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", level=LEVEL)

logging.getLogger("filelock").setLevel(logging.WARNING)


def _transformers_log_filter(record):
	if record.msg.startswith("PyTorch version"):
		return False
	return True


logging.getLogger("transformers.file_utils").addFilter(_transformers_log_filter)


def run():
	from allennlp.commands import main  # noqa
	from allennlp.common.util import install_sigterm_handler

	install_sigterm_handler()

	main(prog="allennlp")


if __name__ == "__main__":
	run()

from allennlp.modules.encoder_base import _EncoderBase
from allennlp.common import Registrable


class Seq2VecEncoder(_EncoderBase, Registrable):

	def get_input_dim(self) -> int:
		raise NotImplementedError

	def get_output_dim(self) -> int:
		raise NotImplementedError

from typing import List, Tuple, Dict, Union

import torch

from allennlp.common.checks import ConfigurationError
import allennlp.nn.util as util

VITERBI_DECODING = Tuple[List[int], float]  # a list of tags, and a viterbi score


def allowed_transitions(constraint_type: str, labels: Dict[int, str]) -> List[Tuple[int, int]]:
	num_labels = len(labels)
	start_tag = num_labels
	end_tag = num_labels + 1
	labels_with_boundaries = list(labels.items()) + [(start_tag, "START"), (end_tag, "END")]

	allowed = []
	for from_label_index, from_label in labels_with_boundaries:
		if from_label in ("START", "END"):
			from_tag = from_label
			from_entity = ""
		else:
			from_tag = from_label[0]
			from_entity = from_label[1:]
		for to_label_index, to_label in labels_with_boundaries:
			if to_label in ("START", "END"):
				to_tag = to_label
				to_entity = ""
			else:
				to_tag = to_label[0]
				to_entity = to_label[1:]
			if is_transition_allowed(constraint_type, from_tag, from_entity, to_tag, to_entity):
				allowed.append((from_label_index, to_label_index))
	return allowed


def is_transition_allowed(
	constraint_type: str, from_tag: str, from_entity: str, to_tag: str, to_entity: str
):

	if to_tag == "START" or from_tag == "END":
		return False

	if constraint_type == "BIOUL":
		if from_tag == "START":
			return to_tag in ("O", "B", "U")
		if to_tag == "END":
			return from_tag in ("O", "L", "U")
		return any(
			[
				from_tag in ("O", "L", "U") and to_tag in ("O", "B", "U"),
				from_tag in ("B", "I") and to_tag in ("I", "L") and from_entity == to_entity,
			]
		)
	elif constraint_type == "BIO":
		if from_tag == "START":
			return to_tag in ("O", "B")
		if to_tag == "END":
			return from_tag in ("O", "B", "I")
		return any(
			[
				to_tag in ("O", "B"),
				to_tag == "I" and from_tag in ("B", "I") and from_entity == to_entity,
			]
		)
	elif constraint_type == "IOB1":
		if from_tag == "START":
			return to_tag in ("O", "I")
		if to_tag == "END":
			return from_tag in ("O", "B", "I")
		return any(
			[
				to_tag in ("O", "I"),
				to_tag == "B" and from_tag in ("B", "I") and from_entity == to_entity,
			]
		)
	elif constraint_type == "BMES":
		if from_tag == "START":
			return to_tag in ("B", "S")
		if to_tag == "END":
			return from_tag in ("E", "S")
		return any(
			[
				to_tag in ("B", "S") and from_tag in ("E", "S"),
				to_tag == "M" and from_tag in ("B", "M") and from_entity == to_entity,
				to_tag == "E" and from_tag in ("B", "M") and from_entity == to_entity,
			]
		)
	else:
		raise ConfigurationError(f"Unknown constraint type: {constraint_type}")


class ConditionalRandomField(torch.nn.Module):

	def __init__(
		self,
		num_tags: int,
		constraints: List[Tuple[int, int]] = None,
		include_start_end_transitions: bool = True,
	) -> None:
		super().__init__()
		self.num_tags = num_tags

		self.transitions = torch.nn.Parameter(torch.empty(num_tags, num_tags))

		if constraints is None:
			constraint_mask = torch.full((num_tags + 2, num_tags + 2), 1.0)
		else:
			constraint_mask = torch.full((num_tags + 2, num_tags + 2), 0.0)
			for i, j in constraints:
				constraint_mask[i, j] = 1.0

		self._constraint_mask = torch.nn.Parameter(constraint_mask, requires_grad=False)

		self.include_start_end_transitions = include_start_end_transitions
		if include_start_end_transitions:
			self.start_transitions = torch.nn.Parameter(torch.Tensor(num_tags))
			self.end_transitions = torch.nn.Parameter(torch.Tensor(num_tags))

		self.reset_parameters()

	def reset_parameters(self):
		torch.nn.init.xavier_normal_(self.transitions)
		if self.include_start_end_transitions:
			torch.nn.init.normal_(self.start_transitions)
			torch.nn.init.normal_(self.end_transitions)

	def _input_likelihood(
		self, logits: torch.Tensor, transitions: torch.Tensor, mask: torch.BoolTensor
	) -> torch.Tensor:
		batch_size, sequence_length, num_tags = logits.size()

		mask = mask.transpose(0, 1).contiguous()
		logits = logits.transpose(0, 1).contiguous()

		if self.include_start_end_transitions:
			alpha = self.start_transitions.view(1, num_tags) + logits[0]
		else:
			alpha = logits[0]

		for i in range(1, sequence_length):
			emit_scores = logits[i].view(batch_size, 1, num_tags)
			transition_scores = transitions.view(1, num_tags, num_tags)
			broadcast_alpha = alpha.view(batch_size, num_tags, 1)

			inner = broadcast_alpha + emit_scores + transition_scores

			alpha = util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) + alpha * (
				~mask[i]
			).view(batch_size, 1)

		if self.include_start_end_transitions:
			stops = alpha + self.end_transitions.view(1, num_tags)
		else:
			stops = alpha

		return util.logsumexp(stops)

	def _joint_likelihood(
		self,
		logits: torch.Tensor,
		transitions: torch.Tensor,
		tags: torch.Tensor,
		mask: torch.BoolTensor,
	) -> torch.Tensor:
		batch_size, sequence_length, _ = logits.data.shape

		logits = logits.transpose(0, 1).contiguous()
		mask = mask.transpose(0, 1).contiguous()
		tags = tags.transpose(0, 1).contiguous()

		if self.include_start_end_transitions:
			score = self.start_transitions.index_select(0, tags[0])
		else:
			score = 0.0

		for i in range(sequence_length - 1):
			current_tag, next_tag = tags[i], tags[i + 1]

			transition_score = transitions[current_tag.view(-1), next_tag.view(-1)]

			emit_score = logits[i].gather(1, current_tag.view(batch_size, 1)).squeeze(1)

			score = score + transition_score * mask[i + 1] + emit_score * mask[i]

		last_tag_index = mask.sum(0).long() - 1
		last_tags = tags.gather(0, last_tag_index.view(1, batch_size)).squeeze(0)

		if self.include_start_end_transitions:
			last_transition_score = self.end_transitions.index_select(0, last_tags)
		else:
			last_transition_score = 0.0

		last_inputs = logits[-1]  # (batch_size, num_tags)
		last_input_score = last_inputs.gather(1, last_tags.view(-1, 1))  # (batch_size, 1)
		last_input_score = last_input_score.squeeze()  # (batch_size,)

		score = score + last_transition_score + last_input_score * mask[-1]

		return score

	def forward(
		self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor = None
	) -> torch.Tensor:
		if mask is None:
			mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
		else:
			mask = mask.to(torch.bool)

		log_denominator = self._input_likelihood(inputs, self.transitions, mask)
		log_numerator = self._joint_likelihood(inputs, self.transitions, tags, mask)

		return torch.sum(log_numerator - log_denominator)

	def viterbi_tags(
		self, logits: torch.Tensor, mask: torch.BoolTensor = None, top_k: int = None
	) -> Union[List[VITERBI_DECODING], List[List[VITERBI_DECODING]]]:
		if mask is None:
			mask = torch.ones(*logits.shape[:2], dtype=torch.bool, device=logits.device)

		if top_k is None:
			top_k = 1
			flatten_output = True
		else:
			flatten_output = False

		_, max_seq_length, num_tags = logits.size()

		logits, mask = logits.data, mask.data

		start_tag = num_tags
		end_tag = num_tags + 1
		transitions = torch.full((num_tags + 2, num_tags + 2), -10000.0, device=logits.device)

		constrained_transitions = self.transitions * self._constraint_mask[
			:num_tags, :num_tags
		] + -10000.0 * (1 - self._constraint_mask[:num_tags, :num_tags])
		transitions[:num_tags, :num_tags] = constrained_transitions.data

		if self.include_start_end_transitions:
			transitions[
				start_tag, :num_tags
			] = self.start_transitions.detach() * self._constraint_mask[
				start_tag, :num_tags
			].data + -10000.0 * (
				1 - self._constraint_mask[start_tag, :num_tags].detach()
			)
			transitions[:num_tags, end_tag] = self.end_transitions.detach() * self._constraint_mask[
				:num_tags, end_tag
			].data + -10000.0 * (1 - self._constraint_mask[:num_tags, end_tag].detach())
		else:
			transitions[start_tag, :num_tags] = -10000.0 * (
				1 - self._constraint_mask[start_tag, :num_tags].detach()
			)
			transitions[:num_tags, end_tag] = -10000.0 * (
				1 - self._constraint_mask[:num_tags, end_tag].detach()
			)

		best_paths = []
		tag_sequence = torch.empty(max_seq_length + 2, num_tags + 2, device=logits.device)

		for prediction, prediction_mask in zip(logits, mask):
			mask_indices = prediction_mask.nonzero(as_tuple=False).squeeze()
			masked_prediction = torch.index_select(prediction, 0, mask_indices)
			sequence_length = masked_prediction.shape[0]

			tag_sequence.fill_(-10000.0)
			tag_sequence[0, start_tag] = 0.0
			tag_sequence[1 : (sequence_length + 1), :num_tags] = masked_prediction
			tag_sequence[sequence_length + 1, end_tag] = 0.0

			viterbi_paths, viterbi_scores = util.viterbi_decode(
				tag_sequence=tag_sequence[: (sequence_length + 2)],
				transition_matrix=transitions,
				top_k=top_k,
			)
			top_k_paths = []
			for viterbi_path, viterbi_score in zip(viterbi_paths, viterbi_scores):
				viterbi_path = viterbi_path[1:-1]
				top_k_paths.append((viterbi_path, viterbi_score.item()))
			best_paths.append(top_k_paths)

		if flatten_output:
			return [top_k_paths[0] for top_k_paths in best_paths]

		return best_paths

import torch
from torch.autograd import Variable


class Predictor(object):

	def __init__(self, model, src_vocab, tgt_vocab):
		if torch.cuda.is_available():
			self.model = model.cuda()
		else:
			self.model = model.cpu()
		self.model.eval()
		self.src_vocab = src_vocab
		self.tgt_vocab = tgt_vocab

	def get_decoder_features(self, src_seq):
		src_id_seq = torch.LongTensor([self.src_vocab.stoi[tok] for tok in src_seq]).view(1, -1)
		if torch.cuda.is_available():
			src_id_seq = src_id_seq.cuda()

		with torch.no_grad():
			softmax_list, _, other = self.model(src_id_seq, [len(src_seq)])

		return other

	def predict(self, src_seq):
		other = self.get_decoder_features(src_seq)

		length = other['length'][0]

		tgt_id_seq = [other['sequence'][di][0].data[0] for di in range(length)]
		tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]
		return tgt_seq

	def predict_n(self, src_seq, n=1):
		other = self.get_decoder_features(src_seq)

		result = []
		for x in range(0, int(n)):
			length = other['topk_length'][0][x]
			tgt_id_seq = [other['topk_sequence'][di][0, x, 0].data[0] for di in range(length)]
			tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]
			result.append(tgt_seq)

		return result

from .multi_head import MultiHeadedAttention
from .single import Attention

import logging
from typing import List, Optional


import torch

from allennlp.training.learning_rate_schedulers.learning_rate_scheduler import LearningRateScheduler


logger = logging.getLogger(__name__)


@LearningRateScheduler.register("slanted_triangular")
class SlantedTriangular(LearningRateScheduler):

	def __init__(
		self,
		optimizer: torch.optim.Optimizer,
		num_epochs: int,
		num_steps_per_epoch: Optional[int] = None,
		cut_frac: float = 0.1,
		ratio: int = 32,
		last_epoch: int = -1,
		gradual_unfreezing: bool = False,
		discriminative_fine_tuning: bool = False,
		decay_factor: float = 0.38,
	) -> None:
		self.num_epochs = num_epochs
		self.num_steps_per_epoch = num_steps_per_epoch
		self.cut_frac = cut_frac
		self.ratio = ratio
		self.gradual_unfreezing = gradual_unfreezing
		self.freezing_current = self.gradual_unfreezing
		self.is_first_epoch = True
		self.batch_num_total_epoch_end: List[int] = []
		if self.gradual_unfreezing:
			assert not optimizer.param_groups[-1]["params"], "The default group should be empty."
		if self.gradual_unfreezing or discriminative_fine_tuning:
			assert len(optimizer.param_groups) > 2, (
				"There should be at least 3 param_groups (2 + empty default group)"
				" for gradual unfreezing / discriminative fine-tuning to make sense."
			)
		super().__init__(optimizer, last_epoch)
		self.step()
		if discriminative_fine_tuning:
			exponent = 0
			for i in range(len(self.base_values) - 1, -1, -1):
				param_group = optimizer.param_groups[i]
				if param_group["params"]:
					param_group["lr"] = self.base_values[i] * decay_factor**exponent
					self.base_values[i] = param_group["lr"]
					exponent += 1
		self.last_batch_num_total = -1
		self.step_batch(0)

	def step(self, metric: float = None) -> None:
		self.last_epoch += 1
		if len(self.batch_num_total_epoch_end) == 0:
			self.batch_num_total_epoch_end.append(0)
		else:
			self.batch_num_total_epoch_end.append(self.last_batch_num_total)

		if self.gradual_unfreezing:
			if self.is_first_epoch:
				num_layers_to_unfreeze = 1
				self.is_first_epoch = False
			else:
				num_layers_to_unfreeze = self.last_epoch + 1
			if num_layers_to_unfreeze >= len(self.optimizer.param_groups) - 1:
				logger.info("Gradual unfreezing finished. Training all layers.")
				self.freezing_current = False
			else:
				logger.info(
					f"Gradual unfreezing. Training only the top {num_layers_to_unfreeze} layers."
				)
			for i, param_group in enumerate(reversed(self.optimizer.param_groups)):
				for param in param_group["params"]:
					param.requires_grad = bool(i <= num_layers_to_unfreeze)

	def step_batch(self, batch_num_total: int = None):
		if batch_num_total is None:
			batch_num_total = self.last_batch_num_total + 1
		self.last_batch_num_total = batch_num_total
		for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_values()):
			param_group["lr"] = learning_rate

	def get_values(self):
		if len(self.batch_num_total_epoch_end) > 1:
			actual_num_steps_per_epoch = int(
				self.batch_num_total_epoch_end[-1] / (len(self.batch_num_total_epoch_end) - 1)
			)
		else:
			actual_num_steps_per_epoch = max(
				self.num_steps_per_epoch or 1, self.last_batch_num_total
			)

		if self.freezing_current:
			num_steps = actual_num_steps_per_epoch
			step = min(self.last_batch_num_total - self.batch_num_total_epoch_end[-1], num_steps)
		else:
			if not self.gradual_unfreezing:
				frozen_steps = 0
			else:
				num_frozen_epochs = len(self.optimizer.param_groups) - 2
				frozen_steps = self.batch_num_total_epoch_end[num_frozen_epochs]
			num_steps = self.num_epochs * actual_num_steps_per_epoch - frozen_steps
			step = min(self.last_batch_num_total - frozen_steps, num_steps)
		cut = int(num_steps * self.cut_frac)
		prop = step / cut if step < cut else 1 - (step - cut) / (num_steps - cut)
		return [lr * (1 + prop * (self.ratio - 1)) / self.ratio for lr in self.base_values]

import torch.nn as nn
import torch.nn.functional as F

class Seq2seq(nn.Module):

	def __init__(self, encoder, decoder, decode_function=F.log_softmax):
		super(Seq2seq, self).__init__()
		self.encoder = encoder
		self.decoder = decoder
		self.decode_function = decode_function

	def flatten_parameters(self):
		self.encoder.rnn.flatten_parameters()
		self.decoder.rnn.flatten_parameters()

	def forward(self, input_variable, input_lengths=None, target_variable=None,
				teacher_forcing_ratio=0):
		encoder_outputs, encoder_hidden = self.encoder(input_variable, input_lengths)
		result = self.decoder(inputs=target_variable,
							  encoder_hidden=encoder_hidden,
							  encoder_outputs=encoder_outputs,
							  function=self.decode_function,
							  teacher_forcing_ratio=teacher_forcing_ratio)
		return result

from typing import Any, Dict, Mapping

from allennlp.common.registrable import Registrable
from allennlp.data.data_loaders.data_loader import DataLoader


class MultiTaskEpochSampler(Registrable):

	def get_task_proportions(self, data_loaders: Mapping[str, DataLoader]) -> Dict[str, float]:
		raise NotImplementedError

	def update_from_epoch_metrics(self, epoch_metrics: Dict[str, Any]) -> None:
		raise NotImplementedError


@MultiTaskEpochSampler.register("uniform")
class UniformSampler(MultiTaskEpochSampler):

	def get_task_proportions(self, data_loaders: Mapping[str, DataLoader]) -> Dict[str, float]:
		return {key: 1 / len(data_loaders) for key in data_loaders}


@MultiTaskEpochSampler.register("weighted")
class WeightedSampler(MultiTaskEpochSampler):

	def __init__(self, weights: Dict[str, float]):
		self.weights = weights

	def get_task_proportions(self, data_loaders: Mapping[str, DataLoader]) -> Dict[str, float]:
		total = sum(self.weights[task] for task in data_loaders.keys())
		return {task: self.weights[task] / total for task in data_loaders.keys()}


@MultiTaskEpochSampler.register("proportional")
class ProportionalSampler(MultiTaskEpochSampler):

	def get_task_proportions(self, data_loaders: Mapping[str, DataLoader]) -> Dict[str, float]:
		try:
			sizes = {key: len(loader) for key, loader in data_loaders.items()}
		except TypeError:
			raise ValueError("ProportionalSampler got passed a data loader without a length")
		total_size = sum(sizes.values())
		return {key: size / total_size for key, size in sizes.items()}

import os
from argparse import ArgumentParser

def makedirs(name):

	import os, errno

	try:
		os.makedirs(name)
	except OSError as ex:
		if ex.errno == errno.EEXIST and os.path.isdir(name):
			pass
		else:
			raise


def get_args():
	parser = ArgumentParser(description='PyTorch/torchtext SNLI example')
	parser.add_argument('--epochs', type=int, default=50,
						help='the number of total epochs to run.')
	parser.add_argument('--batch_size', type=int, default=128,
						help='batch size. (default: 128)')
	parser.add_argument('--d_embed', type=int, default=100,
						help='the size of each embedding vector.')
	parser.add_argument('--d_proj', type=int, default=300,
						help='the size of each projection layer.')
	parser.add_argument('--d_hidden', type=int, default=300,
						help='the number of features in the hidden state.')
	parser.add_argument('--n_layers', type=int, default=1,
						help='the number of recurrent layers. (default: 50)')
	parser.add_argument('--log_every', type=int, default=50,
						help='iteration period to output log.')
	parser.add_argument('--lr',type=float, default=.001,
						help='initial learning rate.')
	parser.add_argument('--dev_every', type=int, default=1000,
						help='log period of validation results.')
	parser.add_argument('--save_every', type=int, default=1000,
						help='model checkpoint period.')
	parser.add_argument('--dp_ratio', type=int, default=0.2,
						help='probability of an element to be zeroed.')
	parser.add_argument('--no-bidirectional', action='store_false', dest='birnn',
						help='disable bidirectional LSTM.')
	parser.add_argument('--preserve-case', action='store_false', dest='lower',
						help='case-sensitivity.')
	parser.add_argument('--no-projection', action='store_false', dest='projection',
						help='disable projection layer.')
	parser.add_argument('--train_embed', action='store_false', dest='fix_emb',
						help='enable embedding word training.')
	parser.add_argument('--gpu', type=int, default=0,
						help='gpu id to use. (default: 0)')
	parser.add_argument('--save_path', type=str, default='results',
						help='save path of results.')
	parser.add_argument('--vector_cache', type=str, default=os.path.join(os.getcwd(), '.vector_cache/input_vectors.pt'),
						help='name of vector cache directory, which saved input word-vectors.')
	parser.add_argument('--word_vectors', type=str, default='glove.6B.100d',
						help='one of or a list containing instantiations of the GloVe, CharNGram, or Vectors classes.'
						'Alternatively, one of or a list of available pretrained vectors: '
						'charngram.100d fasttext.en.300d fasttext.simple.300d'
						'glove.42B.300d glove.840B.300d glove.twitter.27B.25d'
						'glove.twitter.27B.50d glove.twitter.27B.100d glove.twitter.27B.200d'
						'glove.6B.50d glove.6B.100d glove.6B.200d glove.6B.300d')
	parser.add_argument('--resume_snapshot', type=str, default='',
						help='model snapshot to resume.')
	parser.add_argument('--dry-run', action='store_true',
						help='run only a few iterations')
	args = parser.parse_args()
	return args


import logging
import os
import random
import sys
import warnings
from dataclasses import dataclass, field
from typing import List, Optional

import datasets
import evaluate
import numpy as np
from datasets import Value, load_dataset

import transformers
from transformers import (
	AutoConfig,
	AutoModelForSequenceClassification,
	AutoTokenizer,
	DataCollatorWithPadding,
	EvalPrediction,
	HfArgumentParser,
	Trainer,
	TrainingArguments,
	default_data_collator,
	set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/text-classification/requirements.txt")


logger = logging.getLogger(__name__)


@dataclass
class DataTrainingArguments:

	dataset_name: Optional[str] = field(
		default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
	)
	dataset_config_name: Optional[str] = field(
		default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
	)
	do_regression: bool = field(
		default=None,
		metadata={
			"help": "Whether to do regression instead of classification. If None, will be inferred from the dataset."
		},
	)
	text_column_names: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The name of the text column in the input dataset or a CSV/JSON file. "
				'If not specified, will use the "sentence" column for single/multi-label classifcation task.'
			)
		},
	)
	text_column_delimiter: Optional[str] = field(
		default=" ", metadata={"help": "THe delimiter to use to join text columns into a single sentence."}
	)
	train_split_name: Optional[str] = field(
		default=None,
		metadata={
			"help": 'The name of the train split in the input dataset. If not specified, will use the "train" split when do_train is enabled'
		},
	)
	validation_split_name: Optional[str] = field(
		default=None,
		metadata={
			"help": 'The name of the validation split in the input dataset. If not specified, will use the "validation" split when do_eval is enabled'
		},
	)
	test_split_name: Optional[str] = field(
		default=None,
		metadata={
			"help": 'The name of the test split in the input dataset. If not specified, will use the "test" split when do_predict is enabled'
		},
	)
	remove_splits: Optional[str] = field(
		default=None,
		metadata={"help": "The splits to remove from the dataset. Multiple splits should be separated by commas."},
	)
	remove_columns: Optional[str] = field(
		default=None,
		metadata={"help": "The columns to remove from the dataset. Multiple columns should be separated by commas."},
	)
	label_column_name: Optional[str] = field(
		default=None,
		metadata={
			"help": (
				"The name of the label column in the input dataset or a CSV/JSON file. "
				'If not specified, will use the "label" column for single/multi-label classifcation task'
			)
		},
	)
	max_seq_length: int = field(
		default=128,
		metadata={
			"help": (
				"The maximum total input sequence length after tokenization. Sequences longer "
				"than this will be truncated, sequences shorter will be padded."
			)
		},
	)
	overwrite_cache: bool = field(
		default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
	)
	pad_to_max_length: bool = field(
		default=True,
		metadata={
			"help": (
				"Whether to pad all samples to `max_seq_length`. "
				"If False, will pad the samples dynamically when batching to the maximum length in the batch."
			)
		},
	)
	shuffle_train_dataset: bool = field(
		default=False, metadata={"help": "Whether to shuffle the train dataset or not."}
	)
	shuffle_seed: int = field(
		default=42, metadata={"help": "Random seed that will be used to shuffle the train dataset."}
	)
	max_train_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of training examples to this "
				"value if set."
			)
		},
	)
	max_eval_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of evaluation examples to this "
				"value if set."
			)
		},
	)
	max_predict_samples: Optional[int] = field(
		default=None,
		metadata={
			"help": (
				"For debugging purposes or quicker training, truncate the number of prediction examples to this "
				"value if set."
			)
		},
	)
	metric_name: Optional[str] = field(default=None, metadata={"help": "The metric to use for evaluation."})
	train_file: Optional[str] = field(
		default=None, metadata={"help": "A csv or a json file containing the training data."}
	)
	validation_file: Optional[str] = field(
		default=None, metadata={"help": "A csv or a json file containing the validation data."}
	)
	test_file: Optional[str] = field(default=None, metadata={"help": "A csv or a json file containing the test data."})

	def __post_init__(self):
		if self.dataset_name is None:
			if self.train_file is None or self.validation_file is None:
				raise ValueError(" training/validation file or a dataset name.")

			train_extension = self.train_file.split(".")[-1]
			assert train_extension in ["csv", "json"], "`train_file` should be a csv or a json file."
			validation_extension = self.validation_file.split(".")[-1]
			assert (
				validation_extension == train_extension
			), "`validation_file` should have the same extension (csv or json) as `train_file`."


@dataclass
class ModelArguments:

	model_name_or_path: str = field(
		metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
	)
	config_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
	)
	tokenizer_name: Optional[str] = field(
		default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
	)
	cache_dir: Optional[str] = field(
		default=None,
		metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
	)
	use_fast_tokenizer: bool = field(
		default=True,
		metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
	)
	model_revision: str = field(
		default="main",
		metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
	)
	token: str = field(
		default=None,
		metadata={
			"help": (
				"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
				"generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
			)
		},
	)
	use_auth_token: bool = field(
		default=None,
		metadata={
			"help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
		},
	)
	trust_remote_code: bool = field(
		default=False,
		metadata={
			"help": (
				"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
				"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
				"execute code present on the Hub on your local machine."
			)
		},
	)
	ignore_mismatched_sizes: bool = field(
		default=False,
		metadata={"help": "Will enable to load a pretrained model whose head dimensions are different."},
	)


def get_label_list(raw_dataset, split="train") -> List[str]:

	if isinstance(raw_dataset[split]["label"][0], list):
		label_list = [label for sample in raw_dataset[split]["label"] for label in sample]
		label_list = list(set(label_list))
	else:
		label_list = raw_dataset[split].unique("label")
	label_list = [str(label) for label in label_list]
	return label_list


def main():

	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
	if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
		model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	else:
		model_args, data_args, training_args = parser.parse_args_into_dataclasses()

	if model_args.use_auth_token is not None:
		warnings.warn(
			"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
			FutureWarning,
		)
		if model_args.token is not None:
			raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
		model_args.token = model_args.use_auth_token

	send_example_telemetry("run_classification", model_args, data_args)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		handlers=[logging.StreamHandler(sys.stdout)],
	)

	if training_args.should_log:
		transformers.utils.logging.set_verbosity_info()

	log_level = training_args.get_process_log_level()
	logger.setLevel(log_level)
	datasets.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.set_verbosity(log_level)
	transformers.utils.logging.enable_default_handler()
	transformers.utils.logging.enable_explicit_format()

	logger.warning(
		f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
		+ f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
	)
	logger.info(f"Training/evaluation parameters {training_args}")

	last_checkpoint = None
	if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
		last_checkpoint = get_last_checkpoint(training_args.output_dir)
		if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
			raise ValueError(
				f"Output directory ({training_args.output_dir}) already exists and is not empty. "
				"Use --overwrite_output_dir to overcome."
			)
		elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
			logger.info(
				f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
				"the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
			)

	set_seed(training_args.seed)

	if data_args.dataset_name is not None:
		raw_datasets = load_dataset(
			data_args.dataset_name,
			data_args.dataset_config_name,
			cache_dir=model_args.cache_dir,
			token=model_args.token,
		)
		logger.info(f"Dataset loaded: {raw_datasets}")
		logger.info(raw_datasets)
	else:
		data_files = {"train": data_args.train_file, "validation": data_args.validation_file}

		if training_args.do_predict:
			if data_args.test_file is not None:
				train_extension = data_args.train_file.split(".")[-1]
				test_extension = data_args.test_file.split(".")[-1]
				assert (
					test_extension == train_extension
				), "`test_file` should have the same extension (csv or json) as `train_file`."
				data_files["test"] = data_args.test_file
			else:
				raise ValueError("Need either a dataset name or a test file for `do_predict`.")

		for key in data_files.keys():
			logger.info(f"load a local file for {key}: {data_files[key]}")

		if data_args.train_file.endswith(".csv"):
			raw_datasets = load_dataset(
				"csv",
				data_files=data_files,
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)
		else:
			raw_datasets = load_dataset(
				"json",
				data_files=data_files,
				cache_dir=model_args.cache_dir,
				token=model_args.token,
			)


	if data_args.remove_splits is not None:
		for split in data_args.remove_splits.split(","):
			logger.info(f"removing split {split}")
			raw_datasets.pop(split)

	if data_args.train_split_name is not None:
		logger.info(f"using {data_args.validation_split_name} as validation set")
		raw_datasets["train"] = raw_datasets[data_args.train_split_name]
		raw_datasets.pop(data_args.train_split_name)

	if data_args.validation_split_name is not None:
		logger.info(f"using {data_args.validation_split_name} as validation set")
		raw_datasets["validation"] = raw_datasets[data_args.validation_split_name]
		raw_datasets.pop(data_args.validation_split_name)

	if data_args.test_split_name is not None:
		logger.info(f"using {data_args.test_split_name} as test set")
		raw_datasets["test"] = raw_datasets[data_args.test_split_name]
		raw_datasets.pop(data_args.test_split_name)

	if data_args.remove_columns is not None:
		for split in raw_datasets.keys():
			for column in data_args.remove_columns.split(","):
				logger.info(f"removing column {column} from split {split}")
				raw_datasets[split].remove_columns(column)

	if data_args.label_column_name is not None and data_args.label_column_name != "label":
		for key in raw_datasets.keys():
			raw_datasets[key] = raw_datasets[key].rename_column(data_args.label_column_name, "label")


	is_regression = (
		raw_datasets["train"].features["label"].dtype in ["float32", "float64"]
		if data_args.do_regression is None
		else data_args.do_regression
	)

	is_multi_label = False
	if is_regression:
		label_list = None
		num_labels = 1
		for split in raw_datasets.keys():
			if raw_datasets[split].features["label"].dtype not in ["float32", "float64"]:
				logger.warning(
					f"Label type for {split} set to float32, was {raw_datasets[split].features['label'].dtype}"
				)
				features = raw_datasets[split].features
				features.update({"label": Value("float32")})
				try:
					raw_datasets[split] = raw_datasets[split].cast(features)
				except TypeError as error:
					logger.error(
						f"Unable to cast {split} set to float32, please check the labels are correct, or maybe try with --do_regression=False"
					)
					raise error

	else:  # classification
		if raw_datasets["train"].features["label"].dtype == "list":  # multi-label classification
			is_multi_label = True
			logger.info("Label type is list, doing multi-label classification")
		label_list = get_label_list(raw_datasets, split="train")
		for split in ["validation", "test"]:
			if split in raw_datasets:
				val_or_test_labels = get_label_list(raw_datasets, split=split)
				diff = set(val_or_test_labels).difference(set(label_list))
				if len(diff) > 0:
					logger.warning(
						f"Labels {diff} in {split} set but not in training set, adding them to the label list"
					)
					label_list += list(diff)
		for label in label_list:
			if label == -1:
				logger.warning("Label -1 found in label list, removing it.")
				label_list.remove(label)

		label_list.sort()
		num_labels = len(label_list)
		if num_labels <= 1:
			raise ValueError("You need more than one label to do classification.")

	config = AutoConfig.from_pretrained(
		model_args.config_name if model_args.config_name else model_args.model_name_or_path,
		num_labels=num_labels,
		finetuning_task="text-classification",
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)

	if is_regression:
		config.problem_type = "regression"
		logger.info("setting problem type to regression")
	elif is_multi_label:
		config.problem_type = "multi_label_classification"
		logger.info("setting problem type to multi label classification")
	else:
		config.problem_type = "single_label_classification"
		logger.info("setting problem type to single label classification")

	tokenizer = AutoTokenizer.from_pretrained(
		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
		cache_dir=model_args.cache_dir,
		use_fast=model_args.use_fast_tokenizer,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
	)
	model = AutoModelForSequenceClassification.from_pretrained(
		model_args.model_name_or_path,
		from_tf=bool(".ckpt" in model_args.model_name_or_path),
		config=config,
		cache_dir=model_args.cache_dir,
		revision=model_args.model_revision,
		token=model_args.token,
		trust_remote_code=model_args.trust_remote_code,
		ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,
	)

	if data_args.pad_to_max_length:
		padding = "max_length"
	else:
		padding = False

	if training_args.do_train and not is_regression:  # classification, training
		label_to_id = {v: i for i, v in enumerate(label_list)}
		if model.config.label2id != label_to_id:
			logger.warning(
				"The label2id key in the model config.json is not equal to the label2id key of this "
				"run. You can ignore this if you are doing finetuning."
			)
		model.config.label2id = label_to_id
		model.config.id2label = {id: label for label, id in label_to_id.items()}
	elif not is_regression:  # classification, but not training
		logger.info("using label infos in the model config")
		logger.info("label2id: {}".format(model.config.label2id))
		label_to_id = model.config.label2id
	else:  # regression
		label_to_id = None

	if data_args.max_seq_length > tokenizer.model_max_length:
		logger.warning(
			f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
			f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
		)
	max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

	def multi_labels_to_ids(labels: List[str]) -> List[float]:
		ids = [0.0] * len(label_to_id)  # BCELoss requires float as target type
		for label in labels:
			ids[label_to_id[label]] = 1.0
		return ids

	def preprocess_function(examples):
		if data_args.text_column_names is not None:
			text_column_names = data_args.text_column_names.split(",")
			examples["sentence"] = examples[text_column_names[0]]
			for column in text_column_names[1:]:
				for i in range(len(examples[column])):
					examples["sentence"][i] += data_args.text_column_delimiter + examples[column][i]
		result = tokenizer(examples["sentence"], padding=padding, max_length=max_seq_length, truncation=True)
		if label_to_id is not None and "label" in examples:
			if is_multi_label:
				result["label"] = [multi_labels_to_ids(l) for l in examples["label"]]
			else:
				result["label"] = [(label_to_id[str(l)] if l != -1 else -1) for l in examples["label"]]
		return result

	with training_args.main_process_first(desc="dataset map pre-processing"):
		raw_datasets = raw_datasets.map(
			preprocess_function,
			batched=True,
			load_from_cache_file=not data_args.overwrite_cache,
			desc="Running tokenizer on dataset",
		)

	if training_args.do_train:
		if "train" not in raw_datasets:
			raise ValueError("--do_train requires a train dataset.")
		train_dataset = raw_datasets["train"]
		if data_args.shuffle_train_dataset:
			logger.info("Shuffling the training dataset")
			train_dataset = train_dataset.shuffle(seed=data_args.shuffle_seed)
		if data_args.max_train_samples is not None:
			max_train_samples = min(len(train_dataset), data_args.max_train_samples)
			train_dataset = train_dataset.select(range(max_train_samples))

	if training_args.do_eval:
		if "validation" not in raw_datasets and "validation_matched" not in raw_datasets:
			if "test" not in raw_datasets and "test_matched" not in raw_datasets:
				raise ValueError("--do_eval requires a validation or test dataset if validation is not defined.")
			else:
				logger.warning("Validation dataset not found. Falling back to test dataset for validation.")
				eval_dataset = raw_datasets["test"]
		else:
			eval_dataset = raw_datasets["validation"]

		if data_args.max_eval_samples is not None:
			max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
			eval_dataset = eval_dataset.select(range(max_eval_samples))

	if training_args.do_predict or data_args.test_file is not None:
		if "test" not in raw_datasets:
			raise ValueError("--do_predict requires a test dataset")
		predict_dataset = raw_datasets["test"]
		if data_args.max_predict_samples is not None:
			max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)
			predict_dataset = predict_dataset.select(range(max_predict_samples))

	if training_args.do_train:
		for index in random.sample(range(len(train_dataset)), 3):
			logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if data_args.metric_name is not None:
		metric = (
			evaluate.load(data_args.metric_name, config_name="multilabel", cache_dir=model_args.cache_dir)
			if is_multi_label
			else evaluate.load(data_args.metric_name, cache_dir=model_args.cache_dir)
		)
		logger.info(f"Using metric {data_args.metric_name} for evaluation.")
	else:
		if is_regression:
			metric = evaluate.load("mse", cache_dir=model_args.cache_dir)
			logger.info("Using mean squared error (mse) as regression score, you can use --metric_name to overwrite.")
		else:
			if is_multi_label:
				metric = evaluate.load("f1", config_name="multilabel", cache_dir=model_args.cache_dir)
				logger.info(
					"Using multilabel F1 for multi-label classification task, you can use --metric_name to overwrite."
				)
			else:
				metric = evaluate.load("accuracy", cache_dir=model_args.cache_dir)
				logger.info("Using accuracy as classification score, you can use --metric_name to overwrite.")

	def compute_metrics(p: EvalPrediction):
		preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
		if is_regression:
			preds = np.squeeze(preds)
			result = metric.compute(predictions=preds, references=p.label_ids)
		elif is_multi_label:
			preds = np.array([np.where(p > 0, 1, 0) for p in preds])  # convert logits to multi-hot encoding
			result = metric.compute(predictions=preds, references=p.label_ids, average="micro")
		else:
			preds = np.argmax(preds, axis=1)
			result = metric.compute(predictions=preds, references=p.label_ids)
		if len(result) > 1:
			result["combined_score"] = np.mean(list(result.values())).item()
		return result

	if data_args.pad_to_max_length:
		data_collator = default_data_collator
	elif training_args.fp16:
		data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
	else:
		data_collator = None

	trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=train_dataset if training_args.do_train else None,
		eval_dataset=eval_dataset if training_args.do_eval else None,
		compute_metrics=compute_metrics,
		tokenizer=tokenizer,
		data_collator=data_collator,
	)

	if training_args.do_train:
		checkpoint = None
		if training_args.resume_from_checkpoint is not None:
			checkpoint = training_args.resume_from_checkpoint
		elif last_checkpoint is not None:
			checkpoint = last_checkpoint
		train_result = trainer.train(resume_from_checkpoint=checkpoint)
		metrics = train_result.metrics
		max_train_samples = (
			data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
		)
		metrics["train_samples"] = min(max_train_samples, len(train_dataset))
		trainer.save_model()  # Saves the tokenizer too for easy upload
		trainer.log_metrics("train", metrics)
		trainer.save_metrics("train", metrics)
		trainer.save_state()

	if training_args.do_eval:
		logger.info("*** Evaluate ***")
		metrics = trainer.evaluate(eval_dataset=eval_dataset)
		max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
		metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
		trainer.log_metrics("eval", metrics)
		trainer.save_metrics("eval", metrics)

	if training_args.do_predict:
		logger.info("*** Predict ***")
		if "label" in predict_dataset.features:
			predict_dataset = predict_dataset.remove_columns("label")
		predictions = trainer.predict(predict_dataset, metric_key_prefix="predict").predictions
		if is_regression:
			predictions = np.squeeze(predictions)
		elif is_multi_label:
			predictions = np.array([np.where(p > 0, 1, 0) for p in predictions])
		else:
			predictions = np.argmax(predictions, axis=1)
		output_predict_file = os.path.join(training_args.output_dir, "predict_results.txt")
		if trainer.is_world_process_zero():
			with open(output_predict_file, "w") as writer:
				logger.info("***** Predict results *****")
				writer.write("index\tprediction\n")
				for index, item in enumerate(predictions):
					if is_regression:
						writer.write(f"{index}\t{item:3.3f}\n")
					elif is_multi_label:
						item = [label_list[i] for i in range(len(item)) if item[i] == 1]
						writer.write(f"{index}\t{item}\n")
					else:
						item = label_list[item]
						writer.write(f"{index}\t{item}\n")
		logger.info("Predict results saved at {}".format(output_predict_file))
	kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "text-classification"}

	if training_args.push_to_hub:
		trainer.push_to_hub(**kwargs)
	else:
		trainer.create_model_card(**kwargs)


def _mp_fn(index):
	main()


if __name__ == "__main__":
	main()

from typing import List, Iterator, Dict, Tuple, Any, Type, Union, Optional
import logging
from os import PathLike
import json
import re
from contextlib import contextmanager

import numpy
import torch
from torch.utils.hooks import RemovableHandle
from torch import Tensor
from torch import backends

from allennlp.common import Registrable, plugins
from allennlp.common.util import JsonDict, sanitize
from allennlp.data import DatasetReader, Instance
from allennlp.data.batch import Batch
from allennlp.models import Model
from allennlp.models.archival import Archive, load_archive
from allennlp.nn import util

logger = logging.getLogger(__name__)


class Predictor(Registrable):

	def __init__(self, model: Model, dataset_reader: DatasetReader, frozen: bool = True) -> None:
		if frozen:
			model.eval()
		self._model = model
		self._dataset_reader = dataset_reader
		self.cuda_device = next(self._model.named_parameters())[1].get_device()
		self._token_offsets: List[Tensor] = []

	def load_line(self, line: str) -> JsonDict:
		return json.loads(line)

	def dump_line(self, outputs: JsonDict) -> str:
		return json.dumps(outputs) + "\n"

	def predict_json(self, inputs: JsonDict) -> JsonDict:
		instance = self._json_to_instance(inputs)
		return self.predict_instance(instance)

	def json_to_labeled_instances(self, inputs: JsonDict) -> List[Instance]:

		instance = self._json_to_instance(inputs)
		self._dataset_reader.apply_token_indexers(instance)
		outputs = self._model.forward_on_instance(instance)
		new_instances = self.predictions_to_labeled_instances(instance, outputs)
		return new_instances

	def get_gradients(self, instances: List[Instance]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
		original_param_name_to_requires_grad_dict = {}
		for param_name, param in self._model.named_parameters():
			original_param_name_to_requires_grad_dict[param_name] = param.requires_grad
			param.requires_grad = True

		embedding_gradients: List[Tensor] = []
		hooks: List[RemovableHandle] = self._register_embedding_gradient_hooks(embedding_gradients)

		for instance in instances:
			self._dataset_reader.apply_token_indexers(instance)

		dataset = Batch(instances)
		dataset.index_instances(self._model.vocab)
		dataset_tensor_dict = util.move_to_device(dataset.as_tensor_dict(), self.cuda_device)
		with backends.cudnn.flags(enabled=False):
			outputs = self._model.make_output_human_readable(
				self._model.forward(**dataset_tensor_dict)  # type: ignore
			)

			loss = outputs["loss"]
			for p in self._model.parameters():
				p.grad = None
			loss.backward()

		for hook in hooks:
			hook.remove()

		grad_dict = dict()
		for idx, grad in enumerate(embedding_gradients):
			key = "grad_input_" + str(idx + 1)
			grad_dict[key] = grad.detach().cpu().numpy()

		for param_name, param in self._model.named_parameters():
			param.requires_grad = original_param_name_to_requires_grad_dict[param_name]

		return grad_dict, outputs

	def get_interpretable_layer(self) -> torch.nn.Module:
		try:
			return util.find_embedding_layer(self._model)
		except RuntimeError:
			raise RuntimeError(
				"If the model does not use `TextFieldEmbedder`, please override "
				"`get_interpretable_layer` in your predictor to specify the embedding layer."
			)

	def get_interpretable_text_field_embedder(self) -> torch.nn.Module:
		try:
			return util.find_text_field_embedder(self._model)
		except RuntimeError:
			raise RuntimeError(
				"If the model does not use `TextFieldEmbedder`, please override "
				"`get_interpretable_text_field_embedder` in your predictor to specify "
				"the embedding layer."
			)

	def _register_embedding_gradient_hooks(self, embedding_gradients):

		def hook_layers(module, grad_in, grad_out):
			grads = grad_out[0]
			if self._token_offsets:
				offsets = self._token_offsets.pop(0)
				span_grads, span_mask = util.batched_span_select(grads.contiguous(), offsets)
				span_mask = span_mask.unsqueeze(-1)
				span_grads *= span_mask  # zero out paddings

				span_grads_sum = span_grads.sum(2)
				span_grads_len = span_mask.sum(2)
				grads = span_grads_sum / torch.clamp_min(span_grads_len, 1)

				grads[(span_grads_len == 0).expand(grads.shape)] = 0

			embedding_gradients.append(grads)

		def get_token_offsets(module, inputs, outputs):
			offsets = util.get_token_offsets_from_text_field_inputs(inputs)
			if offsets is not None:
				self._token_offsets.append(offsets)

		hooks = []
		text_field_embedder = self.get_interpretable_text_field_embedder()
		hooks.append(text_field_embedder.register_forward_hook(get_token_offsets))
		embedding_layer = self.get_interpretable_layer()
		hooks.append(embedding_layer.register_backward_hook(hook_layers))
		return hooks

	@contextmanager
	def capture_model_internals(self, module_regex: str = ".*") -> Iterator[dict]:
		results = {}
		hooks = []

		def add_output(idx: int):
			def _add_output(mod, _, outputs):
				results[idx] = {"name": str(mod), "output": sanitize(outputs)}

			return _add_output

		regex = re.compile(module_regex)
		for idx, (name, module) in enumerate(self._model.named_modules()):
			if regex.fullmatch(name) and module != self._model:
				hook = module.register_forward_hook(add_output(idx))
				hooks.append(hook)

		yield results

		for hook in hooks:
			hook.remove()

	def predict_instance(self, instance: Instance) -> JsonDict:
		self._dataset_reader.apply_token_indexers(instance)
		outputs = self._model.forward_on_instance(instance)
		return sanitize(outputs)

	def predictions_to_labeled_instances(
		self, instance: Instance, outputs: Dict[str, numpy.ndarray]
	) -> List[Instance]:

		raise RuntimeError("implement this method for model interpretations or attacks")

	def _json_to_instance(self, json_dict: JsonDict) -> Instance:
		raise NotImplementedError

	def predict_batch_json(self, inputs: List[JsonDict]) -> List[JsonDict]:
		instances = self._batch_json_to_instances(inputs)
		return self.predict_batch_instance(instances)

	def predict_batch_instance(self, instances: List[Instance]) -> List[JsonDict]:
		for instance in instances:
			self._dataset_reader.apply_token_indexers(instance)
		outputs = self._model.forward_on_instances(instances)
		return sanitize(outputs)

	def _batch_json_to_instances(self, json_dicts: List[JsonDict]) -> List[Instance]:
		instances = []
		for json_dict in json_dicts:
			instances.append(self._json_to_instance(json_dict))
		return instances

	@classmethod
	def from_path(
		cls,
		archive_path: Union[str, PathLike],
		predictor_name: str = None,
		cuda_device: int = -1,
		dataset_reader_to_load: str = "validation",
		frozen: bool = True,
		import_plugins: bool = True,
		overrides: Union[str, Dict[str, Any]] = "",
		**kwargs,
	) -> "Predictor":
		if import_plugins:
			plugins.import_plugins()
		return Predictor.from_archive(
			load_archive(archive_path, cuda_device=cuda_device, overrides=overrides),
			predictor_name,
			dataset_reader_to_load=dataset_reader_to_load,
			frozen=frozen,
			extra_args=kwargs,
		)

	@classmethod
	def from_archive(
		cls,
		archive: Archive,
		predictor_name: str = None,
		dataset_reader_to_load: str = "validation",
		frozen: bool = True,
		extra_args: Optional[Dict[str, Any]] = None,
	) -> "Predictor":
		config = archive.config.duplicate()

		if not predictor_name:
			model_type = config.get("model").get("type")
			model_class, _ = Model.resolve_class_name(model_type)
			predictor_name = model_class.default_predictor
		predictor_class: Type[Predictor] = (
			Predictor.by_name(predictor_name) if predictor_name is not None else cls  # type: ignore
		)

		if dataset_reader_to_load == "validation":
			dataset_reader = archive.validation_dataset_reader
		else:
			dataset_reader = archive.dataset_reader

		model = archive.model
		if frozen:
			model.eval()

		if extra_args is None:
			extra_args = {}

		return predictor_class(model, dataset_reader, **extra_args)


import argparse
import logging

from allennlp.commands.subcommand import Subcommand

logger = logging.getLogger(__name__)

try:
	from allennlp.commands._checklist_internal import CheckList
except ImportError:

	def _dummy_output(args: argparse.Namespace):
		logger.info(
			"The checklist integration of allennlp is optional; if you're using conda, "
			"it can be installed with `conda install allennlp-checklist`, "
			"otherwise use `pip install allennlp[checklist]`."
		)

	@Subcommand.register("checklist")
	class CheckList(Subcommand):  # type: ignore
		def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
			description = """Dummy command because checklist is not installed."""
			subparser = parser.add_parser(
				self.name,
				description=description,
				help="Run a trained model through a checklist suite.",
			)
			subparser.set_defaults(func=_dummy_output)
			return subparser



import json
import logging
import os
import sys
from unittest.mock import patch

from transformers import ViTMAEForPreTraining, Wav2Vec2ForPreTraining
from transformers.testing_utils import (
	CaptureLogger,
	TestCasePlus,
	backend_device_count,
	is_torch_fp16_available_on_device,
	slow,
	torch_device,
)


SRC_DIRS = [
	os.path.join(os.path.dirname(__file__), dirname)
	for dirname in [
		"text-generation",
		"text-classification",
		"token-classification",
		"language-modeling",
		"multiple-choice",
		"question-answering",
		"summarization",
		"translation",
		"image-classification",
		"speech-recognition",
		"audio-classification",
		"speech-pretraining",
		"image-pretraining",
		"semantic-segmentation",
	]
]
sys.path.extend(SRC_DIRS)


if SRC_DIRS is not None:
	import run_audio_classification
	import run_clm
	import run_generation
	import run_glue
	import run_image_classification
	import run_mae
	import run_mlm
	import run_ner
	import run_qa as run_squad
	import run_semantic_segmentation
	import run_seq2seq_qa as run_squad_seq2seq
	import run_speech_recognition_ctc
	import run_speech_recognition_ctc_adapter
	import run_speech_recognition_seq2seq
	import run_summarization
	import run_swag
	import run_translation
	import run_wav2vec2_pretraining_no_trainer


logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger()


def get_results(output_dir):
	results = {}
	path = os.path.join(output_dir, "all_results.json")
	if os.path.exists(path):
		with open(path, "r") as f:
			results = json.load(f)
	else:
		raise ValueError(f"can't find {path}")
	return results


stream_handler = logging.StreamHandler(sys.stdout)
logger.addHandler(stream_handler)


class ExamplesTests(TestCasePlus):
	def test_run_glue(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_glue.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_accuracy"], 0.75)

	def test_run_clm(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if backend_device_count(torch_device) > 1:
			return

		if torch_device == "cpu":
			testargs.append("--use_cpu")

		with patch.object(sys, "argv", testargs):
			run_clm.main()
			result = get_results(tmp_dir)
			self.assertLess(result["perplexity"], 100)

	def test_run_clm_config_overrides(self):

		tmp_dir = self.get_auto_remove_tmp_dir()

		if torch_device == "cpu":
			testargs.append("--use_cpu")

		logger = run_clm.logger
		with patch.object(sys, "argv", testargs):
			with CaptureLogger(logger) as cl:
				run_clm.main()

		self.assertIn('"n_embd": 10', cl.out)
		self.assertIn('"n_head": 2', cl.out)

	def test_run_mlm(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if torch_device == "cpu":
			testargs.append("--use_cpu")

		with patch.object(sys, "argv", testargs):
			run_mlm.main()
			result = get_results(tmp_dir)
			self.assertLess(result["perplexity"], 42)

	def test_run_ner(self):
		epochs = 7 if backend_device_count(torch_device) > 1 else 2

		tmp_dir = self.get_auto_remove_tmp_dir()

		if torch_device == "cpu":
			testargs.append("--use_cpu")

		with patch.object(sys, "argv", testargs):
			run_ner.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_accuracy"], 0.75)
			self.assertLess(result["eval_loss"], 0.5)

	def test_run_squad(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		with patch.object(sys, "argv", testargs):
			run_squad.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_f1"], 30)
			self.assertGreaterEqual(result["eval_exact"], 30)

	def test_run_squad_seq2seq(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		with patch.object(sys, "argv", testargs):
			run_squad_seq2seq.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_f1"], 30)
			self.assertGreaterEqual(result["eval_exact"], 30)

	def test_run_swag(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		with patch.object(sys, "argv", testargs):
			run_swag.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_accuracy"], 0.8)

	def test_generation(self):
		testargs = ["run_generation.py", "--prompt=Hello", "--length=10", "--seed=42"]

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		model_type, model_name = (
			"--model_type=gpt2",
			"--model_name_or_path=sshleifer/tiny-gpt2",
		)
		with patch.object(sys, "argv", testargs + [model_type, model_name]):
			result = run_generation.main()
			self.assertGreaterEqual(len(result[0]), 10)

	@slow
	def test_run_summarization(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		with patch.object(sys, "argv", testargs):
			run_summarization.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_rouge1"], 10)
			self.assertGreaterEqual(result["eval_rouge2"], 2)
			self.assertGreaterEqual(result["eval_rougeL"], 7)
			self.assertGreaterEqual(result["eval_rougeLsum"], 7)

	@slow
	def test_run_translation(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		with patch.object(sys, "argv", testargs):
			run_translation.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_bleu"], 30)

	def test_run_image_classification(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_image_classification.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_accuracy"], 0.8)

	def test_run_speech_recognition_ctc(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_speech_recognition_ctc.main()
			result = get_results(tmp_dir)
			self.assertLess(result["eval_loss"], result["train_loss"])

	def test_run_speech_recognition_ctc_adapter(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_speech_recognition_ctc_adapter.main()
			result = get_results(tmp_dir)
			self.assertTrue(os.path.isfile(os.path.join(tmp_dir, "./adapter.tur.safetensors")))
			self.assertLess(result["eval_loss"], result["train_loss"])

	def test_run_speech_recognition_seq2seq(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_speech_recognition_seq2seq.main()
			result = get_results(tmp_dir)
			self.assertLess(result["eval_loss"], result["train_loss"])

	def test_run_audio_classification(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_audio_classification.main()
			result = get_results(tmp_dir)
			self.assertLess(result["eval_loss"], result["train_loss"])

	def test_run_wav2vec2_pretraining(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		with patch.object(sys, "argv", testargs):
			run_wav2vec2_pretraining_no_trainer.main()
			model = Wav2Vec2ForPreTraining.from_pretrained(tmp_dir)
			self.assertIsNotNone(model)

	def test_run_vit_mae_pretraining(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_mae.main()
			model = ViTMAEForPreTraining.from_pretrained(tmp_dir)
			self.assertIsNotNone(model)

	def test_run_semantic_segmentation(self):
		tmp_dir = self.get_auto_remove_tmp_dir()

		if is_torch_fp16_available_on_device(torch_device):
			testargs.append("--fp16")

		with patch.object(sys, "argv", testargs):
			run_semantic_segmentation.main()
			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_overall_accuracy"], 0.1)

import torch.nn as nn

from .baseRNN import BaseRNN

class EncoderRNN(BaseRNN):

	def __init__(self, vocab_size, max_len, hidden_size,
				 input_dropout_p=0, dropout_p=0,
				 n_layers=1, bidirectional=False, rnn_cell='gru', variable_lengths=False,
				 embedding=None, update_embedding=True):
		super(EncoderRNN, self).__init__(vocab_size, max_len, hidden_size,
				input_dropout_p, dropout_p, n_layers, rnn_cell)

		self.variable_lengths = variable_lengths
		self.embedding = nn.Embedding(vocab_size, hidden_size)
		if embedding is not None:
			self.embedding.weight = nn.Parameter(embedding)
		self.embedding.weight.requires_grad = update_embedding
		self.rnn = self.rnn_cell(hidden_size, hidden_size, n_layers,
								 batch_first=True, bidirectional=bidirectional, dropout=dropout_p)

	def forward(self, input_var, input_lengths=None):
		embedded = self.embedding(input_var)
		embedded = self.input_dropout(embedded)
		if self.variable_lengths:
			embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)
		output, hidden = self.rnn(embedded)
		if self.variable_lengths:
			output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)
		return output, hidden

from typing import List
import logging
import os
import tempfile
import subprocess
import shutil


from nltk import Tree

from allennlp.common.checks import ConfigurationError
from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum

logger = logging.getLogger(__name__)

DEFAULT_EVALB_DIR = os.path.abspath(
	os.path.join(
		os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir, "tools", "EVALB"
	)
)


@Metric.register("evalb")
class EvalbBracketingScorer(Metric):

	def __init__(
		self,
		evalb_directory_path: str = DEFAULT_EVALB_DIR,
		evalb_param_filename: str = "COLLINS.prm",
		evalb_num_errors_to_kill: int = 10,
	) -> None:
		self._evalb_directory_path = evalb_directory_path
		self._evalb_program_path = os.path.join(evalb_directory_path, "evalb")
		self._evalb_param_path = os.path.join(evalb_directory_path, evalb_param_filename)
		self._evalb_num_errors_to_kill = evalb_num_errors_to_kill

		self._header_line = [
			"ID",
			"Len.",
			"Stat.",
			"Recal",
			"Prec.",
			"Bracket",
			"gold",
			"test",
			"Bracket",
			"Words",
			"Tags",
			"Accracy",
		]

		self._correct_predicted_brackets = 0.0
		self._gold_brackets = 0.0
		self._predicted_brackets = 0.0

	def __call__(self, predicted_trees: List[Tree], gold_trees: List[Tree]) -> None:  # type: ignore
		if not os.path.exists(self._evalb_program_path):
			logger.warning(
				f"EVALB not found at {self._evalb_program_path}.  Attempting to compile it."
			)
			EvalbBracketingScorer.compile_evalb(self._evalb_directory_path)

			if not os.path.exists(self._evalb_program_path):
				compile_command = (
					f"python -c 'from allennlp.training.metrics import EvalbBracketingScorer; "
					f'EvalbBracketingScorer.compile_evalb("{self._evalb_directory_path}")\''
				)
				raise ConfigurationError(
					f"EVALB still not found at {self._evalb_program_path}. "
					"You must compile the EVALB scorer before using it."
					" Run 'make' in the '{}' directory or run: {}".format(
						self._evalb_program_path, compile_command
					)
				)
		tempdir = tempfile.mkdtemp()
		gold_path = os.path.join(tempdir, "gold.txt")
		predicted_path = os.path.join(tempdir, "predicted.txt")
		with open(gold_path, "w") as gold_file:
			for tree in gold_trees:
				gold_file.write(f"{tree.pformat(margin=1000000)}\n")

		with open(predicted_path, "w") as predicted_file:
			for tree in predicted_trees:
				predicted_file.write(f"{tree.pformat(margin=1000000)}\n")

		command = [
			self._evalb_program_path,
			"-p",
			self._evalb_param_path,
			"-e",
			str(self._evalb_num_errors_to_kill),
			gold_path,
			predicted_path,
		]
		completed_process = subprocess.run(
			command, stdout=subprocess.PIPE, universal_newlines=True, check=True
		)

		_correct_predicted_brackets = 0.0
		_gold_brackets = 0.0
		_predicted_brackets = 0.0

		for line in completed_process.stdout.split("\n"):
			stripped = line.strip().split()
			if len(stripped) == 12 and stripped != self._header_line:
				numeric_line = [float(x) for x in stripped]
				_correct_predicted_brackets += numeric_line[5]
				_gold_brackets += numeric_line[6]
				_predicted_brackets += numeric_line[7]

		shutil.rmtree(tempdir)

		self._correct_predicted_brackets += dist_reduce_sum(_correct_predicted_brackets)
		self._gold_brackets += dist_reduce_sum(_gold_brackets)
		self._predicted_brackets += dist_reduce_sum(_predicted_brackets)

	def get_metric(self, reset: bool = False):

		recall = (
			self._correct_predicted_brackets / self._gold_brackets
			if self._gold_brackets > 0
			else 0.0
		)
		precision = (
			self._correct_predicted_brackets / self._predicted_brackets
			if self._gold_brackets > 0
			else 0.0
		)
		f1_measure = (
			2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
		)

		if reset:
			self.reset()
		return {
			"evalb_recall": recall,
			"evalb_precision": precision,
			"evalb_f1_measure": f1_measure,
		}

	def reset(self):
		self._correct_predicted_brackets = 0.0
		self._gold_brackets = 0.0
		self._predicted_brackets = 0.0

	@staticmethod
	def compile_evalb(evalb_directory_path: str = DEFAULT_EVALB_DIR):
		logger.info(f"Compiling EVALB by running make in {evalb_directory_path}.")
		os.system("cd {} && make && cd ../../../".format(evalb_directory_path))

	@staticmethod
	def clean_evalb(evalb_directory_path: str = DEFAULT_EVALB_DIR):
		os.system("rm {}".format(os.path.join(evalb_directory_path, "evalb")))

import torch


class ResidualWithLayerDropout(torch.nn.Module):

	def __init__(self, undecayed_dropout_prob: float = 0.5) -> None:
		super().__init__()
		if undecayed_dropout_prob < 0 or undecayed_dropout_prob > 1:
			raise ValueError(
				f"undecayed dropout probability has to be between 0 and 1, "
				f"but got {undecayed_dropout_prob}"
			)
		self.undecayed_dropout_prob = undecayed_dropout_prob

	def forward(
		self,  # type: ignore
		layer_input: torch.Tensor,
		layer_output: torch.Tensor,
		layer_index: int = None,
		total_layers: int = None,
	) -> torch.Tensor:

		if layer_index is not None and total_layers is not None:
			dropout_prob = 1.0 * self.undecayed_dropout_prob * layer_index / total_layers
		else:
			dropout_prob = 1.0 * self.undecayed_dropout_prob
		if self.training:
			if torch.rand(1) < dropout_prob:
				return layer_input
			else:
				return layer_output + layer_input
		else:
			return (1 - dropout_prob) * layer_output + layer_input

import datetime
import logging
import os
import shutil
import json
from os import PathLike
from typing import Any, Dict, Iterable, Optional, Union, Tuple, Set, List
from collections import Counter

import torch
from torch.nn.utils import clip_grad_norm_

from allennlp.common.checks import check_for_gpu, ConfigurationError
from allennlp.common.params import Params
from allennlp.common.tqdm import Tqdm
from allennlp.common.util import dump_metrics, sanitize, int_to_device
from allennlp.data import Instance, Vocabulary, Batch, DataLoader
from allennlp.data.dataset_readers import DatasetReader
from allennlp.models.archival import CONFIG_NAME
from allennlp.models.model import Model
from allennlp.nn import util as nn_util


logger = logging.getLogger(__name__)


class HasBeenWarned:
	tqdm_ignores_underscores = False


def move_optimizer_to_cuda(optimizer):
	for param_group in optimizer.param_groups:
		for param in param_group["params"]:
			if param.is_cuda:
				param_state = optimizer.state[param]
				for k in param_state.keys():
					if isinstance(param_state[k], torch.Tensor):
						param_state[k] = param_state[k].cuda(device=param.get_device())


def get_batch_size(batch: Union[Dict, torch.Tensor]) -> int:
	if isinstance(batch, torch.Tensor):
		return batch.size(0)  # type: ignore
	elif isinstance(batch, Dict):
		return get_batch_size(next(iter(batch.values())))
	else:
		return 0


def time_to_str(timestamp: int) -> str:
	datetimestamp = datetime.datetime.fromtimestamp(timestamp)
	return "{:04d}-{:02d}-{:02d}-{:02d}-{:02d}-{:02d}".format(
		datetimestamp.year,
		datetimestamp.month,
		datetimestamp.day,
		datetimestamp.hour,
		datetimestamp.minute,
		datetimestamp.second,
	)


def str_to_time(time_str: str) -> datetime.datetime:
	pieces: Any = [int(piece) for piece in time_str.split("-")]
	return datetime.datetime(*pieces)


def data_loaders_from_params(
	params: Params,
	train: bool = True,
	validation: bool = True,
	test: bool = True,
	serialization_dir: Optional[Union[str, PathLike]] = None,
) -> Dict[str, DataLoader]:
	data_loaders: Dict[str, DataLoader] = {}

	train = train and ("train_data_path" in params)
	validation = validation and ("validation_data_path" in params)
	test = test and ("test_data_path" in params)
	if not any((train, validation, test)):
		return data_loaders

	dataset_reader_params = params.pop("dataset_reader")
	dataset_reader = DatasetReader.from_params(
		dataset_reader_params, serialization_dir=serialization_dir
	)
	data_loader_params = params.pop("data_loader")

	if train:
		train_data_path = params.pop("train_data_path")
		logger.info("Reading training data from %s", train_data_path)
		data_loaders["train"] = DataLoader.from_params(
			data_loader_params.duplicate(), reader=dataset_reader, data_path=train_data_path
		)

	if not validation and not test:
		return data_loaders

	validation_and_test_dataset_reader: DatasetReader = dataset_reader
	validation_dataset_reader_params = params.pop("validation_dataset_reader", None)
	if validation_dataset_reader_params is not None:
		logger.info("Using a separate dataset reader to load validation and test data.")
		validation_and_test_dataset_reader = DatasetReader.from_params(
			validation_dataset_reader_params, serialization_dir=serialization_dir
		)

	validation_data_loader_params = params.pop("validation_data_loader", data_loader_params)

	if validation:
		validation_data_path = params.pop("validation_data_path")
		logger.info("Reading validation data from %s", validation_data_path)
		data_loaders["validation"] = DataLoader.from_params(
			validation_data_loader_params.duplicate(),
			reader=validation_and_test_dataset_reader,
			data_path=validation_data_path,
		)

	if test:
		test_data_path = params.pop("test_data_path")
		logger.info("Reading test data from %s", test_data_path)
		data_loaders["test"] = DataLoader.from_params(
			validation_data_loader_params,
			reader=validation_and_test_dataset_reader,
			data_path=test_data_path,
		)

	return data_loaders


def create_serialization_dir(
	params: Params, serialization_dir: Union[str, PathLike], recover: bool, force: bool
) -> None:
	if recover and force:
		raise ConfigurationError("Illegal arguments: both force and recover are true.")

	if os.path.exists(serialization_dir) and force:
		shutil.rmtree(serialization_dir)

	if os.path.exists(serialization_dir) and os.listdir(serialization_dir):
		if not recover:
			raise ConfigurationError(
				f"Serialization directory ({serialization_dir}) already exists and is "
				f"not empty. Specify --recover to recover from an existing output folder."
			)

		logger.info(f"Recovering from prior training at {serialization_dir}.")

		recovered_config_file = os.path.join(serialization_dir, CONFIG_NAME)
		if not os.path.exists(recovered_config_file):
			raise ConfigurationError(
				"The serialization directory already exists but doesn't "
				"contain a config.json. You probably gave the wrong directory."
			)
		loaded_params = Params.from_file(recovered_config_file)

		fail = False
		flat_params = params.as_flat_dict()
		flat_loaded = loaded_params.as_flat_dict()
		for key in flat_params.keys() - flat_loaded.keys():
			logger.error(
				f"Key '{key}' found in training configuration but not in the serialization "
				f"directory we're recovering from."
			)
			fail = True
		for key in flat_loaded.keys() - flat_params.keys():
			logger.error(
				f"Key '{key}' found in the serialization directory we're recovering from "
				f"but not in the training config."
			)
			fail = True
		for key in flat_params.keys():
			if flat_params.get(key) != flat_loaded.get(key):
				logger.error(
					f"Value for '{key}' in training configuration does not match that the value in "
					f"the serialization directory we're recovering from: "
					f"{flat_params[key]} != {flat_loaded[key]}"
				)
				fail = True
		if fail:
			raise ConfigurationError(
				"Training configuration does not match the configuration we're recovering from."
			)
	else:
		if recover:
			raise ConfigurationError(
				f"--recover specified but serialization_dir ({serialization_dir}) "
				"does not exist.  There is nothing to recover from."
			)
		os.makedirs(serialization_dir, exist_ok=True)


def enable_gradient_clipping(model: Model, grad_clipping: Optional[float]) -> None:
	if grad_clipping is not None:
		for parameter in model.parameters():
			if parameter.requires_grad:
				parameter.register_hook(
					lambda grad: nn_util.clamp_tensor(
						grad, minimum=-grad_clipping, maximum=grad_clipping
					)
				)


def rescale_gradients(model: Model, grad_norm: Optional[float] = None) -> Optional[float]:
	if grad_norm:
		parameters_to_clip = [p for p in model.parameters() if p.grad is not None]
		return clip_grad_norm_(parameters_to_clip, grad_norm)
	return None


def get_metrics(
	model: Model,
	total_loss: float,
	total_reg_loss: Optional[float],
	batch_loss: Optional[float],
	batch_reg_loss: Optional[float],
	num_batches: int,
	reset: bool = False,
) -> Dict[str, float]:
	metrics = model.get_metrics(reset=reset)
	if batch_loss is not None:
		metrics["batch_loss"] = batch_loss
	metrics["loss"] = float(total_loss / num_batches) if num_batches > 0 else 0.0
	if total_reg_loss is not None:
		if batch_reg_loss is not None:
			metrics["batch_reg_loss"] = batch_reg_loss
		metrics["reg_loss"] = float(total_reg_loss / num_batches) if num_batches > 0 else 0.0

	return metrics


def get_train_and_validation_metrics(metrics: Dict) -> Tuple[Dict[str, Any], Dict[str, Any]]:
	train_metrics: Dict[str, Any] = {}
	val_metrics: Dict[str, Any] = {}
	for key, value in metrics.items():
		if key.startswith("training_"):
			key = key.replace("training_", "", 1)
			if key not in {"duration", "start_epoch", "epochs"}:
				train_metrics[key] = value
		elif key.startswith("validation_"):
			key = key.replace("validation_", "", 1)
			val_metrics[key] = value
	return train_metrics, val_metrics


def evaluate(
	model: Model,
	data_loader: DataLoader,
	cuda_device: Union[int, torch.device] = -1,
	batch_weight_key: str = None,
	output_file: str = None,
	predictions_output_file: str = None,
) -> Dict[str, Any]:
	check_for_gpu(cuda_device)
	data_loader.set_target_device(int_to_device(cuda_device))
	predictions_file = (
		None if predictions_output_file is None else open(predictions_output_file, "w")
	)

	with torch.no_grad():
		model.eval()

		iterator = iter(data_loader)
		logger.info("Iterating over dataset")
		generator_tqdm = Tqdm.tqdm(iterator)

		batch_count = 0
		loss_count = 0
		total_loss = 0.0
		total_weight = 0.0

		for batch in generator_tqdm:
			batch_count += 1
			batch = nn_util.move_to_device(batch, cuda_device)
			output_dict = model(**batch)
			loss = output_dict.get("loss")

			metrics = model.get_metrics()

			if loss is not None:
				loss_count += 1
				if batch_weight_key:
					weight = output_dict[batch_weight_key].item()
				else:
					weight = 1.0

				total_weight += weight
				total_loss += loss.item() * weight
				metrics["loss"] = total_loss / total_weight

			if not HasBeenWarned.tqdm_ignores_underscores and any(
				metric_name.startswith("_") for metric_name in metrics
			):
				logger.warning(
					'Metrics with names beginning with "_" will '
					"not be logged to the tqdm progress bar."
				)
				HasBeenWarned.tqdm_ignores_underscores = True
			description = (
				", ".join(
					[
						"%s: %.2f" % (name, value)
						for name, value in metrics.items()
						if not name.startswith("_")
					]
				)
				+ " ||"
			)
			generator_tqdm.set_description(description, refresh=False)

			if predictions_file is not None:
				predictions = json.dumps(sanitize(model.make_output_human_readable(output_dict)))
				predictions_file.write(predictions + "\n")

		if predictions_file is not None:
			predictions_file.close()

		final_metrics = model.get_metrics(reset=True)
		if loss_count > 0:
			if loss_count != batch_count:
				raise RuntimeError(
					"The model you are trying to evaluate only sometimes produced a loss!"
				)
			final_metrics["loss"] = total_loss / total_weight

		if output_file is not None:
			dump_metrics(output_file, final_metrics, log=True)

		return final_metrics


def description_from_metrics(metrics: Dict[str, float]) -> str:
	if not HasBeenWarned.tqdm_ignores_underscores and any(
		metric_name.startswith("_") for metric_name in metrics
	):
		logger.warning(
			'Metrics with names beginning with "_" will ' "not be logged to the tqdm progress bar."
		)
		HasBeenWarned.tqdm_ignores_underscores = True
	return (
		", ".join(
			[
				"%s: %.4f" % (name, value)
				for name, value in metrics.items()
				if not name.startswith("_")
			]
		)
		+ " ||"
	)


def make_vocab_from_params(
	params: Params, serialization_dir: Union[str, PathLike], print_statistics: bool = False
) -> Vocabulary:
	vocab_params = params.pop("vocabulary", {})
	os.makedirs(serialization_dir, exist_ok=True)
	vocab_dir = os.path.join(serialization_dir, "vocabulary")

	if os.path.isdir(vocab_dir) and os.listdir(vocab_dir) is not None:
		raise ConfigurationError(
			"The 'vocabulary' directory in the provided serialization directory is non-empty"
		)

	datasets_for_vocab_creation: Optional[List[str]] = params.pop(
		"datasets_for_vocab_creation", None
	)
	if datasets_for_vocab_creation is None and vocab_params.get("type") in {
		"empty",
		"from_files",
		"from_pretrained_transformer",
	}:
		datasets_for_vocab_creation = []

	data_loaders: Dict[str, DataLoader]
	if datasets_for_vocab_creation is None:
		data_loaders = data_loaders_from_params(params, serialization_dir=serialization_dir)
	else:
		for dataset_name in datasets_for_vocab_creation:
			data_path = f"{dataset_name}_data_path"
			if data_path not in params:
				raise ConfigurationError(f"invalid 'datasets_for_vocab_creation' {dataset_name}")
		data_loaders = data_loaders_from_params(
			params,
			serialization_dir=serialization_dir,
			train=("train" in datasets_for_vocab_creation),
			validation=("validation" in datasets_for_vocab_creation),
			test=("test" in datasets_for_vocab_creation),
		)

	instances: Iterable[Instance] = (
		instance
		for key, data_loader in data_loaders.items()
		if datasets_for_vocab_creation is None or key in datasets_for_vocab_creation
		for instance in data_loader.iter_instances()
	)

	if print_statistics:
		instances = list(instances)

	vocab = Vocabulary.from_params(vocab_params, instances=instances)

	logger.info(f"writing the vocabulary to {vocab_dir}.")
	vocab.save_to_files(vocab_dir)
	logger.info("done creating vocab")

	if print_statistics:
		dataset = Batch(instances)
		dataset.index_instances(vocab)
		dataset.print_statistics()
		vocab.print_statistics()

	return vocab


def ngrams(
	tensor: torch.LongTensor, ngram_size: int, exclude_indices: Set[int]
) -> Dict[Tuple[int, ...], int]:
	ngram_counts: Dict[Tuple[int, ...], int] = Counter()
	if ngram_size > tensor.size(-1):
		return ngram_counts
	for start_position in range(ngram_size):
		for tensor_slice in tensor[start_position:].split(ngram_size, dim=-1):
			if tensor_slice.size(-1) < ngram_size:
				break
			ngram = tuple(x.item() for x in tensor_slice)
			if any(x in exclude_indices for x in ngram):
				continue
			ngram_counts[ngram] += 1
	return ngram_counts


def get_valid_tokens_mask(tensor: torch.LongTensor, exclude_indices: Set[int]) -> torch.ByteTensor:
	valid_tokens_mask = torch.ones_like(tensor, dtype=torch.bool)
	for index in exclude_indices:
		valid_tokens_mask &= tensor != index
	return valid_tokens_mask

from typing import List, Iterable, Sequence, Optional

from allennlp.common.registrable import Registrable
from allennlp.data.instance import Instance


class BatchSampler(Registrable):
	def get_batch_indices(self, instances: Sequence[Instance]) -> Iterable[List[int]]:
		raise NotImplementedError

	def get_num_batches(self, instances: Sequence[Instance]) -> int:
		raise NotImplementedError

	def get_batch_size(self) -> Optional[int]:
		return None


from typing import Optional, Tuple, Union, List
import torch
from torch.nn.utils.rnn import PackedSequence
from allennlp.modules.augmented_lstm import AugmentedLstm
from allennlp.common.checks import ConfigurationError

TensorPair = Tuple[torch.Tensor, torch.Tensor]


class StackedAlternatingLstm(torch.nn.Module):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int,
		recurrent_dropout_probability: float = 0.0,
		use_highway: bool = True,
		use_input_projection_bias: bool = True,
	) -> None:
		super().__init__()

		self.input_size = input_size
		self.hidden_size = hidden_size
		self.num_layers = num_layers

		layers = []
		lstm_input_size = input_size
		for layer_index in range(num_layers):
			go_forward = layer_index % 2 == 0
			layer = AugmentedLstm(
				lstm_input_size,
				hidden_size,
				go_forward,
				recurrent_dropout_probability=recurrent_dropout_probability,
				use_highway=use_highway,
				use_input_projection_bias=use_input_projection_bias,
			)
			lstm_input_size = hidden_size
			self.add_module("layer_{}".format(layer_index), layer)
			layers.append(layer)
		self.lstm_layers = layers

	def forward(
		self, inputs: PackedSequence, initial_state: Optional[TensorPair] = None
	) -> Tuple[Union[torch.Tensor, PackedSequence], TensorPair]:
		if not initial_state:
			hidden_states: List[Optional[TensorPair]] = [None] * len(self.lstm_layers)
		elif initial_state[0].size()[0] != len(self.lstm_layers):
			raise ConfigurationError(
				"Initial states were passed to forward() but the number of "
				"initial states does not match the number of layers."
			)
		else:
			hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))

		output_sequence = inputs
		final_states = []
		for i, state in enumerate(hidden_states):
			layer = getattr(self, "layer_{}".format(i))
			output_sequence, final_state = layer(output_sequence, state)
			final_states.append(final_state)

		final_hidden_state, final_cell_state = tuple(
			torch.cat(state_list, 0) for state_list in zip(*final_states)
		)
		return output_sequence, (final_hidden_state, final_cell_state)

import torch

from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder


@Seq2SeqEncoder.register("pass_through")
class PassThroughEncoder(Seq2SeqEncoder):

	def __init__(self, input_dim: int) -> None:
		super().__init__()
		self._input_dim = input_dim

	def get_input_dim(self) -> int:
		return self._input_dim

	def get_output_dim(self) -> int:
		return self._input_dim

	def is_bidirectional(self):
		return False

	def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor = None) -> torch.Tensor:
		if mask is None:
			return inputs
		else:
			return inputs * mask.unsqueeze(dim=-1)

from typing import List, Union, Dict, Any


from allennlp.data.tokenizers.token_class import Token
from allennlp.data.tokenizers.tokenizer import Tokenizer


@Tokenizer.register("character")
class CharacterTokenizer(Tokenizer):

	def __init__(
		self,
		byte_encoding: str = None,
		lowercase_characters: bool = False,
		start_tokens: List[Union[str, int]] = None,
		end_tokens: List[Union[str, int]] = None,
	) -> None:
		self._byte_encoding = byte_encoding
		self._lowercase_characters = lowercase_characters
		self._start_tokens = start_tokens or []
		self._start_tokens.reverse()
		self._end_tokens = end_tokens or []

	def tokenize(self, text: str) -> List[Token]:
		if self._lowercase_characters:
			text = text.lower()
		if self._byte_encoding is not None:
			tokens = [Token(text_id=c + 1) for c in text.encode(self._byte_encoding)]
		else:
			tokens = [Token(t) for t in list(text)]
		for start_token in self._start_tokens:
			if isinstance(start_token, int):
				token = Token(text_id=start_token, idx=0)
			else:
				token = Token(text=start_token, idx=0)
			tokens.insert(0, token)
		for end_token in self._end_tokens:
			if isinstance(end_token, int):
				token = Token(text_id=end_token, idx=0)
			else:
				token = Token(text=end_token, idx=0)
			tokens.append(token)
		return tokens

	def __eq__(self, other) -> bool:
		if isinstance(self, other.__class__):
			return self.__dict__ == other.__dict__
		return NotImplemented

	def _to_params(self) -> Dict[str, Any]:
		return {
			"type": "character",
			"byte_encoding": self._byte_encoding,
			"lowercase_characters": self._lowercase_characters,
			"start_tokens": self._start_tokens,
			"end_tokens": self._end_tokens,
		}

import math
from typing import Optional

import torch


from allennlp.modules.attention.dot_product_attention import DotProductAttention
from allennlp.modules.attention.attention import Attention


@Attention.register("scaled_dot_product")
class ScaledDotProductAttention(DotProductAttention):

	def __init__(self, scaling_factor: Optional[int] = None, normalize: bool = True) -> None:
		super().__init__(normalize)
		self.scaling_factor = scaling_factor

	def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:
		scores = super()._forward_internal(vector, matrix)
		scaling_factor = self.scaling_factor or matrix.size(-1)
		scores = scores / math.sqrt(scaling_factor)
		return scores

import logging
from typing import Optional


import torch


from allennlp.common.util import is_distributed
from allennlp.training.metrics.metric import Metric

logger = logging.getLogger(__name__)


@Metric.register("covariance")
class Covariance(Metric):

	def __init__(self) -> None:
		self._total_prediction_mean = 0.0
		self._total_label_mean = 0.0
		self._total_co_moment = 0.0
		self._total_count = 0.0

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		predictions = predictions.view(-1)
		gold_labels = gold_labels.view(-1)

		if mask is not None:
			mask = mask.view(-1)
			predictions = predictions * mask
			gold_labels = gold_labels * mask
			num_batch_items = torch.sum(mask).item()
		else:
			num_batch_items = gold_labels.numel()

		previous_count = self._total_count
		updated_count = previous_count + num_batch_items

		batch_mean_prediction = torch.sum(predictions) / num_batch_items

		delta_mean_prediction = (
			(batch_mean_prediction - self._total_prediction_mean) * num_batch_items
		) / updated_count
		previous_total_prediction_mean = self._total_prediction_mean

		batch_mean_label = torch.sum(gold_labels) / num_batch_items
		delta_mean_label = (
			(batch_mean_label - self._total_label_mean) * num_batch_items
		) / updated_count
		previous_total_label_mean = self._total_label_mean

		batch_coresiduals = (predictions - batch_mean_prediction) * (gold_labels - batch_mean_label)

		if mask is not None:
			batch_co_moment = torch.sum(batch_coresiduals * mask)
		else:
			batch_co_moment = torch.sum(batch_coresiduals)

		delta_co_moment = batch_co_moment + (
			previous_total_prediction_mean - batch_mean_prediction
		) * (previous_total_label_mean - batch_mean_label) * (
			previous_count * num_batch_items / updated_count
		)




		self._total_prediction_mean += delta_mean_prediction.item()
		self._total_label_mean += delta_mean_label.item()
		self._total_co_moment += delta_co_moment.item()
		self._total_count = updated_count

	def get_metric(self, reset: bool = False) -> float:
		if is_distributed():
			raise RuntimeError("Distributed aggregation for Covariance is currently not supported.")
		covariance = self._total_co_moment / (self._total_count - 1)
		if reset:
			self.reset()
		return covariance

	def reset(self):
		self._total_prediction_mean = 0.0
		self._total_label_mean = 0.0
		self._total_co_moment = 0.0
		self._total_count = 0.0

import os
import time
import requests
import tarfile
import numpy as np
import argparse

import torch
from torch import nn
import torch.nn.functional as F
from torch.optim import Adam


class GraphConv(nn.Module):
	def __init__(self, input_dim, output_dim, use_bias=False):
		super(GraphConv, self).__init__()

		self.kernel = nn.Parameter(torch.Tensor(input_dim, output_dim))
		nn.init.xavier_normal_(self.kernel) # Initialize the weights using Xavier initialization

		self.bias = None
		if use_bias:
			self.bias = nn.Parameter(torch.Tensor(output_dim))
			nn.init.zeros_(self.bias) # Initialize the bias to zeros

	def forward(self, input_tensor, adj_mat):

		support = torch.mm(input_tensor, self.kernel) # Matrix multiplication between input and weight matrix
		output = torch.spmm(adj_mat, support) # Sparse matrix multiplication between adjacency matrix and support
		if self.bias is not None:
			output = output + self.bias

		return output


class GCN(nn.Module):
	def __init__(self, input_dim, hidden_dim, output_dim, use_bias=True, dropout_p=0.1):
		super(GCN, self).__init__()

		self.gc1 = GraphConv(input_dim, hidden_dim, use_bias=use_bias)
		self.gc2 = GraphConv(hidden_dim, output_dim, use_bias=use_bias)

		self.dropout = nn.Dropout(dropout_p)

	def forward(self, input_tensor, adj_mat):

		x = self.gc1(input_tensor, adj_mat)
		x = F.relu(x) # Apply ReLU activation function
		x = self.dropout(x) # Apply dropout regularization

		x = self.gc2(x, adj_mat)

		return F.log_softmax(x, dim=1)


def load_cora(path='./cora', device='cpu'):

	content_path = os.path.join(path, 'cora.content')
	cites_path = os.path.join(path, 'cora.cites')

	content_tensor = np.genfromtxt(content_path, dtype=np.dtype(str))
	cites_tensor = np.genfromtxt(cites_path, dtype=np.int32)

	features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32)) # Extract feature values
	scale_vector = torch.sum(features, dim=1) # Compute sum of features for each node
	scale_vector = 1 / scale_vector # Compute reciprocal of the sums
	scale_vector[scale_vector == float('inf')] = 0 # Handle division by zero cases
	scale_vector = torch.diag(scale_vector).to_sparse() # Convert the scale vector to a sparse diagonal matrix
	features = scale_vector @ features # Scale the features using the scale vector

	classes, labels = np.unique(content_tensor[:, -1], return_inverse=True) # Extract unique classes and map labels to indices
	labels = torch.LongTensor(labels) # Convert labels to a tensor

	idx = content_tensor[:, 0].astype(np.int32) # Extract node indices
	idx_map = {id: pos for pos, id in enumerate(idx)} # Create a dictionary to map indices to positions

	edges = np.array(
		list(map(lambda edge: [idx_map[edge[0]], idx_map[edge[1]]], 
			cites_tensor)), dtype=np.int32)

	V = len(idx) # Number of nodes
	E = edges.shape[0] # Number of edges
	adj_mat = torch.sparse_coo_tensor(edges.T, torch.ones(E), (V, V), dtype=torch.int64) # Create the initial adjacency matrix as a sparse tensor
	adj_mat = torch.eye(V) + adj_mat # Add self-loops to the adjacency matrix

	degree_mat = torch.sum(adj_mat, dim=1) # Compute the sum of each row in the adjacency matrix (degree matrix)
	degree_mat = torch.sqrt(1 / degree_mat) # Compute the reciprocal square root of the degrees
	degree_mat[degree_mat == float('inf')] = 0 # Handle division by zero cases
	degree_mat = torch.diag(degree_mat).to_sparse() # Convert the degree matrix to a sparse diagonal matrix

	adj_mat = degree_mat @ adj_mat @ degree_mat # Apply the renormalization trick

	return features.to_sparse().to(device), labels.to(device), adj_mat.to_sparse().to(device)


def train_iter(epoch, model, optimizer, criterion, input, target, mask_train, mask_val, print_every=10):
	start_t = time.time()
	model.train()
	optimizer.zero_grad()

	output = model(*input) 
	loss = criterion(output[mask_train], target[mask_train]) # Compute the loss using the training mask

	loss.backward()
	optimizer.step()

	loss_train, acc_train = test(model, criterion, input, target, mask_train)
	loss_val, acc_val = test(model, criterion, input, target, mask_val)

	if epoch % print_every == 0:
		print(f'Epoch: {epoch:04d} ({(time.time() - start_t):.4f}s) loss_train: {loss_train:.4f} acc_train: {acc_train:.4f} loss_val: {loss_val:.4f} acc_val: {acc_val:.4f}')


def test(model, criterion, input, target, mask):
	model.eval()
	with torch.no_grad():
		output = model(*input)
		output, target = output[mask], target[mask]

		loss = criterion(output, target)
		acc = (output.argmax(dim=1) == target).float().sum() / len(target)
	return loss.item(), acc.item()


if __name__ == '__main__':
	device = 'cuda' if torch.cuda.is_available() else 'cpu'

	parser = argparse.ArgumentParser(description='PyTorch Graph Convolutional Network')
	parser.add_argument('--epochs', type=int, default=200,
						help='number of epochs to train (default: 200)')
	parser.add_argument('--lr', type=float, default=0.01,
						help='learning rate (default: 0.01)')
	parser.add_argument('--l2', type=float, default=5e-4,
						help='weight decay (default: 5e-4)')
	parser.add_argument('--dropout-p', type=float, default=0.5,
						help='dropout probability (default: 0.5)')
	parser.add_argument('--hidden-dim', type=int, default=16,
						help='dimension of the hidden representation (default: 16)')
	parser.add_argument('--val-every', type=int, default=20,
						help='epochs to wait for print training and validation evaluation (default: 20)')
	parser.add_argument('--include-bias', action='store_true', default=False,
						help='use bias term in convolutions (default: False)')
	parser.add_argument('--no-cuda', action='store_true', default=False,
						help='disables CUDA training')
	parser.add_argument('--no-mps', action='store_true', default=False,
						help='disables macOS GPU training')
	parser.add_argument('--dry-run', action='store_true', default=False,
						help='quickly check a single pass')
	parser.add_argument('--seed', type=int, default=42, metavar='S',
						help='random seed (default: 42)')
	args = parser.parse_args()

	use_cuda = not args.no_cuda and torch.cuda.is_available()
	use_mps = not args.no_mps and torch.backends.mps.is_available()

	torch.manual_seed(args.seed)

	if use_cuda:
		device = torch.device('cuda')
	elif use_mps:
		device = torch.device('mps')
	else:
		device = torch.device('cpu')
	print(f'Using {device} device')

	cora_url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'
	print('Downloading dataset...')
	with requests.get(cora_url, stream=True) as tgz_file:
		with tarfile.open(fileobj=tgz_file.raw, mode='r:gz') as tgz_object:
			tgz_object.extractall()

	print('Loading dataset...')
	features, labels, adj_mat = load_cora(device=device)
	idx = torch.randperm(len(labels)).to(device)
	idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:]

	gcn = GCN(features.shape[1], args.hidden_dim, labels.max().item() + 1,args.include_bias, args.dropout_p).to(device)
	optimizer = Adam(gcn.parameters(), lr=args.lr, weight_decay=args.l2)
	criterion = nn.NLLLoss()

	for epoch in range(args.epochs):
		train_iter(epoch + 1, gcn, optimizer, criterion, (features, adj_mat), labels, idx_train, idx_val, args.val_every)
		if args.dry_run:
			break

	loss_test, acc_test = test(gcn, criterion, (features, adj_mat), labels, idx_test)
	print(f'Test set results: loss {loss_test:.4f} accuracy {acc_test:.4f}')
import os
from typing import (
	Union,
	Tuple,
	OrderedDict,
	Dict,
	NamedTuple,
	List,
	Optional,
	Any,
	Sequence,
	TYPE_CHECKING,
)


import torch
import torch.distributed as dist
from torch.cuda import amp
from torch.nn.utils import clip_grad_norm_

from allennlp.common import Registrable
from allennlp.common.util import int_to_device
from allennlp.nn.parallel.sharded_module_mixin import ShardedModuleMixin

if TYPE_CHECKING:
	from allennlp.models import Model


StateDictType = Union[Dict[str, torch.Tensor], OrderedDict[str, torch.Tensor]]


class LoadStateDictReturnType(NamedTuple):
	missing_keys: List[str]
	unexpected_keys: List[str]


class DdpWrappedModel:

	def __init__(
		self,
		model: torch.nn.Module,
		local_rank: Optional[int] = None,
		world_size: Optional[int] = None,
	) -> None:
		self.model = model
		self.local_rank: int = local_rank if local_rank is not None else dist.get_rank()
		self.world_size: int = world_size if world_size is not None else dist.get_world_size()
		self.is_primary: bool = self.local_rank == 0

	@property
	def is_sharded(self) -> bool:
		return isinstance(self.model, ShardedModuleMixin)

	@staticmethod
	def consolidate_sharded_state(
		sharded_state_files: Sequence[Union[str, os.PathLike]]
	) -> StateDictType:
		raise NotImplementedError

	def load_state_dict(
		self, state_dict: StateDictType, strict: bool = True
	) -> LoadStateDictReturnType:
		return self.model.load_state_dict(state_dict, strict=strict)  # type: ignore[arg-type]

	def state_dict(self, *args, **kwargs) -> StateDictType:
		return self.model.state_dict(*args, **kwargs)

	def clip_grad_norm_(self, max_norm: Union[float, int]) -> torch.Tensor:
		return clip_grad_norm_([p for p in self.model.parameters() if p.grad is not None], max_norm)

	def init_grad_scaler(self) -> amp.GradScaler:
		return amp.GradScaler()


class DdpAccelerator(Registrable):

	default_implementation = "torch"

	def __init__(
		self,
		local_rank: Optional[int] = None,
		world_size: Optional[int] = None,
		cuda_device: Union[torch.device, int] = -1,
	) -> None:
		self.local_rank: int = local_rank if local_rank is not None else dist.get_rank()
		self.world_size: int = world_size if world_size is not None else dist.get_world_size()
		self.is_primary: bool = local_rank == 0
		self.cuda_device = int_to_device(cuda_device)

	def wrap_model(self, model: "Model") -> Tuple["Model", DdpWrappedModel]:
		raise NotImplementedError

	def wrap_module(self, module: torch.nn.Module) -> torch.nn.Module:
		return module


@DdpAccelerator.register("torch")
class TorchDdpAccelerator(DdpAccelerator):

	def __init__(
		self,
		*,
		find_unused_parameters: bool = False,
		local_rank: Optional[int] = None,
		world_size: Optional[int] = None,
		cuda_device: Union[torch.device, int] = -1,
	) -> None:
		super().__init__(local_rank=local_rank, world_size=world_size, cuda_device=cuda_device)
		self._ddp_kwargs = {
			"find_unused_parameters": find_unused_parameters,
		}

	def wrap_model(self, model: "Model") -> Tuple["Model", DdpWrappedModel]:
		if self.cuda_device != torch.device("cpu"):
			model = model.cuda(self.cuda_device)
		wrapped_model = torch.nn.parallel.DistributedDataParallel(
			model,
			device_ids=None if self.cuda_device == torch.device("cpu") else [self.cuda_device],
			**self._ddp_kwargs,
		)
		wrapped_model._register_state_dict_hook(TorchDdpAccelerator._remove_torch_ddp_prefix)
		wrapped_model._register_load_state_dict_pre_hook(TorchDdpAccelerator._add_torch_ddp_prefix)
		return model, DdpWrappedModel(
			wrapped_model,
			local_rank=self.local_rank,
			world_size=self.world_size,
		)

	@staticmethod
	def _add_torch_ddp_prefix(state_dict: StateDictType, prefix: str, *args: Any) -> None:
		for key in list(state_dict.keys()):
			if key.startswith(prefix + "module."):
				continue
			new_key = prefix + "module." + key
			state_dict[new_key] = state_dict[key]
			del state_dict[key]

	@staticmethod
	def _remove_torch_ddp_prefix(
		module: torch.nn.Module, state_dict: StateDictType, prefix: str, *args: Any
	) -> None:
		for key in list(state_dict.keys()):
			if not key.startswith(prefix + "module."):
				continue
			new_key = key.replace(prefix + "module.", "", 1)
			state_dict[new_key] = state_dict[key]
			del state_dict[key]


import argparse
import json
import math
import os
import random
from pathlib import Path

import datasets
import evaluate
import numpy as np
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo, hf_hub_download
from PIL import Image
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.transforms import functional
from tqdm.auto import tqdm

import transformers
from transformers import (
	AutoConfig,
	AutoImageProcessor,
	AutoModelForSemanticSegmentation,
	SchedulerType,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)

require_version("datasets>=2.0.0", "To fix: pip install -r examples/pytorch/semantic-segmentation/requirements.txt")


def pad_if_smaller(img, size, fill=0):
	min_size = min(img.size)
	if min_size < size:
		original_width, original_height = img.size
		pad_height = size - original_height if original_height < size else 0
		pad_width = size - original_width if original_width < size else 0
		img = functional.pad(img, (0, 0, pad_width, pad_height), fill=fill)
	return img


class Compose:
	def __init__(self, transforms):
		self.transforms = transforms

	def __call__(self, image, target):
		for t in self.transforms:
			image, target = t(image, target)
		return image, target


class Identity:
	def __init__(self):
		pass

	def __call__(self, image, target):
		return image, target


class Resize:
	def __init__(self, size):
		self.size = size

	def __call__(self, image, target):
		image = functional.resize(image, self.size)
		target = functional.resize(target, self.size, interpolation=transforms.InterpolationMode.NEAREST)
		return image, target


class RandomResize:
	def __init__(self, min_size, max_size=None):
		self.min_size = min_size
		if max_size is None:
			max_size = min_size
		self.max_size = max_size

	def __call__(self, image, target):
		size = random.randint(self.min_size, self.max_size)
		image = functional.resize(image, size)
		target = functional.resize(target, size, interpolation=transforms.InterpolationMode.NEAREST)
		return image, target


class RandomCrop:
	def __init__(self, size):
		self.size = size

	def __call__(self, image, target):
		image = pad_if_smaller(image, self.size)
		target = pad_if_smaller(target, self.size, fill=255)
		crop_params = transforms.RandomCrop.get_params(image, (self.size, self.size))
		image = functional.crop(image, *crop_params)
		target = functional.crop(target, *crop_params)
		return image, target


class RandomHorizontalFlip:
	def __init__(self, flip_prob):
		self.flip_prob = flip_prob

	def __call__(self, image, target):
		if random.random() < self.flip_prob:
			image = functional.hflip(image)
			target = functional.hflip(target)
		return image, target


class PILToTensor:
	def __call__(self, image, target):
		image = functional.pil_to_tensor(image)
		target = torch.as_tensor(np.array(target), dtype=torch.int64)
		return image, target


class ConvertImageDtype:
	def __init__(self, dtype):
		self.dtype = dtype

	def __call__(self, image, target):
		image = functional.convert_image_dtype(image, self.dtype)
		return image, target


class Normalize:
	def __init__(self, mean, std):
		self.mean = mean
		self.std = std

	def __call__(self, image, target):
		image = functional.normalize(image, mean=self.mean, std=self.std)
		return image, target


class ReduceLabels:
	def __call__(self, image, target):
		if not isinstance(target, np.ndarray):
			target = np.array(target).astype(np.uint8)
		target[target == 0] = 255
		target = target - 1
		target[target == 254] = 255

		target = Image.fromarray(target)
		return image, target


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a text classification task")
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to a pretrained model or model identifier from huggingface.co/models.",
		default="nvidia/mit-b0",
	)
	parser.add_argument(
		"--dataset_name",
		type=str,
		help="Name of the dataset on the hub.",
		default="segments/sidewalk-semantic",
	)
	parser.add_argument(
		"--reduce_labels",
		action="store_true",
		help="Whether or not to reduce all labels by 1 and replace background by 255.",
	)
	parser.add_argument(
		"--train_val_split",
		type=float,
		default=0.15,
		help="Fraction of the dataset to be used for validation.",
	)
	parser.add_argument(
		"--cache_dir",
		type=str,
		help="Path to a folder in which the model and dataset will be cached.",
	)
	parser.add_argument(
		"--use_auth_token",
		action="store_true",
		help="Whether to use an authentication token to access the model repository.",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument(
		"--adam_beta1",
		type=float,
		default=0.9,
		help="Beta1 for AdamW optimizer",
	)
	parser.add_argument(
		"--adam_beta2",
		type=float,
		default=0.999,
		help="Beta2 for AdamW optimizer",
	)
	parser.add_argument(
		"--adam_epsilon",
		type=float,
		default=1e-8,
		help="Epsilon for AdamW optimizer",
	)
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="polynomial",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		required=False,
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	args = parser.parse_args()

	if args.push_to_hub or args.with_tracking:
		if args.output_dir is None:
			raise ValueError(
				"Need an `output_dir` to create a repo when `--push_to_hub` or `with_tracking` is specified."
			)

	if args.output_dir is not None:
		os.makedirs(args.output_dir, exist_ok=True)

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_semantic_segmentation_no_trainer", args)

	accelerator_log_kwargs = {}

	if args.with_tracking:
		accelerator_log_kwargs["log_with"] = args.report_to
		accelerator_log_kwargs["project_dir"] = args.output_dir

	accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)

	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed, device_specific=True)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	dataset = load_dataset(args.dataset_name, cache_dir=args.cache_dir)

	if "pixel_values" in dataset["train"].column_names:
		dataset = dataset.rename_columns({"pixel_values": "image"})
	if "annotation" in dataset["train"].column_names:
		dataset = dataset.rename_columns({"annotation": "label"})

	args.train_val_split = None if "validation" in dataset.keys() else args.train_val_split
	if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:
		split = dataset["train"].train_test_split(args.train_val_split)
		dataset["train"] = split["train"]
		dataset["validation"] = split["test"]

	if args.dataset_name == "scene_parse_150":
		repo_id = "huggingface/label-files"
		filename = "ade20k-id2label.json"
	else:
		repo_id = args.dataset_name
		filename = "id2label.json"
	id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
	id2label = {int(k): v for k, v in id2label.items()}
	label2id = {v: k for k, v in id2label.items()}

	config = AutoConfig.from_pretrained(
		args.model_name_or_path, id2label=id2label, label2id=label2id, trust_remote_code=args.trust_remote_code
	)
	image_processor = AutoImageProcessor.from_pretrained(
		args.model_name_or_path, trust_remote_code=args.trust_remote_code
	)
	model = AutoModelForSemanticSegmentation.from_pretrained(
		args.model_name_or_path, config=config, trust_remote_code=args.trust_remote_code
	)

	if "shortest_edge" in image_processor.size:
		size = (image_processor.size["shortest_edge"], image_processor.size["shortest_edge"])
	else:
		size = (image_processor.size["height"], image_processor.size["width"])
	train_transforms = Compose(
		[
			ReduceLabels() if args.reduce_labels else Identity(),
			RandomCrop(size=size),
			RandomHorizontalFlip(flip_prob=0.5),
			PILToTensor(),
			ConvertImageDtype(torch.float),
			Normalize(mean=image_processor.image_mean, std=image_processor.image_std),
		]
	)
	val_transforms = Compose(
		[
			ReduceLabels() if args.reduce_labels else Identity(),
			Resize(size=size),
			PILToTensor(),
			ConvertImageDtype(torch.float),
			Normalize(mean=image_processor.image_mean, std=image_processor.image_std),
		]
	)

	def preprocess_train(example_batch):
		pixel_values = []
		labels = []
		for image, target in zip(example_batch["image"], example_batch["label"]):
			image, target = train_transforms(image.convert("RGB"), target)
			pixel_values.append(image)
			labels.append(target)

		encoding = {}
		encoding["pixel_values"] = torch.stack(pixel_values)
		encoding["labels"] = torch.stack(labels)

		return encoding

	def preprocess_val(example_batch):
		pixel_values = []
		labels = []
		for image, target in zip(example_batch["image"], example_batch["label"]):
			image, target = val_transforms(image.convert("RGB"), target)
			pixel_values.append(image)
			labels.append(target)

		encoding = {}
		encoding["pixel_values"] = torch.stack(pixel_values)
		encoding["labels"] = torch.stack(labels)

		return encoding

	with accelerator.main_process_first():
		train_dataset = dataset["train"].with_transform(preprocess_train)
		eval_dataset = dataset["validation"].with_transform(preprocess_val)

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(
		eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size
	)

	optimizer = torch.optim.AdamW(
		list(model.parameters()),
		lr=args.learning_rate,
		betas=[args.adam_beta1, args.adam_beta2],
		eps=args.adam_epsilon,
	)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,
		num_training_steps=args.max_train_steps
		if overrode_max_train_steps
		else args.max_train_steps * accelerator.num_processes,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	metric = evaluate.load("mean_iou", cache_dir=args.cache_dir)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("semantic_segmentation_no_trainer", experiment_config)

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			with accelerator.accumulate(model):
				outputs = model(**batch)
				loss = outputs.loss
				if args.with_tracking:
					total_loss += loss.detach().float()
				accelerator.backward(loss)
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()

			if accelerator.sync_gradients:
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

					if args.push_to_hub and epoch < args.num_train_epochs - 1:
						accelerator.wait_for_everyone()
						unwrapped_model = accelerator.unwrap_model(model)
						unwrapped_model.save_pretrained(
							args.output_dir,
							is_main_process=accelerator.is_main_process,
							save_function=accelerator.save,
						)
						if accelerator.is_main_process:
							image_processor.save_pretrained(args.output_dir)
							repo.push_to_hub(
								commit_message=f"Training in progress {completed_steps} steps",
								blocking=False,
								auto_lfs_prune=True,
							)

			if completed_steps >= args.max_train_steps:
				break

		logger.info("***** Running evaluation *****")
		model.eval()
		for step, batch in enumerate(tqdm(eval_dataloader, disable=not accelerator.is_local_main_process)):
			with torch.no_grad():
				outputs = model(**batch)

			upsampled_logits = torch.nn.functional.interpolate(
				outputs.logits, size=batch["labels"].shape[-2:], mode="bilinear", align_corners=False
			)
			predictions = upsampled_logits.argmax(dim=1)

			predictions, references = accelerator.gather_for_metrics((predictions, batch["labels"]))

			metric.add_batch(
				predictions=predictions,
				references=references,
			)

		eval_metrics = metric.compute(
			num_labels=len(id2label),
			ignore_index=255,
			reduce_labels=False,  # we've already reduced the labels before
		)
		logger.info(f"epoch {epoch}: {eval_metrics}")

		if args.with_tracking:
			accelerator.log(
				{
					"mean_iou": eval_metrics["mean_iou"],
					"mean_accuracy": eval_metrics["mean_accuracy"],
					"overall_accuracy": eval_metrics["overall_accuracy"],
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				image_processor.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			image_processor.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

			all_results = {
				f"eval_{k}": v.tolist() if isinstance(v, np.ndarray) else v for k, v in eval_metrics.items()
			}
			with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
				json.dump(all_results, f)


if __name__ == "__main__":
	main()

import os

import torch
import torch.distributed.autograd as dist_autograd
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.optim as optim
from torch.distributed.optim import DistributedOptimizer

import rnn


def _run_trainer():
	batch = 5
	ntoken = 7
	ninp = 2

	nhid = 3
	nindices = 6
	nlayers = 4
	hidden = (
		torch.randn(nlayers, nindices, nhid),
		torch.randn(nlayers, nindices, nhid)
	)

	model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)

	opt = DistributedOptimizer(
		optim.SGD,
		model.parameter_rrefs(),
		lr=0.05,
	)

	criterion = torch.nn.CrossEntropyLoss()

	def get_next_batch():
		for _ in range(5):
			data = torch.LongTensor(batch, nindices) % ntoken
			target = torch.LongTensor(batch, ntoken) % nindices
			yield data, target

	for epoch in range(10):
		for data, target in get_next_batch():
			with dist_autograd.context() as context_id:
				hidden[0].detach_()
				hidden[1].detach_()
				output, hidden = model(data, hidden)
				loss = criterion(output, target)
				dist_autograd.backward(context_id, [loss])
				opt.step(context_id)
		print("Training epoch {}".format(epoch))


def run_worker(rank, world_size):
	os.environ['MASTER_ADDR'] = 'localhost'
	os.environ['MASTER_PORT'] = '29500'
	if rank == 1:
		rpc.init_rpc("trainer", rank=rank, world_size=world_size)
		_run_trainer()
	else:
		rpc.init_rpc("ps", rank=rank, world_size=world_size)
		pass

	rpc.shutdown()


if __name__ == "__main__":
	world_size = 2
	mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)


from allennlp.modules.transformer.layer_norm import LayerNorm
from allennlp.modules.transformer.positional_encoding import SinusoidalPositionalEncoding
from allennlp.modules.transformer.transformer_module import TransformerModule
from allennlp.modules.transformer.transformer_embeddings import (
	Embeddings,
	TransformerEmbeddings,
	ImageFeatureEmbeddings,
)
from allennlp.modules.transformer.attention_module import SelfAttention, T5Attention
from allennlp.modules.transformer.activation_layer import ActivationLayer
from allennlp.modules.transformer.transformer_layer import AttentionLayer, TransformerLayer
from allennlp.modules.transformer.transformer_stack import TransformerStack
from allennlp.modules.transformer.transformer_pooler import TransformerPooler
from allennlp.modules.transformer.output_layer import OutputLayer

from allennlp.modules.transformer.bimodal_attention import BiModalAttention
from allennlp.modules.transformer.bimodal_encoder import BiModalEncoder
from allennlp.modules.transformer.t5 import T5

from typing import Optional
from checklist.test_suite import TestSuite
from checklist.test_types import MFT as MinimumFunctionalityTest
from allennlp.confidence_checks.task_checklists.task_suite import TaskSuite


@TaskSuite.register("fake-task-suite")
class FakeTaskSuite(TaskSuite):

	def __init__(
		self,
		suite: Optional[TestSuite] = None,
		fake_arg1: Optional[int] = None,
		fake_arg2: Optional[int] = None,
	):
		self._fake_arg1 = fake_arg1
		self._fake_arg2 = fake_arg2

		if not suite:
			suite = TestSuite()

		test = MinimumFunctionalityTest(
			["sentence 1", "sentence 2"],
			labels=0,
			name="fake test 1",
			capability="fake capability",
			description="Test's description",
		)
		suite.add(test)

		super().__init__(suite)

import bz2
import gzip
import lzma
import weakref
from contextlib import contextmanager
import glob
import io
import os
import logging
import json
from abc import ABC
from collections import defaultdict
from datetime import timedelta
from fnmatch import fnmatch
from os import PathLike
from pathlib import Path
from typing import (
	Optional,
	Tuple,
	Union,
	Callable,
	Set,
	List,
	Iterator,
	Iterable,
	Dict,
	NamedTuple,
	MutableMapping,
)
from weakref import WeakValueDictionary
import shutil
import pickle
import time
import warnings

import cached_path as _cached_path
from cached_path import (  # noqa: F401
	resource_to_filename as _resource_to_filename,
	check_tarfile,
	is_url_or_existing_file,
	find_latest_cached as _find_latest_cached,
)
from cached_path.cache_file import CacheFile
from cached_path.common import PathOrStr
from cached_path.file_lock import FileLock
from cached_path.meta import Meta as _Meta
import torch
import numpy as np
import lmdb
from torch import Tensor


logger = logging.getLogger(__name__)

CACHE_ROOT = Path(os.getenv("ALLENNLP_CACHE_ROOT", Path.home() / ".allennlp"))
CACHE_DIRECTORY = str(CACHE_ROOT / "cache")
DEPRECATED_CACHE_DIRECTORY = str(CACHE_ROOT / "datasets")

DATASET_CACHE = CACHE_DIRECTORY

if os.path.exists(DEPRECATED_CACHE_DIRECTORY):
	logger.warning(
		f"Deprecated cache directory found ({DEPRECATED_CACHE_DIRECTORY}).  "
		f"Please remove this directory from your system to free up space."
	)


def filename_to_url(filename: str, cache_dir: Union[str, Path] = None) -> Tuple[str, str]:
	return _cached_path.filename_to_url(filename, cache_dir=cache_dir or CACHE_DIRECTORY)


def cached_path(
	url_or_filename: Union[str, PathLike],
	cache_dir: Union[str, Path] = None,
	extract_archive: bool = False,
	force_extract: bool = False,
) -> str:
	return str(
		_cached_path.cached_path(
			url_or_filename,
			cache_dir=cache_dir or CACHE_DIRECTORY,
			extract_archive=extract_archive,
			force_extract=force_extract,
		)
	)


def _serialize(data):
	buffer = pickle.dumps(data, protocol=-1)
	return np.frombuffer(buffer, dtype=np.uint8)


_active_tensor_caches: MutableMapping[int, "TensorCache"] = weakref.WeakValueDictionary()


def _unique_file_id(path: Union[str, PathLike]) -> int:
	result = os.stat(path).st_ino
	assert result != 0
	return result


class TensorCache(MutableMapping[str, Tensor], ABC):

	def __new__(cls, filename: Union[str, PathLike], *, read_only: bool = False, **kwargs):
		filename = str(filename)
		try:
			result = _active_tensor_caches.get(_unique_file_id(filename))
		except FileNotFoundError:
			result = None
		if result is None:
			result = super(TensorCache, cls).__new__(cls)
		return result

	def __init__(
		self,
		filename: Union[str, PathLike],
		*,
		map_size: int = 1024 * 1024 * 1024 * 1024,
		read_only: bool = False,
	) -> None:
		self.lmdb_env: lmdb.Environment
		if hasattr(self, "lmdb_env"):
			if read_only:
				return
			if not self.read_only:
				return

			filename = self.lmdb_env.path()
			old_info = self.lmdb_env.info()

			self.lmdb_env.close()
			self.lmdb_env = lmdb.open(
				filename,
				map_size=old_info["map_size"],
				subdir=False,
				metasync=False,
				sync=True,
				readahead=False,
				meminit=False,
				readonly=False,
				lock=True,
			)
		else:
			filename = str(filename)

			cpu_count = os.cpu_count() or 1
			if os.path.exists(filename):
				if os.path.isfile(filename):
					if not os.access(filename, os.W_OK):
						if not read_only:
							warnings.warn(
								f"File '{filename}' is read-only, so cache will be read-only",
								UserWarning,
							)
						read_only = True
				else:
					raise ValueError("Expect a file, found a directory instead")

			use_lock = True
			if read_only:

				lock_filename = filename + "-lock"
				if os.path.isfile(lock_filename):
					use_lock = os.access(lock_filename, os.W_OK)
				else:
					use_lock = os.access(os.path.dirname(lock_filename), os.W_OK)

			if not use_lock:
				warnings.warn(
					f"Lacking permissions to use lock file on cache '{filename}'.\nUse at your own risk!",
					UserWarning,
				)

			self.lmdb_env = lmdb.open(
				filename,
				subdir=False,
				map_size=map_size,
				max_readers=cpu_count * 4,
				max_spare_txns=cpu_count * 4,
				metasync=False,
				sync=True,
				readahead=False,
				meminit=False,
				readonly=read_only,
				lock=use_lock,
			)
			_active_tensor_caches[_unique_file_id(filename)] = self

			self.cache_cache: MutableMapping[str, Tensor] = WeakValueDictionary()

	@property
	def read_only(self) -> bool:
		return self.lmdb_env.flags()["readonly"]

	def __contains__(self, key: object):
		if not isinstance(key, str):
			return False
		if key in self.cache_cache:
			return True
		encoded_key = key.encode()
		with self.lmdb_env.begin(write=False) as txn:
			result = txn.get(encoded_key)
			return result is not None

	def __getitem__(self, key: str):
		try:
			return self.cache_cache[key]
		except KeyError:
			encoded_key = key.encode()
			with self.lmdb_env.begin(write=False) as txn:
				buffer = txn.get(encoded_key)
				if buffer is None:
					raise KeyError()
				tensor = torch.load(io.BytesIO(buffer), map_location="cpu")
			self.cache_cache[key] = tensor
			return tensor

	def __setitem__(self, key: str, tensor: torch.Tensor):
		if self.read_only:
			raise ValueError("cannot write to a read-only cache")

		tensor = tensor.cpu()
		encoded_key = key.encode()
		buffer = io.BytesIO()
		if tensor.storage().size() != np.prod(tensor.size()):
			tensor = tensor.clone()
		assert tensor.storage().size() == np.prod(tensor.size())
		torch.save(tensor.detach(), buffer, pickle_protocol=pickle.HIGHEST_PROTOCOL)
		with self.lmdb_env.begin(write=True) as txn:
			txn.put(encoded_key, buffer.getbuffer())

		self.cache_cache[key] = tensor

	def __delitem__(self, key: str):
		if self.read_only:
			raise ValueError("cannot write to a read-only cache")

		encoded_key = key.encode()
		with self.lmdb_env.begin(write=True) as txn:
			txn.delete(encoded_key)

		try:
			del self.cache_cache[key]
		except KeyError:
			pass

	def __del__(self):
		if self.lmdb_env is not None:
			self.lmdb_env.close()
			self.lmdb_env = None

	def __len__(self):
		return self.lmdb_env.stat()["entries"]

	def __iter__(self):
		raise NotImplementedError()


class LocalCacheResource:

	def __init__(self, resource_name: str, version: str, cache_dir: str = CACHE_DIRECTORY) -> None:
		self.resource_name = resource_name
		self.version = version
		self.cache_dir = cache_dir
		self.path = os.path.join(self.cache_dir, _resource_to_filename(resource_name, version))
		self.file_lock = FileLock(self.path + ".lock")

	def cached(self) -> bool:
		return os.path.exists(self.path)

	@contextmanager
	def writer(self, mode="w"):
		if self.cached():
			raise ValueError(
				f"local cache of {self.resource_name} (version '{self.version}') already exists!"
			)

		with CacheFile(self.path, mode=mode) as f:
			yield f

		meta = _Meta(
			resource=self.resource_name,
			cached_path=self.path,
			creation_time=time.time(),
			etag=self.version,
			size=_get_resource_size(self.path),
		)
		meta.to_file()

	@contextmanager
	def reader(self, mode="r"):
		if not self.cached():
			raise ValueError(
				f"local cache of {self.resource_name} (version '{self.version}') does not exist yet!"
			)

		with open(self.path, mode) as f:
			yield f

	def __enter__(self):
		self.file_lock.acquire()
		return self

	def __exit__(self, exc_type, exc_value, traceback):
		self.file_lock.release()
		if exc_value is None:
			return True
		return False


def read_set_from_file(filename: str) -> Set[str]:
	collection = set()
	with open(filename, "r") as file_:
		for line in file_:
			collection.add(line.rstrip())
	return collection


def get_file_extension(path: str, dot=True, lower: bool = True):
	ext = os.path.splitext(path)[1]
	ext = ext if dot else ext[1:]
	return ext.lower() if lower else ext


_SUFFIXES: Dict[Callable, str] = {
	open: "",
	gzip.open: ".gz",
	bz2.open: ".bz2",
	lzma.open: ".xz",
}


def open_compressed(
	filename: Union[str, PathLike],
	mode: str = "rt",
	encoding: Optional[str] = "UTF-8",
	**kwargs,
):
	if not isinstance(filename, str):
		filename = str(filename)

	open_fn: Callable
	filename = str(filename)
	for open_fn, suffix in _SUFFIXES.items():
		if len(suffix) > 0 and filename.endswith(suffix):
			break
	else:
		open_fn = open

	return open_fn(cached_path(filename), mode=mode, encoding=encoding, **kwargs)


def text_lines_from_file(filename: Union[str, PathLike], strip_lines: bool = True) -> Iterator[str]:
	with open_compressed(filename, "rt", encoding="UTF-8", errors="replace") as p:
		if strip_lines:
			for line in p:
				yield line.strip()
		else:
			yield from p


def json_lines_from_file(filename: Union[str, PathLike]) -> Iterable[Union[list, dict]]:
	return (json.loads(line) for line in text_lines_from_file(filename))


def _get_resource_size(path: str) -> int:
	if os.path.isfile(path):
		return os.path.getsize(path)
	inodes: Set[int] = set()
	total_size = 0
	for dirpath, dirnames, filenames in os.walk(path):
		for f in filenames:
			fp = os.path.join(dirpath, f)
			inode = os.stat(fp).st_ino
			if not os.path.islink(fp) and inode not in inodes:
				inodes.add(inode)
				total_size += os.path.getsize(fp)
	return total_size


class _CacheEntry(NamedTuple):
	regular_files: List[_Meta]
	extraction_dirs: List[_Meta]


def _find_entries(
	patterns: List[str] = None,
	cache_dir: Union[str, Path] = None,
) -> Tuple[int, Dict[str, _CacheEntry]]:
	cache_dir = os.path.expanduser(cache_dir or CACHE_DIRECTORY)

	total_size: int = 0
	cache_entries: Dict[str, _CacheEntry] = defaultdict(lambda: _CacheEntry([], []))
	for meta_path in glob.glob(str(cache_dir) + "/*.json"):
		meta = _Meta.from_path(meta_path)
		if patterns and not any(fnmatch(meta.resource, p) for p in patterns):
			continue
		if meta.extraction_dir:
			cache_entries[meta.resource].extraction_dirs.append(meta)
		else:
			cache_entries[meta.resource].regular_files.append(meta)
		total_size += meta.size

	for entry in cache_entries.values():
		entry.regular_files.sort(key=lambda meta: meta.creation_time, reverse=True)
		entry.extraction_dirs.sort(key=lambda meta: meta.creation_time, reverse=True)

	return total_size, cache_entries


def remove_cache_entries(patterns: List[str], cache_dir: Union[str, Path] = None) -> int:
	total_size, cache_entries = _find_entries(patterns=patterns, cache_dir=cache_dir)
	for resource, entry in cache_entries.items():
		for meta in entry.regular_files:
			logger.info("Removing cached version of %s at %s", resource, meta.cached_path)
			os.remove(meta.cached_path)
			if os.path.exists(meta.cached_path + ".lock"):
				os.remove(meta.cached_path + ".lock")
			os.remove(meta.cached_path + ".json")
		for meta in entry.extraction_dirs:
			logger.info("Removing extracted version of %s at %s", resource, meta.cached_path)
			shutil.rmtree(meta.cached_path)
			if os.path.exists(meta.cached_path + ".lock"):
				os.remove(meta.cached_path + ".lock")
			os.remove(meta.cached_path + ".json")
	return total_size


def inspect_cache(patterns: List[str] = None, cache_dir: Union[str, Path] = None):
	from allennlp.common.util import format_timedelta, format_size

	cache_dir = os.path.expanduser(cache_dir or CACHE_DIRECTORY)

	total_size, cache_entries = _find_entries(patterns=patterns, cache_dir=cache_dir)

	if patterns:
		print(f"Cached resources matching {patterns}:")
	else:
		print("Cached resources:")

	for resource, entry in sorted(
		cache_entries.items(),
		key=lambda x: max(
			0 if not x[1][0] else x[1][0][0].creation_time,
			0 if not x[1][1] else x[1][1][0].creation_time,
		),
		reverse=True,
	):
		print("\n-", resource)
		if entry.regular_files:
			td = timedelta(seconds=time.time() - entry.regular_files[0].creation_time)
			n_versions = len(entry.regular_files)
			size = entry.regular_files[0].size
			print(
				f"  {n_versions} {'versions' if n_versions > 1 else 'version'} cached, "
				f"latest {format_size(size)} from {format_timedelta(td)} ago"
			)
		if entry.extraction_dirs:
			td = timedelta(seconds=time.time() - entry.extraction_dirs[0].creation_time)
			n_versions = len(entry.extraction_dirs)
			size = entry.extraction_dirs[0].size
			print(
				f"  {n_versions} {'versions' if n_versions > 1 else 'version'} extracted, "
				f"latest {format_size(size)} from {format_timedelta(td)} ago"
			)
	print(f"\nTotal size: {format_size(total_size)}")


def hardlink_or_copy(source: PathOrStr, dest: PathOrStr):
	try:
		os.link(source, dest)
	except OSError as e:
		if e.errno in {18, 95}:  # Cross-device link and Windows
			shutil.copy(source, dest)
		else:
			raise


from __future__ import print_function

import argparse

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torchvision import datasets, transforms


class Net(nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.rnn = nn.LSTM(input_size=28, hidden_size=64, batch_first=True)
		self.batchnorm = nn.BatchNorm1d(64)
		self.dropout1 = nn.Dropout2d(0.25)
		self.dropout2 = nn.Dropout2d(0.5)
		self.fc1 = nn.Linear(64, 32)
		self.fc2 = nn.Linear(32, 10)

	def forward(self, input):
		input = input.reshape(-1, 28, 28)
		output, hidden = self.rnn(input)

		output = output[:, -1, :]
		output = self.batchnorm(output)
		output = self.dropout1(output)
		output = self.fc1(output)
		output = F.relu(output)
		output = self.dropout2(output)
		output = self.fc2(output)
		output = F.log_softmax(output, dim=1)
		return output


def train(args, model, device, train_loader, optimizer, epoch):
	model.train()
	for batch_idx, (data, target) in enumerate(train_loader):
		data, target = data.to(device), target.to(device)
		optimizer.zero_grad()
		output = model(data)
		loss = F.nll_loss(output, target)
		loss.backward()
		optimizer.step()
		if batch_idx % args.log_interval == 0:
			print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
				epoch, batch_idx * len(data), len(train_loader.dataset),
					   100. * batch_idx / len(train_loader), loss.item()))
			if args.dry_run:
				break


def test(args, model, device, test_loader):
	model.eval()
	test_loss = 0
	correct = 0
	with torch.no_grad():
		for data, target in test_loader:
			data, target = data.to(device), target.to(device)
			output = model(data)
			test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
			pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
			correct += pred.eq(target.view_as(pred)).sum().item()
			if args.dry_run:
				break

	test_loss /= len(test_loader.dataset)

	print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
		test_loss, correct, len(test_loader.dataset),
		100. * correct / len(test_loader.dataset)))


def main():
	parser = argparse.ArgumentParser(description='PyTorch MNIST Example using RNN')
	parser.add_argument('--batch-size', type=int, default=64, metavar='N',
						help='input batch size for training (default: 64)')
	parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
						help='input batch size for testing (default: 1000)')
	parser.add_argument('--epochs', type=int, default=14, metavar='N',
						help='number of epochs to train (default: 14)')
	parser.add_argument('--lr', type=float, default=0.1, metavar='LR',
						help='learning rate (default: 0.1)')
	parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
						help='learning rate step gamma (default: 0.7)')
	parser.add_argument('--cuda', action='store_true', default=False,
						help='enables CUDA training')
	parser.add_argument('--mps', action="store_true", default=False,
						help="enables MPS training")
	parser.add_argument('--dry-run', action='store_true', default=False,
						help='quickly check a single pass')
	parser.add_argument('--seed', type=int, default=1, metavar='S',
						help='random seed (default: 1)')
	parser.add_argument('--log-interval', type=int, default=10, metavar='N',
						help='how many batches to wait before logging training status')
	parser.add_argument('--save-model', action='store_true', default=False,
						help='for Saving the current Model')
	args = parser.parse_args()

	if args.cuda and not args.mps:
		device = "cuda"
	elif args.mps and not args.cuda:
		device = "mps"
	else:
		device = "cpu"

	device = torch.device(device)

	torch.manual_seed(args.seed)

	kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}
	train_loader = torch.utils.data.DataLoader(
		datasets.MNIST('../data', train=True, download=True,
					   transform=transforms.Compose([
						   transforms.ToTensor(),
						   transforms.Normalize((0.1307,), (0.3081,))
					   ])),
		batch_size=args.batch_size, shuffle=True, **kwargs)
	test_loader = torch.utils.data.DataLoader(
		datasets.MNIST('../data', train=False, transform=transforms.Compose([
			transforms.ToTensor(),
			transforms.Normalize((0.1307,), (0.3081,))
		])),
		batch_size=args.test_batch_size, shuffle=True, **kwargs)

	model = Net().to(device)
	optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

	scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
	for epoch in range(1, args.epochs + 1):
		train(args, model, device, train_loader, optimizer, epoch)
		test(args, model, device, test_loader)
		scheduler.step()

	if args.save_model:
		torch.save(model.state_dict(), "mnist_rnn.pt")


if __name__ == '__main__':
	main()

import argparse
import json
import logging
import math
import os
import random
from pathlib import Path

import datasets
import evaluate
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	AutoConfig,
	AutoModelForSequenceClassification,
	AutoTokenizer,
	DataCollatorWithPadding,
	PretrainedConfig,
	SchedulerType,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/text-classification/requirements.txt")

task_to_keys = {
	"cola": ("sentence", None),
	"mnli": ("premise", "hypothesis"),
	"mrpc": ("sentence1", "sentence2"),
	"qnli": ("question", "sentence"),
	"qqp": ("question1", "question2"),
	"rte": ("sentence1", "sentence2"),
	"sst2": ("sentence", None),
	"stsb": ("sentence1", "sentence2"),
	"wnli": ("sentence1", "sentence2"),
}


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a text classification task")
	parser.add_argument(
		"--task_name",
		type=str,
		default=None,
		help="The name of the glue task to train on.",
		choices=list(task_to_keys.keys()),
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--max_length",
		type=int,
		default=128,
		help=(
			"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
			" sequences shorter will be padded if `--pad_to_max_length` is passed."
		),
	)
	parser.add_argument(
		"--pad_to_max_length",
		action="store_true",
		help="If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.",
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=True,
	)
	parser.add_argument(
		"--use_slow_tokenizer",
		action="store_true",
		help="If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	parser.add_argument(
		"--ignore_mismatched_sizes",
		action="store_true",
		help="Whether or not to enable to load a pretrained model whose head dimensions are different.",
	)
	args = parser.parse_args()

	if args.task_name is None and args.train_file is None and args.validation_file is None:
		raise ValueError("Need either a task name or a training/validation file.")
	else:
		if args.train_file is not None:
			extension = args.train_file.split(".")[-1]
			assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
		if args.validation_file is not None:
			extension = args.validation_file.split(".")[-1]
			assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


def main():
	args = parse_args()
	send_example_telemetry("run_glue_no_trainer", args)

	accelerator = (
		Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()
	)
	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()




	if args.task_name is not None:
		raw_datasets = load_dataset("glue", args.task_name)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		extension = (args.train_file if args.train_file is not None else args.validation_file).split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files)

	if args.task_name is not None:
		is_regression = args.task_name == "stsb"
		if not is_regression:
			label_list = raw_datasets["train"].features["label"].names
			num_labels = len(label_list)
		else:
			num_labels = 1
	else:
		is_regression = raw_datasets["train"].features["label"].dtype in ["float32", "float64"]
		if is_regression:
			num_labels = 1
		else:
			label_list = raw_datasets["train"].unique("label")
			label_list.sort()  # Let's sort it for determinism
			num_labels = len(label_list)

	config = AutoConfig.from_pretrained(
		args.model_name_or_path,
		num_labels=num_labels,
		finetuning_task=args.task_name,
		trust_remote_code=args.trust_remote_code,
	)
	tokenizer = AutoTokenizer.from_pretrained(
		args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
	)
	model = AutoModelForSequenceClassification.from_pretrained(
		args.model_name_or_path,
		from_tf=bool(".ckpt" in args.model_name_or_path),
		config=config,
		ignore_mismatched_sizes=args.ignore_mismatched_sizes,
		trust_remote_code=args.trust_remote_code,
	)

	if args.task_name is not None:
		sentence1_key, sentence2_key = task_to_keys[args.task_name]
	else:
		non_label_column_names = [name for name in raw_datasets["train"].column_names if name != "label"]
		if "sentence1" in non_label_column_names and "sentence2" in non_label_column_names:
			sentence1_key, sentence2_key = "sentence1", "sentence2"
		else:
			if len(non_label_column_names) >= 2:
				sentence1_key, sentence2_key = non_label_column_names[:2]
			else:
				sentence1_key, sentence2_key = non_label_column_names[0], None

	label_to_id = None
	if (
		model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
		and args.task_name is not None
		and not is_regression
	):
		label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
		if sorted(label_name_to_id.keys()) == sorted(label_list):
			logger.info(
				f"The configuration of the model provided the following label correspondence: {label_name_to_id}. "
				"Using it!"
			)
			label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}
		else:
			logger.warning(
				"Your model seems to have been trained with labels, but they don't match the dataset: ",
				f"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}."
				"\nIgnoring the model labels as a result.",
			)
	elif args.task_name is None and not is_regression:
		label_to_id = {v: i for i, v in enumerate(label_list)}

	if label_to_id is not None:
		model.config.label2id = label_to_id
		model.config.id2label = {id: label for label, id in config.label2id.items()}
	elif args.task_name is not None and not is_regression:
		model.config.label2id = {l: i for i, l in enumerate(label_list)}
		model.config.id2label = {id: label for label, id in config.label2id.items()}

	padding = "max_length" if args.pad_to_max_length else False

	def preprocess_function(examples):
		texts = (
			(examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])
		)
		result = tokenizer(*texts, padding=padding, max_length=args.max_length, truncation=True)

		if "label" in examples:
			if label_to_id is not None:
				result["labels"] = [label_to_id[l] for l in examples["label"]]
			else:
				result["labels"] = examples["label"]
		return result

	with accelerator.main_process_first():
		processed_datasets = raw_datasets.map(
			preprocess_function,
			batched=True,
			remove_columns=raw_datasets["train"].column_names,
			desc="Running tokenizer on dataset",
		)

	train_dataset = processed_datasets["train"]
	eval_dataset = processed_datasets["validation_matched" if args.task_name == "mnli" else "validation"]

	for index in random.sample(range(len(train_dataset)), 3):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	if args.pad_to_max_length:
		data_collator = default_data_collator
	else:
		data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)

	no_decay = ["bias", "LayerNorm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps,
		num_training_steps=args.max_train_steps,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		experiment_config = vars(args)
		experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
		accelerator.init_trackers("glue_no_trainer", experiment_config)

	if args.task_name is not None:
		metric = evaluate.load("glue", args.task_name)
	else:
		metric = evaluate.load("accuracy")

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0
	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			outputs = model(**batch)
			loss = outputs.loss
			if args.with_tracking:
				total_loss += loss.detach().float()
			loss = loss / args.gradient_accumulation_steps
			accelerator.backward(loss)
			if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()
		samples_seen = 0
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				outputs = model(**batch)
			predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()
			predictions, references = accelerator.gather((predictions, batch["labels"]))
			if accelerator.num_processes > 1:
				if step == len(eval_dataloader) - 1:
					predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]
					references = references[: len(eval_dataloader.dataset) - samples_seen]
				else:
					samples_seen += references.shape[0]
			metric.add_batch(
				predictions=predictions,
				references=references,
			)

		eval_metric = metric.compute()
		logger.info(f"epoch {epoch}: {eval_metric}")

		if args.with_tracking:
			accelerator.log(
				{
					"accuracy" if args.task_name is not None else "glue": eval_metric,
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)

	if args.task_name == "mnli":
		eval_dataset = processed_datasets["validation_mismatched"]
		eval_dataloader = DataLoader(
			eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
		)
		eval_dataloader = accelerator.prepare(eval_dataloader)

		model.eval()
		for step, batch in enumerate(eval_dataloader):
			outputs = model(**batch)
			predictions = outputs.logits.argmax(dim=-1)
			metric.add_batch(
				predictions=accelerator.gather(predictions),
				references=accelerator.gather(batch["labels"]),
			)

		eval_metric = metric.compute()
		logger.info(f"mnli-mm: {eval_metric}")

	if args.output_dir is not None:
		all_results = {f"eval_{k}": v for k, v in eval_metric.items()}
		with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
			json.dump(all_results, f)


if __name__ == "__main__":
	main()

from typing import Dict, Optional, Any


import torch

from allennlp.data.fields.text_field import TextFieldTensors
from allennlp.data.vocabulary import Vocabulary
from allennlp.modules.backbones.backbone import Backbone
from allennlp.modules.token_embedders.pretrained_transformer_embedder import (
	PretrainedTransformerEmbedder,
)
from allennlp.nn import util


@Backbone.register("pretrained_transformer")
class PretrainedTransformerBackbone(Backbone):

	def __init__(
		self,
		vocab: Vocabulary,
		model_name: str,
		*,
		max_length: int = None,
		sub_module: str = None,
		train_parameters: bool = True,
		last_layer_only: bool = True,
		override_weights_file: Optional[str] = None,
		override_weights_strip_prefix: Optional[str] = None,
		tokenizer_kwargs: Optional[Dict[str, Any]] = None,
		transformer_kwargs: Optional[Dict[str, Any]] = None,
		output_token_strings: bool = True,
		vocab_namespace: str = "tags",
	) -> None:
		super().__init__()
		self._vocab = vocab
		self._namespace = vocab_namespace
		self._embedder = PretrainedTransformerEmbedder(
			model_name=model_name,
			max_length=max_length,
			sub_module=sub_module,
			train_parameters=train_parameters,
			last_layer_only=last_layer_only,
			override_weights_file=override_weights_file,
			override_weights_strip_prefix=override_weights_strip_prefix,
			tokenizer_kwargs=tokenizer_kwargs,
			transformer_kwargs=transformer_kwargs,
		)
		self._output_token_strings = output_token_strings

	def forward(self, text: TextFieldTensors) -> Dict[str, torch.Tensor]:  # type: ignore
		if len(text) != 1:
			raise ValueError(
				"PretrainedTransformerBackbone is only compatible with using a single TokenIndexer"
			)
		text_inputs = next(iter(text.values()))
		mask = util.get_text_field_mask(text)
		encoded_text = self._embedder(**text_inputs)
		outputs = {"encoded_text": encoded_text, "encoded_text_mask": mask}
		if self._output_token_strings:
			outputs["token_ids"] = util.get_token_ids_from_text_field_tensors(text)
		return outputs

	def make_output_human_readable(
		self, output_dict: Dict[str, torch.Tensor]
	) -> Dict[str, torch.Tensor]:
		if not self._output_token_strings:
			return output_dict

		tokens = []
		for instance_tokens in output_dict["token_ids"]:
			tokens.append(
				[
					self._vocab.get_token_from_index(token_id.item(), namespace=self._namespace)
					for token_id in instance_tokens
				]
			)
		output_dict["tokens"] = tokens
		return output_dict

from typing import Dict, Any, Optional, Union, Tuple, List
import torch
from torch.testing import assert_allclose
import pytest

from allennlp.common.testing.test_case import AllenNlpTestCase
from allennlp.common.testing.model_test_case import ModelTestCase
from allennlp.common.testing.distributed_test import run_distributed_test

from allennlp.modules.transformer import TransformerModule

from allennlp.training.metrics import Metric


_available_devices = ["cuda:0"] if torch.cuda.is_available() else ["cpu"]


def multi_device(test_method):
	return pytest.mark.parametrize("device", _available_devices)(pytest.mark.gpu(test_method))


def requires_gpu(test_method):
	return pytest.mark.gpu(
		pytest.mark.skipif(not torch.cuda.is_available(), reason="No CUDA device registered.")(
			test_method
		)
	)


def requires_multi_gpu(test_method):
	return pytest.mark.gpu(
		pytest.mark.skipif(torch.cuda.device_count() < 2, reason="2 or more GPUs required.")(
			test_method
		)
	)


def cpu_or_gpu(test_method):
	return pytest.mark.gpu(test_method)




def assert_metrics_values(
	metrics: Dict[str, Any],
	desired_values: Dict[str, Any],
	rtol: float = 0.0001,
	atol: float = 1e-05,
):
	for key in metrics:
		if isinstance(metrics[key], Dict) and isinstance(desired_values[key], Dict):
			for subkey in metrics[key]:
				assert_allclose(
					metrics[key][subkey], desired_values[key][subkey], rtol=rtol, atol=atol
				)
		else:
			assert_allclose(metrics[key], desired_values[key], rtol=rtol, atol=atol)


def global_distributed_metric(
	global_rank: int,
	world_size: int,
	gpu_id: Union[int, torch.device],
	metric: Metric,
	metric_kwargs: Dict[str, List[Any]],
	desired_values: Dict[str, Any],
	exact: Union[bool, Tuple[float, float]] = True,
	number_of_runs: int = 1,
):
	kwargs = {}

	for argname in metric_kwargs:
		kwargs[argname] = metric_kwargs[argname][global_rank]

	for _ in range(number_of_runs):
		metric(**kwargs)

	metrics = metric.get_metric(False)
	if not isinstance(metrics, Dict) and not isinstance(desired_values, Dict):
		metrics = {"metric_value": metrics}
		desired_values = {"metric_value": desired_values}

	if isinstance(exact, bool):
		if exact:
			rtol = 0.0
			atol = 0.0
		else:
			rtol = 0.0001
			atol = 1e-05
	else:
		rtol = exact[0]
		atol = exact[1]

	assert_metrics_values(metrics, desired_values, rtol, atol)  # type: ignore


def assert_equal_parameters(
	old_module: torch.nn.Module,
	new_module: TransformerModule,
	ignore_missing: bool = False,
	mapping: Optional[Dict] = None,
):
	mapping = mapping or {}

	old_parameters = dict(old_module.named_parameters())
	present_only_in_old = set(old_parameters.keys())

	for name, parameter in new_module.named_parameters():
		for key, val in mapping.items():
			name = name.replace(key, val)
		if ignore_missing:
			if name not in old_parameters:
				continue
		present_only_in_old.remove(name)
		assert torch.allclose(old_parameters[name], parameter)
	return present_only_in_old

import argparse
import gym
import numpy as np
from itertools import count
from collections import namedtuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical


parser = argparse.ArgumentParser(description='PyTorch actor-critic example')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
					help='discount factor (default: 0.99)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
					help='random seed (default: 543)')
parser.add_argument('--render', action='store_true',
					help='render the environment')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
					help='interval between training status logs (default: 10)')
args = parser.parse_args()


env = gym.make('CartPole-v1')
env.reset(seed=args.seed)
torch.manual_seed(args.seed)


SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])


class Policy(nn.Module):
	def __init__(self):
		super(Policy, self).__init__()
		self.affine1 = nn.Linear(4, 128)

		self.action_head = nn.Linear(128, 2)

		self.value_head = nn.Linear(128, 1)

		self.saved_actions = []
		self.rewards = []

	def forward(self, x):
		x = F.relu(self.affine1(x))

		action_prob = F.softmax(self.action_head(x), dim=-1)

		state_values = self.value_head(x)

		return action_prob, state_values


model = Policy()
optimizer = optim.Adam(model.parameters(), lr=3e-2)
eps = np.finfo(np.float32).eps.item()


def select_action(state):
	state = torch.from_numpy(state).float()
	probs, state_value = model(state)

	m = Categorical(probs)

	action = m.sample()

	model.saved_actions.append(SavedAction(m.log_prob(action), state_value))

	return action.item()


def finish_episode():
	R = 0
	saved_actions = model.saved_actions
	policy_losses = [] # list to save actor (policy) loss
	value_losses = [] # list to save critic (value) loss
	returns = [] # list to save the true values

	for r in model.rewards[::-1]:
		R = r + args.gamma * R
		returns.insert(0, R)

	returns = torch.tensor(returns)
	returns = (returns - returns.mean()) / (returns.std() + eps)

	for (log_prob, value), R in zip(saved_actions, returns):
		advantage = R - value.item()

		policy_losses.append(-log_prob * advantage)

		value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))

	optimizer.zero_grad()

	loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()

	loss.backward()
	optimizer.step()

	del model.rewards[:]
	del model.saved_actions[:]


def main():
	running_reward = 10

	for i_episode in count(1):

		state, _ = env.reset()
		ep_reward = 0

		for t in range(1, 10000):

			action = select_action(state)

			state, reward, done, _, _ = env.step(action)

			if args.render:
				env.render()

			model.rewards.append(reward)
			ep_reward += reward
			if done:
				break

		running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward

		finish_episode()

		if i_episode % args.log_interval == 0:
			print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
				  i_episode, ep_reward, running_reward))

		if running_reward > env.spec.reward_threshold:
			print("Solved! Running reward is now {} and "
				  "the last episode runs to {} time steps!".format(running_reward, t))
			break


if __name__ == '__main__':
	main()

import hashlib
import io
from datetime import timedelta
import importlib
import json
import logging
import os
import pkgutil
import random
import sys
import signal
from contextlib import contextmanager
from itertools import islice, zip_longest
from pathlib import Path
from typing import (
	Any,
	Callable,
	Dict,
	Generator,
	Iterable,
	Iterator,
	List,
	Optional,
	Tuple,
	TypeVar,
	Union,
	Sequence,
	Set,
)

import dill
import numpy
import spacy
import torch
import torch.distributed as dist
from spacy.cli.download import download as spacy_download
from spacy.language import Language as SpacyModelType
import base58

from allennlp.common.checks import log_pytorch_version_info
from allennlp.common.params import Params

try:
	import resource
except ImportError:
	resource = None  # type: ignore

logger = logging.getLogger(__name__)

JsonDict = Dict[str, Any]

START_SYMBOL = "@start@"
END_SYMBOL = "@end@"


PathType = Union[os.PathLike, str]
T = TypeVar("T")
ContextManagerFunctionReturnType = Generator[T, None, None]


def sanitize(x: Any) -> Any:
	from allennlp.data.tokenizers import Token

	if isinstance(x, (str, float, int, bool)):
		return x
	elif isinstance(x, torch.Tensor):
		return x.cpu().tolist()
	elif isinstance(x, numpy.ndarray):
		return x.tolist()
	elif isinstance(x, numpy.number):
		return x.item()
	elif isinstance(x, dict):
		return {key: sanitize(value) for key, value in x.items()}
	elif isinstance(x, numpy.bool_):
		return bool(x)
	elif isinstance(x, (spacy.tokens.Token, Token)):
		return x.text
	elif isinstance(x, (list, tuple, set)):
		return [sanitize(x_i) for x_i in x]
	elif x is None:
		return "None"
	elif hasattr(x, "to_json"):
		return x.to_json()
	else:
		raise ValueError(
			f"Cannot sanitize {x} of type {type(x)}. "
			"If this is your own custom class, add a `to_json(self)` method "
			"that returns a JSON-like object."
		)


def group_by_count(iterable: List[Any], count: int, default_value: Any) -> List[List[Any]]:
	return [list(x) for x in zip_longest(*[iter(iterable)] * count, fillvalue=default_value)]


A = TypeVar("A")


def lazy_groups_of(iterable: Iterable[A], group_size: int) -> Iterator[List[A]]:
	iterator = iter(iterable)
	while True:
		s = list(islice(iterator, group_size))
		if len(s) > 0:
			yield s
		else:
			break


def pad_sequence_to_length(
	sequence: Sequence,
	desired_length: int,
	default_value: Callable[[], Any] = lambda: 0,
	padding_on_right: bool = True,
) -> List:
	sequence = list(sequence)
	if padding_on_right:
		padded_sequence = sequence[:desired_length]
	else:
		padded_sequence = sequence[-desired_length:]
	pad_length = desired_length - len(padded_sequence)
	values_to_pad = [default_value()] * pad_length
	if padding_on_right:
		padded_sequence = padded_sequence + values_to_pad
	else:
		padded_sequence = values_to_pad + padded_sequence
	return padded_sequence


def add_noise_to_dict_values(dictionary: Dict[A, float], noise_param: float) -> Dict[A, float]:
	new_dict = {}
	for key, value in dictionary.items():
		noise_value = value * noise_param
		noise = random.uniform(-noise_value, noise_value)
		new_dict[key] = value + noise
	return new_dict


def namespace_match(pattern: str, namespace: str):
	if pattern[0] == "*" and namespace.endswith(pattern[1:]):
		return True
	elif pattern == namespace:
		return True
	return False


def prepare_environment(params: Params):
	seed = params.pop_int("random_seed", 13370)
	numpy_seed = params.pop_int("numpy_seed", 1337)
	torch_seed = params.pop_int("pytorch_seed", 133)

	if seed is not None:
		random.seed(seed)
	if numpy_seed is not None:
		numpy.random.seed(numpy_seed)
	if torch_seed is not None:
		torch.manual_seed(torch_seed)
		if torch.cuda.is_available():
			torch.cuda.manual_seed_all(torch_seed)

	log_pytorch_version_info()


LOADED_SPACY_MODELS: Dict[Tuple[str, bool, bool, bool], SpacyModelType] = {}


def get_spacy_model(
	spacy_model_name: str, pos_tags: bool = True, parse: bool = False, ner: bool = False
) -> SpacyModelType:

	options = (spacy_model_name, pos_tags, parse, ner)
	if options not in LOADED_SPACY_MODELS:
		disable = ["vectors", "textcat"]
		if not pos_tags:
			disable.append("tagger")
		if not parse:
			disable.append("parser")
		if not ner:
			disable.append("ner")
		try:
			spacy_model = spacy.load(spacy_model_name, disable=disable)
		except OSError:
			logger.warning(
				f"Spacy models '{spacy_model_name}' not found.  Downloading and installing."
			)
			spacy_download(spacy_model_name)

			spacy_model_module = __import__(spacy_model_name)
			spacy_model = spacy_model_module.load(disable=disable)  # type: ignore

		LOADED_SPACY_MODELS[options] = spacy_model
	return LOADED_SPACY_MODELS[options]


@contextmanager
def pushd(new_dir: PathType, verbose: bool = False) -> ContextManagerFunctionReturnType[None]:
	previous_dir = os.getcwd()
	if verbose:
		logger.info(f"Changing directory to {new_dir}")  # type: ignore
	os.chdir(new_dir)
	try:
		yield
	finally:
		if verbose:
			logger.info(f"Changing directory back to {previous_dir}")
		os.chdir(previous_dir)


@contextmanager
def push_python_path(path: PathType) -> ContextManagerFunctionReturnType[None]:
	path = Path(path).resolve()
	path = str(path)
	sys.path.insert(0, path)
	try:
		yield
	finally:
		sys.path.remove(path)


def import_module_and_submodules(package_name: str, exclude: Optional[Set[str]] = None) -> None:
	exclude = exclude if exclude else set()
	try:
		import checklist  # noqa
	except ImportError:
		exclude |= {"allennlp.confidence_checks.task_checklists"}

	if package_name in exclude:
		return

	importlib.invalidate_caches()

	with push_python_path("."):
		module = importlib.import_module(package_name)
		path = getattr(module, "__path__", [])
		path_string = "" if not path else path[0]

		for module_finder, name, _ in pkgutil.walk_packages(path):
			if path_string and module_finder.path != path_string:  # type: ignore[union-attr]
				continue
			if name.startswith("_"):
				continue
			if name.startswith("test"):
				continue
			subpackage = f"{package_name}.{name}"
			import_module_and_submodules(subpackage, exclude=exclude)


def peak_cpu_memory() -> Dict[int, int]:
	if resource is None or sys.platform not in ("linux", "darwin"):
		peak_bytes = 0
	else:
		peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
		if sys.platform == "darwin":
			peak_bytes = peak
		else:
			peak_bytes = peak * 1_024

	if is_distributed():
		global_rank = dist.get_rank()
		world_size = dist.get_world_size()

		peak_bytes_tensor = torch.tensor([global_rank, peak_bytes])
		gather_results = [torch.tensor([0, 0]) for _ in range(world_size)]

		if dist.get_backend() == "nccl":
			peak_bytes_tensor = peak_bytes_tensor.cuda()
			gather_results = [x.cuda() for x in gather_results]

		dist.all_gather(gather_results, peak_bytes_tensor)

		results_dict: Dict[int, int] = {}
		for peak_bytes_tensor in gather_results:
			results_dict[int(peak_bytes_tensor[0])] = int(peak_bytes_tensor[1])

		return results_dict
	else:
		return {0: peak_bytes}


def peak_gpu_memory() -> Dict[int, int]:
	if not torch.cuda.is_available():
		return {}

	device = torch.cuda.current_device()

	results_dict: Dict[int, int] = {}
	if is_distributed():
		if dist.get_backend() != "nccl":
			return {}

		global_rank = dist.get_rank()
		world_size = dist.get_world_size()
		peak_bytes = torch.cuda.max_memory_allocated(device)
		peak_bytes_tensor = torch.tensor([global_rank, peak_bytes], device=device)
		gather_results = [torch.tensor([0, 0], device=device) for _ in range(world_size)]

		dist.all_gather(gather_results, peak_bytes_tensor)

		for peak_bytes_tensor in gather_results:
			results_dict[int(peak_bytes_tensor[0])] = int(peak_bytes_tensor[1])
	else:
		results_dict = {0: torch.cuda.max_memory_allocated()}

	torch.cuda.reset_max_memory_allocated(device)

	return results_dict


def ensure_list(iterable: Iterable[A]) -> List[A]:
	if isinstance(iterable, list):
		return iterable
	else:
		return list(iterable)


def is_lazy(iterable: Iterable[A]) -> bool:
	return not isinstance(iterable, list)


def int_to_device(device: Union[int, torch.device]) -> torch.device:
	if isinstance(device, torch.device):
		return device
	if device < 0:
		return torch.device("cpu")
	return torch.device(device)


def log_frozen_and_tunable_parameter_names(model: torch.nn.Module) -> None:
	frozen_parameter_names, tunable_parameter_names = get_frozen_and_tunable_parameter_names(model)

	logger.info("The following parameters are Frozen (without gradient):")
	for name in frozen_parameter_names:
		logger.info(name)

	logger.info("The following parameters are Tunable (with gradient):")
	for name in tunable_parameter_names:
		logger.info(name)


def get_frozen_and_tunable_parameter_names(
	model: torch.nn.Module,
) -> Tuple[Iterable[str], Iterable[str]]:
	frozen_parameter_names = (
		name for name, parameter in model.named_parameters() if not parameter.requires_grad
	)
	tunable_parameter_names = (
		name for name, parameter in model.named_parameters() if parameter.requires_grad
	)
	return frozen_parameter_names, tunable_parameter_names


def dump_metrics(file_path: Optional[str], metrics: Dict[str, Any], log: bool = False) -> None:
	metrics_json = json.dumps(metrics, indent=2)
	if file_path:
		with open(file_path, "w") as metrics_file:
			metrics_file.write(metrics_json)
	if log:
		logger.info("Metrics: %s", metrics_json)


def flatten_filename(file_path: str) -> str:
	return file_path.replace("/", "_SLASH_")


def is_distributed() -> bool:
	return dist.is_available() and dist.is_initialized()


def is_global_primary() -> bool:
	if not is_distributed():
		return True
	else:
		return dist.get_rank() == 0


def sanitize_wordpiece(wordpiece: str) -> str:
	if wordpiece.startswith("##"):
		return wordpiece[2:]
	elif wordpiece.startswith("Ġ"):
		return wordpiece[1:]
	elif wordpiece.startswith("▁"):
		return wordpiece[1:]
	else:
		return wordpiece


def sanitize_ptb_tokenized_string(text: str) -> str:
	tokens = text.split(" ")
	if len(tokens) == 0:
		return text

	token_map = {
		"``": '"',
		"''": '"',
		"-lrb-": "(",
		"-rrb-": ")",
		"-lsb-": "[",
		"-rsb-": "]",
		"-lcb-": "{",
		"-rcb-": "}",
		"<s>": "",
		"</s>": "",
	}

	punct_forward = {"`", "$", "#"}
	punct_backward = {".", ",", "!", "?", ":", ";", "%", "'"}

	em_forward = {"(", "[", "{"}
	em_backward = {"n't", "na", ")", "]", "}"}

	new_tokens: List[str] = []

	merge_fwd = False
	for i, orig_token in enumerate(tokens):
		tokens[i] = token_map[orig_token.lower()] if orig_token.lower() in token_map else orig_token
		new_token = tokens[i].lower()

		if merge_fwd:
			tokens[i] = tokens[i - 1] + tokens[i]

		if len(tokens[i]) == 0:
			continue

		merge_bckwd = not merge_fwd and (
			orig_token == "''"
			or new_token in em_backward
			or new_token.startswith("'")
			or all(c in punct_backward for c in new_token)
		)
		merge_fwd = (
			orig_token == "``"
			or new_token in em_forward
			or all(c in punct_forward for c in new_token)
		)

		if merge_bckwd and new_tokens:
			new_tokens[-1] += tokens[i]
		elif not new_tokens or not merge_fwd or i == len(tokens) - 1:
			new_tokens.append(tokens[i])

	return " ".join(new_tokens)


def find_open_port() -> int:
	import socket

	with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
		sock.bind(("", 0))
		return sock.getsockname()[1]


def format_timedelta(td: timedelta) -> str:
	if td.days > 1:
		return f"{td.days} days"
	elif td.days > 0:
		return f"{td.days} day"
	else:
		hours, remainder = divmod(td.seconds, 3600)
		minutes, _ = divmod(remainder, 60)
		if hours > 1:
			return f"{hours} hours"
		elif hours > 0:
			return f"{hours} hour, {minutes} mins"
		else:
			return f"{minutes} mins"


def format_size(size: int) -> str:
	GBs = size / (1024 * 1024 * 1024)
	if GBs >= 10:
		return f"{int(round(GBs, 0))}G"
	if GBs >= 1:
		return f"{round(GBs, 1):.1f}G"
	MBs = size / (1024 * 1024)
	if MBs >= 10:
		return f"{int(round(MBs, 0))}M"
	if MBs >= 1:
		return f"{round(MBs, 1):.1f}M"
	KBs = size / 1024
	if KBs >= 10:
		return f"{int(round(KBs, 0))}K"
	if KBs >= 1:
		return f"{round(KBs, 1):.1f}K"
	return f"{size}B"


def nan_safe_tensor_divide(numerator, denominator):
	result = numerator / denominator
	mask = denominator == 0.0
	if not mask.any():
		return result

	result[mask] = 0.0
	return result


def shuffle_iterable(i: Iterable[T], pool_size: int = 1024) -> Iterable[T]:
	import random

	i = iter(i)
	pool = []

	for item in i:
		pool.append(item)
		if len(pool) >= pool_size:
			break

	while len(pool) > 0:
		index = random.randrange(len(pool))
		yield pool[index]
		try:
			pool[index] = next(i)
		except StopIteration:
			del pool[index]
			break

	random.shuffle(pool)
	yield from pool


def cycle_iterator_function(iterator_function: Callable[[], Iterable[T]]) -> Iterator[T]:
	iterator = iter(iterator_function())
	while True:
		try:
			yield next(iterator)
		except StopIteration:
			iterator = iter(iterator_function())


def hash_object(o: Any) -> str:
	m = hashlib.blake2b()
	with io.BytesIO() as buffer:
		dill.dump(o, buffer)
		m.update(buffer.getbuffer())
		return base58.b58encode(m.digest()).decode()


class SigTermReceived(Exception):
	pass


def _handle_sigterm(sig, frame):
	raise SigTermReceived


def install_sigterm_handler():
	signal.signal(signal.SIGTERM, _handle_sigterm)

from typing import Optional


import torch

from allennlp.common.checks import ConfigurationError
from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("sequence_accuracy")
class SequenceAccuracy(Metric):

	def __init__(self) -> None:
		self.correct_count = 0.0
		self.total_count = 0.0

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		if gold_labels.dim() != predictions.dim() - 1:
			raise ConfigurationError(
				"gold_labels must have dimension == predictions.dim() - 1 but "
				"found tensor of shape: {}".format(gold_labels.size())
			)
		if mask is not None and mask.size() != gold_labels.size():
			raise ConfigurationError(
				"mask must have the same size as predictions but "
				"found tensor of shape: {}".format(mask.size())
			)

		k = predictions.size()[1]
		expanded_size = list(gold_labels.size())
		expanded_size.insert(1, k)
		expanded_gold = gold_labels.unsqueeze(1).expand(expanded_size)

		if mask is not None:
			expanded_mask = mask.unsqueeze(1).expand(expanded_size)
			masked_gold = expanded_mask * expanded_gold
			masked_predictions = expanded_mask * predictions
		else:
			masked_gold = expanded_gold
			masked_predictions = predictions

		eqs = masked_gold.eq(masked_predictions)
		matches_per_question = eqs.min(dim=2)[0]
		some_match = matches_per_question.max(dim=1)[0]
		correct = some_match.sum().item()

		_total_count = predictions.size()[0]
		_correct_count = correct

		self.correct_count += dist_reduce_sum(_correct_count)
		self.total_count += dist_reduce_sum(_total_count)

	def get_metric(self, reset: bool = False):
		if self.total_count > 0:
			accuracy = self.correct_count / self.total_count
		else:
			accuracy = 0
		if reset:
			self.reset()
		return {"accuracy": accuracy}

	def reset(self):
		self.correct_count = 0.0
		self.total_count = 0.0

import torch
from torchvision import models

model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

for param in model.parameters():
	param.requires_grad = False

resnet18 = torch.nn.Sequential(*list(model.children())[:-1])

example_input = torch.rand(1, 3, 224, 224)
script_module = torch.jit.trace(resnet18, example_input)
script_module.save('resnet18_without_last_layer.pt')


from typing import Optional, Tuple, List

import torch

from allennlp.nn.util import get_dropout_mask
from allennlp.nn.initializers import block_orthogonal


class LstmCellWithProjection(torch.nn.Module):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		cell_size: int,
		go_forward: bool = True,
		recurrent_dropout_probability: float = 0.0,
		memory_cell_clip_value: Optional[float] = None,
		state_projection_clip_value: Optional[float] = None,
	) -> None:
		super().__init__()
		self.input_size = input_size
		self.hidden_size = hidden_size
		self.cell_size = cell_size

		self.go_forward = go_forward
		self.state_projection_clip_value = state_projection_clip_value
		self.memory_cell_clip_value = memory_cell_clip_value
		self.recurrent_dropout_probability = recurrent_dropout_probability

		self.input_linearity = torch.nn.Linear(input_size, 4 * cell_size, bias=False)
		self.state_linearity = torch.nn.Linear(hidden_size, 4 * cell_size, bias=True)

		self.state_projection = torch.nn.Linear(cell_size, hidden_size, bias=False)
		self.reset_parameters()

	def reset_parameters(self):
		block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])
		block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])

		self.state_linearity.bias.data.fill_(0.0)
		self.state_linearity.bias.data[self.cell_size : 2 * self.cell_size].fill_(1.0)

	def forward(
		self,
		inputs: torch.FloatTensor,
		batch_lengths: List[int],
		initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
	):
		batch_size = inputs.size()[0]
		total_timesteps = inputs.size()[1]

		output_accumulator = inputs.new_zeros(batch_size, total_timesteps, self.hidden_size)

		if initial_state is None:
			full_batch_previous_memory = inputs.new_zeros(batch_size, self.cell_size)
			full_batch_previous_state = inputs.new_zeros(batch_size, self.hidden_size)
		else:
			full_batch_previous_state = initial_state[0].squeeze(0)
			full_batch_previous_memory = initial_state[1].squeeze(0)

		current_length_index = batch_size - 1 if self.go_forward else 0
		if self.recurrent_dropout_probability > 0.0 and self.training:
			dropout_mask = get_dropout_mask(
				self.recurrent_dropout_probability, full_batch_previous_state
			)
		else:
			dropout_mask = None

		for timestep in range(total_timesteps):
			index = timestep if self.go_forward else total_timesteps - timestep - 1

			if self.go_forward:
				while batch_lengths[current_length_index] <= index:
					current_length_index -= 1
			else:
				while (
					current_length_index < (len(batch_lengths) - 1)
					and batch_lengths[current_length_index + 1] > index
				):
					current_length_index += 1

			previous_memory = full_batch_previous_memory[0 : current_length_index + 1].clone()
			previous_state = full_batch_previous_state[0 : current_length_index + 1].clone()
			timestep_input = inputs[0 : current_length_index + 1, index]

			projected_input = self.input_linearity(timestep_input)
			projected_state = self.state_linearity(previous_state)

			input_gate = torch.sigmoid(
				projected_input[:, (0 * self.cell_size) : (1 * self.cell_size)]
				+ projected_state[:, (0 * self.cell_size) : (1 * self.cell_size)]
			)
			forget_gate = torch.sigmoid(
				projected_input[:, (1 * self.cell_size) : (2 * self.cell_size)]
				+ projected_state[:, (1 * self.cell_size) : (2 * self.cell_size)]
			)
			memory_init = torch.tanh(
				projected_input[:, (2 * self.cell_size) : (3 * self.cell_size)]
				+ projected_state[:, (2 * self.cell_size) : (3 * self.cell_size)]
			)
			output_gate = torch.sigmoid(
				projected_input[:, (3 * self.cell_size) : (4 * self.cell_size)]
				+ projected_state[:, (3 * self.cell_size) : (4 * self.cell_size)]
			)
			memory = input_gate * memory_init + forget_gate * previous_memory


			if self.memory_cell_clip_value:

				memory = torch.clamp(
					memory, -self.memory_cell_clip_value, self.memory_cell_clip_value
				)

			pre_projection_timestep_output = output_gate * torch.tanh(memory)

			timestep_output = self.state_projection(pre_projection_timestep_output)
			if self.state_projection_clip_value:

				timestep_output = torch.clamp(
					timestep_output,
					-self.state_projection_clip_value,
					self.state_projection_clip_value,
				)

			if dropout_mask is not None:
				timestep_output = timestep_output * dropout_mask[0 : current_length_index + 1]

			full_batch_previous_memory = full_batch_previous_memory.clone()
			full_batch_previous_state = full_batch_previous_state.clone()
			full_batch_previous_memory[0 : current_length_index + 1] = memory
			full_batch_previous_state[0 : current_length_index + 1] = timestep_output
			output_accumulator[0 : current_length_index + 1, index] = timestep_output

		final_state = (
			full_batch_previous_state.unsqueeze(0),
			full_batch_previous_memory.unsqueeze(0),
		)

		return output_accumulator, final_state


from .environment import bfloat_support
from .train_utils import setup, cleanup, get_date_of_run, format_metrics_to_gb, train, validation,setup_model
						  
						  
import math
import random
from typing import Optional, List, Iterator


import torch

from allennlp.common.util import lazy_groups_of
from allennlp.common.tqdm import Tqdm
from allennlp.data.data_loaders.data_loader import DataLoader, TensorDict
from allennlp.data.data_loaders.data_collator import DefaultDataCollator
from allennlp.data.dataset_readers import DatasetReader
from allennlp.data.instance import Instance
from allennlp.data.vocabulary import Vocabulary
import allennlp.nn.util as nn_util


@DataLoader.register("simple", constructor="from_dataset_reader")
class SimpleDataLoader(DataLoader):

	def __init__(
		self,
		instances: List[Instance],
		batch_size: int,
		*,
		shuffle: bool = False,
		batches_per_epoch: Optional[int] = None,
		vocab: Optional[Vocabulary] = None,
	) -> None:
		self.instances = instances
		self.batch_size = batch_size
		self.shuffle = shuffle
		self.batches_per_epoch = batches_per_epoch
		self.vocab = vocab
		self.cuda_device: Optional[torch.device] = None
		self._batch_generator: Optional[Iterator[TensorDict]] = None
		self.collate_fn = DefaultDataCollator()

	def __len__(self) -> int:
		if self.batches_per_epoch is not None:
			return self.batches_per_epoch
		return math.ceil(len(self.instances) / self.batch_size)

	def __iter__(self) -> Iterator[TensorDict]:
		if self.batches_per_epoch is None:
			yield from self._iter_batches()
		else:
			if self._batch_generator is None:
				self._batch_generator = self._iter_batches()
			for i in range(self.batches_per_epoch):
				try:
					yield next(self._batch_generator)
				except StopIteration:  # data_generator is exhausted
					self._batch_generator = self._iter_batches()  # so refresh it
					yield next(self._batch_generator)

	def _iter_batches(self) -> Iterator[TensorDict]:
		if self.shuffle:
			random.shuffle(self.instances)
		for batch in lazy_groups_of(self.iter_instances(), self.batch_size):
			tensor_dict = self.collate_fn(batch)
			if self.cuda_device is not None:
				tensor_dict = nn_util.move_to_device(tensor_dict, self.cuda_device)
			yield tensor_dict

	def iter_instances(self) -> Iterator[Instance]:
		for instance in self.instances:
			if self.vocab is not None:
				instance.index_fields(self.vocab)
			yield instance

	def index_with(self, vocab: Vocabulary) -> None:
		self.vocab = vocab
		for instance in self.instances:
			instance.index_fields(self.vocab)

	def set_target_device(self, device: torch.device) -> None:
		self.cuda_device = device

	@classmethod
	def from_dataset_reader(
		cls,
		reader: DatasetReader,
		data_path: str,
		batch_size: int,
		shuffle: bool = False,
		batches_per_epoch: Optional[int] = None,
		quiet: bool = False,
	) -> "SimpleDataLoader":
		instance_iter = reader.read(data_path)
		if not quiet:
			instance_iter = Tqdm.tqdm(instance_iter, desc="loading instances")
		instances = list(instance_iter)
		return cls(instances, batch_size, shuffle=shuffle, batches_per_epoch=batches_per_epoch)


import argparse
import logging


from allennlp.commands.subcommand import Subcommand
from allennlp.common.push_to_hf import push_to_hf

logger = logging.getLogger(__name__)


@Subcommand.register("push-to-hf")
class PushToHf(Subcommand):
	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		subparser = parser.add_parser(self.name, description=description, help=description)
		subparser.set_defaults(func=push)

		subparser.add_argument(
			"-n",
			"--repo-name",
			required=True,
			type=str,
			default="Name of the repository",
			help="Name of the repository",
		)

		model_dir_group = subparser.add_mutually_exclusive_group(required=True)
		model_dir_group.add_argument(
			"-s",
			"--serialization-dir",
			type=str,
			help="directory in which to save the model and its logs are saved",
		)

		model_dir_group.add_argument(
			"-a",
			"--archive-path",
			type=str,
			help="full path to the zipped model, using serialization_dir instead is recommended",
		)

		subparser.add_argument(
			"-o",
			"--organization",
			required=False,
			type=str,
			help="name of organization to which the model should be uploaded",
		)

		subparser.add_argument(
			"-c",
			"--commit-message",
			required=False,
			type=str,
			default="Update repository",
			help="Commit message to use for the push",
		)

		subparser.add_argument(
			"-l",
			"--local-repo-path",
			required=False,
			type=str,
			default="hub",
			help="local path for creating repo",
		)

		return subparser


def push(args: argparse.Namespace):
	push_to_hf(
		args.repo_name,
		serialization_dir=args.serialization_dir,
		archive_path=args.archive_path,
		organization=args.organization,
		commit_message=args.commit_message,
		local_repo_path=args.local_repo_path,
	)

import torch

from allennlp.common.checks import ConfigurationError
from allennlp.modules.augmented_lstm import AugmentedLstm
from allennlp.modules.seq2vec_encoders.seq2vec_encoder import Seq2VecEncoder
from allennlp.modules.stacked_alternating_lstm import StackedAlternatingLstm
from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm


class PytorchSeq2VecWrapper(Seq2VecEncoder):

	def __init__(self, module: torch.nn.modules.RNNBase) -> None:
		super().__init__(stateful=False)
		self._module = module
		try:
			if not self._module.batch_first:
				raise ConfigurationError("Our encoder semantics assumes batch is always first!")
		except AttributeError:
			pass

	def get_input_dim(self) -> int:
		return self._module.input_size

	def get_output_dim(self) -> int:
		try:
			is_bidirectional = self._module.bidirectional
		except AttributeError:
			is_bidirectional = False
		return self._module.hidden_size * (2 if is_bidirectional else 1)

	def forward(
		self, inputs: torch.Tensor, mask: torch.BoolTensor, hidden_state: torch.Tensor = None
	) -> torch.Tensor:

		if mask is None:
			return self._module(inputs, hidden_state)[0][:, -1, :]

		batch_size = mask.size(0)

		(
			_,
			state,
			restoration_indices,
		) = self.sort_and_run_forward(self._module, inputs, mask, hidden_state)

		if isinstance(state, tuple):
			state = state[0]

		num_layers_times_directions, num_valid, encoding_dim = state.size()
		if num_valid < batch_size:
			zeros = state.new_zeros(
				num_layers_times_directions, batch_size - num_valid, encoding_dim
			)
			state = torch.cat([state, zeros], 1)


		unsorted_state = state.transpose(0, 1).index_select(0, restoration_indices)

		try:
			last_state_index = 2 if self._module.bidirectional else 1
		except AttributeError:
			last_state_index = 1
		last_layer_state = unsorted_state[:, -last_state_index:, :]
		return last_layer_state.contiguous().view([-1, self.get_output_dim()])


@Seq2VecEncoder.register("gru")
class GruSeq2VecEncoder(PytorchSeq2VecWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int = 1,
		bias: bool = True,
		dropout: float = 0.0,
		bidirectional: bool = False,
	):
		module = torch.nn.GRU(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			bias=bias,
			batch_first=True,
			dropout=dropout,
			bidirectional=bidirectional,
		)
		super().__init__(module=module)


@Seq2VecEncoder.register("lstm")
class LstmSeq2VecEncoder(PytorchSeq2VecWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int = 1,
		bias: bool = True,
		dropout: float = 0.0,
		bidirectional: bool = False,
	):
		module = torch.nn.LSTM(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			bias=bias,
			batch_first=True,
			dropout=dropout,
			bidirectional=bidirectional,
		)
		super().__init__(module=module)


@Seq2VecEncoder.register("rnn")
class RnnSeq2VecEncoder(PytorchSeq2VecWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int = 1,
		nonlinearity: str = "tanh",
		bias: bool = True,
		dropout: float = 0.0,
		bidirectional: bool = False,
	):
		module = torch.nn.RNN(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			nonlinearity=nonlinearity,
			bias=bias,
			batch_first=True,
			dropout=dropout,
			bidirectional=bidirectional,
		)
		super().__init__(module=module)


@Seq2VecEncoder.register("augmented_lstm")
class AugmentedLstmSeq2VecEncoder(PytorchSeq2VecWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		go_forward: bool = True,
		recurrent_dropout_probability: float = 0.0,
		use_highway: bool = True,
		use_input_projection_bias: bool = True,
	) -> None:
		module = AugmentedLstm(
			input_size=input_size,
			hidden_size=hidden_size,
			go_forward=go_forward,
			recurrent_dropout_probability=recurrent_dropout_probability,
			use_highway=use_highway,
			use_input_projection_bias=use_input_projection_bias,
		)
		super().__init__(module=module)


@Seq2VecEncoder.register("alternating_lstm")
class StackedAlternatingLstmSeq2VecEncoder(PytorchSeq2VecWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int,
		recurrent_dropout_probability: float = 0.0,
		use_highway: bool = True,
		use_input_projection_bias: bool = True,
	) -> None:
		module = StackedAlternatingLstm(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			recurrent_dropout_probability=recurrent_dropout_probability,
			use_highway=use_highway,
			use_input_projection_bias=use_input_projection_bias,
		)
		super().__init__(module=module)


@Seq2VecEncoder.register("stacked_bidirectional_lstm")
class StackedBidirectionalLstmSeq2VecEncoder(PytorchSeq2VecWrapper):

	def __init__(
		self,
		input_size: int,
		hidden_size: int,
		num_layers: int,
		recurrent_dropout_probability: float = 0.0,
		layer_dropout_probability: float = 0.0,
		use_highway: bool = True,
	) -> None:
		module = StackedBidirectionalLstm(
			input_size=input_size,
			hidden_size=hidden_size,
			num_layers=num_layers,
			recurrent_dropout_probability=recurrent_dropout_probability,
			layer_dropout_probability=layer_dropout_probability,
			use_highway=use_highway,
		)
		super().__init__(module=module)


import argparse
import logging


from allennlp.commands.subcommand import Subcommand
from allennlp.common.file_utils import (
	cached_path,
	CACHE_DIRECTORY,
	inspect_cache,
	remove_cache_entries,
)


logger = logging.getLogger(__name__)


@Subcommand.register("cached-path")
class CachedPath(Subcommand):
	requires_plugins: bool = False

	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:
		description = """Cache remote files to the AllenNLP cache."""
		subparser = parser.add_parser(
			self.name,
			description=description,
			help=description,
		)
		subparser.set_defaults(func=_cached_path)
		subparser.add_argument(
			"resources",
			type=str,
			nargs="*",
		)
		subparser.add_argument(
			"-d",
			"--cache-dir",
			type=str,
			help="""Use a custom cache directory.""",
			default=CACHE_DIRECTORY,
		)
		subparser.add_argument(
			"-x",
			"--extract-archive",
			action="store_true",
			help="""Automatically extract zip or tar.gz archive files.""",
		)
		subparser.add_argument(
			"-f",
			"--force-extract",
			action="store_true",
			help="""Extract archives regardless of whether or not they already exist.""",
		)
		subparser.add_argument(
			"--inspect",
			action="store_true",
			help="""Print some useful information about the cache.""",
		)
		subparser.add_argument(
			"--remove",
			action="store_true",
			help="""Remove any cache entries matching the given resource patterns.""",
		)
		return subparser


def _cached_path(args: argparse.Namespace):
	logger.info("Cache directory: %s", args.cache_dir)
	if args.inspect:
		if args.extract_archive or args.force_extract or args.remove:
			raise RuntimeError(
				"cached-path cannot accept --extract-archive, --force-extract, or --remove "
				"options when --inspect flag is used."
			)
		inspect_cache(patterns=args.resources, cache_dir=args.cache_dir)
	elif args.remove:
		from allennlp.common.util import format_size

		if args.extract_archive or args.force_extract or args.inspect:
			raise RuntimeError(
				"cached-path cannot accept --extract-archive, --force-extract, or --inspect "
				"options when --remove flag is used."
			)
		if not args.resources:
			raise RuntimeError(
				"Missing positional argument(s) 'resources'. 'resources' is required when using "
				"the --remove option. If you really want to remove everything, pass '*' for 'resources'."
			)
		reclaimed_space = remove_cache_entries(args.resources, cache_dir=args.cache_dir)
		print(f"Reclaimed {format_size(reclaimed_space)} of space")
	else:
		for resource in args.resources:
			print(
				cached_path(
					resource,
					cache_dir=args.cache_dir,
					extract_archive=args.extract_archive,
					force_extract=args.force_extract,
				)
			)

from typing import List, Optional, Union

import torch


from allennlp.common.util import nan_safe_tensor_divide
from allennlp.common.checks import ConfigurationError
from allennlp.training.metrics.metric import Metric
from allennlp.nn.util import dist_reduce_sum


@Metric.register("fbeta")
class FBetaMeasure(Metric):

	def __init__(self, beta: float = 1.0, average: str = None, labels: List[int] = None) -> None:
		average_options = {None, "micro", "macro", "weighted"}
		if average not in average_options:
			raise ConfigurationError(f"`average` has to be one of {average_options}.")
		if beta <= 0:
			raise ConfigurationError("`beta` should be >0 in the F-beta score.")
		if labels is not None and len(labels) == 0:
			raise ConfigurationError("`labels` cannot be an empty list.")
		self._beta = beta
		self._average = average
		self._labels = labels

		self._true_positive_sum: Union[None, torch.Tensor] = None
		self._total_sum: Union[None, torch.Tensor] = None
		self._pred_sum: Union[None, torch.Tensor] = None
		self._true_sum: Union[None, torch.Tensor] = None

	def __call__(
		self,
		predictions: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)

		num_classes = predictions.size(-1)
		if (gold_labels >= num_classes).any():
			raise ConfigurationError(
				"A gold label passed to FBetaMeasure contains "
				f"an id >= {num_classes}, the number of classes."
			)

		if self._true_positive_sum is None:
			self._true_positive_sum = torch.zeros(num_classes, device=predictions.device)
			self._true_sum = torch.zeros(num_classes, device=predictions.device)
			self._pred_sum = torch.zeros(num_classes, device=predictions.device)
			self._total_sum = torch.zeros(num_classes, device=predictions.device)

		if mask is None:
			mask = torch.ones_like(gold_labels).bool()
		gold_labels = gold_labels.float()

		pred_mask = predictions.sum(dim=-1) != 0
		argmax_predictions = predictions.max(dim=-1)[1].float()

		true_positives = (gold_labels == argmax_predictions) & mask & pred_mask
		true_positives_bins = gold_labels[true_positives]

		if true_positives_bins.shape[0] == 0:
			true_positive_sum = torch.zeros(num_classes, device=predictions.device)
		else:
			true_positive_sum = torch.bincount(
				true_positives_bins.long(), minlength=num_classes
			).float()

		pred_bins = argmax_predictions[mask & pred_mask].long()
		if pred_bins.shape[0] != 0:
			pred_sum = torch.bincount(pred_bins, minlength=num_classes).float()
		else:
			pred_sum = torch.zeros(num_classes, device=predictions.device)

		gold_labels_bins = gold_labels[mask].long()
		if gold_labels.shape[0] != 0:
			true_sum = torch.bincount(gold_labels_bins, minlength=num_classes).float()
		else:
			true_sum = torch.zeros(num_classes, device=predictions.device)

		self._total_sum += mask.sum().to(torch.float)

		self._true_positive_sum += dist_reduce_sum(true_positive_sum)
		self._pred_sum += dist_reduce_sum(pred_sum)
		self._true_sum += dist_reduce_sum(true_sum)

	def get_metric(self, reset: bool = False):
		if self._true_positive_sum is None:
			raise RuntimeError("You have never called this metric before.")
		else:
			tp_sum = self._true_positive_sum
			pred_sum = self._pred_sum
			true_sum = self._true_sum

		if self._labels is not None:
			tp_sum = tp_sum[self._labels]
			pred_sum = pred_sum[self._labels]  # type: ignore
			true_sum = true_sum[self._labels]  # type: ignore

		if self._average == "micro":
			tp_sum = tp_sum.sum()
			pred_sum = pred_sum.sum()  # type: ignore
			true_sum = true_sum.sum()  # type: ignore

		beta2 = self._beta**2
		precision = nan_safe_tensor_divide(tp_sum, pred_sum)
		recall = nan_safe_tensor_divide(tp_sum, true_sum)
		fscore = (1 + beta2) * precision * recall / (beta2 * precision + recall)
		fscore[tp_sum == 0] = 0.0

		if self._average == "macro":
			precision = precision.mean()
			recall = recall.mean()
			fscore = fscore.mean()
		elif self._average == "weighted":
			weights = true_sum
			weights_sum = true_sum.sum()  # type: ignore
			precision = nan_safe_tensor_divide((weights * precision).sum(), weights_sum)
			recall = nan_safe_tensor_divide((weights * recall).sum(), weights_sum)
			fscore = nan_safe_tensor_divide((weights * fscore).sum(), weights_sum)

		if reset:
			self.reset()

		if self._average is None:
			return {
				"precision": precision.tolist(),
				"recall": recall.tolist(),
				"fscore": fscore.tolist(),
			}
		else:
			return {"precision": precision.item(), "recall": recall.item(), "fscore": fscore.item()}

	def reset(self) -> None:
		self._true_positive_sum = None
		self._pred_sum = None
		self._true_sum = None
		self._total_sum = None

	@property
	def _true_negative_sum(self):
		if self._total_sum is None:
			return None
		else:
			true_negative_sum = (
				self._total_sum - self._pred_sum - self._true_sum + self._true_positive_sum
			)
			return true_negative_sum

import itertools
import random
from collections import OrderedDict
from typing import NamedTuple, Optional, List, Tuple

import torch
from torch import nn, FloatTensor, IntTensor, Tensor
import torch.nn.functional as F
import torchvision
import torchvision.ops.boxes as box_ops

from allennlp.common import Registrable


class RegionDetectorOutput(NamedTuple):

	features: List[Tensor]

	boxes: List[Tensor]

	class_probs: Optional[List[Tensor]] = None

	class_labels: Optional[List[Tensor]] = None


class RegionDetector(nn.Module, Registrable):

	def forward(
		self,
		images: FloatTensor,
		sizes: IntTensor,
		image_features: "OrderedDict[str, FloatTensor]",
	) -> RegionDetectorOutput:
		raise NotImplementedError()


@RegionDetector.register("random")
class RandomRegionDetector(RegionDetector):

	def __init__(self, seed: Optional[int] = None):
		super().__init__()
		self.random = random.Random(seed)

	def _seeded_random_tensor(self, *shape: int, device) -> torch.FloatTensor:
		result = torch.zeros(*shape, dtype=torch.float32, device=device)
		for coordinates in itertools.product(*(range(size) for size in result.shape)):
			result[coordinates] = self.random.uniform(-1, 1)
		return result

	def forward(
		self,
		images: FloatTensor,
		sizes: IntTensor,
		image_features: "OrderedDict[str, FloatTensor]",
	) -> RegionDetectorOutput:
		batch_size, num_features, height, width = images.size()
		features = [
			self._seeded_random_tensor(2, 10, device=images.device) for _ in range(batch_size)
		]
		boxes = [
			torch.zeros(2, 4, dtype=torch.float32, device=images.device) for _ in range(batch_size)
		]
		for image_num in range(batch_size):
			boxes[image_num][0, 2] = sizes[image_num, 0]
			boxes[image_num][0, 3] = sizes[image_num, 1]
			boxes[image_num][1, 2] = sizes[image_num, 0]
			boxes[image_num][1, 3] = sizes[image_num, 1]
		return RegionDetectorOutput(features, boxes)


@RegionDetector.register("faster_rcnn")
class FasterRcnnRegionDetector(RegionDetector):

	def __init__(
		self,
		*,
		box_score_thresh: float = 0.05,
		box_nms_thresh: float = 0.5,
		max_boxes_per_image: int = 100,
	):
		super().__init__()
		self.detector = torchvision.models.detection.fasterrcnn_resnet50_fpn(
			pretrained=True,
			box_score_thresh=box_score_thresh,
			box_nms_thresh=box_nms_thresh,
			box_detections_per_img=max_boxes_per_image,
		)
		del self.detector.backbone
		for parameter in self.detector.parameters():
			parameter.requires_grad = False

	def forward(
		self,
		images: FloatTensor,
		sizes: IntTensor,
		image_features: "OrderedDict[str, FloatTensor]",
	) -> RegionDetectorOutput:
		if self.training:
			raise RuntimeError(
				"FasterRcnnRegionDetector can not be used for training at the moment"
			)


		image_shapes: List[Tuple[int, int]] = list((int(h), int(w)) for (h, w) in sizes)
		image_list = torchvision.models.detection.image_list.ImageList(images, image_shapes)

		proposals: List[Tensor]
		proposals, _ = self.detector.rpn(image_list, image_features)

		box_features = self.detector.roi_heads.box_roi_pool(image_features, proposals, image_shapes)

		box_features = self.detector.roi_heads.box_head(box_features)

		class_logits, box_regression = self.detector.roi_heads.box_predictor(box_features)

		boxes, features, scores, labels = self._postprocess_detections(
			class_logits, box_features, box_regression, proposals, image_shapes
		)

		return RegionDetectorOutput(features, boxes, scores, labels)

	def _postprocess_detections(
		self,
		class_logits: Tensor,
		box_features: Tensor,
		box_regression: Tensor,
		proposals: List[Tensor],
		image_shapes: List[Tuple[int, int]],
	) -> Tuple[List[Tensor], List[Tensor], List[Tensor], List[Tensor]]:
		device = class_logits.device
		num_classes = class_logits.shape[-1]

		boxes_per_image = [boxes_in_image.shape[0] for boxes_in_image in proposals]

		pred_boxes = self.detector.roi_heads.box_coder.decode(box_regression, proposals)

		pred_scores = F.softmax(class_logits, -1)

		pred_boxes_list = pred_boxes.split(boxes_per_image, 0)
		features_list = box_features.split(boxes_per_image, dim=0)
		pred_scores_list = pred_scores.split(boxes_per_image, 0)

		all_boxes = []
		all_features = []
		all_scores = []
		all_labels = []
		for boxes, features, scores, image_shape in zip(
			pred_boxes_list, features_list, pred_scores_list, image_shapes
		):
			boxes = box_ops.clip_boxes_to_image(boxes, image_shape)

			features = features.unsqueeze(1).expand(boxes.shape[0], boxes.shape[1], -1)

			labels = torch.arange(num_classes, device=device)
			labels = labels.view(1, -1).expand_as(scores)

			boxes = boxes[:, 1:]
			features = features[:, 1:]
			scores = scores[:, 1:]
			labels = labels[:, 1:]

			boxes = boxes.reshape(-1, 4)
			features = features.reshape(boxes.shape[0], -1)
			scores = scores.reshape(-1)
			labels = labels.reshape(-1)

			inds = torch.where(scores > self.detector.roi_heads.score_thresh)[0]
			boxes, features, scores, labels = (
				boxes[inds],
				features[inds],
				scores[inds],
				labels[inds],
			)

			keep = box_ops.remove_small_boxes(boxes, min_size=1e-2)
			boxes, features, scores, labels = (
				boxes[keep],
				features[keep],
				scores[keep],
				labels[keep],
			)

			keep = box_ops.batched_nms(boxes, scores, labels, self.detector.roi_heads.nms_thresh)
			keep = keep[: self.detector.roi_heads.detections_per_img]
			boxes, features, scores, labels = (
				boxes[keep],
				features[keep],
				scores[keep],
				labels[keep],
			)

			all_boxes.append(boxes)
			all_features.append(features)
			all_scores.append(scores)
			all_labels.append(labels)

		return all_boxes, all_features, all_scores, all_labels

import torch

from allennlp.nn import util


class LayerNorm(torch.nn.Module):


	def __init__(self, dimension: int) -> None:
		super().__init__()
		self.gamma = torch.nn.Parameter(torch.ones(dimension))
		self.beta = torch.nn.Parameter(torch.zeros(dimension))

	def forward(self, tensor: torch.Tensor):
		mean = tensor.mean(-1, keepdim=True)
		std = tensor.std(-1, unbiased=False, keepdim=True)
		return (
			self.gamma * (tensor - mean) / (std + util.tiny_value_of_dtype(std.dtype)) + self.beta
		)

import math
import time

from transformers import Trainer, is_torch_tpu_available
from transformers.trainer_utils import PredictionOutput, speed_metrics


if is_torch_tpu_available(check_device=False):
	import torch_xla.core.xla_model as xm
	import torch_xla.debug.metrics as met


class QuestionAnsweringTrainer(Trainer):
	def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):
		super().__init__(*args, **kwargs)
		self.eval_examples = eval_examples
		self.post_process_function = post_process_function

	def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval"):
		eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
		eval_dataloader = self.get_eval_dataloader(eval_dataset)
		eval_examples = self.eval_examples if eval_examples is None else eval_examples

		compute_metrics = self.compute_metrics
		self.compute_metrics = None
		eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
		start_time = time.time()
		try:
			output = eval_loop(
				eval_dataloader,
				description="Evaluation",
				prediction_loss_only=True if compute_metrics is None else None,
				ignore_keys=ignore_keys,
				metric_key_prefix=metric_key_prefix,
			)
		finally:
			self.compute_metrics = compute_metrics
		total_batch_size = self.args.eval_batch_size * self.args.world_size
		if f"{metric_key_prefix}_jit_compilation_time" in output.metrics:
			start_time += output.metrics[f"{metric_key_prefix}_jit_compilation_time"]
		output.metrics.update(
			speed_metrics(
				metric_key_prefix,
				start_time,
				num_samples=output.num_samples,
				num_steps=math.ceil(output.num_samples / total_batch_size),
			)
		)
		if self.post_process_function is not None and self.compute_metrics is not None and self.args.should_save:
			eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)
			metrics = self.compute_metrics(eval_preds)

			for key in list(metrics.keys()):
				if not key.startswith(f"{metric_key_prefix}_"):
					metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
			metrics.update(output.metrics)
		else:
			metrics = output.metrics

		if self.args.should_log:
			self.log(metrics)

		if self.args.tpu_metrics_debug or self.args.debug:
			xm.master_print(met.metrics_report())

		self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
		return metrics

	def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_key_prefix: str = "test"):
		predict_dataloader = self.get_test_dataloader(predict_dataset)

		compute_metrics = self.compute_metrics
		self.compute_metrics = None
		eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
		start_time = time.time()
		try:
			output = eval_loop(
				predict_dataloader,
				description="Prediction",
				prediction_loss_only=True if compute_metrics is None else None,
				ignore_keys=ignore_keys,
				metric_key_prefix=metric_key_prefix,
			)
		finally:
			self.compute_metrics = compute_metrics
		total_batch_size = self.args.eval_batch_size * self.args.world_size
		if f"{metric_key_prefix}_jit_compilation_time" in output.metrics:
			start_time += output.metrics[f"{metric_key_prefix}_jit_compilation_time"]
		output.metrics.update(
			speed_metrics(
				metric_key_prefix,
				start_time,
				num_samples=output.num_samples,
				num_steps=math.ceil(output.num_samples / total_batch_size),
			)
		)

		if self.post_process_function is None or self.compute_metrics is None:
			return output

		predictions = self.post_process_function(predict_examples, predict_dataset, output.predictions, "predict")
		metrics = self.compute_metrics(predictions)

		for key in list(metrics.keys()):
			if not key.startswith(f"{metric_key_prefix}_"):
				metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
		metrics.update(output.metrics)
		return PredictionOutput(predictions=predictions.predictions, label_ids=predictions.label_ids, metrics=metrics)



import json
import logging
import os
import sys
from time import time
from unittest.mock import patch

from transformers.testing_utils import TestCasePlus, require_torch_tpu


logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger()


def get_results(output_dir):
	results = {}
	path = os.path.join(output_dir, "all_results.json")
	if os.path.exists(path):
		with open(path, "r") as f:
			results = json.load(f)
	else:
		raise ValueError(f"can't find {path}")
	return results


stream_handler = logging.StreamHandler(sys.stdout)
logger.addHandler(stream_handler)


@require_torch_tpu
class TorchXLAExamplesTests(TestCasePlus):
	def test_run_glue(self):
		import xla_spawn

		tmp_dir = self.get_auto_remove_tmp_dir()

		with patch.object(sys, "argv", testargs):
			start = time()
			xla_spawn.main()
			end = time()

			result = get_results(tmp_dir)
			self.assertGreaterEqual(result["eval_accuracy"], 0.75)

			self.assertLess(end - start, 500)

	def test_trainer_tpu(self):
		import xla_spawn

		with patch.object(sys, "argv", testargs):
			xla_spawn.main()


import importlib
import logging
import os
from pathlib import Path
import sys
from typing import Iterable, Set

from allennlp.common.util import push_python_path, import_module_and_submodules


logger = logging.getLogger(__name__)


LOCAL_PLUGINS_FILENAME = ".allennlp_plugins"

GLOBAL_PLUGINS_FILENAME = str(Path.home() / ".allennlp" / "plugins")

DEFAULT_PLUGINS = ("allennlp_models", "allennlp_semparse", "allennlp_server")


def discover_file_plugins(plugins_filename: str = LOCAL_PLUGINS_FILENAME) -> Iterable[str]:
	with open(plugins_filename) as file_:
		for module_name in file_.readlines():
			module_name = module_name.strip()
			if module_name:
				yield module_name


def discover_plugins() -> Iterable[str]:
	plugins: Set[str] = set()
	if os.path.isfile(LOCAL_PLUGINS_FILENAME):
		with push_python_path("."):
			for plugin in discover_file_plugins(LOCAL_PLUGINS_FILENAME):
				if plugin in plugins:
					continue
				yield plugin
				plugins.add(plugin)
	if os.path.isfile(GLOBAL_PLUGINS_FILENAME):
		for plugin in discover_file_plugins(GLOBAL_PLUGINS_FILENAME):
			if plugin in plugins:
				continue
			yield plugin
			plugins.add(plugin)


def import_plugins() -> None:
	import_module_and_submodules(
		"allennlp",
		exclude={
			"allennlp.sanity_checks",  # deprecated
			"allennlp.tools",  # things in here are usually run as commands themselves
		},
	)

	cwd = os.getcwd()
	if cwd not in sys.path:
		sys.path.append(cwd)

	for module_name in DEFAULT_PLUGINS:
		try:
			import_module_and_submodules(module_name)
			logger.info("Plugin %s available", module_name)
		except ModuleNotFoundError as e:
			if e.name != module_name:
				logger.error(f"Plugin {module_name} could not be loaded: {e}")
	for module_name in discover_plugins():
		try:
			importlib.import_module(module_name)
			logger.info("Plugin %s available", module_name)
		except ModuleNotFoundError as e:
			logger.error(f"Plugin {module_name} could not be loaded: {e}")

from os import PathLike
from typing import Dict, Iterator, Union, Optional

from allennlp.data.instance import Instance
from allennlp.data.dataset_readers.dataset_reader import (
	DatasetReader,
	WorkerInfo,
	DatasetReaderInput,
)


@DatasetReader.register("multitask")
class MultiTaskDatasetReader(DatasetReader):

	def __init__(self, readers: Dict[str, DatasetReader]) -> None:
		self.readers = {
			task: _MultitaskDatasetReaderShim(reader, task) for task, reader in readers.items()
		}

	def read(  # type: ignore
		self,
		file_paths: Union[PathLike, str, Dict[str, Union[PathLike, str]]],
		*,
		force_task: Optional[str] = None
	) -> Union[Iterator[Instance], Dict[str, Iterator[Instance]]]:
		if force_task is None:
			raise RuntimeError("This class is not designed to be called like this.")
		return self.readers[force_task].read(file_paths)


@DatasetReader.register("multitask_shim")
class _MultitaskDatasetReaderShim(DatasetReader):

	def __init__(self, inner: DatasetReader, head: str, **kwargs):
		super().__init__(**kwargs)
		self.inner = inner
		self.head = head

	def _set_worker_info(self, info: Optional[WorkerInfo]) -> None:
		super()._set_worker_info(info)
		self.inner._set_worker_info(info)

	def read(self, file_path: DatasetReaderInput) -> Iterator[Instance]:
		from allennlp.data.fields import MetadataField

		for instance in self.inner.read(file_path):
			instance.add_field("task", MetadataField(self.head))
			yield instance

	def text_to_instance(self, *inputs) -> Instance:
		from allennlp.data.fields import MetadataField

		instance = self.inner.text_to_instance(*inputs)
		instance.add_field("task", MetadataField(self.head))
		return instance

	def apply_token_indexers(self, instance: Instance) -> None:
		self.inner.apply_token_indexers(instance)

from __future__ import print_function
from itertools import count

import torch
import torch.nn.functional as F

POLY_DEGREE = 4
W_target = torch.randn(POLY_DEGREE, 1) * 5
b_target = torch.randn(1) * 5


def make_features(x):
	x = x.unsqueeze(1)
	return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)


def f(x):
	return x.mm(W_target) + b_target.item()


def poly_desc(W, b):
	result = 'y = '
	for i, w in enumerate(W):
		result += '{:+.2f} x^{} '.format(w, i + 1)
	result += '{:+.2f}'.format(b[0])
	return result


def get_batch(batch_size=32):
	random = torch.randn(batch_size)
	x = make_features(random)
	y = f(x)
	return x, y


fc = torch.nn.Linear(W_target.size(0), 1)

for batch_idx in count(1):
	batch_x, batch_y = get_batch()

	fc.zero_grad()

	output = F.smooth_l1_loss(fc(batch_x), batch_y)
	loss = output.item()

	output.backward()

	for param in fc.parameters():
		param.data.add_(-0.1 * param.grad)

	if loss < 1e-3:
		break

print('Loss: {:.6f} after {} batches'.format(loss, batch_idx))
print('==> Learned function:\t' + poly_desc(fc.weight.view(-1), fc.bias))
print('==> Actual function:\t' + poly_desc(W_target.view(-1), b_target))

from __future__ import division
import logging
import os
import random
import time

import torch
import torchtext
from torch import optim

import seq2seq
from seq2seq.evaluator import Evaluator
from seq2seq.loss import NLLLoss
from seq2seq.optim import Optimizer
from seq2seq.util.checkpoint import Checkpoint

class SupervisedTrainer(object):
	def __init__(self, expt_dir='experiment', loss=NLLLoss(), batch_size=64,
				 random_seed=None,
				 checkpoint_every=100, print_every=100):
		self._trainer = "Simple Trainer"
		self.random_seed = random_seed
		if random_seed is not None:
			random.seed(random_seed)
			torch.manual_seed(random_seed)
		self.loss = loss
		self.evaluator = Evaluator(loss=self.loss, batch_size=batch_size)
		self.optimizer = None
		self.checkpoint_every = checkpoint_every
		self.print_every = print_every

		if not os.path.isabs(expt_dir):
			expt_dir = os.path.join(os.getcwd(), expt_dir)
		self.expt_dir = expt_dir
		if not os.path.exists(self.expt_dir):
			os.makedirs(self.expt_dir)
		self.batch_size = batch_size

		self.logger = logging.getLogger(__name__)

	def _train_batch(self, input_variable, input_lengths, target_variable, model, teacher_forcing_ratio):
		loss = self.loss
		decoder_outputs, decoder_hidden, other = model(input_variable, input_lengths, target_variable,
													   teacher_forcing_ratio=teacher_forcing_ratio)
		loss.reset()
		for step, step_output in enumerate(decoder_outputs):
			batch_size = target_variable.size(0)
			loss.eval_batch(step_output.contiguous().view(batch_size, -1), target_variable[:, step + 1])
		model.zero_grad()
		loss.backward()
		self.optimizer.step()

		return loss.get_loss()

	def _train_epoches(self, data, model, n_epochs, start_epoch, start_step,
					   dev_data=None, teacher_forcing_ratio=0):
		log = self.logger

		print_loss_total = 0  # Reset every print_every
		epoch_loss_total = 0  # Reset every epoch

		device = None if torch.cuda.is_available() else -1
		batch_iterator = torchtext.data.BucketIterator(
			dataset=data, batch_size=self.batch_size,
			sort=False, sort_within_batch=True,
			sort_key=lambda x: len(x.src),
			device=device, repeat=False)

		steps_per_epoch = len(batch_iterator)
		total_steps = steps_per_epoch * n_epochs

		step = start_step
		step_elapsed = 0
		for epoch in range(start_epoch, n_epochs + 1):
			log.debug("Epoch: %d, Step: %d" % (epoch, step))

			batch_generator = batch_iterator.__iter__()
			for _ in range((epoch - 1) * steps_per_epoch, step):
				next(batch_generator)

			model.train(True)
			for batch in batch_generator:
				step += 1
				step_elapsed += 1

				input_variables, input_lengths = getattr(batch, seq2seq.src_field_name)
				target_variables = getattr(batch, seq2seq.tgt_field_name)

				loss = self._train_batch(input_variables, input_lengths.tolist(), target_variables, model, teacher_forcing_ratio)

				print_loss_total += loss
				epoch_loss_total += loss

				if step % self.print_every == 0 and step_elapsed > self.print_every:
					print_loss_avg = print_loss_total / self.print_every
					print_loss_total = 0
					log_msg = 'Progress: %d%%, Train %s: %.4f' % (
						step / total_steps * 100,
						self.loss.name,
						print_loss_avg)
					log.info(log_msg)

				if step % self.checkpoint_every == 0 or step == total_steps:
					Checkpoint(model=model,
							   optimizer=self.optimizer,
							   epoch=epoch, step=step,
							   input_vocab=data.fields[seq2seq.src_field_name].vocab,
							   output_vocab=data.fields[seq2seq.tgt_field_name].vocab).save(self.expt_dir)

			if step_elapsed == 0: continue

			epoch_loss_avg = epoch_loss_total / min(steps_per_epoch, step - start_step)
			epoch_loss_total = 0
			log_msg = "Finished epoch %d: Train %s: %.4f" % (epoch, self.loss.name, epoch_loss_avg)
			if dev_data is not None:
				dev_loss, accuracy = self.evaluator.evaluate(model, dev_data)
				self.optimizer.update(dev_loss, epoch)
				log_msg += ", Dev %s: %.4f, Accuracy: %.4f" % (self.loss.name, dev_loss, accuracy)
				model.train(mode=True)
			else:
				self.optimizer.update(epoch_loss_avg, epoch)

			log.info(log_msg)

	def train(self, model, data, num_epochs=5,
			  resume=False, dev_data=None,
			  optimizer=None, teacher_forcing_ratio=0):
		if resume:
			latest_checkpoint_path = Checkpoint.get_latest_checkpoint(self.expt_dir)
			resume_checkpoint = Checkpoint.load(latest_checkpoint_path)
			model = resume_checkpoint.model
			self.optimizer = resume_checkpoint.optimizer

			resume_optim = self.optimizer.optimizer
			defaults = resume_optim.param_groups[0]
			defaults.pop('params', None)
			defaults.pop('initial_lr', None)
			self.optimizer.optimizer = resume_optim.__class__(model.parameters(), **defaults)

			start_epoch = resume_checkpoint.epoch
			step = resume_checkpoint.step
		else:
			start_epoch = 1
			step = 0
			if optimizer is None:
				optimizer = Optimizer(optim.Adam(model.parameters()), max_grad_norm=5)
			self.optimizer = optimizer

		self.logger.info("Optimizer: %s, Scheduler: %s" % (self.optimizer.optimizer, self.optimizer.scheduler))

		self._train_epoches(data, model, num_epochs,
							start_epoch, step, dev_data=dev_data,
							teacher_forcing_ratio=teacher_forcing_ratio)
		return model

import collections.abc
from copy import deepcopy
from pathlib import Path
from typing import (
	Any,
	Callable,
	cast,
	Dict,
	Iterable,
	List,
	Mapping,
	Set,
	Tuple,
	Type,
	TypeVar,
	Union,
	Optional,
)
import inspect
import logging

from allennlp.common.checks import ConfigurationError
from allennlp.common.lazy import Lazy
from allennlp.common.params import Params

logger = logging.getLogger(__name__)

T = TypeVar("T", bound="FromParams")

_NO_DEFAULT = inspect.Parameter.empty


def takes_arg(obj, arg: str) -> bool:
	if inspect.isclass(obj):
		signature = inspect.signature(obj.__init__)
	elif inspect.ismethod(obj) or inspect.isfunction(obj):
		signature = inspect.signature(obj)
	else:
		raise ConfigurationError(f"object {obj} is not callable")
	return arg in signature.parameters


def takes_kwargs(obj) -> bool:
	if inspect.isclass(obj):
		signature = inspect.signature(obj.__init__)
	elif inspect.ismethod(obj) or inspect.isfunction(obj):
		signature = inspect.signature(obj)
	else:
		raise ConfigurationError(f"object {obj} is not callable")
	return any(
		p.kind == inspect.Parameter.VAR_KEYWORD  # type: ignore
		for p in signature.parameters.values()
	)


def can_construct_from_params(type_: Type) -> bool:
	if type_ in [str, int, float, bool]:
		return True
	origin = getattr(type_, "__origin__", None)
	if origin == Lazy:
		return True
	elif origin:
		if hasattr(type_, "from_params"):
			return True
		args = getattr(type_, "__args__")
		return all(can_construct_from_params(arg) for arg in args)

	return hasattr(type_, "from_params")


def is_base_registrable(cls) -> bool:
	from allennlp.common.registrable import Registrable  # import here to avoid circular imports

	if not issubclass(cls, Registrable):
		return False
	method_resolution_order = inspect.getmro(cls)[1:]
	for base_class in method_resolution_order:
		if issubclass(base_class, Registrable) and base_class is not Registrable:
			return False
	return True


def remove_optional(annotation: type):
	origin = getattr(annotation, "__origin__", None)
	args = getattr(annotation, "__args__", ())

	if origin == Union:
		return Union[tuple([arg for arg in args if arg != type(None)])]  # noqa: E721
	else:
		return annotation


def infer_constructor_params(
	cls: Type[T], constructor: Union[Callable[..., T], Callable[[T], None]] = None
) -> Dict[str, inspect.Parameter]:
	if constructor is None:
		constructor = cls.__init__
	return infer_method_params(cls, constructor)


infer_params = infer_constructor_params  # Legacy name


def infer_method_params(cls: Type[T], method: Callable) -> Dict[str, inspect.Parameter]:
	signature = inspect.signature(method)
	parameters = dict(signature.parameters)

	has_kwargs = False
	var_positional_key = None
	for param in parameters.values():
		if param.kind == param.VAR_KEYWORD:
			has_kwargs = True
		elif param.kind == param.VAR_POSITIONAL:
			var_positional_key = param.name

	if var_positional_key:
		del parameters[var_positional_key]

	if not has_kwargs:
		return parameters

	super_class = None
	for super_class_candidate in cls.mro()[1:]:
		if issubclass(super_class_candidate, FromParams):
			super_class = super_class_candidate
			break
	if super_class:
		super_parameters = infer_params(super_class)
	else:
		super_parameters = {}

	return {**super_parameters, **parameters}  # Subclass parameters overwrite superclass ones


def create_kwargs(
	constructor: Callable[..., T], cls: Type[T], params: Params, **extras
) -> Dict[str, Any]:

	kwargs: Dict[str, Any] = {}

	parameters = infer_params(cls, constructor)
	accepts_kwargs = False

	for param_name, param in parameters.items():
		if param_name == "self":
			continue

		if param.kind == param.VAR_KEYWORD:
			accepts_kwargs = True
			continue

		annotation = remove_optional(param.annotation)

		explicitly_set = param_name in params
		constructed_arg = pop_and_construct_arg(
			cls.__name__, param_name, annotation, param.default, params, **extras
		)

		if explicitly_set or constructed_arg is not param.default:
			kwargs[param_name] = constructed_arg

	if accepts_kwargs:
		kwargs.update(params)
	else:
		params.assert_empty(cls.__name__)
	return kwargs


def create_extras(cls: Type[T], extras: Dict[str, Any]) -> Dict[str, Any]:
	subextras: Dict[str, Any] = {}
	if hasattr(cls, "from_params"):
		from_params_method = cls.from_params  # type: ignore
	else:
		from_params_method = cls
	if takes_kwargs(from_params_method):
		subextras = extras
	else:
		subextras = {k: v for k, v in extras.items() if takes_arg(from_params_method, k)}
	return subextras


def pop_and_construct_arg(
	class_name: str, argument_name: str, annotation: Type, default: Any, params: Params, **extras
) -> Any:
	from allennlp.models.archival import load_archive  # import here to avoid circular imports

	name = argument_name

	if name in extras:
		if name not in params:
			return extras[name]
		else:
			logger.warning(
				f"Parameter {name} for class {class_name} was found in both "
				"**extras and in params. Using the specification found in params, "
				"but you probably put a key in a config file that you didn't need, "
				"and if it is different from what we get from **extras, you might "
				"get unexpected behavior."
			)
	elif (
		name in params
		and isinstance(params.get(name), Params)
		and "_pretrained" in params.get(name)
	):
		load_module_params = params.pop(name).pop("_pretrained")
		archive_file = load_module_params.pop("archive_file")
		module_path = load_module_params.pop("module_path")
		freeze = load_module_params.pop("freeze", True)
		archive = load_archive(archive_file)
		result = archive.extract_module(module_path, freeze)
		if not isinstance(result, annotation):
			raise ConfigurationError(
				f"The module from model at {archive_file} at path {module_path} "
				f"was expected of type {annotation} but is of type {type(result)}"
			)
		return result

	popped_params = params.pop(name, default) if default != _NO_DEFAULT else params.pop(name)
	if popped_params is None:
		return None

	return construct_arg(class_name, name, popped_params, annotation, default, **extras)


def construct_arg(
	class_name: str,
	argument_name: str,
	popped_params: Params,
	annotation: Type,
	default: Any,
	**extras,
) -> Any:
	origin = getattr(annotation, "__origin__", None)
	args = getattr(annotation, "__args__", [])

	optional = default != _NO_DEFAULT

	if hasattr(annotation, "from_params"):
		if popped_params is default:
			return default
		elif popped_params is not None:

			subextras = create_extras(annotation, extras)

			if isinstance(popped_params, str):
				popped_params = Params({"type": popped_params})
			elif isinstance(popped_params, dict):
				popped_params = Params(popped_params)
			result = annotation.from_params(params=popped_params, **subextras)

			return result
		elif not optional:
			raise ConfigurationError(f"expected key {argument_name} for {class_name}")
		else:
			return default

	elif annotation in {int, bool}:
		if type(popped_params) in {int, bool}:
			return annotation(popped_params)
		else:
			raise TypeError(f"Expected {argument_name} to be a {annotation.__name__}.")
	elif annotation == str:
		if type(popped_params) == str or isinstance(popped_params, Path):
			return str(popped_params)  # type: ignore
		else:
			raise TypeError(f"Expected {argument_name} to be a string.")
	elif annotation == float:
		if type(popped_params) in {int, float}:
			return popped_params
		else:
			raise TypeError(f"Expected {argument_name} to be numeric.")

	elif (
		origin in {collections.abc.Mapping, Mapping, Dict, dict}
		and len(args) == 2
		and can_construct_from_params(args[-1])
	):
		value_cls = annotation.__args__[-1]
		value_dict = {}
		if not isinstance(popped_params, Mapping):
			raise TypeError(
				f"Expected {argument_name} to be a Mapping (probably a dict or a Params object)."
			)

		for key, value_params in popped_params.items():
			value_dict[key] = construct_arg(
				str(value_cls),
				argument_name + "." + key,
				value_params,
				value_cls,
				_NO_DEFAULT,
				**extras,
			)

		return value_dict

	elif origin in (Tuple, tuple) and all(can_construct_from_params(arg) for arg in args):
		value_list = []

		for i, (value_cls, value_params) in enumerate(zip(annotation.__args__, popped_params)):
			value = construct_arg(
				str(value_cls),
				argument_name + f".{i}",
				value_params,
				value_cls,
				_NO_DEFAULT,
				**extras,
			)
			value_list.append(value)

		return tuple(value_list)

	elif origin in (Set, set) and len(args) == 1 and can_construct_from_params(args[0]):
		value_cls = annotation.__args__[0]

		value_set = set()

		for i, value_params in enumerate(popped_params):
			value = construct_arg(
				str(value_cls),
				argument_name + f".{i}",
				value_params,
				value_cls,
				_NO_DEFAULT,
				**extras,
			)
			value_set.add(value)

		return value_set

	elif origin == Union:
		backup_params = deepcopy(popped_params)

		error_chain: Optional[Exception] = None
		for arg_annotation in args:
			try:
				return construct_arg(
					str(arg_annotation),
					argument_name,
					popped_params,
					arg_annotation,
					default,
					**extras,
				)
			except (ValueError, TypeError, ConfigurationError, AttributeError) as e:
				popped_params = deepcopy(backup_params)
				e.args = (f"While constructing an argument of type {arg_annotation}",) + e.args
				e.__cause__ = error_chain
				error_chain = e

		config_error = ConfigurationError(
			f"Failed to construct argument {argument_name} with type {annotation}."
		)
		config_error.__cause__ = error_chain
		raise config_error
	elif origin == Lazy:
		if popped_params is default:
			return default

		value_cls = args[0]
		subextras = create_extras(value_cls, extras)
		return Lazy(value_cls, params=deepcopy(popped_params), constructor_extras=subextras)  # type: ignore

	elif (
		origin in {collections.abc.Iterable, Iterable, List, list}
		and len(args) == 1
		and can_construct_from_params(args[0])
	):
		value_cls = annotation.__args__[0]

		value_list = []

		for i, value_params in enumerate(popped_params):
			value = construct_arg(
				str(value_cls),
				argument_name + f".{i}",
				value_params,
				value_cls,
				_NO_DEFAULT,
				**extras,
			)
			value_list.append(value)

		return value_list

	else:
		if isinstance(popped_params, Params):
			return popped_params.as_dict()
		return popped_params


class FromParams:

	@classmethod
	def from_params(
		cls: Type[T],
		params: Params,
		constructor_to_call: Callable[..., T] = None,
		constructor_to_inspect: Union[Callable[..., T], Callable[[T], None]] = None,
		**extras,
	) -> T:

		from allennlp.common.registrable import Registrable  # import here to avoid circular imports

		logger.debug(
			f"instantiating class {cls} from params {getattr(params, 'params', params)} "
			f"and extras {set(extras.keys())}"
		)

		if params is None:
			return None

		if isinstance(params, str):
			params = Params({"type": params})

		if not isinstance(params, Params):
			raise ConfigurationError(
				"from_params was passed a `params` object that was not a `Params`. This probably "
				"indicates malformed parameters in a configuration file, where something that "
				"should have been a dictionary was actually a list, or something else. "
				f"This happened when constructing an object of type {cls}."
			)

		registered_subclasses = Registrable._registry.get(cls)

		if is_base_registrable(cls) and registered_subclasses is None:
			raise ConfigurationError(
				"Tried to construct an abstract Registrable base class that has no registered "
				"concrete types. This might mean that you need to use --include-package to get "
				"your concrete classes actually registered."
			)

		if registered_subclasses is not None and not constructor_to_call:

			as_registrable = cast(Type[Registrable], cls)
			default_to_first_choice = as_registrable.default_implementation is not None
			choice = params.pop_choice(
				"type",
				choices=as_registrable.list_available(),
				default_to_first_choice=default_to_first_choice,
			)
			subclass, constructor_name = as_registrable.resolve_class_name(choice)
			if not constructor_name:
				constructor_to_inspect = subclass.__init__
				constructor_to_call = subclass  # type: ignore
			else:
				constructor_to_inspect = cast(Callable[..., T], getattr(subclass, constructor_name))
				constructor_to_call = constructor_to_inspect

			if hasattr(subclass, "from_params"):
				extras = create_extras(subclass, extras)
				retyped_subclass = cast(Type[T], subclass)
				return retyped_subclass.from_params(
					params=params,
					constructor_to_call=constructor_to_call,
					constructor_to_inspect=constructor_to_inspect,
					**extras,
				)
			else:
				return subclass(**params)  # type: ignore
		else:

			if not constructor_to_inspect:
				constructor_to_inspect = cls.__init__
			if not constructor_to_call:
				constructor_to_call = cls

			if constructor_to_inspect == object.__init__:
				kwargs: Dict[str, Any] = {}
				params.assert_empty(cls.__name__)
			else:
				constructor_to_inspect = cast(Callable[..., T], constructor_to_inspect)
				kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)

			return constructor_to_call(**kwargs)  # type: ignore

	def to_params(self) -> Params:

		def replace_object_with_params(o: Any) -> Any:
			if isinstance(o, FromParams):
				return o.to_params()
			elif isinstance(o, List):
				return [replace_object_with_params(i) for i in o]
			elif isinstance(o, Set):
				return {replace_object_with_params(i) for i in o}
			elif isinstance(o, Dict):
				return {key: replace_object_with_params(value) for key, value in o.items()}
			else:
				return o

		return Params(replace_object_with_params(self._to_params()))

	def _to_params(self) -> Dict[str, Any]:
		raise NotImplementedError()

from allennlp.common.from_params import FromParams
from allennlp.common.lazy import Lazy
from allennlp.common.params import Params
from allennlp.common.registrable import Registrable
from allennlp.common.tqdm import Tqdm
from allennlp.common.util import JsonDict
from allennlp.common.meta import Meta
from allennlp.common.push_to_hf import push_to_hf


import argparse
import json
import logging
import os


from allennlp.commands.subcommand import Subcommand

logger = logging.getLogger(__name__)


@Subcommand.register("print-results")
class PrintResults(Subcommand):
	requires_plugins: bool = False

	def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:

		description = """Print results from allennlp training runs in a helpful CSV format."""
		subparser = parser.add_parser(
			self.name,
			description=description,
			help="Print results from allennlp serialization directories to the console.",
		)
		subparser.add_argument(
			"path",
			type=str,
			help="Path to recursively search for allennlp serialization directories.",
		)

		subparser.add_argument(
			"-k",
			"--keys",
			type=str,
			nargs="+",
			help="Keys to print from metrics.json."
			'Keys not present in all metrics.json will result in "N/A"',
			default=None,
			required=False,
		)
		subparser.add_argument(
			"-m",
			"--metrics-filename",
			type=str,
			help="Name of the metrics file to inspect.",
			default="metrics.json",
			required=False,
		)

		subparser.set_defaults(func=print_results_from_args)
		return subparser


def print_results_from_args(args: argparse.Namespace):
	path = args.path
	metrics_name = args.metrics_filename
	keys = args.keys

	results_dict = {}
	for root, _, files in os.walk(path):
		if metrics_name in files:
			full_name = os.path.join(root, metrics_name)
			with open(full_name) as file_:
				metrics = json.load(file_)
			results_dict[full_name] = metrics

	sorted_keys = sorted(list(results_dict.keys()))
	print(f"model_run, {', '.join(keys)}")
	for name in sorted_keys:
		results = results_dict[name]
		keys_to_print = (str(results.get(key, "N/A")) for key in keys)
		print(f"{name}, {', '.join(keys_to_print)}")

import torch


class ShardedModuleMixin:

	def get_original_module(self) -> torch.nn.Module:
		raise NotImplementedError

from allennlp.confidence_checks.verification_base import VerificationBase
from allennlp.confidence_checks.normalization_bias_verification import NormalizationBiasVerification



from allennlp.fairness.bias_mitigator_wrappers import BiasMitigatorWrapper

from allennlp.common.lazy import Lazy
from allennlp.data import Vocabulary
from allennlp.models import Model
from allennlp.nn.util import find_embedding_layer


@Model.register("bias_mitigator_applicator")
class BiasMitigatorApplicator(Model):

	def __init__(
		self,
		vocab: Vocabulary,
		base_model: Model,
		bias_mitigator: Lazy[BiasMitigatorWrapper],
		**kwargs
	):
		super().__init__(vocab, **kwargs)

		self.base_model = base_model
		embedding_layer = find_embedding_layer(self.base_model)

		self.bias_mitigator = bias_mitigator.construct(embedding_layer=embedding_layer)
		embedding_layer.register_forward_hook(self.bias_mitigator)

		self.vocab = self.base_model.vocab
		self._regularizer = self.base_model._regularizer

	def train(self, mode: bool = True):
		super().train(mode)
		self.base_model.train(mode)
		self.bias_mitigator.train(mode)


	def forward(self, *args, **kwargs):
		return self.base_model.forward(*args, **kwargs)

	def forward_on_instance(self, *args, **kwargs):
		return self.base_model.forward_on_instance(*args, **kwargs)

	def forward_on_instances(self, *args, **kwargs):
		return self.base_model.forward_on_instances(*args, **kwargs)

	def get_regularization_penalty(self, *args, **kwargs):
		return self.base_model.get_regularization_penalty(*args, **kwargs)

	def get_parameters_for_histogram_logging(self, *args, **kwargs):
		return self.base_model.get_parameters_for_histogram_logging(*args, **kwargs)

	def get_parameters_for_histogram_tensorboard_logging(self, *args, **kwargs):
		return self.base_model.get_parameters_for_histogram_tensorboard_logging(*args, **kwargs)

	def make_output_human_readable(self, *args, **kwargs):
		return self.base_model.make_output_human_readable(*args, **kwargs)

	def get_metrics(self, *args, **kwargs):
		return self.base_model.get_metrics(*args, **kwargs)

	def _get_prediction_device(self, *args, **kwargs):
		return self.base_model._get_prediction_device(*args, **kwargs)

	def _maybe_warn_for_unseparable_batches(self, *args, **kwargs):
		return self.base_model._maybe_warn_for_unseparable_batches(*args, **kwargs)

	def extend_embedder_vocab(self, *args, **kwargs):
		return self.base_model.extend_embedder_vocab(*args, **kwargs)


from allennlp.data.fields.field import Field
from allennlp.data.fields.adjacency_field import AdjacencyField
from allennlp.data.fields.tensor_field import TensorField
from allennlp.data.fields.flag_field import FlagField
from allennlp.data.fields.index_field import IndexField
from allennlp.data.fields.label_field import LabelField
from allennlp.data.fields.list_field import ListField
from allennlp.data.fields.metadata_field import MetadataField
from allennlp.data.fields.multilabel_field import MultiLabelField
from allennlp.data.fields.namespace_swapping_field import NamespaceSwappingField
from allennlp.data.fields.sequence_field import SequenceField
from allennlp.data.fields.sequence_label_field import SequenceLabelField
from allennlp.data.fields.span_field import SpanField
from allennlp.data.fields.text_field import TextField
from allennlp.data.fields.array_field import ArrayField
from allennlp.data.fields.transformer_text_field import TransformerTextField

import torch.nn as nn

from .attention import MultiHeadedAttention
from .utils import SublayerConnection, PositionwiseFeedForward


class TransformerBlock(nn.Module):

	def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):

		super().__init__()
		self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)
		self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)
		self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)
		self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)
		self.dropout = nn.Dropout(p=dropout)

	def forward(self, x, mask):
		x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))
		x = self.output_sublayer(x, self.feed_forward)
		return self.dropout(x)

from .mixed_precision import *
from .wrapping import *
from .activation_checkpointing_functions import apply_fsdp_checkpointing

import torch
from torch.fx import Proxy, Graph, GraphModule




graph = Graph()
tracer = torch.fx.proxy.GraphAppendingTracer(graph)

raw1 = graph.placeholder('x')
raw2 = graph.placeholder('y')

y = Proxy(raw1, tracer)
z = Proxy(raw2, tracer)

a = torch.cat([y, z])
b = torch.tanh(a)
c = torch.neg(b)
z = torch.add(b, c)

graph.output(c.node)

mod = GraphModule(torch.nn.Module(), graph)

import copy
import json
from os import PathLike
import random
from typing import Any, Dict, Iterable, Set, Union

import torch
import numpy
from numpy.testing import assert_allclose

from allennlp.commands.train import train_model_from_file
from allennlp.common import Params
from allennlp.common.testing.test_case import AllenNlpTestCase
from allennlp.data import DatasetReader, Vocabulary
from allennlp.data import DataLoader
from allennlp.data.batch import Batch
from allennlp.models import load_archive, Model
from allennlp.training import GradientDescentTrainer
from allennlp.confidence_checks.normalization_bias_verification import NormalizationBiasVerification


class ModelTestCase(AllenNlpTestCase):

	def set_up_model(
		self,
		param_file: PathLike,
		dataset_file: PathLike,
		serialization_dir: PathLike = None,
		seed: int = None,
	):
		if seed is not None:
			random.seed(seed)
			numpy.random.seed(seed)
			torch.manual_seed(seed)

		self.param_file = str(param_file)
		params = Params.from_file(self.param_file)

		reader = DatasetReader.from_params(
			params["dataset_reader"], serialization_dir=serialization_dir
		)
		instances = list(reader.read(str(dataset_file)))
		if "vocabulary" in params:
			vocab_params = params["vocabulary"]
			vocab = Vocabulary.from_params(params=vocab_params, instances=instances)
		else:
			vocab = Vocabulary.from_instances(instances)
		self.vocab = vocab
		self.instances = instances
		self.model = Model.from_params(
			vocab=self.vocab, params=params["model"], serialization_dir=serialization_dir
		)

		self.dataset = Batch(self.instances)
		self.dataset.index_instances(self.vocab)

	def test_model_batch_norm_verification(self):
		if hasattr(self, "model"):  # TODO: can this be done using pytest.skipif?
			verification = NormalizationBiasVerification(self.model)
			assert verification.check(inputs=self.dataset.as_tensor_dict())

	def ensure_model_can_train_save_and_load(
		self,
		param_file: Union[PathLike, str],
		tolerance: float = 1e-4,
		cuda_device: int = -1,
		gradients_to_ignore: Set[str] = None,
		overrides: str = "",
		metric_to_check: str = None,
		metric_terminal_value: float = None,
		metric_tolerance: float = 1e-4,
		disable_dropout: bool = True,
		which_loss: str = "loss",
		seed: int = None,
	):
		save_dir = self.TEST_DIR / "save_and_load_test"
		archive_file = save_dir / "model.tar.gz"
		model = train_model_from_file(param_file, save_dir, overrides=overrides, return_model=True)
		assert model is not None

		metrics_file = save_dir / "metrics.json"
		if metric_to_check is not None:
			metrics = json.loads(metrics_file.read_text())
			metric_value = metrics.get(f"best_validation_{metric_to_check}") or metrics.get(
				f"training_{metric_to_check}"
			)
			assert metric_value is not None, f"Cannot find {metric_to_check} in metrics.json file"
			assert metric_terminal_value is not None, "Please specify metric terminal value"
			assert abs(metric_value - metric_terminal_value) < metric_tolerance
		archive = load_archive(archive_file, cuda_device=cuda_device)
		loaded_model = archive.model
		state_keys = model.state_dict().keys()
		loaded_state_keys = loaded_model.state_dict().keys()
		assert state_keys == loaded_state_keys
		for key in state_keys:
			assert_allclose(
				model.state_dict()[key].cpu().numpy(),
				loaded_model.state_dict()[key].cpu().numpy(),
				err_msg=key,
			)
		reader = archive.dataset_reader
		params = Params.from_file(param_file, params_overrides=overrides)

		data_loader_params = params["data_loader"]
		data_loader_params["shuffle"] = False
		data_loader_params2 = Params(copy.deepcopy(data_loader_params.as_dict()))

		if seed is not None:
			random.seed(seed)
			numpy.random.seed(seed)
			torch.manual_seed(seed)

		print("Reading with original model")
		data_loader = DataLoader.from_params(
			params=data_loader_params, reader=reader, data_path=params["validation_data_path"]
		)
		data_loader.index_with(model.vocab)

		if seed is not None:
			random.seed(seed)
			numpy.random.seed(seed)
			torch.manual_seed(seed)

		print("Reading with loaded model")
		data_loader2 = DataLoader.from_params(
			params=data_loader_params2, reader=reader, data_path=params["validation_data_path"]
		)
		data_loader2.index_with(loaded_model.vocab)

		model_batch = next(iter(data_loader))

		loaded_batch = next(iter(data_loader2))

		self.check_model_computes_gradients_correctly(
			model, model_batch, gradients_to_ignore, disable_dropout, which_loss
		)

		assert model_batch.keys() == loaded_batch.keys()
		for key in model_batch.keys():
			self.assert_fields_equal(model_batch[key], loaded_batch[key], key, 1e-6)

		model.eval()
		loaded_model.eval()
		for model_ in [model, loaded_model]:
			for module in model_.modules():
				if hasattr(module, "stateful") and module.stateful:
					module.reset_states()
		print("Predicting with original model")
		model_predictions = model(**model_batch)
		print("Predicting with loaded model")
		loaded_model_predictions = loaded_model(**loaded_batch)

		for key in model_predictions.keys():
			self.assert_fields_equal(
				model_predictions[key], loaded_model_predictions[key], name=key, tolerance=tolerance
			)

		loaded_model.train()
		loaded_model_predictions = loaded_model(**loaded_batch)
		loaded_model_loss = loaded_model_predictions[which_loss]
		assert loaded_model_loss is not None
		loaded_model_loss.backward()

		return model, loaded_model

	def ensure_model_can_train(
		self,
		trainer: GradientDescentTrainer,
		gradients_to_ignore: Set[str] = None,
		metric_to_check: str = None,
		metric_terminal_value: float = None,
		metric_tolerance: float = 1e-4,
		disable_dropout: bool = True,
	):
		metrics = trainer.train()
		if metric_to_check is not None:
			metric_value = metrics.get(f"best_validation_{metric_to_check}") or metrics.get(
				f"training_{metric_to_check}"
			)
			assert metric_value is not None, f"Cannot find {metric_to_check} in metrics.json file"
			assert metric_terminal_value is not None, "Please specify metric terminal value"
			assert abs(metric_value - metric_terminal_value) < metric_tolerance

		model_batch = next(iter(trainer.data_loader))

		self.check_model_computes_gradients_correctly(
			trainer.model, model_batch, gradients_to_ignore, disable_dropout
		)

	def assert_fields_equal(self, field1, field2, name: str, tolerance: float = 1e-6) -> None:
		if isinstance(field1, torch.Tensor):
			assert_allclose(
				field1.detach().cpu().numpy(),
				field2.detach().cpu().numpy(),
				rtol=tolerance,
				err_msg=name,
			)
		elif isinstance(field1, dict):
			assert field1.keys() == field2.keys()
			for key in field1:
				self.assert_fields_equal(
					field1[key], field2[key], tolerance=tolerance, name=name + "." + str(key)
				)
		elif isinstance(field1, (list, tuple)):
			assert len(field1) == len(field2)
			for i, (subfield1, subfield2) in enumerate(zip(field1, field2)):
				self.assert_fields_equal(
					subfield1, subfield2, tolerance=tolerance, name=name + f"[{i}]"
				)
		elif isinstance(field1, (float, int)):
			assert_allclose([field1], [field2], rtol=tolerance, err_msg=name)
		else:
			if field1 != field2:
				for key in field1.__dict__:
					print(key, getattr(field1, key) == getattr(field2, key))
			assert field1 == field2, f"{name}, {type(field1)}, {type(field2)}"

	@staticmethod
	def check_model_computes_gradients_correctly(
		model: Model,
		model_batch: Dict[str, Union[Any, Dict[str, Any]]],
		params_to_ignore: Set[str] = None,
		disable_dropout: bool = True,
		which_loss: str = "loss",
	):
		print("Checking gradients")
		for p in model.parameters():
			p.grad = None
		model.train()

		original_dropouts: Dict[str, float] = {}

		if disable_dropout:
			for name, module in model.named_modules():
				if isinstance(module, torch.nn.Dropout):
					original_dropouts[name] = getattr(module, "p")
					setattr(module, "p", 0)

		result = model(**model_batch)
		result[which_loss].backward()
		has_zero_or_none_grads = {}
		for name, parameter in model.named_parameters():
			zeros = torch.zeros(parameter.size())
			if params_to_ignore and name in params_to_ignore:
				continue
			if parameter.requires_grad:

				if parameter.grad is None:
					has_zero_or_none_grads[
						name
					] = "No gradient computed (i.e parameter.grad is None)"

				elif parameter.grad.is_sparse or parameter.grad.data.is_sparse:
					pass

				elif (parameter.grad.cpu() == zeros).all():
					has_zero_or_none_grads[
						name
					] = f"zeros with shape ({tuple(parameter.grad.size())})"
			else:
				assert parameter.grad is None

		if has_zero_or_none_grads:
			for name, grad in has_zero_or_none_grads.items():
				print(f"Parameter: {name} had incorrect gradient: {grad}")
			raise Exception("Incorrect gradients found. See stdout for more info.")

		if disable_dropout:
			for name, module in model.named_modules():
				if name in original_dropouts:
					setattr(module, "p", original_dropouts[name])

	def ensure_batch_predictions_are_consistent(self, keys_to_ignore: Iterable[str] = ()):
		self.model.eval()
		single_predictions = []
		for i, instance in enumerate(self.instances):
			dataset = Batch([instance])
			tensors = dataset.as_tensor_dict(dataset.get_padding_lengths())
			result = self.model(**tensors)
			single_predictions.append(result)
		full_dataset = Batch(self.instances)
		batch_tensors = full_dataset.as_tensor_dict(full_dataset.get_padding_lengths())
		batch_predictions = self.model(**batch_tensors)
		for i, instance_predictions in enumerate(single_predictions):
			for key, single_predicted in instance_predictions.items():
				tolerance = 1e-6
				if "loss" in key:
					continue
				if key in keys_to_ignore:
					continue
				single_predicted = single_predicted[0]
				batch_predicted = batch_predictions[key][i]
				if isinstance(single_predicted, torch.Tensor):
					if single_predicted.size() != batch_predicted.size():
						slices = tuple(slice(0, size) for size in single_predicted.size())
						batch_predicted = batch_predicted[slices]
					assert_allclose(
						single_predicted.data.numpy(),
						batch_predicted.data.numpy(),
						atol=tolerance,
						err_msg=key,
					)
				else:
					assert single_predicted == batch_predicted, key

import argparse
import time
import math
import os
import torch
import torch.nn as nn
import torch.onnx

import data
import model

parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM/GRU/Transformer Language Model')
parser.add_argument('--data', type=str, default='./data/wikitext-2',
					help='location of the data corpus')
parser.add_argument('--model', type=str, default='LSTM',
					help='type of network (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)')
parser.add_argument('--emsize', type=int, default=200,
					help='size of word embeddings')
parser.add_argument('--nhid', type=int, default=200,
					help='number of hidden units per layer')
parser.add_argument('--nlayers', type=int, default=2,
					help='number of layers')
parser.add_argument('--lr', type=float, default=20,
					help='initial learning rate')
parser.add_argument('--clip', type=float, default=0.25,
					help='gradient clipping')
parser.add_argument('--epochs', type=int, default=40,
					help='upper epoch limit')
parser.add_argument('--batch_size', type=int, default=20, metavar='N',
					help='batch size')
parser.add_argument('--bptt', type=int, default=35,
					help='sequence length')
parser.add_argument('--dropout', type=float, default=0.2,
					help='dropout applied to layers (0 = no dropout)')
parser.add_argument('--tied', action='store_true',
					help='tie the word embedding and softmax weights')
parser.add_argument('--seed', type=int, default=1111,
					help='random seed')
parser.add_argument('--cuda', action='store_true', default=False,
					help='use CUDA')
parser.add_argument('--mps', action='store_true', default=False,
						help='enables macOS GPU training')
parser.add_argument('--log-interval', type=int, default=200, metavar='N',
					help='report interval')
parser.add_argument('--save', type=str, default='model.pt',
					help='path to save the final model')
parser.add_argument('--onnx-export', type=str, default='',
					help='path to export the final model in onnx format')
parser.add_argument('--nhead', type=int, default=2,
					help='the number of heads in the encoder/decoder of the transformer model')
parser.add_argument('--dry-run', action='store_true',
					help='verify the code and the model')
args = parser.parse_args()

torch.manual_seed(args.seed)
if torch.cuda.is_available():
	if not args.cuda:
		print("WARNING: You have a CUDA device, so you should probably run with --cuda.")
if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
	if not args.mps:
		print("WARNING: You have mps device, to enable macOS GPU run with --mps.")

use_mps = args.mps and torch.backends.mps.is_available()
if args.cuda:
	device = torch.device("cuda")
elif use_mps:
	device = torch.device("mps")
else:
	device = torch.device("cpu")


corpus = data.Corpus(args.data)


def batchify(data, bsz):
	nbatch = data.size(0) // bsz
	data = data.narrow(0, 0, nbatch * bsz)
	data = data.view(bsz, -1).t().contiguous()
	return data.to(device)

eval_batch_size = 10
train_data = batchify(corpus.train, args.batch_size)
val_data = batchify(corpus.valid, eval_batch_size)
test_data = batchify(corpus.test, eval_batch_size)


ntokens = len(corpus.dictionary)
if args.model == 'Transformer':
	model = model.TransformerModel(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)
else:
	model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)

criterion = nn.NLLLoss()


def repackage_hidden(h):

	if isinstance(h, torch.Tensor):
		return h.detach()
	else:
		return tuple(repackage_hidden(v) for v in h)



def get_batch(source, i):
	seq_len = min(args.bptt, len(source) - 1 - i)
	data = source[i:i+seq_len]
	target = source[i+1:i+1+seq_len].view(-1)
	return data, target


def evaluate(data_source):
	model.eval()
	total_loss = 0.
	ntokens = len(corpus.dictionary)
	if args.model != 'Transformer':
		hidden = model.init_hidden(eval_batch_size)
	with torch.no_grad():
		for i in range(0, data_source.size(0) - 1, args.bptt):
			data, targets = get_batch(data_source, i)
			if args.model == 'Transformer':
				output = model(data)
				output = output.view(-1, ntokens)
			else:
				output, hidden = model(data, hidden)
				hidden = repackage_hidden(hidden)
			total_loss += len(data) * criterion(output, targets).item()
	return total_loss / (len(data_source) - 1)


def train():
	model.train()
	total_loss = 0.
	start_time = time.time()
	ntokens = len(corpus.dictionary)
	if args.model != 'Transformer':
		hidden = model.init_hidden(args.batch_size)
	for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):
		data, targets = get_batch(train_data, i)
		model.zero_grad()
		if args.model == 'Transformer':
			output = model(data)
			output = output.view(-1, ntokens)
		else:
			hidden = repackage_hidden(hidden)
			output, hidden = model(data, hidden)
		loss = criterion(output, targets)
		loss.backward()

		torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)
		for p in model.parameters():
			p.data.add_(p.grad, alpha=-lr)

		total_loss += loss.item()

		if batch % args.log_interval == 0 and batch > 0:
			cur_loss = total_loss / args.log_interval
			elapsed = time.time() - start_time
			print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '
					'loss {:5.2f} | ppl {:8.2f}'.format(
				epoch, batch, len(train_data) // args.bptt, lr,
				elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))
			total_loss = 0
			start_time = time.time()
		if args.dry_run:
			break


def export_onnx(path, batch_size, seq_len):
	print('The model is also exported in ONNX format at {}.'.format(os.path.realpath(args.onnx_export)))
	model.eval()
	dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)
	hidden = model.init_hidden(batch_size)
	torch.onnx.export(model, (dummy_input, hidden), path)


lr = args.lr
best_val_loss = None

try:
	for epoch in range(1, args.epochs+1):
		epoch_start_time = time.time()
		train()
		val_loss = evaluate(val_data)
		print('-' * 89)
		print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
				'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
										   val_loss, math.exp(val_loss)))
		print('-' * 89)
		if not best_val_loss or val_loss < best_val_loss:
			with open(args.save, 'wb') as f:
				torch.save(model, f)
			best_val_loss = val_loss
		else:
			lr /= 4.0
except KeyboardInterrupt:
	print('-' * 89)
	print('Exiting from training early')

with open(args.save, 'rb') as f:
	model = torch.load(f)
	if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:
		model.rnn.flatten_parameters()

test_loss = evaluate(test_data)
print('=' * 89)
print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(
	test_loss, math.exp(test_loss)))
print('=' * 89)

if len(args.onnx_export) > 0:
	export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)

from typing import Dict


import torch

from allennlp.data.fields.field import Field
from allennlp.data.fields.sequence_field import SequenceField
from allennlp.common.checks import ConfigurationError


class IndexField(Field[torch.Tensor]):

	__slots__ = ["sequence_index", "sequence_field"]

	def __init__(self, index: int, sequence_field: SequenceField) -> None:
		self.sequence_index = index
		self.sequence_field = sequence_field

		if not isinstance(index, int):
			raise ConfigurationError(
				"IndexFields must be passed integer indices. "
				"Found index: {} with type: {}.".format(index, type(index))
			)

	def get_padding_lengths(self) -> Dict[str, int]:
		return {}

	def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:
		return torch.LongTensor([self.sequence_index])

	def empty_field(self):
		return IndexField(-1, self.sequence_field.empty_field())

	def __str__(self) -> str:
		return f"IndexField with index: {self.sequence_index}."

	def __eq__(self, other) -> bool:
		if isinstance(other, int):
			return self.sequence_index == other
		return super().__eq__(other)

	def __len__(self):
		return 1

	def human_readable_repr(self):
		return self.sequence_index

from typing import Optional, List


import torch

from allennlp.nn.util import dist_reduce_sum
from allennlp.training.metrics.metric import Metric


@Metric.register("attachment_scores")
class AttachmentScores(Metric):

	def __init__(self, ignore_classes: List[int] = None) -> None:
		self._labeled_correct = 0.0
		self._unlabeled_correct = 0.0
		self._exact_labeled_correct = 0.0
		self._exact_unlabeled_correct = 0.0
		self._total_words = 0.0
		self._total_sentences = 0.0

		self._ignore_classes: List[int] = ignore_classes or []

	def __call__(  # type: ignore
		self,
		predicted_indices: torch.Tensor,
		predicted_labels: torch.Tensor,
		gold_indices: torch.Tensor,
		gold_labels: torch.Tensor,
		mask: Optional[torch.BoolTensor] = None,
	):
		detached = self.detach_tensors(
			predicted_indices, predicted_labels, gold_indices, gold_labels, mask
		)
		predicted_indices, predicted_labels, gold_indices, gold_labels, mask = detached

		if mask is None:
			mask = torch.ones_like(predicted_indices).bool()

		predicted_indices = predicted_indices.long()
		predicted_labels = predicted_labels.long()
		gold_indices = gold_indices.long()
		gold_labels = gold_labels.long()

		for label in self._ignore_classes:
			label_mask = gold_labels.eq(label)
			mask = mask & ~label_mask

		correct_indices = predicted_indices.eq(gold_indices).long() * mask
		unlabeled_exact_match = (correct_indices + ~mask).prod(dim=-1)
		correct_labels = predicted_labels.eq(gold_labels).long() * mask
		correct_labels_and_indices = correct_indices * correct_labels
		labeled_exact_match = (correct_labels_and_indices + ~mask).prod(dim=-1)
		total_sentences = correct_indices.size(0)
		total_words = correct_indices.numel() - (~mask).sum()

		self._unlabeled_correct += dist_reduce_sum(correct_indices).sum()
		self._exact_unlabeled_correct += dist_reduce_sum(unlabeled_exact_match).sum()
		self._labeled_correct += dist_reduce_sum(correct_labels_and_indices).sum()
		self._exact_labeled_correct += dist_reduce_sum(labeled_exact_match).sum()
		self._total_sentences += dist_reduce_sum(total_sentences)
		self._total_words += dist_reduce_sum(total_words)

	def get_metric(
		self,
		reset: bool = False,
	):
		unlabeled_attachment_score = 0.0
		labeled_attachment_score = 0.0
		unlabeled_exact_match = 0.0
		labeled_exact_match = 0.0

		if self._total_words > 0.0:
			unlabeled_attachment_score = float(self._unlabeled_correct) / float(self._total_words)
			labeled_attachment_score = float(self._labeled_correct) / float(self._total_words)
		if self._total_sentences > 0:
			unlabeled_exact_match = float(self._exact_unlabeled_correct) / float(
				self._total_sentences
			)
			labeled_exact_match = float(self._exact_labeled_correct) / float(self._total_sentences)
		if reset:
			self.reset()
		metrics = {
			"UAS": unlabeled_attachment_score,
			"LAS": labeled_attachment_score,
			"UEM": unlabeled_exact_match,
			"LEM": labeled_exact_match,
		}
		return metrics

	def reset(self):
		self._labeled_correct = 0.0
		self._unlabeled_correct = 0.0
		self._exact_labeled_correct = 0.0
		self._exact_unlabeled_correct = 0.0
		self._total_words = 0.0
		self._total_sentences = 0.0

from typing import List, Dict, Any, Optional, TYPE_CHECKING

from allennlp.training.callbacks.callback import TrainerCallback
from allennlp.data import TensorDict
from allennlp.confidence_checks.normalization_bias_verification import NormalizationBiasVerification


if TYPE_CHECKING:
	from allennlp.training.gradient_descent_trainer import GradientDescentTrainer


@TrainerCallback.register("sanity_checks")
@TrainerCallback.register("confidence_checks")
class ConfidenceChecksCallback(TrainerCallback):

	def on_start(
		self, trainer: "GradientDescentTrainer", is_primary: bool = True, **kwargs
	) -> None:
		self.trainer = trainer
		if is_primary:
			self._verification = NormalizationBiasVerification(self.trainer._pytorch_model)
			self._verification.register_hooks()

	def on_batch(
		self,
		trainer: "GradientDescentTrainer",
		batch_inputs: List[TensorDict],
		batch_outputs: List[Dict[str, Any]],
		batch_metrics: Dict[str, Any],
		epoch: int,
		batch_number: int,
		is_training: bool,
		is_primary: bool = True,
		batch_grad_norm: Optional[float] = None,
		**kwargs,
	) -> None:
		if not is_primary:
			return None

		if epoch == 0 and batch_number == 1 and is_training:
			self._verification.destroy_hooks()
			detected_pairs = self._verification.collect_detections()
			if len(detected_pairs) > 0:
				raise ConfidenceCheckError(
					"The NormalizationBiasVerification check failed. See logs for more details."
				)


class ConfidenceCheckError(Exception):

	def __init__(self, message) -> None:
		super().__init__(
			message
			+ "\nYou can disable these checks by setting the trainer parameter `run_confidence_checks` to `False`."
		)

from os import PathLike
from dataclasses import dataclass, asdict
import json
import logging
from typing import Union

from allennlp.version import VERSION


logger = logging.getLogger(__name__)


META_NAME = "meta.json"


@dataclass
class Meta:

	version: str

	@classmethod
	def new(cls) -> "Meta":
		return cls(version=VERSION)

	def to_file(self, path: Union[PathLike, str]) -> None:
		with open(path, "w") as meta_file:
			json.dump(asdict(self), meta_file)

	@classmethod
	def from_path(cls, path: Union[PathLike, str]) -> "Meta":
		with open(path) as meta_file:
			data = json.load(meta_file)
		return cls(**data)

from allennlp.modules.vision.grid_embedder import GridEmbedder, ResnetBackbone
from allennlp.modules.vision.image2image import (
	Image2ImageModule,
	NormalizeImage,
)
from allennlp.modules.vision.region_detector import (
	RegionDetector,
	FasterRcnnRegionDetector,
)


import argparse
import json
import logging
import math
import os
import random
from pathlib import Path

import datasets
import evaluate
import numpy as np
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from datasets import load_dataset
from huggingface_hub import Repository, create_repo
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

import transformers
from transformers import (
	CONFIG_MAPPING,
	MODEL_MAPPING,
	AutoConfig,
	AutoModelForSeq2SeqLM,
	AutoTokenizer,
	DataCollatorForSeq2Seq,
	MBartTokenizer,
	MBartTokenizerFast,
	SchedulerType,
	default_data_collator,
	get_scheduler,
)
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


check_min_version("4.38.0.dev0")

logger = get_logger(__name__)
require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/translation/requirements.txt")

MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


def parse_args():
	parser = argparse.ArgumentParser(description="Finetune a transformers model on a text classification task")
	parser.add_argument(
		"--dataset_name",
		type=str,
		default=None,
		help="The name of the dataset to use (via the datasets library).",
	)

	parser.add_argument(
		"--predict_with_generate",
		type=bool,
		default=True,
		help="",
	)
	parser.add_argument(
		"--dataset_config_name",
		type=str,
		default=None,
		help="The configuration name of the dataset to use (via the datasets library).",
	)
	parser.add_argument(
		"--train_file", type=str, default=None, help="A csv or a json file containing the training data."
	)

	parser.add_argument(
		"--num_beams",
		type=int,
		default=None,
		help=(
			"Number of beams to use for evaluation. This argument will be "
			"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``."
		),
	)

	parser.add_argument(
		"--max_source_length",
		type=int,
		default=1024,
		help=(
			"The maximum total input sequence length after "
			"tokenization.Sequences longer than this will be truncated, sequences shorter will be padded."
		),
	)
	parser.add_argument(
		"--max_target_length",
		type=int,
		default=128,
		help=(
			"The maximum total sequence length for target text after "
			"tokenization. Sequences longer than this will be truncated, sequences shorter will be padded "
			"during ``evaluate`` and ``predict``."
		),
	)
	parser.add_argument(
		"--val_max_target_length",
		type=int,
		default=None,
		help=(
			"The maximum total sequence length for validation "
			"target text after tokenization.Sequences longer than this will be truncated, sequences shorter will be "
			"padded. Will default to `max_target_length`.This argument is also used to override the ``max_length`` "
			"param of ``model.generate``, which is used during ``evaluate`` and ``predict``."
		),
	)
	parser.add_argument(
		"--pad_to_max_length",
		type=bool,
		default=False,
		help=(
			"Whether to pad all samples to model maximum sentence "
			"length. If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
			"efficient on GPU but very bad for TPU."
		),
	)
	parser.add_argument(
		"--validation_file", type=str, default=None, help="A csv or a json file containing the validation data."
	)
	parser.add_argument(
		"--ignore_pad_token_for_loss",
		type=bool,
		default=True,
		help="Whether to ignore the tokens corresponding to padded labels in the loss computation or not.",
	)
	parser.add_argument("--source_lang", type=str, default=None, help="Source language id for translation.")
	parser.add_argument("--target_lang", type=str, default=None, help="Target language id for translation.")
	parser.add_argument(
		"--source_prefix",
		type=str,
		default=None,
		help="A prefix to add before every source text (useful for T5 models).",
	)
	parser.add_argument(
		"--preprocessing_num_workers",
		type=int,
		default=None,
		help="The number of processes to use for the preprocessing.",
	)
	parser.add_argument(
		"--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
	)
	parser.add_argument(
		"--max_length",
		type=int,
		default=128,
		help=(
			"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
			" sequences shorter will be padded if `--pad_to_max_lengh` is passed."
		),
	)
	parser.add_argument(
		"--model_name_or_path",
		type=str,
		help="Path to pretrained model or model identifier from huggingface.co/models.",
		required=False,
	)
	parser.add_argument(
		"--config_name",
		type=str,
		default=None,
		help="Pretrained config name or path if not the same as model_name",
	)
	parser.add_argument(
		"--tokenizer_name",
		type=str,
		default=None,
		help="Pretrained tokenizer name or path if not the same as model_name",
	)
	parser.add_argument(
		"--use_slow_tokenizer",
		action="store_true",
		help="If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).",
	)
	parser.add_argument(
		"--per_device_train_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the training dataloader.",
	)
	parser.add_argument(
		"--per_device_eval_batch_size",
		type=int,
		default=8,
		help="Batch size (per device) for the evaluation dataloader.",
	)
	parser.add_argument(
		"--learning_rate",
		type=float,
		default=5e-5,
		help="Initial learning rate (after the potential warmup period) to use.",
	)
	parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
	parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
	parser.add_argument(
		"--max_train_steps",
		type=int,
		default=None,
		help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
	)
	parser.add_argument(
		"--gradient_accumulation_steps",
		type=int,
		default=1,
		help="Number of updates steps to accumulate before performing a backward/update pass.",
	)
	parser.add_argument(
		"--lr_scheduler_type",
		type=SchedulerType,
		default="linear",
		help="The scheduler type to use.",
		choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
	)
	parser.add_argument(
		"--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
	)
	parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
	parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
	parser.add_argument(
		"--model_type",
		type=str,
		default=None,
		help="Model type to use if training from scratch.",
		choices=MODEL_TYPES,
	)
	parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
	parser.add_argument(
		"--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
	)
	parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
	parser.add_argument(
		"--trust_remote_code",
		type=bool,
		default=False,
		help=(
			"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
			"should only be set to `True` for repositories you trust and in which you have read the code, as it will "
			"execute code present on the Hub on your local machine."
		),
	)
	parser.add_argument(
		"--checkpointing_steps",
		type=str,
		default=None,
		help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
	)
	parser.add_argument(
		"--resume_from_checkpoint",
		type=str,
		default=None,
		help="If the training should continue from a checkpoint folder.",
	)
	parser.add_argument(
		"--with_tracking",
		action="store_true",
		help="Whether to enable experiment trackers for logging.",
	)
	parser.add_argument(
		"--report_to",
		type=str,
		default="all",
		help=(
			'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
			' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
			"Only applicable when `--with_tracking` is passed."
		),
	)
	args = parser.parse_args()


	if args.dataset_name is None and args.train_file is None and args.validation_file is None:
		raise ValueError("Need either a task name or a training/validation file.")

	if args.train_file is not None:
		extension = args.train_file.split(".")[-1]
		assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
	if args.validation_file is not None:
		extension = args.validation_file.split(".")[-1]
		assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."

	if args.push_to_hub:
		assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."

	return args


def main():
	args = parse_args()

	send_example_telemetry("run_translation_no_trainer", args)

	accelerator = (
		Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()
	)

	logging.basicConfig(
		format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
		datefmt="%m/%d/%Y %H:%M:%S",
		level=logging.INFO,
	)
	logger.info(accelerator.state, main_process_only=False)
	if accelerator.is_local_main_process:
		datasets.utils.logging.set_verbosity_warning()
		transformers.utils.logging.set_verbosity_info()
	else:
		datasets.utils.logging.set_verbosity_error()
		transformers.utils.logging.set_verbosity_error()

	if args.seed is not None:
		set_seed(args.seed)

	if accelerator.is_main_process:
		if args.push_to_hub:
			repo_name = args.hub_model_id
			if repo_name is None:
				repo_name = Path(args.output_dir).absolute().name
			repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id
			repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)

			with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
				if "step_*" not in gitignore:
					gitignore.write("step_*\n")
				if "epoch_*" not in gitignore:
					gitignore.write("epoch_*\n")
		elif args.output_dir is not None:
			os.makedirs(args.output_dir, exist_ok=True)
	accelerator.wait_for_everyone()

	if args.dataset_name is not None:
		raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
	else:
		data_files = {}
		if args.train_file is not None:
			data_files["train"] = args.train_file
		if args.validation_file is not None:
			data_files["validation"] = args.validation_file
		extension = args.train_file.split(".")[-1]
		raw_datasets = load_dataset(extension, data_files=data_files)

	if args.config_name:
		config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)
	elif args.model_name_or_path:
		config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)
	else:
		config = CONFIG_MAPPING[args.model_type]()
		logger.warning("You are instantiating a new config instance from scratch.")

	if args.tokenizer_name:
		tokenizer = AutoTokenizer.from_pretrained(
			args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	elif args.model_name_or_path:
		tokenizer = AutoTokenizer.from_pretrained(
			args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code
		)
	else:
		raise ValueError(
			"You are instantiating a new tokenizer from scratch. This is not supported by this script. "
			"You can do it from another script, save it, and load it from here, using --tokenizer_name."
		)

	if args.model_name_or_path:
		model = AutoModelForSeq2SeqLM.from_pretrained(
			args.model_name_or_path,
			from_tf=bool(".ckpt" in args.model_name_or_path),
			config=config,
			trust_remote_code=args.trust_remote_code,
		)
	else:
		logger.info("Training new model from scratch")
		model = AutoModelForSeq2SeqLM.from_config(config, trust_remote_code=args.trust_remote_code)

	embedding_size = model.get_input_embeddings().weight.shape[0]
	if len(tokenizer) > embedding_size:
		model.resize_token_embeddings(len(tokenizer))

	if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):
		assert (
			args.target_lang is not None and args.source_lang is not None
		), "mBart requires --target_lang and --source_lang"
		if isinstance(tokenizer, MBartTokenizer):
			model.config.decoder_start_token_id = tokenizer.lang_code_to_id[args.target_lang]
		else:
			model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(args.target_lang)

	if model.config.decoder_start_token_id is None:
		raise ValueError("Make sure that `config.decoder_start_token_id` is correctly defined")

	prefix = args.source_prefix if args.source_prefix is not None else ""

	column_names = raw_datasets["train"].column_names

	if isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):
		if args.source_lang is not None:
			tokenizer.src_lang = args.source_lang
		if args.target_lang is not None:
			tokenizer.tgt_lang = args.target_lang

	source_lang = args.source_lang.split("_")[0]
	target_lang = args.target_lang.split("_")[0]

	padding = "max_length" if args.pad_to_max_length else False

	max_target_length = args.max_target_length
	padding = "max_length" if args.pad_to_max_length else False

	def preprocess_function(examples):
		inputs = [ex[source_lang] for ex in examples["translation"]]
		targets = [ex[target_lang] for ex in examples["translation"]]
		inputs = [prefix + inp for inp in inputs]
		model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)

		labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)

		if padding == "max_length" and args.ignore_pad_token_for_loss:
			labels["input_ids"] = [
				[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
			]

		model_inputs["labels"] = labels["input_ids"]
		return model_inputs

	with accelerator.main_process_first():
		processed_datasets = raw_datasets.map(
			preprocess_function,
			batched=True,
			num_proc=args.preprocessing_num_workers,
			remove_columns=column_names,
			load_from_cache_file=not args.overwrite_cache,
			desc="Running tokenizer on dataset",
		)

	train_dataset = processed_datasets["train"]
	eval_dataset = processed_datasets["validation"]

	for index in random.sample(range(len(train_dataset)), 3):
		logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

	label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id
	if args.pad_to_max_length:
		data_collator = default_data_collator
	else:
		data_collator = DataCollatorForSeq2Seq(
			tokenizer,
			model=model,
			label_pad_token_id=label_pad_token_id,
			pad_to_multiple_of=8 if accelerator.use_fp16 else None,
		)

	train_dataloader = DataLoader(
		train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
	)
	eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)

	no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]
	optimizer_grouped_parameters = [
		{
			"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
			"weight_decay": args.weight_decay,
		},
		{
			"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
			"weight_decay": 0.0,
		},
	]
	optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

	overrode_max_train_steps = False
	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if args.max_train_steps is None:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
		overrode_max_train_steps = True

	lr_scheduler = get_scheduler(
		name=args.lr_scheduler_type,
		optimizer=optimizer,
		num_warmup_steps=args.num_warmup_steps,
		num_training_steps=args.max_train_steps,
	)

	model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
		model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
	)

	num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
	if overrode_max_train_steps:
		args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
	args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
	checkpointing_steps = args.checkpointing_steps
	if checkpointing_steps is not None and checkpointing_steps.isdigit():
		checkpointing_steps = int(checkpointing_steps)

	if args.with_tracking:
		if accelerator.is_main_process:
			experiment_config = vars(args)
			experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
			accelerator.init_trackers("translation_no_trainer", experiment_config)

	metric = evaluate.load("sacrebleu")

	def postprocess_text(preds, labels):
		preds = [pred.strip() for pred in preds]
		labels = [[label.strip()] for label in labels]

		return preds, labels

	total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

	logger.info("***** Running training *****")
	logger.info(f"  Num examples = {len(train_dataset)}")
	logger.info(f"  Num Epochs = {args.num_train_epochs}")
	logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
	logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
	logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
	logger.info(f"  Total optimization steps = {args.max_train_steps}")
	progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
	completed_steps = 0
	starting_epoch = 0

	if args.resume_from_checkpoint:
		if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
			checkpoint_path = args.resume_from_checkpoint
			path = os.path.basename(args.resume_from_checkpoint)
		else:
			dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
			dirs.sort(key=os.path.getctime)
			path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
			checkpoint_path = path
			path = os.path.basename(checkpoint_path)

		accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
		accelerator.load_state(checkpoint_path)
		training_difference = os.path.splitext(path)[0]

		if "epoch" in training_difference:
			starting_epoch = int(training_difference.replace("epoch_", "")) + 1
			resume_step = None
			completed_steps = starting_epoch * num_update_steps_per_epoch
		else:
			resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
			starting_epoch = resume_step // len(train_dataloader)
			completed_steps = resume_step // args.gradient_accumulation_steps
			resume_step -= starting_epoch * len(train_dataloader)

	progress_bar.update(completed_steps)

	for epoch in range(starting_epoch, args.num_train_epochs):
		model.train()
		if args.with_tracking:
			total_loss = 0
		if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
			active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
		else:
			active_dataloader = train_dataloader
		for step, batch in enumerate(active_dataloader):
			outputs = model(**batch)
			loss = outputs.loss
			if args.with_tracking:
				total_loss += loss.detach().float()
			loss = loss / args.gradient_accumulation_steps
			accelerator.backward(loss)
			if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
				optimizer.step()
				lr_scheduler.step()
				optimizer.zero_grad()
				progress_bar.update(1)
				completed_steps += 1

			if isinstance(checkpointing_steps, int):
				if completed_steps % checkpointing_steps == 0:
					output_dir = f"step_{completed_steps}"
					if args.output_dir is not None:
						output_dir = os.path.join(args.output_dir, output_dir)
					accelerator.save_state(output_dir)

			if completed_steps >= args.max_train_steps:
				break

		model.eval()

		if args.val_max_target_length is None:
			args.val_max_target_length = args.max_target_length

		gen_kwargs = {
			"max_length": args.val_max_target_length if args is not None else config.max_length,
			"num_beams": args.num_beams,
		}
		samples_seen = 0
		for step, batch in enumerate(eval_dataloader):
			with torch.no_grad():
				generated_tokens = accelerator.unwrap_model(model).generate(
					batch["input_ids"],
					attention_mask=batch["attention_mask"],
					**gen_kwargs,
				)

				generated_tokens = accelerator.pad_across_processes(
					generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
				)
				labels = batch["labels"]
				if not args.pad_to_max_length:
					labels = accelerator.pad_across_processes(batch["labels"], dim=1, pad_index=tokenizer.pad_token_id)

				generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
				labels = accelerator.gather(labels).cpu().numpy()

				if args.ignore_pad_token_for_loss:
					labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

				decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
				decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

				decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

				if accelerator.num_processes > 1:
					if step == len(eval_dataloader) - 1:
						decoded_preds = decoded_preds[: len(eval_dataloader.dataset) - samples_seen]
						decoded_labels = decoded_labels[: len(eval_dataloader.dataset) - samples_seen]
					else:
						samples_seen += len(decoded_labels)

				metric.add_batch(predictions=decoded_preds, references=decoded_labels)
		eval_metric = metric.compute()
		logger.info({"bleu": eval_metric["score"]})

		if args.with_tracking:
			accelerator.log(
				{
					"bleu": eval_metric["score"],
					"train_loss": total_loss.item() / len(train_dataloader),
					"epoch": epoch,
					"step": completed_steps,
				},
				step=completed_steps,
			)

		if args.push_to_hub and epoch < args.num_train_epochs - 1:
			accelerator.wait_for_everyone()
			unwrapped_model = accelerator.unwrap_model(model)
			unwrapped_model.save_pretrained(
				args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
			)
			if accelerator.is_main_process:
				tokenizer.save_pretrained(args.output_dir)
				repo.push_to_hub(
					commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
				)

		if args.checkpointing_steps == "epoch":
			output_dir = f"epoch_{epoch}"
			if args.output_dir is not None:
				output_dir = os.path.join(args.output_dir, output_dir)
			accelerator.save_state(output_dir)

	if args.with_tracking:
		accelerator.end_training()

	if args.output_dir is not None:
		accelerator.wait_for_everyone()
		unwrapped_model = accelerator.unwrap_model(model)
		unwrapped_model.save_pretrained(
			args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
		)
		if accelerator.is_main_process:
			tokenizer.save_pretrained(args.output_dir)
			if args.push_to_hub:
				repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)
		with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
			json.dump({"eval_bleu": eval_metric["score"]}, f)


if __name__ == "__main__":
	main()

import os

_MAJOR = "2"
_MINOR = "10"
_PATCH = "1"
_SUFFIX = os.environ.get("ALLENNLP_VERSION_SUFFIX", "")

VERSION_SHORT = "{0}.{1}".format(_MAJOR, _MINOR)
VERSION = "{0}.{1}.{2}{3}".format(_MAJOR, _MINOR, _PATCH, _SUFFIX)

import torch.nn as nn


class BaseRNN(nn.Module):
	SYM_MASK = "MASK"
	SYM_EOS = "EOS"

	def __init__(self, vocab_size, max_len, hidden_size, input_dropout_p, dropout_p, n_layers, rnn_cell):
		super(BaseRNN, self).__init__()
		self.vocab_size = vocab_size
		self.max_len = max_len
		self.hidden_size = hidden_size
		self.n_layers = n_layers
		self.input_dropout_p = input_dropout_p
		self.input_dropout = nn.Dropout(p=input_dropout_p)
		if rnn_cell.lower() == 'lstm':
			self.rnn_cell = nn.LSTM
		elif rnn_cell.lower() == 'gru':
			self.rnn_cell = nn.GRU
		else:
			raise ValueError("Unsupported RNN Cell: {0}".format(rnn_cell))

		self.dropout_p = dropout_p

	def forward(self, *args, **kwargs):
		raise NotImplementedError()

