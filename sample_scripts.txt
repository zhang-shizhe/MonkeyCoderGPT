<START>
from __future__ import annotations

import logging
from typing import Optional, Sequence

import torch
from torch import _prims, Tensor

log = logging.getLogger(__name__)


def make_prim(
	schema: str,
	impl_aten,
	return_type=_prims.RETURN_TYPE.NEW,
	doc: str = "",
	tags: Optional[Sequence[torch.Tag]] = None,
):
	def meta(*args, **kwargs):
		return _prims.TensorMeta(impl_aten(*args, **kwargs))

	return _prims._make_prim(
		schema=schema,
		return_type=return_type,
		meta=meta,
		impl_aten=impl_aten,
		doc=doc,
		tags=tags,
	)


def eager_force_stride(input_tensor: Tensor, stride) -> Tensor:
	if input_tensor.stride() == stride:
		return input_tensor
	new_tensor = input_tensor.clone().as_strided(
		input_tensor.shape,
		stride,
	)
	new_tensor.copy_(input_tensor)
	return new_tensor


seed = make_prim(
	"inductor_seed(Device device) -> Tensor",
	lambda device: torch.randint(2**63 - 1, [], device=device),
	doc="create a fresh seed (one per call) for use with inductor_rand",
	tags=(torch.Tag.nondeterministic_seeded,),
)
seeds = make_prim(
	"inductor_seeds(int count, Device device) -> Tensor",
	lambda count, device: torch.randint(2**63 - 1, [count], device=device),
	doc="Horizontal fusion of many inductor_seed() calls",
	tags=(torch.Tag.nondeterministic_seeded,),
)
lookup_seed = make_prim(
	"inductor_lookup_seed(Tensor seeds, int index) -> Tensor",
	lambda seeds, index: seeds[index],
	doc="Extract a single seed from the result of inductor_seeds()",
)
random = make_prim(
	"inductor_random(SymInt[] size, Tensor seed, str mode) -> Tensor",
	lambda size, seed, mode: getattr(torch, mode)(size, device=seed.device),
	doc="torch.rand()/torch.randn() using backend-specific RNG that can be fused",
)
randint = make_prim(
	"inductor_randint(SymInt low, SymInt high, SymInt[] size, Tensor seed) -> Tensor",
	lambda low, high, size, seed: torch.randint(low, high, size, device=seed.device),
	doc="torch.randint() using backend-specific RNG that can be fused",
)
force_stride_order = make_prim(
	"inductor_force_stride_order(Tensor input, SymInt[] stride) -> Tensor",
	eager_force_stride,
	doc="Force the stride order for input tensor. No-op if the input tensor already has the stride. Do a copy otherwise",
)
masked_scatter_with_index = make_prim(
	"inductor_masked_scatter_with_index(Tensor input, Tensor mask, Tensor source_idx, Tensor source) -> Tensor",
	lambda input_tensor, mask, index, source: torch.masked_scatter(
		input_tensor, mask, source
	),
	doc="masked_scatter with precomputed indices",
)
_unsafe_index_put_ = make_prim(
	"_unsafe_index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)",
	lambda self, indices, values, accumulate=False: torch.ops.aten.index_put_(
		self, indices, values, accumulate
	),
	doc="Unsafe index_put_ (doesn't issue device asserts)",
)

<END>

<START>
import inspect
import logging

import math
import re
from typing import Dict, List

from torch._streambase import _StreamBase
from ..guards import install_guard

try:
	import numpy as np
except ModuleNotFoundError:
	np = None

import torch._C
import torch._refs
import torch.fx
import torch.nn
import torch.onnx.operators

from .. import config, polyfill, variables
from ..device_interface import get_registered_device_interfaces
from ..exc import unimplemented
from ..guards import GuardBuilder
from ..utils import (
	check_constant_args,
	check_unspec_python_args,
	has_torch_function,
	product,
	proxy_args_kwargs,
)
from .base import VariableTracker
from .ctx_manager import (
	AutocastModeVariable,
	NullContextVariable,
	TorchFunctionDisableVariable,
)
from .distributed import is_constant_pg_functions, is_from_local, ProcessGroupVariable
from .higher_order_ops import TorchHigherOrderOperatorVariable
from .lists import ListVariable, TupleVariable
from .torch_function import can_dispatch_torch_function, dispatch_torch_function

log = logging.getLogger(__name__)


REWRITE_OPS_TO_TENSOR_SIZE_METHOD = [
	torch.onnx.operators.shape_as_tensor,
	torch._shape_as_tensor,
]

constant_fold_functions = [
	torch._assert,
	torch._utils._get_device_index,
	torch.cuda.is_available,
	torch.distributed.is_available,
	torch.get_autocast_gpu_dtype,
	torch.get_default_dtype,
	torch.is_autocast_cache_enabled,
	torch.is_autocast_cpu_enabled,
	torch.is_autocast_enabled,
	torch.is_complex,
	torch.is_floating_point,
	torch.nn.functional._Reduction.get_enum,
	torch.promote_types,
	torch._C._get_privateuse1_backend_name,
]


if torch.distributed.is_available():
	constant_fold_functions.extend(
		[
			torch.distributed.is_initialized,
			torch.distributed.get_rank,
			torch.distributed.get_world_size,
		]
	)


tracing_state_functions = {
	torch.jit.is_scripting: False,
	torch.jit.is_tracing: False,
	torch._C._get_tracing_state: None,
	torch.fx._symbolic_trace.is_fx_tracing: False,
	torch.onnx.is_in_onnx_export: False,
	torch._dynamo.external_utils.is_compiling: True,
	torch._utils.is_compiling: True,
}


class BaseTorchVariable(VariableTracker):

	def __repr__(self):
		return f"TorchCtxManagerClassVariable({self.value})"

	def call_function(
		self, tx, args: "List[VariableTracker]", kwargs: "Dict[str, VariableTracker]"
	) -> "VariableTracker":
		from . import GradModeVariable, InferenceModeVariable, StreamVariable

		if self.value is torch.no_grad:
			if len(args) == 1 and isinstance(
				args[0], variables.functions.BaseUserFunctionVariable
			):
				ctx = GradModeVariable.create(tx, False)
				return ctx.call_function(tx, args, kwargs)
			else:
				return GradModeVariable.create(tx, False)
		elif self.value is torch.enable_grad:
			if len(args) == 1 and isinstance(
				args[0], variables.functions.BaseUserFunctionVariable
			):
				ctx = GradModeVariable.create(tx, True)
				return ctx.call_function(tx, args, kwargs)
			return GradModeVariable.create(tx, True)
		elif self.value is torch.set_grad_enabled and len(args) == 1:
			return GradModeVariable.create(
				tx, args[0].as_python_constant(), initialized=True
			)
		elif self.value is torch.inference_mode:
			return InferenceModeVariable.create(tx, args[0].as_python_constant())
		elif inspect.isclass(self.value) and issubclass(self.value, _StreamBase):
			from torch._dynamo.variables.builder import wrap_fx_proxy_cls

			return wrap_fx_proxy_cls(
				StreamVariable,
				tx,
				tx.output.create_proxy(
					"call_function",
					self.value,
					(),
					{},
				),
			)
		elif self.value in [
			torch.amp.autocast_mode.autocast,
			torch.cuda.amp.autocast,
			torch.cpu.amp.autocast,
		]:
			return AutocastModeVariable.create(self.value, args, kwargs)
		elif self.value in (
			torch.profiler.profile,
			torch.profiler.record_function,
			torch.autograd.profiler.profile,
			torch.autograd.profiler.record_function,
		):
			log.warning("Profiler function %s will be ignored", self.value)
			return NullContextVariable()
		elif self.value is torch._C.DisableTorchFunctionSubclass:
			assert not (args or kwargs)
			return TorchFunctionDisableVariable.create(tx)


class TorchInGraphFunctionVariable(BaseTorchVariable):
Calling {str(self.value)} on only torch.SymInt arguments is not yet supported.
To support this behavior, we need to allow const-propping tensors that store symint data.
For now, dynamo will explicitly graph break when it encounters user code with this behavior.
		if self.value is torch.nn.modules.utils._ntuple:
			count = args[0].as_python_constant()
		else:
			count = self.value.__closure__[0].cell_contents
		assert isinstance(count, int)
		assert not kwargs

		def handle_ntuple(value):
			if value.has_unpack_var_sequence(tx):
				return variables.TupleVariable(
					list(value.unpack_var_sequence(tx)),
				)
			elif value.is_python_constant():
				return variables.ConstantVariable.create(
					torch.nn.modules.utils._ntuple(count)(value.as_python_constant()),
				)
			else:
				unimplemented(f"torch.nn.modules.utils._ntuple({value})")

		if self.value is torch.nn.modules.utils._ntuple:
			return variables.LambdaVariable(handle_ntuple)
		else:
			return handle_ntuple(args[0])

<END>

<START>


import functools

import torch
from torch.nn.functional import (
	GRID_SAMPLE_INTERPOLATION_MODES,
	GRID_SAMPLE_PADDING_MODES,
)
from torch.onnx import _type_utils, errors, symbolic_helper, utils
from torch.onnx._internal import _beartype, jit_utils, registration

_onnx_symbolic = functools.partial(registration.onnx_symbolic, opset=16)


@_onnx_symbolic("aten::grid_sampler")
@symbolic_helper.parse_args("v", "v", "i", "i", "b")
@_beartype.beartype
def grid_sampler(
	g: jit_utils.GraphContext,
	input,
	grid,
	mode_enum,
	padding_mode_enum,
	align_corners,
):
	if symbolic_helper._get_tensor_rank(input) == 5:
		return symbolic_helper._onnx_unsupported("GridSample with 5D volumetric input")
	mode_s = {v: k for k, v in GRID_SAMPLE_INTERPOLATION_MODES.items()}[mode_enum]  # type: ignore[call-arg]
	padding_mode_s = {v: k for k, v in GRID_SAMPLE_PADDING_MODES.items()}[padding_mode_enum]  # type: ignore[call-arg]
	return g.op(
		"GridSample",
		input,
		grid,
		align_corners_i=int(align_corners),
		mode_s=mode_s,
		padding_mode_s=padding_mode_s,
	)


@_onnx_symbolic("aten::scatter_add")
@symbolic_helper.parse_args("v", "i", "v", "v")
@_beartype.beartype
def scatter_add(g: jit_utils.GraphContext, self, dim, index, src):
	if symbolic_helper.is_caffe2_aten_fallback():
		return g.at("scatter", self, dim, index, src, overload_name="src")

	src_type = _type_utils.JitScalarType.from_value(
		src, _type_utils.JitScalarType.UNDEFINED
	)
	src_sizes = symbolic_helper._get_tensor_sizes(src)
	index_sizes = symbolic_helper._get_tensor_sizes(index)

	if len(src_sizes) != len(index_sizes):
		return symbolic_helper._unimplemented(
			"scatter_add",
			f"`index` ({index_sizes}) should have the same dimensionality as `src` ({src_sizes})",
		)

	if src_sizes != index_sizes or None in index_sizes:
		adjusted_shape = g.op("Shape", index)
		starts = g.op("Constant", value_t=torch.tensor([0] * len(index_sizes)))
		src = g.op("Slice", src, starts, adjusted_shape)

	src = symbolic_helper._maybe_get_scalar(src)
	if symbolic_helper._is_value(src):
		return g.op("ScatterElements", self, index, src, axis_i=dim, reduction_s="add")
	else:
		if _type_utils.JitScalarType.from_value(self) != src_type:
			src = g.op(
				"Cast",
				src,
				to_i=_type_utils.JitScalarType.from_value(self).onnx_type(),
			)

		return g.op(
			"ScatterElements",
			self,
			index,
			src,
			axis_i=dim,
			reduction_s="add",
		)


@_onnx_symbolic("aten::scatter_reduce")
@symbolic_helper.parse_args("v", "i", "v", "v", "s", "b")
@_beartype.beartype
def scatter_reduce(
	g: jit_utils.GraphContext,
	self: torch._C.Value,
	dim: int,
	index: torch._C.Value,
	src: torch._C.Value,
	reduce: str,
	include_self: bool,
):
	if reduce == "mean":
		raise errors.OnnxExporterError(
			"ONNX does not support mean reduction for scatter_reduce"
		)
	if not include_self:
		raise errors.OnnxExporterError(
			"ONNX does not support include_self=False for scatter_reduce"
		)

	reduce_mode = {  # convert torch string name to onnx string name
		"mean": "none",  # 'mean' doesn't support in ONNX 1.14 definition
		"sum": "add",
		"prod": "mul",
		"amin": "min",
		"amax": "max",
	}
	onnx_reduce = reduce_mode[reduce]

	self_rank = g.op("Size", g.op("Shape", self))

	self_rank_is_zero = g.op(
		"Equal", self_rank, g.op("Constant", value_t=torch.tensor(0, dtype=torch.int64))
	)
	if_op, (if_context, else_context), _ = jit_utils.add_op_with_blocks(
		g, "If", self_rank_is_zero, n_blocks=2, outputs=3
	)
	neg_1 = if_context.op("Constant", value_t=torch.tensor([-1], dtype=torch.int64))

	self_reshape = if_context.op("Reshape", self, neg_1)
	utils._add_output_to_block(if_context.block, self_reshape)
	index_reshape = if_context.op("Reshape", index, neg_1)
	utils._add_output_to_block(if_context.block, index_reshape)
	src_reshape = if_context.op("Reshape", src, neg_1)
	utils._add_output_to_block(if_context.block, src_reshape)

	self_identity = else_context.op("Identity", self)
	utils._add_output_to_block(else_context.block, self_identity)
	index_identitye = else_context.op("Identity", index)
	utils._add_output_to_block(else_context.block, index_identitye)
	src_identity = else_context.op("Identity", src)
	utils._add_output_to_block(else_context.block, src_identity)

	result = g.op("ScatterElements", *if_op, axis_i=dim, reduction_s=onnx_reduce)

	if_op, (if_context, else_context), _ = jit_utils.add_op_with_blocks(
		g, "If", self_rank_is_zero, n_blocks=2, outputs=1
	)
	result_squeezed = if_context.op("Squeeze", result)
	utils._add_output_to_block(if_context.block, result_squeezed)
	result_identity = else_context.op("Identity", result)
	utils._add_output_to_block(else_context.block, result_identity)
	result_final = if_op.node().output()

	return result_final

<END>

<START>

__all__ = [
	'ConvBn1d',
	'ConvBnReLU1d',
	'ConvReLU1d',
	'ConvBn2d',
	'ConvBnReLU2d',
	'ConvReLU2d',
	'ConvBn3d',
	'ConvBnReLU3d',
	'ConvReLU3d',
	'freeze_bn_stats',
	'update_bn_stats',
]

from torch.ao.nn.intrinsic.qat import ConvBn1d
from torch.ao.nn.intrinsic.qat import ConvBnReLU1d
from torch.ao.nn.intrinsic.qat import ConvReLU1d
from torch.ao.nn.intrinsic.qat import ConvBn2d
from torch.ao.nn.intrinsic.qat import ConvBnReLU2d
from torch.ao.nn.intrinsic.qat import ConvReLU2d
from torch.ao.nn.intrinsic.qat import ConvBn3d
from torch.ao.nn.intrinsic.qat import ConvBnReLU3d
from torch.ao.nn.intrinsic.qat import ConvReLU3d
from torch.ao.nn.intrinsic.qat import freeze_bn_stats
from torch.ao.nn.intrinsic.qat import update_bn_stats

<END>

<START>

from __future__ import annotations

import dataclasses
from typing import List, Optional

from torch.onnx._internal.diagnostics.infra.sarif import (
	_location_relationship,
	_logical_location,
	_message,
	_physical_location,
	_property_bag,
	_region,
)


@dataclasses.dataclass
class Location(object):

<END>

<START>
import copyreg
import enum
import functools
import warnings
from collections import OrderedDict
from copy import deepcopy
from numbers import Number
from typing import Any, Dict, Optional, Tuple, Union

import torch
import torch._C as _C
import torch.utils.hooks as hooks
from torch._namedtensor_internals import (
	check_serializing_named_tensor,
	is_ellipsis,
	resolve_ellipsis,
	single_ellipsis_index,
	unzip_namedshape,
	update_names,
)
from torch.overrides import (
	get_default_nowrap_functions,
	handle_torch_function,
	has_torch_function,
	has_torch_function_unary,
	has_torch_function_variadic,
)
from torch.utils.dlpack import DLDeviceType


def _handle_torch_function_and_wrap_type_error_to_not_implemented(f):
	assigned = functools.WRAPPER_ASSIGNMENTS

	@functools.wraps(f, assigned=assigned)
	def wrapped(*args, **kwargs):
		try:
			if has_torch_function(args):
				return handle_torch_function(wrapped, args, *args, **kwargs)
			return f(*args, **kwargs)
		except TypeError:
			return NotImplemented

	return wrapped


def _rebuild_from_type(func, type, args, dict):
	if type is Tensor:
		return func(*args)

	ret = func(*args).as_subclass(type)
	ret.__dict__ = dict
	return ret


def _rebuild_from_type_v2(func, new_type, args, state):
	ret = func(*args)
	if type(ret) is not new_type:
		ret = ret.as_subclass(new_type)
	if (
		getattr(ret.__class__, "__setstate__", Tensor.__setstate__)
		is not Tensor.__setstate__
	):
		ret.__setstate__(state)
	else:
		ret = torch._utils._set_obj_state(ret, state)
	return ret


class Tensor(torch._C.TensorBase):
	def __deepcopy__(self, memo):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.__deepcopy__, (self,), self, memo)
		if not self.is_leaf:
			raise RuntimeError(
				"Only Tensors created explicitly by the user "
				"(graph leaves) support the deepcopy protocol at the moment.  "
				"If you were attempting to deepcopy a module, this may be because "
				"of a torch.nn.utils.weight_norm usage, "
				"see https://github.com/pytorch/pytorch/pull/103001"
			)
		if id(self) in memo:
			return memo[id(self)]
		with torch.no_grad():
			if (
				self.is_sparse
				or self.device.type
				in ["lazy", "xla", "mtia", "mps", "ort", "meta", "ipu"]
				or (
					not torch._C._has_storage(self)
					and self.device.type == torch._C._get_privateuse1_backend_name()
				)
				or (type(self) is not Tensor and self.data_ptr() == 0)
			):
				new_tensor = self.clone()
				if type(new_tensor) is not type(self):
					raise RuntimeError(
						"The default implementation of __deepcopy__() for wrapper subclasses "
						"only works for subclass types that implement clone() and for which "
						"cloning returns another instance of the same subclass. You should either "
						"properly implement clone() for your subclass or override __deepcopy__() "
						"if it is intended behavior for clone() to return an instance of a "
						"different type."
					)
			else:
				new_storage = self._typed_storage()._deepcopy(memo)
				if self.is_quantized:
					quantizer_params: Union[
						Tuple[torch.qscheme, float, int],
						Tuple[torch.qscheme, Tensor, Tensor, int],
					]
					if self.qscheme() == torch.per_tensor_affine:
						quantizer_params = (
							self.qscheme(),
							self.q_scale(),
							self.q_zero_point(),
						)
					elif self.qscheme() in (
						torch.per_channel_affine,
						torch.per_channel_affine_float_qparams,
					):
						quantizer_params = (
							self.qscheme(),
							self.q_per_channel_scales(),
							self.q_per_channel_zero_points(),
							self.q_per_channel_axis(),
						)
					else:
						raise RuntimeError(
							f"Unsupported qscheme {self.qscheme()} in deepcopy"
						)
					new_tensor = torch._utils._rebuild_qtensor(
						torch.storage.TypedStorage(
							wrap_storage=new_storage._untyped_storage,
							dtype=self.dtype,
							_internal=True,
						),
						self.storage_offset(),
						self.size(),
						self.stride(),
						quantizer_params,
						self.requires_grad,
						self._backward_hooks,
					)
					if type(new_tensor) is not type(self):
						raise RuntimeError(
							"The default implementation of __deepcopy__() for quantized tensors "
							"expects the tensor returned by torch._utils._rebuild_qtensor() to "
							"match the type of the instance being copied. If you encounter this, "
							"please open an issue on PyTorch's GitHub."
						)
				else:
					new_tensor = self.new_empty([])
					if type(new_tensor) is not type(self):
						raise RuntimeError(
							"The default implementation of __deepcopy__() for non-wrapper subclasses "
							"only works for subclass types that implement new_empty() and for which "
							"that function returns another instance of the same subclass. You should "
							"either properly implement new_empty() for your subclass or override "
							"__deepcopy__() if it is intended behavior for new_empty() to return "
							"an instance of a different type."
						)
					new_tensor.set_(
						new_storage, self.storage_offset(), self.size(), self.stride()
					)
					if self.is_conj():
						new_tensor = new_tensor.conj_physical()
					if self.is_neg():
						new_tensor = new_tensor.neg()
			if self.requires_grad:
				new_tensor.requires_grad_()
			if self.grad is not None:
				new_tensor.grad = self.grad.__deepcopy__(memo)

			if type(self) is not Tensor:
				if type(new_tensor) is not type(self):
					raise RuntimeError(
						"Type of deepcopy result does not match the type of the source tensor. "
						"If you encounter this, please open an issue on PyTorch's GitHub."
					)

				slots_to_save = copyreg._slotnames(self.__class__)  # type: ignore[attr-defined]
				for slot in slots_to_save:
					if hasattr(self, slot):
						setattr(new_tensor, slot, deepcopy(getattr(self, slot), memo))

			new_tensor.__dict__ = deepcopy(self.__dict__, memo)

			memo[id(self)] = new_tensor
			return new_tensor

	def __reduce_ex__(self, proto):
		state = torch._utils._get_obj_state(self)
		if type(self) is Tensor and not state:
			return self._reduce_ex_internal(proto)
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.__reduce_ex__, (self,), self, proto)
		func, args = self._reduce_ex_internal(proto)
		return (_rebuild_from_type_v2, (func, type(self), args, state))

	def storage(self):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.storage, (self,), self)

		torch.storage._warn_typed_storage_removal(stacklevel=2)
		return self._typed_storage()

	def _typed_storage(self):
		untyped_storage = self.untyped_storage()
		return torch.TypedStorage(
			wrap_storage=untyped_storage, dtype=self.dtype, _internal=True
		)

	def _reduce_ex_internal(self, proto):
		check_serializing_named_tensor(self)
		torch.utils.hooks.warn_if_has_hooks(self)
		backward_hooks: Dict[Any, Any] = OrderedDict()
		if self.device.type in ["xla", "mtia", "ort"] or (
			not torch._C._has_storage(self)
			and self.device.type == torch._C._get_privateuse1_backend_name()
		):
			numpy_tensor = (
				self.cpu().numpy()
				if self.dtype != torch.bfloat16
				else self.cpu().to(torch.float32).numpy()
			)
			return (
				torch._utils._rebuild_device_tensor_from_numpy,
				(numpy_tensor, self.dtype, str(self.device), self.requires_grad),
			)
		if self.device.type == "meta":
			arg_meta = (
				self.dtype,
				tuple(self.size()),
				self.stride(),
				self.requires_grad,
			)
			return (torch._utils._rebuild_meta_tensor_no_storage, arg_meta)
		if self.is_quantized:
			quantizer_params: Union[
				Tuple[torch.qscheme, float, int], Tuple[Any, Tensor, Tensor, int]
			]
			if self.qscheme() == torch.per_tensor_affine:
				quantizer_params = (
					torch.per_tensor_affine,
					self.q_scale(),
					self.q_zero_point(),
				)
			elif self.qscheme() in (
				torch.per_channel_affine,
				torch.per_channel_affine_float_qparams,
			):
				quantizer_params = (
					torch.per_channel_affine,
					self.q_per_channel_scales(),
					self.q_per_channel_zero_points(),
					self.q_per_channel_axis(),
				)
			else:
				raise RuntimeError(
					f"Serialization is not supported for tensors of type {self.qscheme()}"
				)
			args_qtensor = (
				torch.storage.TypedStorage(
					wrap_storage=self._typed_storage()._untyped_storage,
					dtype=self.dtype,
					_internal=True,
				),
				self.storage_offset(),
				tuple(self.size()),
				self.stride(),
				quantizer_params,
				self.requires_grad,
				backward_hooks,
			)
			return (torch._utils._rebuild_qtensor, args_qtensor)
		elif self.is_sparse:
			if self.layout == torch.sparse_coo:
				args_sparse = (
					self.layout,
					(self._indices(), self._values(), self.size(), self.is_coalesced()),
				)
			else:
				raise NotImplementedError(
					f"sparse tensor __reduce_ex__ for layout `{self.layout}`"
				)
			return (torch._utils._rebuild_sparse_tensor, args_sparse)
		elif self.layout in {
			torch.sparse_csr,
			torch.sparse_csc,
			torch.sparse_bsr,
			torch.sparse_bsc,
		}:
			if self.layout in {torch.sparse_csr, torch.sparse_bsr}:
				compressed_indices, plain_indices = (
					self.crow_indices(),
					self.col_indices(),
				)
			else:
				compressed_indices, plain_indices = (
					self.ccol_indices(),
					self.row_indices(),
				)
			args_sparse_compressed = (
				self.layout,
				(
					compressed_indices,
					plain_indices,
					self.values(),
					self.size(),
				),
			)
			return (torch._utils._rebuild_sparse_tensor, args_sparse_compressed)
		elif self.is_nested:
			args_nested = (
				self.values(),
				self._nested_tensor_size(),
				self._nested_tensor_strides(),
				self._nested_tensor_storage_offsets(),
			)
			return (torch._utils._rebuild_nested_tensor, args_nested)
		elif (
			self.data_ptr() == 0
			and type(self) is not torch.Tensor
			and type(self).__torch_dispatch__ is not torch.Tensor.__torch_dispatch__
		):
			arg_wrapper_subclass = (
				type(self),
				self.dtype,
				tuple(self.size()),
				self.stride(),
				self.storage_offset(),
				self.layout,
				self.device,
				self.requires_grad,
			)
			return (torch._utils._rebuild_wrapper_subclass, arg_wrapper_subclass)
		else:
			v3_dtypes = [
				torch.float8_e5m2,
				torch.float8_e4m3fn,
				torch.bits8,
				torch.bits16,
				torch.bits1x8,
				torch.bits2x4,
				torch.bits4x2,
			]
			if self.dtype in v3_dtypes:
				rebuild_func = torch._utils._rebuild_tensor_v3
				storage = self.untyped_storage()
			else:
				rebuild_func = torch._utils._rebuild_tensor_v2  # type: ignore[assignment]
				storage = torch.storage.TypedStorage(
					wrap_storage=self._typed_storage()._untyped_storage,
					dtype=self.dtype,
					_internal=True,
				)  # type: ignore[assignment]
			args = (
				storage,
				self.storage_offset(),
				tuple(self.size()),
				self.stride(),
				self.requires_grad,
				backward_hooks,
			)  # previously was self._backward_hooks

			if isinstance(storage, torch.storage.UntypedStorage):
				args = args + (self.dtype,)  # type: ignore[assignment]

			metadata = torch._utils.get_tensor_metadata(self)
			if metadata:
				args = args + (metadata,)  # type: ignore[assignment]

			return (rebuild_func, args)

	def __setstate__(self, state):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.__setstate__, (self,), self, state)
		if not self.is_leaf:
			raise RuntimeError("__setstate__ can be only called on leaf Tensors")
		if len(state) == 4:
			self.set_(*state)
			return
		elif len(state) == 5:
			self.data = state[0]
			state = (state[3], state[4], state[2])
		self.requires_grad, _, self._backward_hooks = state

	def __repr__(self, *, tensor_contents=None):
		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor.__repr__, (self,), self, tensor_contents=tensor_contents
			)
		return torch._tensor_str._str(self, tensor_contents=tensor_contents)

	def backward(
		self, gradient=None, retain_graph=None, create_graph=False, inputs=None
	):
		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor.backward,
				(self,),
				self,
				gradient=gradient,
				retain_graph=retain_graph,
				create_graph=create_graph,
				inputs=inputs,
			)
		torch.autograd.backward(
			self, gradient, retain_graph, create_graph, inputs=inputs
		)

	def register_hook(self, hook):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.register_hook, (self,), self, hook)
		if not self.requires_grad:
			raise RuntimeError(
				"cannot register a hook on a tensor that doesn't require gradient"
			)
		if self._backward_hooks is None:
			self._backward_hooks = OrderedDict()
			if self.grad_fn is not None:
				self.grad_fn._register_hook_dict(self)
		handle = hooks.RemovableHandle(self._backward_hooks)
		self._backward_hooks[handle.id] = hook
		return handle

	def register_post_accumulate_grad_hook(self, hook):
		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor.register_post_accumulate_grad_hook, (self,), self, hook
			)
		if not self.requires_grad:
			raise RuntimeError(
				"cannot register a hook on a tensor that doesn't require gradient"
			)
		if self.grad_fn is not None:
			raise RuntimeError(
				"post accumulate grad hooks cannot be registered on non-leaf tensors"
			)
		if self._post_accumulate_grad_hooks is None:
			self._post_accumulate_grad_hooks: Dict[Any, Any] = OrderedDict()
		handle = hooks.RemovableHandle(self._post_accumulate_grad_hooks)
		self._post_accumulate_grad_hooks[handle.id] = hook
		return handle

	def reinforce(self, reward):
		def trim(str):
			return "\n".join([line.strip() for line in str.split("\n")])

		raise RuntimeError(
			trim(
			)
		)

	detach = _C._add_docstr(
		_C.TensorBase.detach,
	)

	detach_ = _C._add_docstr(
		_C.TensorBase.detach_,
	)

	def is_shared(self):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.is_shared, (self,), self)
		return self._typed_storage()._is_shared()

	def share_memory_(self):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.share_memory_, (self,), self)
		self._typed_storage()._share_memory_()
		return self

	def __reversed__(self):
		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor.norm, (self,), self, p=p, dim=dim, keepdim=keepdim, dtype=dtype
			)
		return torch.norm(self, p, dim, keepdim, dtype=dtype)

	def solve(self, other):
		from ._linalg_utils import solve

		return solve(self, other)

	def lstsq(self, other):
		from ._linalg_utils import lstsq

		return lstsq(self, other)

	def eig(self, eigenvectors=False):
		from ._linalg_utils import eig

		return eig(self, eigenvectors=eigenvectors)

	def symeig(self, eigenvectors=False):
		from ._linalg_utils import _symeig

		return _symeig(self, eigenvectors=eigenvectors)

	def lu(self, pivot=True, get_infos=False):

		.. warning::
		  This function changed signature at version 0.4.1. Calling with
		  the previous signature may cause error or return incorrect result.
		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor.istft,
				(self,),
				self,
				n_fft,
				hop_length=hop_length,
				win_length=win_length,
				window=window,
				center=center,
				normalized=normalized,
				onesided=onesided,
				length=length,
				return_complex=return_complex,
			)
		return torch.istft(
			self,
			n_fft,
			hop_length,
			win_length,
			window,
			center,
			normalized,
			onesided,
			length,
			return_complex=return_complex,
		)

	def resize(self, *sizes):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.resize, (self,), self, *sizes)
		warnings.warn("non-inplace resize is deprecated")
		from torch.autograd._functions import Resize

		return Resize.apply(self, sizes)

	def resize_as(self, tensor):
		if has_torch_function_variadic(self, tensor):
			return handle_torch_function(Tensor.resize_as, (self, tensor), self, tensor)
		warnings.warn("non-inplace resize_as is deprecated")
		from torch.autograd._functions import Resize

		return Resize.apply(self, tensor.size())

	def split(self, split_size, dim=0):

		See :func:`torch.unique`

		See :func:`torch.unique_consecutive`

		Args:
			element (Tensor or scalar): element to be checked
				for presence in current tensor"

		See:
		https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html

		Returns the type of the underlying storage.


		Refining is a special case of renaming that "lifts" unnamed dimensions.
		A ``None`` dim can be refined to have any name; a named dim can only be
		refined to have the same name.

		Because named tensors can coexist with unnamed tensors, refining names
		gives a nice way to write named-tensor-aware code that works with both
		named and unnamed tensors.

		:attr:`names` may contain up to one Ellipsis (``...``).
		The Ellipsis is expanded greedily; it is expanded in-place to fill
		:attr:`names` to the same length as ``self.dim()`` using names from the
		corresponding indices of ``self.names``.

		Python 2 does not support Ellipsis but one may use a string literal
		instead (``'...'``).

		Args:
			names (iterable of str): The desired names of the output tensor. May
				contain up to one Ellipsis.

		Examples::

			>>> imgs = torch.randn(32, 3, 128, 128)
			>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')
			>>> named_imgs.names
			('N', 'C', 'H', 'W')

			>>> tensor = torch.randn(2, 3, 5, 7, 11)
			>>> tensor = tensor.refine_names('A', ..., 'B', 'C')
			>>> tensor.names
			('A', None, None, 'B', 'C')

		.. warning::
			The named tensor API is experimental and subject to change.

		specified in :attr:`names`, adding size-one dims for any new names.

		All of the dims of :attr:`self` must be named in order to use this method.
		The resulting tensor is a view on the original tensor.

		All dimension names of :attr:`self` must be present in :attr:`names`.
		:attr:`names` may contain additional names that are not in ``self.names``;
		the output tensor has a size-one dimension for each of those new names.

		:attr:`names` may contain up to one Ellipsis (``...``).
		The Ellipsis is expanded to be equal to all dimension names of :attr:`self`
		that are not mentioned in :attr:`names`, in the order that they appear
		in :attr:`self`.

		Python 2 does not support Ellipsis but one may use a string literal
		instead (``'...'``).

		Args:
			names (iterable of str): The desired dimension ordering of the
				output tensor. May contain up to one Ellipsis that is expanded
				to all unmentioned dim names of :attr:`self`.

		Examples::

			>>> tensor = torch.randn(2, 2, 2, 2, 2, 2)
			>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')

			>>> named_tensor.align_to('F', 'E', ...)

		.. warning::
			The named tensor API is experimental and subject to change.

		unflatten(dim, sizes) -> Tensor

		See :func:`torch.unflatten`.


		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor.rename_, (self,), self, *names, **rename_map
			)

		return update_names(self, names, rename_map, inplace=True)

	def rename(self, *names, **rename_map):
		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor.rename, (self,), self, *names, **rename_map
			)

		return update_names(self, names, rename_map, inplace=False)

	def to_sparse_coo(self):
		return self.to_sparse()

	def dim_order(self):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.dim_order, (self,), self)

		import torch._prims_common as utils

		return tuple(utils.compute_elementwise_output_logical_to_physical_perm(self))

	def _update_names(self, names, inplace):
		if has_torch_function_unary(self):
			return handle_torch_function(
				Tensor._update_names, (self,), self, names, inplace
			)

		if inplace:
			return super().rename_(names)
		else:
			return super().rename(names)

	@classmethod
	def __torch_function__(cls, func, types, args=(), kwargs=None):
		if kwargs is None:
			kwargs = {}

		if not all(issubclass(cls, t) for t in types):
			return NotImplemented

		with _C.DisableTorchFunctionSubclass():
			ret = func(*args, **kwargs)
			if func in get_default_nowrap_functions():
				return ret
			else:
				return _convert(ret, cls)

	__torch_dispatch__ = _C._disabled_torch_dispatch_impl

	def __dlpack__(self, stream=None):
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.__dlpack__, (self,), self, stream)

		if self.requires_grad:
			raise RuntimeError(
				"Can't export tensors that require gradient, use tensor.detach()"
			)
		if self.is_conj():
			raise RuntimeError("Can't export tensors with the conjugate bit set")
		if self.layout != torch.strided:
			raise RuntimeError(
				"Can't export tensors with layout other than torch.strided"
			)

		if stream is not None and type(stream) is not int:
			raise TypeError("stream must be ``int`` or ``none``")
		elif stream is not None and stream != -1:
			if self.device.type == "cuda":
				if stream == 1 and torch.version.hip is None:
					stream = torch.cuda.default_stream()
				elif stream == 0 and torch.version.hip is not None:
					stream = torch.cuda.default_stream()
				else:
					stream = torch.cuda.ExternalStream(stream)
				sync_stream = torch.cuda.current_stream()
				if stream != sync_stream:
					event = torch.cuda.Event()
					event.record(sync_stream)
					stream.wait_event(event)
		return torch.to_dlpack(self)

	def __dlpack_device__(self) -> Tuple[enum.IntEnum, int]:
		if has_torch_function_unary(self):
			return handle_torch_function(Tensor.__dlpack_device__, (self,), self)
		device = self.device
		idx = device.index if device.index is not None else 0
		torch_device_type = device.type
		if torch_device_type == "cuda" and torch.version.hip is not None:
			device_type = DLDeviceType.kDLROCM
		elif torch_device_type == "cpu" and self.is_pinned():
			device_type = DLDeviceType.kDLCPUPinned
		elif torch_device_type == "cuda":
			device_type = DLDeviceType.kDLGPU
		elif torch_device_type == "cpu":
			device_type = DLDeviceType.kDLCPU
		elif self.device.type == "xpu":
			device_type = DLDeviceType.kDLOneAPI
		else:
			raise ValueError(f"Unknown device type {torch_device_type} for Dlpack")
		return (device_type, idx)

	__module__ = "torch"


def _convert(ret, cls):
	if cls is Tensor:
		return ret

	if isinstance(ret, Tensor) and not isinstance(ret, cls):
		ret = ret.as_subclass(cls)

	if isinstance(ret, (tuple, list)):
		ret = type(ret)(_convert(r, cls) for r in ret)

	return ret

<END>

<START>
import torch


def show():
	return torch._C._show_config()


def _cxx_flags():
	return torch._C._parallel_info()

<END>

<START>

import __future__  # noqa: F404

import collections
import functools
import types
import warnings
from typing import Dict, Set, List, Any, Callable, Iterable, Type, Tuple
from functools import wraps
import contextlib

import torch
from torch._C import (
	_has_torch_function, _has_torch_function_unary,
	_has_torch_function_variadic, _add_docstr,
	_push_on_torch_function_stack, _pop_torch_function_stack, _get_function_stack_at, _len_torch_function_stack,
	_is_torch_function_mode_enabled)

__all__ = [
	"get_ignored_functions",
	"get_overridable_functions",
	"get_testing_overrides",
	"handle_torch_function",
	"has_torch_function",
	"resolve_name",
	"is_tensor_like",
	"is_tensor_method_or_property",
	"wrap_torch_function",
	"enable_reentrant_dispatch",
]


def _disable_user_warnings(
		func: Callable, regex: str = '.*is deprecated, please use.*', module: str = 'torch') -> Callable:

	@wraps(func)
	def wrapper(*args, **kwargs):
		with warnings.catch_warnings():
			warnings.filterwarnings("ignore", category=UserWarning, message=regex, module=module)
			return func(*args, **kwargs)
	return wrapper


@functools.lru_cache(None)
@_disable_user_warnings
def get_ignored_functions() -> Set[Callable]:
	Tensor = torch.Tensor
	return {
		torch.typename,
		torch.is_tensor,
		torch.is_storage,
		torch.set_default_tensor_type,
		torch.set_default_device,
		torch.get_default_device,
		torch.set_rng_state,
		torch.get_rng_state,
		torch.manual_seed,
		torch.initial_seed,
		torch.seed,
		torch.save,
		torch.load,
		torch.set_printoptions,
		torch.fork,
		torch.get_default_dtype,
		torch.get_num_interop_threads,
		torch.get_num_threads,
		torch.init_num_threads,
		torch.import_ir_module,
		torch.import_ir_module_from_buffer,
		torch.is_anomaly_enabled,
		torch.is_anomaly_check_nan_enabled,
		torch.is_grad_enabled,
		torch.merge_type_from_type_comment,
		torch.parse_ir,
		torch.parse_schema,
		torch.parse_type_comment,
		torch.set_anomaly_enabled,
		torch.set_flush_denormal,
		torch.set_num_interop_threads,
		torch.set_num_threads,
		torch.wait,
		torch.as_tensor,
		torch.from_numpy,
		torch.get_device,
		torch.tensor,
		torch.default_generator,
		torch.has_cuda,
		torch.has_cudnn,
		torch.has_lapack,
		torch.device,
		torch.dtype,
		torch.finfo,
		torch.has_mkl,
		torch.has_mps,
		torch.has_mkldnn,
		torch.has_openmp,
		torch.iinfo,
		torch.memory_format,
		torch.qscheme,
		torch.set_grad_enabled,
		torch.no_grad,
		torch.enable_grad,
		torch.inference_mode,
		torch.is_inference_mode_enabled,
		torch.layout,
		torch.align_tensors,
		torch.arange,
		torch.as_strided,
		torch.bartlett_window,
		torch.blackman_window,
		torch.broadcast_shapes,
		torch.can_cast,
		torch.compile,
		torch.cudnn_affine_grid_generator,
		torch.cudnn_batch_norm,
		torch.cudnn_convolution,
		torch.cudnn_convolution_transpose,
		torch.cudnn_convolution_relu,
		torch.cudnn_convolution_add_relu,
		torch.cudnn_grid_sampler,
		torch.cudnn_is_acceptable,
		torch.empty,
		torch.empty_permuted,
		torch.empty_strided,
		torch.empty_quantized,
		torch.export.dynamic_dim,
		torch.export.export,
		torch.export.load,
		torch.export.register_dataclass,
		torch.export.save,
		torch.eye,
		torch.fft.fftfreq,
		torch.fft.rfftfreq,
		torch.from_file,
		torch.full,
		torch.fill,
		torch.hamming_window,
		torch.hann_window,
		torch.kaiser_window,
		torch.linspace,
		torch.logspace,
		torch.mkldnn_adaptive_avg_pool2d,
		torch.mkldnn_convolution,
		torch.mkldnn_max_pool2d,
		torch.mkldnn_max_pool3d,
		torch.mkldnn_linear_backward_weights,
		torch.mkldnn_rnn_layer,
		torch.normal,
		torch.ones,
		torch.promote_types,
		torch.rand,
		torch.randn,
		torch.randint,
		torch.randperm,
		torch.range,
		torch.result_type,
		torch.scalar_tensor,
		torch.sparse_coo_tensor,
		torch.sparse_compressed_tensor,
		torch.sparse_csr_tensor,
		torch.sparse_csc_tensor,
		torch.sparse_bsr_tensor,
		torch.sparse_bsc_tensor,
		torch.sym_constrain_range,
		torch.sym_constrain_range_for_size,
		torch.tril_indices,
		torch.triu_indices,
		torch.vander,
		torch.zeros,
		torch._jit_internal.boolean_dispatch,
		torch.nn.functional.assert_int_or_pair,
		torch.nn.functional.upsample,
		torch.nn.functional.upsample_bilinear,
		torch.nn.functional.upsample_nearest,
		torch.nn.functional.has_torch_function,
		torch.nn.functional.has_torch_function_unary,
		torch.nn.functional.has_torch_function_variadic,
		torch.nn.functional.handle_torch_function,
		torch.nn.functional.sigmoid,
		torch.nn.functional.hardsigmoid,
		torch.nn.functional.tanh,
		torch.nn.functional._canonical_mask,
		torch.nn.functional._none_or_dtype,
		torch.nn.init.calculate_gain,
		torch.nn.init.uniform,
		torch.nn.init.normal,
		torch.nn.init.constant,
		torch.nn.init.eye,
		torch.nn.init.dirac,
		torch.nn.init.xavier_uniform,
		torch.nn.init.xavier_normal,
		torch.nn.init.kaiming_uniform,
		torch.nn.init.kaiming_normal,
		torch.nn.init.orthogonal,
		torch.nn.init.sparse,
		torch.nested.to_padded_tensor,
		has_torch_function,
		handle_torch_function,
		torch.set_autocast_enabled,
		torch.is_autocast_enabled,
		torch.clear_autocast_cache,
		torch.set_autocast_cpu_enabled,
		torch.is_autocast_cpu_enabled,
		torch.set_autocast_xla_enabled,
		torch.is_autocast_xla_enabled,
		torch.set_autocast_ipu_enabled,
		torch.is_autocast_ipu_enabled,
		torch.set_autocast_cpu_dtype,
		torch.get_autocast_cpu_dtype,
		torch.set_autocast_ipu_dtype,
		torch.get_autocast_ipu_dtype,
		torch.get_autocast_gpu_dtype,
		torch.set_autocast_gpu_dtype,
		torch.get_autocast_xla_dtype,
		torch.set_autocast_xla_dtype,
		torch.autocast_increment_nesting,
		torch.autocast_decrement_nesting,
		torch.is_autocast_cache_enabled,
		torch.set_autocast_cache_enabled,
		torch.nn.functional.hardswish,
		torch.is_vulkan_available,
		torch.are_deterministic_algorithms_enabled,
		torch.use_deterministic_algorithms,
		torch.is_deterministic_algorithms_warn_only_enabled,
		torch.set_deterministic_debug_mode,
		torch.get_deterministic_debug_mode,
		torch.set_float32_matmul_precision,
		torch.get_float32_matmul_precision,
		torch.unify_type_list,
		torch.is_warn_always_enabled,
		torch.set_warn_always,
		torch.vitals_enabled,
		torch.set_vital,
		torch.read_vitals,
		torch.vmap,
		torch.cond,
		torch.frombuffer,
		torch.asarray,
		torch._functional_sym_constrain_range,
		torch._make_dep_token,
		Tensor.__delitem__,
		Tensor.__dir__,
		Tensor.__getattribute__,
		Tensor.__init__,
		Tensor.__iter__,
		Tensor.__init_subclass__,
		Tensor.__delattr__,
		Tensor.__setattr__,
		Tensor.__torch_function__,
		Tensor.__torch_dispatch__,
		Tensor.__new__,
		Tensor.__class__,
		Tensor.__subclasshook__,
		Tensor.__hash__,
		Tensor.as_subclass,
		Tensor.eig,
		Tensor.lstsq,
		Tensor.reinforce,
		Tensor.new,
		Tensor.new_tensor,
		Tensor.new_empty,
		Tensor.new_empty_strided,
		Tensor.new_zeros,
		Tensor.new_ones,
		Tensor.new_full,
		Tensor._make_subclass,
		Tensor.solve,
		Tensor.symeig,
		Tensor.stride,
		Tensor.unflatten,
		Tensor.to_sparse_coo,
		Tensor.to_sparse_csr,
		Tensor.to_sparse_csc,
		Tensor.to_sparse_bsr,
		Tensor.to_sparse_bsc,
		Tensor._to_sparse,
		Tensor._to_sparse_csr,
		Tensor._to_sparse_csc,
		Tensor._to_sparse_bsr,
		Tensor._to_sparse_bsc,
		Tensor._typed_storage,
		Tensor._reduce_ex_internal,
		Tensor._fix_weakref,
		Tensor._view_func,
		Tensor._view_func_unsafe,
		Tensor._make_wrapper_subclass,
		Tensor._python_dispatch.__get__,
		Tensor._has_symbolic_sizes_strides.__get__,
		Tensor._conj,
		Tensor._conj_physical,
		Tensor._neg_view,
		Tensor._is_zerotensor,
		Tensor._is_all_true,
		Tensor._is_any_true,
		Tensor._addmm_activation,
		Tensor.to_padded_tensor,
	}


@functools.lru_cache(None)
def get_default_nowrap_functions() -> Set[Callable]:
	Tensor = torch.Tensor
	return {
		Tensor._base.__get__,
		Tensor.grad.__get__,
		Tensor._grad.__get__,
	}


@functools.lru_cache(None)
@_disable_user_warnings
def get_testing_overrides() -> Dict[Callable, Callable]:
	Tensor = torch.Tensor
	ret: Dict[Callable, Callable] = {
		torch.abs: lambda input, out=None: -1,
		torch.absolute: lambda input, out=None: -1,
		torch.adaptive_avg_pool1d: lambda input, output_size: -1,
		torch.adaptive_max_pool1d: lambda inputs, output_size: -1,
		torch.acos: lambda input, out=None: -1,
		torch.adjoint: lambda input: -1,
		torch.arccos: lambda input, out=None: -1,
		torch.acosh: lambda input, out=None: -1,
		torch.arccosh: lambda input, out=None: -1,
		torch.add: lambda input, other, out=None: -1,
		torch.addbmm: lambda input, batch1, batch2, alpha=1, beta=1, out=None: -1,
		torch.addcdiv: lambda input, tensor1, tensor2, value=1, out=None: -1,
		torch.addcmul: lambda input, tensor1, tensor2, value=1, out=None: -1,
		torch.addmm: lambda input, mat1, mat2, beta=1, alpha=1, out=None: -1,
		torch.addmv: lambda input, mat, vec, beta=1, alpha=1, out=None: -1,
		torch.addr: lambda input, vec1, vec2, beta=1, alpha=1, out=None: -1,
		torch.affine_grid_generator: lambda theta, size, align_corners: -1,
		torch.all: lambda input, dim=None: -1,
		torch.allclose: lambda input, other, trol=1e-05, atol=1e-08, equal_nan=False: -1,
		torch.alpha_dropout: lambda input, p, train, inplace=False: -1,
		torch.amax: lambda input, dim=None: -1,
		torch.amin: lambda input, dim=None: -1,
		torch.aminmax: lambda input, dim=None, keepdim=False, out=None: -1,
		torch.angle: lambda input, out=None: -1,
		torch.any: lambda input, dim=None, keepdim=False, out=None: -1,
		torch.argmax: lambda input: -1,
		torch.argmin: lambda input: -1,
		torch.argsort: lambda input, dim=None: -1,
		torch.asin: lambda input, out=None: -1,
		torch._assert_async: lambda input, msg: -1,
		torch.arcsin: lambda input, out=None: -1,
		torch.asinh: lambda input, out=None: -1,
		torch.arcsinh: lambda input, out=None: -1,
		torch.atan: lambda input, out=None: -1,
		torch.arctan: lambda input, out=None: -1,
		torch.atan2: lambda input, other, out=None: -1,
		torch.arctan2: lambda input, other, out=None: -1,
		torch.atanh: lambda input, out=None: -1,
		torch.arctanh: lambda input, out=None: -1,
		torch.atleast_1d: lambda *tensors: -1,
		torch.atleast_2d: lambda *tensors: -1,
		torch.atleast_3d: lambda *tensors: -1,
		torch.avg_pool1d: lambda input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True: -1,
		torch.baddbmm: lambda input, batch1, batch2, alpha=1, beta=1, out=None: -1,
		torch.batch_norm: lambda input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled: -1,
		torch.batch_norm_backward_elemt: lambda grad_out, input, mean, invstd, weight, sum_dy, sum_dy_xmu, count_tensor: -1,
		torch.batch_norm_backward_reduce: lambda grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g: -1,
		torch.batch_norm_elemt: lambda input, weight, bias, mean, invstd, eps: -1,
		torch.batch_norm_gather_stats: lambda input, mean, invstd, running_mean, running_var, momentum, eps, count: -1,
		torch.batch_norm_gather_stats_with_counts: lambda input, mean, invstd, running_mean, running_var, momentum, eps, count: -1,
		torch.batch_norm_stats: lambda input, eps: -1,
		torch.batch_norm_update_stats: lambda input, running_mean, running_var, momentum: -1,
		torch.bernoulli: lambda input, generator=None, out=None: -1,
		torch.bilinear: lambda input1, input2, weight, bias: -1,
		torch.binary_cross_entropy_with_logits: (lambda input, target, weight=None, size_average=None, reduce=None,
												 reduction='mean', pos_weight=None: -1),
		torch.bincount: lambda input, weights=None, minlength=0: -1,
		torch.binomial: lambda count, prob, generator=None: -1,
		torch.bitwise_and: lambda input, other, out=None: -1,
		torch.bitwise_not: lambda input, out=None: -1,
		torch.bitwise_or: lambda input, other, out=None: -1,
		torch.bitwise_xor: lambda input, other, out=None: -1,
		torch.bitwise_left_shift: lambda input, other, out=None: -1,
		torch.bitwise_right_shift: lambda input, other, out=None: -1,
		torch.block_diag: lambda *tensors: -1,
		torch.bmm: lambda input, mat2, out=None: -1,
		torch.broadcast_tensors: lambda *tensors: -1,
		torch.broadcast_to: lambda self, size: -1,
		torch.bucketize: lambda input, boundaries, out_int32=False, right=False, out=None: -1,
		torch.cartesian_prod: lambda *tensors: -1,
		torch.cat: lambda tensors, dim=0, out=None: -1,
		torch.concat: lambda tensors, dim=0, out=None: -1,  # alias for torch.cat
		torch.concatenate: lambda tensors, dim=0, out=None: -1,  # alias for torch.concatenate
		torch.cdist: lambda x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary': -1,
		torch.ceil: lambda input, out=None: -1,
		torch.celu: lambda input, alpha=1., inplace=False: -1,
		torch.chain_matmul: lambda *matrices, out=None: -1,
		torch.channel_shuffle: lambda input, groups : -1,
		torch.cholesky: lambda input, upper=False, out=None: -1,
		torch.linalg.cholesky: lambda input, out=None: -1,
		torch.linalg.cholesky_ex: lambda input, check_errors=False, out=None: -1,
		torch.cholesky_inverse: lambda input, upper=False, out=None: -1,
		torch.cholesky_solve: lambda input1, input2, upper=False, out=None: -1,
		torch.choose_qparams_optimized: lambda input, numel, n_bins, ratio, bit_width: -1,
		torch.chunk: lambda input, chunks, dim=0: -1,
		torch.clamp: lambda input, min=None, max=None, out=None: -1,
		torch.clip: lambda input, min=None, max=None, out=None: -1,
		torch.clamp_min: lambda input, min, out=None: -1,
		torch.clamp_max: lambda input, max, out=None: -1,
		torch.column_stack: lambda tensors, out=None: -1,
		torch.cov: lambda input, correction=1, fweights=None, aweights=None: -1,
		torch.clone: lambda input: -1,
		torch.combinations: lambda input, r=2, with_replacement=False: -1,
		torch.complex: lambda real, imag: -1,
		torch.copysign: lambda input, other, out=None: -1,
		torch.polar: lambda abs, ang: -1,
		torch.linalg.cond: lambda input, ord=None: -1,
		torch.conj: lambda input, out=None: -1,
		torch.conj_physical: lambda input, out=None: -1,
		torch.resolve_conj: lambda input, out=None: -1,
		torch.resolve_neg: lambda input, out=None: -1,
		torch.constant_pad_nd: lambda input, pad, value=0: -1,
		torch.conv1d: lambda input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1: -1,
		torch.conv2d: lambda input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1: -1,
		torch.conv3d: lambda input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1: -1,
		torch.convolution: lambda input, weight, bias, stride, padding, dilation, transposed, output_adding, groups: -1,
		torch.conv_tbc: lambda input, weight, bias, pad=0: -1,
		torch.conv_transpose1d: lambda input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1: -1,
		torch.conv_transpose2d: lambda input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1: -1,
		torch.conv_transpose3d: lambda input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1: -1,
		torch.corrcoef: lambda input: -1,
		torch.cos: lambda input, out=None: -1,
		torch.cosine_embedding_loss: lambda input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean': -1,
		torch.cosh: lambda input, out=None: -1,
		torch.cosine_similarity: lambda x1, x2, dim=1, eps=1e-8: -1,
		torch.count_nonzero: lambda input: -1,
		torch.cross: lambda input, other, dim=None, out=None: -1,
		torch.linalg.cross: lambda input, other, dim=-1, out=None: -1,
		torch.ctc_loss: (lambda log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean',
						 zero_infinity=False: -1),
		torch.cummax: lambda input, dim, out=None: -1,
		torch.cummin: lambda input, dim, out=None: -1,
		torch.cumprod: lambda input, dim, out=None, dtype=None: -1,
		torch.cumsum: lambda input, dim, out=None, dtype=None: -1,
		torch.cumulative_trapezoid: lambda y, x=None, dim=-1: -1,
		torch.logcumsumexp: lambda input, dim, out=None: -1,
		torch.deg2rad: lambda input, out=None: -1,
		torch.dequantize: lambda input: -1,
		torch.det: lambda input: -1,
		torch.linalg.det: lambda input: -1,  # alias for torch.det  # type: ignore[attr-defined]
		torch.detach: lambda input: -1,
		torch.diag: lambda input, diagonal=0, out=None: -1,
		torch.diag_embed: lambda input, diagonal=0, out=None: -1,
		torch.diagflat: lambda input, offset=0: -1,
		torch.diff: lambda input, n=1, dim=-1, prepend=None, append=None, out=None: -1,
		torch.diagonal: lambda input, offset=0, dim1=0, dim2=1: -1,
		torch.linalg.diagonal: lambda input, offset=0, dim1=-2, dim2=-1: -1,
		torch.diagonal_scatter: lambda input, src, offset=0, dim1=0, dim2=1: -1,
		torch.as_strided_scatter: lambda self, src, size, stride, storage_offset=None: -1,
		torch.digamma: lambda input, out=None: -1,
		torch.dist: lambda input, other, p=2: -1,
		torch.div: lambda input, other, rounding_mode=None, out=None: -1,
		torch.divide: lambda input, other, rounding_mode=None, out=None: -1,
		torch.dot: lambda input, other, out=None: -1,
		torch.dropout: lambda input, p, train, inplace=False: -1,
		torch.dsmm: lambda input, mat2: -1,
		torch.hsmm: lambda mat1, mat2: -1,
		torch.dsplit: lambda input, indices_or_sections: -1,
		torch.dstack: lambda tensors, out=None: -1,
		torch.linalg.eig: lambda input, out=None: -1,
		torch.linalg.eigvals: lambda input, out=None: -1,
		torch.linalg.eigh: lambda input, UPLO="L", out=None: -1,
		torch.linalg.eigvalsh: lambda input, UPLO="L", out=None: -1,
		torch.einsum: lambda equation, *operands: -1,
		torch.embedding: (lambda input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False,
						  sparse=False: -1),
		torch.embedding_bag: (lambda input, weight, offsets, max_norm=None, norm_type=2, scale_grad_by_freq=False,
							  mode='mean', sparse=False, per_sample_weights=None, padding_idx=None: -1),
		torch.empty_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
		torch.eq: lambda input, other, out=None: -1,
		torch.equal: lambda input, other: -1,
		torch.erf: lambda input, out=None: -1,
		torch.erfc: lambda input, out=None: -1,
		torch.erfinv: lambda input, out=None: -1,
		torch.exp: lambda input, out=None: -1,
		torch.exp2: lambda input, out=None: -1,
		torch.expm1: lambda input, out=None: -1,
		torch.fake_quantize_per_channel_affine: lambda input, scale, zero_point, axis, quant_min, quant_max: -1,
		torch.fake_quantize_per_tensor_affine: lambda input, scale, zero_point, quant_min, quant_max: -1,
		torch.fused_moving_avg_obs_fake_quant: (lambda x, observer_on, fake_quant_on, averaging_const, running_min,
												running_max, scale, zero_point, quant_min, quant_max, ch_axis,
												per_row_fake_quant=False, symmetric_quant=False: -1),
		torch.fbgemm_linear_fp16_weight: lambda input, packed_weight, bias: -1,
		torch.fbgemm_linear_fp16_weight_fp32_activation: lambda input, packed_weight, bias: -1,
		torch.fbgemm_linear_int8_weight: lambda input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias: -1,
		torch.fbgemm_linear_int8_weight_fp32_activation: (lambda input, weight, packed, col_offsets, weight_scale,
														  weight_zero_point, bias: -1),
		torch.fbgemm_linear_quantize_weight: lambda input: -1,
		torch.fbgemm_pack_gemm_matrix_fp16: lambda input: -1,
		torch.fbgemm_pack_quantized_matrix: lambda input, a, b: -1,
		torch.feature_alpha_dropout: lambda input, p, train: -1,
		torch.feature_dropout: lambda input, p, train: -1,
		torch.fft.ifft: lambda input, n=None, dim=-1, norm=None: -1,
		torch.fft.rfft: lambda input, n=None, dim=-1, norm=None: -1,
		torch.fft.irfft: lambda input, n=None, dim=-1, norm=None: -1,
		torch.fft.hfft: lambda input, n=None, dim=-1, norm=None: -1,
		torch.fft.ihfft: lambda input, n=None, dim=-1, norm=None: -1,
		torch.fft.hfft2: lambda input, s=None, dim=(-2, -1), norm=None: -1,
		torch.fft.ihfft2: lambda input, s=None, dim=(-2, -1), norm=None: -1,
		torch.fft.hfftn: lambda input, s=None, dim=-1, norm=None: -1,
		torch.fft.ihfftn: lambda input, s=None, dim=-1, norm=None: -1,
		torch.fft.fftn: lambda input, s=None, dim=None, norm=None: -1,
		torch.fft.ifftn: lambda input, s=None, dim=None, norm=None: -1,
		torch.fft.rfftn: lambda input, s=None, dim=None, norm=None: -1,
		torch.fft.irfftn: lambda input, s=None, dim=None, norm=None: -1,
		torch.fft.fft2: lambda input, s=None, dim=(-2, -1), norm=None: -1,
		torch.fft.ifft2: lambda input, s=None, dim=(-2, -1), norm=None: -1,
		torch.fft.rfft2: lambda input, s=None, dim=(-2, -1), norm=None: -1,
		torch.fft.irfft2: lambda input, s=None, dim=(-2, -1), norm=None: -1,
		torch.fft.fftshift: lambda input, dim=None: -1,
		torch.fft.ifftshift: lambda input, dim=None: -1,
		torch.fft.fft: lambda input, n=None, dim=-1, norm=None: -1,
		torch.fix: lambda input, out=None: -1,
		torch.flatten: lambda input, start_dim=0, end_dim=-1: -1,
		torch.flip: lambda input, dims: -1,
		torch.fliplr: lambda input: -1,
		torch.flipud: lambda input: -1,
		torch.frobenius_norm: lambda input, dim=None, keepdim=False, out=None: -1,
		torch.floor: lambda input, out=None: -1,
		torch.floor_divide: lambda input, other: -1,
		torch.float_power: lambda input, exponent, out=None: -1,
		torch.fmod: lambda input, other, out=None: -1,
		torch.frac: lambda input, out=None: -1,
		torch.frexp: lambda input, out=None: -1,
		torch.full_like: lambda input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False: -1,
		torch._functional_assert_async: lambda input, msg, dep_token: -1,
		torch.lu_unpack: lambda LU_data, LU_pivots, unpack_data=True, unpack_pivots=True: -1,
		torch.gather: lambda input, dim, index, out=None, sparse_grad=False: -1,
		torch.gcd: lambda input, other, out=None: -1,
		torch.ge: lambda input, other, out=None: -1,
		torch.greater_equal: lambda input, other, out=None: -1,
		torch.geqrf: lambda input, out=None: -1,
		torch.i0: lambda input, out=None: -1,
		torch.inner: lambda input, other, out=None: -1,
		torch.outer: lambda input, vec2, out=None: -1,
		torch.ger: lambda input, vec2, out=None: -1,  # alias for torch.outer
		torch.gradient: lambda input, spacing=None, dim=None, edge_order=1: -1,
		torch.grid_sampler: lambda input, grid, interpolation_mode, padding_mode, align_corners: -1,
		torch.grid_sampler_2d: lambda input, grid, interpolation_mode, padding_mode, align_corners: -1,
		torch.grid_sampler_3d: lambda input, grid, interpolation_mode, padding_mode, align_corners: -1,
		torch.group_norm: lambda input, num_groups, weight=None, bias=None, eps=1e-05, cudnn_enabled=True: -1,
		torch.gru: lambda input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first: -1,
		torch.gru_cell: lambda input, hx, w_ih, w_hh, b_ih=None, b_hh=None: -1,
		torch.gt: lambda input, other, out=None: -1,
		torch.greater: lambda input, other, out=None: -1,
		torch.hardshrink: lambda input, lambd=0.5: -1,
		torch.heaviside: lambda input, values, out=None: -1,
		torch.hinge_embedding_loss: lambda input, target, margin=1.0, size_average=None, reduce=None, reduction='mean': -1,
		torch.histc: lambda input, bins=100, min=0, max=0, out=None: -1,
		torch.histogram: lambda input, bins=100, min=None, max=None, weight=None, density=False, out=None: -1,
		torch.histogramdd: lambda input, bins, range=None, weight=None, density=False: -1,
		torch.linalg.householder_product: lambda input, tau: -1,
		torch.hspmm: lambda mat1, mat2, out=None: -1,
		torch.hsplit: lambda input, indices_or_sections: -1,
		torch.hstack: lambda tensors, out=None: -1,
		torch.hypot: lambda input, other, out=None: -1,
		torch.igamma: lambda input, other, out=None: -1,
		torch.igammac: lambda input, other, out=None: -1,
		torch.imag: lambda input, out=None: -1,
		torch.index_add: lambda input, dim, index, source: -1,
		torch.index_copy: lambda input, dim, index, source: -1,
		torch.index_put: lambda input, indices, values, accumulate=False: -1,
		torch.index_select: lambda input, dim, index, out=None: -1,
		torch.index_fill: lambda input, dim, index, value: -1,
		torch.index_reduce: lambda input, dim, index, source, reduce, include_input=True: -1,
		torch.isfinite: lambda tensor: -1,
		torch.isin: lambda e, te, assume_unique=False, invert=False: -1,
		torch.isinf: lambda tensor: -1,
		torch.isreal: lambda tensor: -1,
		torch.isposinf: lambda input, out=None: -1,
		torch.isneginf: lambda input, out=None: -1,
		torch.instance_norm: (lambda input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps,
							  cudnn_enabled: -1),
		torch.int_repr: lambda input: -1,
		torch.inverse: lambda input, out=None: -1,
		torch.linalg.inv: lambda input, out=None: -1,
		torch.linalg.inv_ex: lambda input, check_errors=False, out=None: -1,
		torch.is_complex: lambda input: -1,
		torch.is_conj: lambda input: -1,
		torch.is_neg: lambda input: -1,
		torch.is_distributed: lambda input: -1,
		torch.is_inference: lambda input: -1,
		torch.is_floating_point: lambda input: -1,
		torch.is_nonzero: lambda input: -1,
		torch.is_same_size: lambda input, other: -1,
		torch.is_signed: lambda input: -1,
		torch.isclose: lambda input, other, rtol=1e-05, atol=1e-08, equal_nan=False: -1,
		torch.isnan: lambda input: -1,
		torch.istft: (lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True,
					  normalized=False, onesided=None, length=None, return_complex=False: -1),
		torch.kl_div: lambda input, target, size_average=None, reduce=None, reduction='mean', log_target=False: -1,
		torch.kron: lambda input, other: -1,
		torch.kthvalue: lambda input, k, dim=None, keepdim=False, out=None: -1,
		torch.linalg.ldl_factor_ex: lambda input, hermitian=False, check_errors=False, out=None: -1,
		torch.linalg.ldl_factor: lambda input, hermitian=False, out=None: -1,
		torch.linalg.ldl_solve: lambda LD, pivots, B, hermitian=False, out=None: -1,
		torch.layer_norm: lambda input, normalized_shape, weight=None, bias=None, esp=1e-05, cudnn_enabled=True: -1,
		torch.lcm: lambda input, other, out=None: -1,
		torch.ldexp: lambda input, other, out=None: -1,
		torch.le: lambda input, other, out=None: -1,
		torch.less_equal: lambda input, other, out=None: -1,
		torch.lerp: lambda input, end, weight, out=None: -1,
		torch.lgamma: lambda input, out=None: -1,
		torch.lobpcg: lambda input, k=None, B=None, X=None, n=None, iK=None, niter=None, tol=None, largest=None, method=None,
		tracker=None, ortho_iparams=None, ortho_fparams=None, ortho_bparams=None: -1,
		torch.log: lambda input, out=None: -1,
		torch.log_softmax: lambda input, dim, dtype=None: -1,
		torch.log10: lambda input, out=None: -1,
		torch.log1p: lambda input, out=None: -1,
		torch.log2: lambda input, out=None: -1,
		torch.logaddexp: lambda input, other, out=None: -1,
		torch.logaddexp2: lambda input, other, out=None: -1,
		torch.logdet: lambda input: -1,
		torch.xlogy: lambda x, y, out=None: -1,
		torch.logical_and: lambda input, other, out=None: -1,
		torch.logical_not: lambda input, out=None: -1,
		torch.logical_or: lambda input, other, out=None: -1,
		torch.logical_xor: lambda input, other, out=None: -1,
		torch.logit: lambda input, eps=None: -1,
		torch.logsumexp: lambda input, names, keepdim=False, out=None: -1,
		torch.lstm: lambda data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional: -1,
		torch.lstm_cell: lambda input, hx, w_ih, w_hh, b_ih=None, b_hh=None: -1,
		torch.lt: lambda input, other, out=None: -1,
		torch.less: lambda input, other, out=None: -1,
		torch.lu: lambda A, pivot=True, get_infos=False, out=None: -1,
		torch.lu_solve: lambda b, LU_data, LU_pivots, out=None: -1,
		torch.margin_ranking_loss: lambda input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean': -1,  # type: ignore[attr-defined]  # noqa: B950
		torch.masked_fill: lambda input, mask, value: -1,
		torch.masked_scatter: lambda input, mask, source: -1,
		torch.masked_select: lambda input, mask, out=None: -1,
		torch.matmul: lambda input, other, out=None: -1,
		torch.linalg.lu: lambda input, pivot=True, out=None: -1,
		torch.linalg.lu_factor: lambda input, pivot=True, out=None: -1,
		torch.linalg.lu_factor_ex: lambda input, pivot=True, check_errors=False, out=None: -1,
		torch.linalg.lu_solve: lambda LU, pivots, B, left=True, adjoint=False, out=None: -1,
		torch.linalg.matmul: lambda input, other, out=None: -1,  # alias for torch.matmul
		torch.matrix_power: lambda input, n: -1,
		torch.linalg.matrix_power: lambda input, n, out=None: -1,
		torch.linalg.matrix_rank: lambda input, tol=None, hermitian=False: -1,
		torch.linalg.multi_dot: lambda tensors, out=None: -1,
		torch.matrix_exp: lambda input: -1,
		torch.linalg.matrix_exp: lambda input: -1,
		torch.max: lambda input, out=None: -1,
		torch.maximum: lambda input, other, out=None: -1,
		torch.fmax: lambda input, other, out=None: -1,
		torch.max_pool1d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
		torch.max_pool2d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
		torch.max_pool3d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
		torch.max_pool1d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
										return_indices=False, ceil_mode=False: -1),
		torch.mean: lambda input, dim=None: -1,
		torch.nanmean: lambda input, dim=None, keepdim=False, dtype=None, out=None: -1,
		torch.median: lambda input, dim=None: -1,
		torch.nanmedian: lambda input, dim=None: -1,
		torch.meshgrid: lambda *tensors, **kwargs: -1,
		torch.min: lambda input, out=None: -1,
		torch.minimum: lambda input, other, out=None: -1,
		torch.fmin: lambda input, other, out=None: -1,
		torch.miopen_batch_norm: (lambda input, weight, bias, running_mean, running_var, training,
								  exponential_average_factor, epsilon: -1),
		torch.miopen_convolution: lambda input, weight, bias, padding, stride, dilation, groups, benchmark, deterministic: -1,
		torch.miopen_convolution_add_relu: lambda input, weight, z, alpha, bias, stride, padding, dilation, groups: -1,
		torch.miopen_convolution_relu: lambda input, weight, bias, stride, padding, dilation, groups: -1,
		torch.miopen_convolution_transpose: (lambda input, weight, bias, padding, output_padding, stride, dilation,
											 groups, benchmark, deterministic: -1),
		torch.miopen_depthwise_convolution: (lambda input, weight, bias, padding, stride, dilation, groups, benchmark,
											 deterministic: -1),
		torch.miopen_rnn: (lambda input, weight, weight_stride0, hx, cx, mode, hidden_size, num_layers, batch_first,
						   dropout, train, bidirectional, batch_sizes, dropout_state: -1),
		torch.mm: lambda input, mat2, out=None: -1,
		torch.mode: lambda input, dim=-1, keepdim=False, out=None: -1,
		torch.movedim: lambda input, source, destination: -1,
		torch.moveaxis: lambda input, source, destination: -1,
		torch.msort: lambda input, descending=False, out=None: -1,
		torch.mul: lambda input, other, out=None: -1,
		torch.multiply: lambda input, other, out=None: -1,
		torch.multinomial: lambda input, num_samples, replacement=False, out=None: -1,
		torch.mv: lambda input, vec, out=None: -1,
		torch.mvlgamma: lambda input, p: -1,
		torch.narrow: lambda input, dim, start, length: -1,
		torch.nan_to_num: lambda input, nan=0.0, posinf=None, neginf=None, out=None: -1,
		torch.native_batch_norm: lambda input, weight, bias, running_mean, running_var, training, momentum, eps: -1,
		torch._native_batch_norm_legit: lambda input, weight, bias, training, momentum, eps: -1,
		torch.native_dropout: lambda input, p, train: -1,
		torch.native_layer_norm: lambda input, normalized_shape, weight=None, bias=None, eps=1e-05: -1,
		torch.native_group_norm: lambda input, weight, bias, N, C, HxW, group, eps: -1,
		torch.native_norm: lambda input, p=2, dim=None, keepdim=False, dtype=None: -1,
		torch.native_channel_shuffle: lambda input, groups : -1,
		torch.ne: lambda input, other, out=None: -1,
		torch.not_equal: lambda input, other, out=None: -1,
		torch.neg: lambda input, out=None: -1,
		torch.negative: lambda input, out=None: -1,
		torch.nextafter: lambda input, other, out=None: -1,
		torch.nn.functional.adaptive_avg_pool2d: lambda input, output_size: -1,
		torch.nn.functional.adaptive_avg_pool3d: lambda input, output_size: -1,
		torch.nn.functional.adaptive_max_pool1d: lambda input, output_size, return_indices=False: -1,
		torch.nn.functional.adaptive_max_pool1d_with_indices: lambda input, output_size, return_indices=False: -1,
		torch.nn.functional.adaptive_max_pool2d: lambda input, output_size, return_indices=False: -1,
		torch.nn.functional.adaptive_max_pool2d_with_indices: lambda input, output_size, return_indices=False: -1,
		torch.nn.functional.adaptive_max_pool3d: lambda input, output_size, return_indices=False: -1,
		torch.nn.functional.adaptive_max_pool3d_with_indices: lambda input, output_size, return_indices=False: -1,
		torch.nn.functional.affine_grid: lambda theta, size, align_corners=None: -1,
		torch.nn.functional.alpha_dropout: lambda input, p=0.5, training=False, inplace=False: -1,
		torch.nn.functional.avg_pool2d: (lambda input, kernel_size, stride=None, padding=0, ceil_mode=False,
										 count_include_pad=True, divisor_override=None: -1),
		torch.nn.functional.avg_pool3d: (lambda input, kernel_size, stride=None, padding=0, ceil_mode=False,
										 count_include_pad=True, divisor_override=None: -1),
		torch.nn.functional.batch_norm: (lambda input, running_mean, running_var, weight=None, bias=None, training=False,
										 momentum=0.1, eps=1e-05: -1),
		torch.nn.functional.bilinear: lambda input1, input2, weight, bias=None: -1,
		torch.nn.functional.binary_cross_entropy: (lambda input, target, weight=None, size_average=None, reduce=None,
												   reduction="mean": -1),
		torch.nn.functional.binary_cross_entropy_with_logits: (lambda input, target, weight=None, size_average=None,
															   reduce=None, reduction="mean", pos_weight=None: -1),
		torch.nn.functional.celu: lambda input, alpha=1.0, inplace=False: -1,
		torch.nn.functional.cosine_embedding_loss: (lambda input1, input2, target, margin=0, size_average=None,
													reduce=None, reduction='mean': -1),
		torch.nn.functional.cross_entropy: (lambda input, target, weight=None, size_average=None, ignore_index=-100,
											reduce=None, reduction="mean", label_smoothing=0.0: -1),
		torch.nn.functional.ctc_loss: (lambda log_probs, targets, input_lengths, target_lengths, blank=0,
									   reduction='mean', zero_infinity=False: -1),
		torch.nn.functional.dropout: lambda input, p=0.5, training=True, inplace=False: -1,
		torch.nn.functional.dropout1d: lambda input, p=0.5, training=True, inplace=False: -1,
		torch.nn.functional.dropout2d: lambda input, p=0.5, training=True, inplace=False: -1,
		torch.nn.functional.dropout3d: lambda input, p=0.5, training=True, inplace=False: -1,
		torch.nn.functional.elu: lambda input, alpha=1.0, inplace=False: -1,
		torch.nn.functional.embedding: (lambda input, weight, padding_idx=None, max_norm=None, norm_type=2.0,
										scale_grad_by_freq=False, sparse=False: -1),
		torch.nn.functional.embedding_bag: (lambda input, weight, offsets=None, max_norm=None, norm_type=2,
											scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None,
											include_last_offset=False, padding_idx=None: -1),
		torch.nn.functional.feature_alpha_dropout: lambda input, p=0.5, training=False, inplace=False: -1,
		torch.nn.functional.fold: lambda input, output_size, kernel_size, dilation=1, padding=0, stride=1: -1,
		torch.nn.functional.fractional_max_pool2d: (lambda input, kernel_size, output_size=None, output_ratio=None,
													return_indices=False, _random_samples=None: -1),
		torch.nn.functional.fractional_max_pool2d_with_indices: (
			lambda input, kernel_size, output_size=None, output_ratio=None, return_indices=False,
			_random_samples=None: -1),
		torch.nn.functional.fractional_max_pool3d: (lambda input, kernel_size, output_size=None, output_ratio=None,
													return_indices=False, _random_samples=None: -1),
		torch.nn.functional.fractional_max_pool3d_with_indices: (
			lambda input, kernel_size, output_size=None, output_ratio=None, return_indices=False,
			_random_samples=None: -1),
		torch.nn.functional.gaussian_nll_loss: lambda input, target, var, full=False, eps=1e-06, reduction='mean': -1,
		torch.nn.functional.gelu: lambda input, approximate='none': -1,
		torch.nn.functional.glu: lambda input, dim=-1: -1,
		torch.nn.functional.grid_sample: lambda input, grid, mode='bilinear', padding_mode='zeros', align_corners=None: -1,
		torch.nn.functional.group_norm: lambda input, num_groups, weight=None, bias=None, eps=1e-05: -1,
		torch.nn.functional.gumbel_softmax: lambda logits, tau=1, hard=False, eps=1e-10, dim=-1: -1,
		torch.nn.functional.hardshrink: lambda input, lambd=0.5: -1,
		torch.nn.functional.hardtanh: lambda input, min_val=-1., max_val=1., inplace=False: -1,
		torch.nn.functional.hinge_embedding_loss: (lambda input, target, margin=1.0, size_average=None, reduce=None,
												   reduction='mean': -1),
		torch.nn.functional.instance_norm: (lambda input, running_mean=None, running_var=None, weight=None, bias=None,
											use_input_stats=True, momentum=0.1, eps=1e-05: -1),
		torch.nn.functional.interpolate: (lambda input, size=None, scale_factor=None, mode='nearest', align_corners=None,
										  recompute_scale_factor=None, antialias=False: -1),
		torch.nn.functional.kl_div: lambda input, target, size_average=None, reduce=None, reduction='mean', log_target=False: -1,
		torch.nn.functional.l1_loss: lambda input, target, size_average=None, reduce=None, reduction='mean': -1,
		torch.nn.functional.layer_norm: lambda input, normalized_shape, weight=None, bias=None, eps=1e-05: -1,
		torch.nn.functional.leaky_relu: lambda input, negative_slope=0.01, inplace=False: -1,
		torch.nn.functional.linear: lambda input, weight, bias=None: -1,
		torch.nn.functional.local_response_norm: lambda input, size, alpha=0.0001, beta=0.75, k=1.0: -1,
		torch.nn.functional.log_softmax: lambda input, dim=None, _stacklevel=3, dtype=None: -1,
		torch.nn.functional.logsigmoid: lambda input: -1,
		torch.nn.functional.lp_pool1d: lambda input, norm_type, kernel_size, stride=None, ceil_mode=False: -1,
		torch.nn.functional.lp_pool2d: lambda input, norm_type, kernel_size, stride=None, ceil_mode=False: -1,
		torch.nn.functional.lp_pool3d: lambda input, norm_type, kernel_size, stride=None, ceil_mode=False: -1,
		torch.nn.functional.margin_ranking_loss: (lambda input1, input2, target, margin=0, size_average=None,
												  reduce=None, reduction='mean': -1),
		torch.nn.functional.max_pool1d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
										 ceil_mode=False, return_indices=False: -1),
		torch.nn.functional.max_pool1d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
													  return_indices=False, ceil_mode=False: -1),
		torch.nn.functional.max_pool2d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
										 ceil_mode=False, return_indices=False: -1),
		torch.nn.functional.max_pool2d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
													  return_indices=False, ceil_mode=False: -1),
		torch.nn.functional.max_pool3d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
										 return_indices=False, ceil_mode=False: -1),
		torch.nn.functional.max_pool3d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
													  return_indices=False, ceil_mode=False: -1),
		torch.nn.functional.max_unpool1d: lambda input, indices, kernel_size, stride=None, padding=0, output_size=None: -1,
		torch.nn.functional.max_unpool2d: lambda input, indices, kernel_size, stride=None, padding=0, output_size=None: -1,
		torch.nn.functional.max_unpool3d: lambda input, indices, kernel_size, stride=None, padding=0, output_size=None: -1,
		torch.nn.functional.mse_loss: lambda input, target, size_average=None, reduce=None, reduction='mean': -1,
		torch.nn.functional.multi_head_attention_forward: (
			lambda query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v,
			add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training=True, key_padding_mask=None,
			need_weights=True, attn_mask=None, use_separate_proj_weight=False, q_proj_weight=None, k_proj_weight=None,
			v_proj_weight=None, static_k=None, static_v=None, average_attn_weights=None, is_causal=False: -1),
		torch.nn.functional.multi_margin_loss: (lambda input, target, p=1, margin=1.0, weight=None, size_average=None,
												reduce=None, reduction='mean': -1),
		torch.nn.functional.multilabel_margin_loss: (lambda input, target, size_average=None, reduce=None,
													 reduction='mean': -1),
		torch.nn.functional.multilabel_soft_margin_loss: (lambda input, target, weight=None, size_average=None,
														  reduce=None, reduction='mean': -1),
		torch.nn.functional.nll_loss: (lambda input, target, weight=None, size_average=None, ignore_index=-100,
									   reduce=None, reduction='mean': -1),
		torch.nn.functional.normalize: lambda input, p=2, dim=1, eps=1e-12, out=None: -1,
		torch.nn.functional.one_hot: lambda tensor, num_classes=-1: -1,
		torch.nn.functional.pad: lambda input, pad, mode='constant', value=0: -1,
		torch.nn.functional.pairwise_distance: lambda x1, x2, p=2.0, eps=1e-06, keepdim=False: -1,
		torch.nn.functional.poisson_nll_loss: (lambda input, target, log_input=True, full=False, size_average=None,
											   eps=1e-08, reduce=None, reduction='mean': -1),
		torch.nn.functional.prelu: lambda input, weight: -1,
		torch.nn.functional.relu: lambda input, inplace=False: -1,
		torch.nn.functional.relu6: lambda input, inplace=False: -1,
		torch.nn.functional.rrelu: lambda input, lower=0.125, upper=0.3333333333333333, training=False, inplace=False: -1,
		torch.nn.functional.selu: lambda input, inplace=False: -1,
		torch.nn.functional.silu: lambda input, inplace=False: -1,
		torch.nn.functional.mish: lambda input, inplace=False: -1,
		torch.nn.functional.scaled_dot_product_attention: lambda query, key, value, attn_mask=None, dropout_p=0.0: -1,
		torch.nn.functional.smooth_l1_loss: lambda input, target, size_average=None, reduce=None, reduction='mean', beta=1.: -1,
		torch.nn.functional.huber_loss: lambda input, target, reduction='mean', delta=1.: -1,
		torch.nn.functional.soft_margin_loss: lambda input, target, size_average=None, reduce=None, reduction='mean': -1,
		torch.nn.functional.softmax: lambda input, dim=None, _stacklevel=3, dtype=None: -1,
		torch.nn.functional.softmin: lambda input, dim=None, _stacklevel=3, dtype=None: -1,
		torch.nn.functional.softplus: lambda input, beta=1, threshold=20: -1,
		torch.nn.functional.softshrink: lambda input, lambd=0.5: -1,
		torch.nn.functional.softsign: lambda input: -1,
		torch.nn.functional.tanhshrink: lambda input: -1,
		torch.nn.functional.threshold: lambda input, threshold, value, inplace=False: -1,
		torch.nn.functional.triplet_margin_loss: (lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06,
												  swap=False, size_average=None, reduce=None, reduction='mean': -1),
		torch.nn.functional.triplet_margin_with_distance_loss: (lambda anchor, positive, negative, *,
																distance_function=None, margin=1.0,
																swap=False, reduction='mean': -1),
		torch.nn.functional.unfold: lambda input, kernel_size, dilation=1, padding=0, stride=1: -1,
		torch.nn.init.uniform_: lambda tensor, a=0., b=1., generator=None: -1,
		torch.nn.init.normal_: lambda tensor, mean=0., std=1., generator=None: -1,
		torch.nn.init.constant_: lambda tensor, val: -1,
		torch.nn.init.kaiming_uniform_: lambda tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', generator=None: -1,
		torch.nonzero: lambda input, as_tuple=False: -1,
		torch.nonzero_static: lambda input, *, size, fill_value=-1: -1,
		torch.argwhere: lambda input: -1,
		torch.norm: lambda input, p='fro', dim=None, keepdim=False, out=None, dtype=None: -1,
		torch.linalg.norm: lambda input, ord=None, dim=None, keepdim=False, out=None, dtype=None: -1,
		torch.linalg.vector_norm: lambda input, ord=2, dim=None, keepdim=False, out=None, dtype=None: -1,
		torch.linalg.matrix_norm: lambda input, ord='fro', dim=(-2, -1), keepdim=False, out=None, dtype=None: -1,
		torch.norm_except_dim: lambda v, pow=2, dim=0: -1,
		torch.nuclear_norm: lambda input, p='fro', dim=None, keepdim=False, out=None, dtype=None: -1,
		torch.numel: lambda input: -1,
		torch.orgqr: lambda input, tau: -1,
		torch.ormqr: lambda input, input2, input3, left=True, transpose=False: -1,
		torch.pairwise_distance: lambda x1, x2, p=2.0, eps=1e-06, keepdim=False: -1,
		torch.permute: lambda self, dim: -1,
		torch.pca_lowrank: lambda input, q=None, center=True, niter=2: -1,
		torch.pdist: lambda input, p=2: -1,
		torch.pinverse: lambda input, rcond=1e-15: -1,
		torch.linalg.pinv: lambda input, rcond=1e-15, hermitian=False: -1,
		torch.pixel_shuffle: lambda input, upscale_factor: -1,
		torch.pixel_unshuffle: lambda input, downscale_factor: -1,
		torch.poisson: lambda input, generator=None: -1,
		torch.poisson_nll_loss: lambda input, target, log_input, full, eps, reduction: -1,
		torch.polygamma: lambda input, n, out=None: -1,
		torch.positive: lambda input, out=None: -1,
		torch.prelu: lambda input, weight: -1,
		torch.ones_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
		torch.pow: lambda input, exponent, out=None: -1,
		torch.prod: lambda input, dtype=None: -1,
		torch.put: lambda input, index, source, accumulate=False: -1,
		torch.q_per_channel_axis: lambda input: -1,
		torch.q_per_channel_scales: lambda input: -1,
		torch.q_per_channel_zero_points: lambda input: -1,
		torch.q_scale: lambda input: -1,
		torch.q_zero_point: lambda input: -1,
		torch.qr: lambda input, some=True, out=None: -1,
		torch.linalg.qr: lambda input, mode='reduced', out=None: -1,
		torch.quantile: lambda input, q, dim=None, keepdim=False, interpolation='linear', out=None: -1,
		torch.nanquantile: lambda input, q, dim=None, keepdim=False, interpolation='linear', out=None: -1,
		torch.quantize_per_channel: lambda input, scales, zero_points, axis, dtype: -1,
		torch.quantize_per_tensor: lambda input, scale, zero_point, dtype: -1,
		torch.quantize_per_tensor_dynamic: lambda input, dtype, reduce_range: -1,
		torch.quantized_batch_norm: lambda input, weight, bias, mean, var, eps, output_scale, output_zero_point: -1,
		torch.quantized_gru_cell: (lambda input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih,
								   col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh: -1),

		torch.quantized_lstm_cell: (lambda input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih,
									col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh: -1),
		torch.quantized_max_pool1d: (lambda input, kernel_size, stride=tuple(), padding=(0,),
									 dilation=(1,), ceil_mode=False: -1),
		torch.quantized_max_pool2d: (lambda input, kernel_size, stride=tuple(), padding=(0, 0),
									 dilation=(1, 1), ceil_mode=False: -1),
		torch.quantized_max_pool3d: (lambda input, kernel_size, stride=tuple(), padding=(0, 0, 0),
									 dilation=(1, 1, 1), ceil_mode=False: -1),
		torch.quantized_rnn_relu_cell: (lambda input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih,
										col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh: -1),
		torch.quantized_rnn_tanh_cell: (lambda input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih,
										col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh: -1),
		torch.rad2deg: lambda input, out=None: -1,
		torch.rand_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
		torch.randint_like: lambda input, high, dtype=None, layout=torch.strided, device=None, requires_grad=False: -1,
		torch.randn_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
		torch.ravel: lambda input: -1,
		torch.real: lambda input, out=None: -1,
		torch.vdot: lambda input, other, out=None: -1,
		torch.linalg.vecdot: lambda input, other, dim=-1, out=None: -1,
		torch.view_as_real: lambda input: -1,
		torch.view_as_complex: lambda input: -1,
		torch.reciprocal: lambda input, out=None: -1,
		torch.relu: lambda input, inplace=False: -1,
		torch.remainder: lambda input, other, out=None: -1,
		torch.renorm: lambda input, p, dim, maxnorm, out=None: -1,
		torch.repeat_interleave: lambda input, dim=None: -1,
		torch.reshape: lambda input, shape: -1,
		torch.rnn_relu: lambda input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first: -1,
		torch.rnn_relu_cell: lambda input, hx, w_ih, w_hh, b_ih=None, b_hh=None: -1,
		torch.rnn_tanh: lambda input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first: -1,
		torch.rnn_tanh_cell: lambda input, hx, w_ih, w_hh, b_ih=None, b_hh=None: -1,
		torch.roll: lambda input, shifts, dims=None: -1,
		torch.rot90: lambda input, k=1, dims=(0, 1): -1,
		torch.round: lambda input, out=None: -1,
		torch.row_stack: lambda tensors, out=None: -1,  # alias for torch.vstack
		torch._rowwise_prune: (lambda weight, mask, compressed_indices_dtype: -1),
		torch.rrelu: lambda input, lower=1. / 8, upper=1. / 3, training=False, inplace=False: -1,
		torch.rsqrt: lambda input, out=None: -1,
		torch.rsub: lambda input, other, alpha=1: -1,
		torch.saddmm: lambda input, mat1, mat2, beta=1, alpha=1, out=None: -1,
		torch.scatter: lambda input, dim, index, src: -1,
		torch.scatter_add: lambda input, dim, index, src: -1,
		torch.scatter_reduce: lambda input, dim, index, src, reduce, include_self=True: -1,
		torch.searchsorted: lambda sorted_sequence, input, out_int32=False, right=False, out=None: -1,
		torch._segment_reduce: lambda data, reduce="max", lengths=None, indices=None, offsets=None, axis=0, unsafe=False: -1,
		torch.select: lambda input, dim, index: -1,
		torch.select_scatter: lambda input, src, dim, index: -1,
		torch.slice_scatter: lambda input, src, dim=0, start=None, end=None, step=1: -1,
		torch.selu: lambda input, inplace=False: -1,
		torch.sigmoid: lambda input, out=None: -1,
		torch.sign: lambda input, out=None: -1,
		torch.signbit: lambda input, out=None: -1,
		torch.sgn: lambda input, out=None: -1,
		torch.sin: lambda input, out=None: -1,
		torch.sinc: lambda input, out=None: -1,
		torch.sinh: lambda input, out=None: -1,
		torch.slogdet: lambda input: -1,
		torch.linalg.slogdet: lambda input: -1,
		torch.smm: lambda input, mat2: -1,
		torch.spmm: lambda input, mat2: -1,
		torch.softmax: lambda input, dim, dtype=None: -1,
		torch.linalg.solve: lambda A, B, left=True, out=None: -1,
		torch.linalg.solve_ex: lambda A, B, left=True, check_errors=False, out=None: -1,
		torch.sort: lambda input, dim=-1, descending=False, *, stable=False, out=None: -1,
		torch.split: lambda tensor, split_size_or_sections, dim=0: -1,
		torch.split_with_sizes: lambda tensor, split_size_or_sections, dim=0: -1,
		torch.sqrt: lambda input, out=None: -1,
		torch.square: lambda input, out=None: -1,
		torch.squeeze: lambda input, dim=None, out=None: -1,
		torch.sspaddmm: lambda input, mat1, mat2, beta=1, alpha=1, out=None: -1,
		torch.stack: lambda tensors, dim=0, out=None: -1,
		torch.std: lambda input, dim=None: -1,
		torch.std_mean: lambda input, dim=None: -1,
		torch.stft: (lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True,
					 pad_mode='reflect', normalized=False, onesided=True, return_complex=None: -1),
		torch.sub: lambda input, other, out=None: -1,
		torch.subtract: lambda input, other, out=None: -1,
		torch.sum: lambda input, dim=None: -1,
		torch.sym_float: lambda input: -1,
		torch.sym_int: lambda input: -1,
		torch.sym_max: lambda a, b: -1,
		torch.sym_min: lambda a, b: -1,
		torch.sym_not: lambda input: -1,
		torch.sym_ite: lambda a, b, c: -1,
		torch.sym_sqrt: lambda input: -1,
		torch.nansum: lambda input, dim=None: -1,
		torch.svd: lambda input, some=True, compute_uv=True, out=None: -1,
		torch.svd_lowrank: lambda input, q=6, niter=2, M=None: -1,
		torch.linalg.svd: lambda input, full_matrices=True, out=None: -1,
		torch.linalg.svdvals: lambda input, out=None: -1,
		torch.swapaxes: lambda input, dim0, dim1: -1,
		torch.swapdims: lambda input, axis0, axis1: -1,
		torch.special.airy_ai: lambda input: -1,
		torch.special.bessel_j0: lambda input: -1,
		torch.special.bessel_j1: lambda input: -1,
		torch.special.bessel_y0: lambda input: -1,
		torch.special.bessel_y1: lambda input: -1,
		torch.special.chebyshev_polynomial_t: lambda input, n, out=None: -1,
		torch.special.chebyshev_polynomial_u: lambda input, n, out=None: -1,
		torch.special.chebyshev_polynomial_v: lambda input, n, out=None: -1,
		torch.special.chebyshev_polynomial_w: lambda input, n, out=None: -1,
		torch.special.digamma: lambda input: -1,
		torch.special.entr: lambda input: -1,
		torch.special.erf: lambda input: -1,
		torch.special.erfc: lambda input: -1,
		torch.special.erfcx: lambda input: -1,
		torch.special.erfinv: lambda input: -1,
		torch.special.exp2: lambda input: -1,
		torch.special.expit: lambda input: -1,
		torch.special.expm1: lambda input: -1,
		torch.special.gammainc: lambda input, other, out=None: -1,
		torch.special.gammaincc: lambda input, other, out=None: -1,
		torch.special.gammaln: lambda input: -1,
		torch.special.hermite_polynomial_h: lambda input, n, out=None: -1,
		torch.special.hermite_polynomial_he: lambda input, n, out=None: -1,
		torch.special.i0: lambda input: -1,
		torch.special.i0e: lambda input: -1,
		torch.special.i1: lambda input: -1,
		torch.special.i1e: lambda input: -1,
		torch.special.laguerre_polynomial_l: lambda input, n, out=None: -1,
		torch.special.legendre_polynomial_p: lambda input, n, out=None: -1,
		torch.special.log1p: lambda input: -1,
		torch.special.log_ndtr: lambda input: -1,
		torch.special.log_softmax: lambda input, dim, dtype=None: -1,
		torch.special.logit: lambda input: -1,
		torch.special.logsumexp: lambda input, dim, keepdim=False, out=None: -1,
		torch.special.modified_bessel_i0: lambda input: -1,
		torch.special.modified_bessel_i1: lambda input: -1,
		torch.special.modified_bessel_k0: lambda input: -1,
		torch.special.modified_bessel_k1: lambda input: -1,
		torch.special.multigammaln: lambda input, p: -1,
		torch.special.ndtr: lambda input: -1,
		torch.special.ndtri: lambda input: -1,
		torch.special.polygamma: lambda input, n, out=None: -1,
		torch.special.psi: lambda input: -1,
		torch.special.round: lambda input: -1,
		torch.special.scaled_modified_bessel_k0: lambda input: -1,
		torch.special.scaled_modified_bessel_k1: lambda input: -1,
		torch.special.shifted_chebyshev_polynomial_t: lambda input, n, out=None: -1,
		torch.special.shifted_chebyshev_polynomial_u: lambda input, n, out=None: -1,
		torch.special.shifted_chebyshev_polynomial_v: lambda input, n, out=None: -1,
		torch.special.shifted_chebyshev_polynomial_w: lambda input, n, out=None: -1,
		torch.special.sinc: lambda input: -1,
		torch.special.softmax: lambda input, dim, dtype=None: -1,
		torch.special.spherical_bessel_j0: lambda input: -1,
		torch.special.xlog1py: lambda input, other, out=None: -1,
		torch.special.xlogy: lambda input, other, out=None: -1,
		torch.special.zeta: lambda self, other, out=None: -1,
		torch.t: lambda input: -1,
		torch.take: lambda input, index: -1,
		torch.take_along_dim: lambda input, indices, dim=None, out=None: -1,
		torch.tan: lambda input, out=None: -1,
		torch.tanh: lambda input, out=None: -1,
		torch.linalg.tensorinv: lambda a, ind=2: -1,
		torch.linalg.tensorsolve: lambda a, b, dims=None: -1,
		torch.tensordot: lambda a, b, dims=2, out=None: -1,
		torch.tensor_split: lambda input, indices_or_sections, dim=0: -1,
		torch.threshold: lambda input, threshold, value, inplace=False: -1,
		torch.tile: lambda input, dims: -1,
		torch.topk: lambda input, k, dim=-1, descending=False, out=None: -1,
		torch.trace: lambda input: -1,
		torch.transpose: lambda input, dim0, dim1: -1,
		torch.trapz: lambda y, x=None, dim=-1: -1,
		torch.trapezoid: lambda y, x=None, dim=-1: -1,
		torch.triangular_solve: lambda input, A, upper=True, transpose=False, unitriangular=False: -1,
		torch.linalg.solve_triangular: lambda input, B, upper, left=True, unitriangular=False: -1,
		torch.tril: lambda input, diagonal=0, out=None: -1,
		torch.triplet_margin_loss: (lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False,

									size_average=None, reduce=None, reduction='mean': -1),
		torch.triu: lambda input, diagonal=0, out=None: -1,
		torch.true_divide: lambda input, other: -1,
		torch.trunc: lambda input, out=None: -1,
		torch.unbind: lambda input, dim=0: -1,
		torch.unflatten: lambda input, dim, sizes, names: -1,
		torch.unique: lambda input, sorted=True, return_inverse=False, return_counts=False, dim=None: -1,
		torch.unique_consecutive: lambda input, return_inverse=False, return_counts=False, dim=None: -1,
		torch.unravel_index: lambda indices, shape: -1,
		torch.unsafe_chunk: lambda input, chunks, dim=0: -1,
		torch.unsafe_split: lambda tensor, split_size_or_sections, dim=0: -1,
		torch.unsafe_split_with_sizes: lambda tensor, split_size_or_sections, dim=0: -1,
		torch.unsqueeze: lambda input, dim, out=None: -1,
		torch.linalg.vander: lambda x, N=None: -1,
		torch.var: lambda input, dim=None: -1,
		torch.var_mean: lambda input, dim=None: -1,
		torch.vsplit: lambda input, indices_or_sections: -1,
		torch.vstack: lambda tensors, out=None: -1,
		torch.where: lambda condition, x=None, y=None: -1,
		torch.zeros_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
		torch._fw_primal_copy: lambda self, level: -1,
		torch._make_dual_copy: lambda primal, tangent, level: -1,
		torch.view_as_real_copy: lambda self: -1,
		torch.view_as_complex_copy: lambda self: -1,
		torch._conj_copy: lambda self: -1,
		torch._neg_view_copy: lambda self: -1,
		torch.as_strided_copy: lambda self, size, stride, storage_offset=None: -1,
		torch._sparse_broadcast_to_copy: lambda self, size: -1,
		torch.diagonal_copy: lambda self, offset=0, dim1=0, dim2=1: -1,
		torch.expand_copy: lambda self, size, *, implicit=False: -1,
		torch.narrow_copy: lambda self, dim, start, length: -1,
		torch.permute_copy: lambda self, dims: -1,
		torch._reshape_alias_copy: lambda self, size, stride: -1,
		torch.select_copy: lambda self, dim, index: -1,
		torch.detach_copy: lambda self: -1,
		torch.slice_copy: lambda self, dim=0, start=None, end=None, step=1: -1,
		torch.split_copy: lambda self, split_size, dim=0: -1,
		torch.split_with_sizes_copy: lambda self, split_sizes, dim=0: -1,
		torch.squeeze_copy: lambda self, dim: -1,
		torch.t_copy: lambda self: -1,
		torch.transpose_copy: lambda self, dim0, dim1: -1,
		torch.unsqueeze_copy: lambda self, dim: -1,
		torch._indices_copy: lambda self: -1,
		torch._values_copy: lambda self: -1,
		torch.indices_copy: lambda self: -1,
		torch.values_copy: lambda self: -1,
		torch.crow_indices_copy: lambda self: -1,
		torch.col_indices_copy: lambda self: -1,
		torch.ccol_indices_copy: lambda self: -1,
		torch.row_indices_copy: lambda self: -1,
		torch.unbind_copy: lambda self, dim=0: -1,
		torch.view_copy: lambda self, dtype: -1,
		torch.unfold_copy: lambda self, dimension, size, step: -1,
		torch.alias_copy: lambda self: -1,
		Tensor.__floordiv__: lambda self, other: -1,
		Tensor.__rfloordiv__: lambda self, other: -1,
		Tensor.__ifloordiv__: lambda self, other: -1,
		Tensor.__truediv__: lambda self, other: -1,
		Tensor.__rtruediv__: lambda self, other: -1,
		Tensor.__itruediv__: lambda self, other: -1,
		Tensor.__lshift__: lambda self, other: -1,
		Tensor.__rlshift__: lambda self, other: -1,
		Tensor.__ilshift__: lambda self, other: -1,
		Tensor.__rshift__: lambda self, other: -1,
		Tensor.__rrshift__: lambda self, other: -1,
		Tensor.__irshift__: lambda self, other: -1,
		Tensor.__and__: lambda self, other: -1,
		Tensor.__or__: lambda self, other: -1,
		Tensor.__xor__: lambda self, other: -1,
		Tensor.__float__: lambda self: -1,
		Tensor.__complex__: lambda self: -1,
		Tensor.__array__: lambda self, dtype: -1,
		Tensor.__bool__: lambda self: -1,
		Tensor.__contains__: lambda self, other: -1,
		Tensor.__neg__: lambda self: -1,
		Tensor.__invert__: lambda self: -1,
		Tensor.__mod__: lambda self, other: -1,
		Tensor.__rmod__: lambda self, other: -1,
		Tensor.__imod__: lambda self, other: -1,
		Tensor.__array_wrap__: lambda self, array: -1,
		Tensor.__getitem__: lambda self, idx: -1,
		Tensor.__deepcopy__: lambda self, memo: -1,
		Tensor.__int__: lambda self: -1,
		Tensor.__long__: lambda self: -1,
		Tensor.__index__: lambda self: -1,
		Tensor.__len__: lambda self: -1,
		Tensor.__format__: lambda self, format_spec: -1,
		Tensor.__reduce_ex__: lambda self, proto: -1,
		Tensor.__reversed__: lambda self: -1,
		Tensor.__repr__: lambda self, *, tensor_contents=None: -1,
		Tensor.__setitem__: lambda self, k, v: -1,
		Tensor.__setstate__: lambda self, d: -1,
		Tensor.T.__get__: lambda self: -1,
		Tensor.H.__get__: lambda self: -1,
		Tensor.mT.__get__: lambda self: -1,
		Tensor.mH.__get__: lambda self: -1,
		Tensor._backward_hooks.__get__: lambda self: -1,
		Tensor._post_accumulate_grad_hooks.__get__: lambda self: -1,
		Tensor._base.__get__: lambda self: -1,
		Tensor._cdata.__get__: lambda self: -1,
		Tensor.grad.__get__: lambda self: -1,
		Tensor._grad.__get__: lambda self: -1,
		Tensor._grad_fn.__get__: lambda self: -1,
		Tensor.grad_fn.__get__: lambda self: -1,
		Tensor._version.__get__: lambda self: -1,
		Tensor._autocast_to_reduced_precision: lambda self, cuda_enabled, cpu_enabled, cuda_dtype, cpu_dtype: -1,
		Tensor._autocast_to_full_precision: lambda self, cuda_enabled, cpu_enabled: -1,
		Tensor.data.__get__: lambda self: -1,
		Tensor.device.__get__: lambda self: -1,
		Tensor.dtype.__get__: lambda self: -1,
		Tensor.is_cuda.__get__: lambda self: -1,
		Tensor.is_cpu.__get__: lambda self: -1,
		Tensor.is_xla.__get__: lambda self: -1,
		Tensor.is_xpu.__get__: lambda self: -1,
		Tensor.is_ipu.__get__: lambda self: -1,
		Tensor.is_leaf.__get__: lambda self: -1,
		Tensor.retains_grad.__get__: lambda self: -1,
		Tensor.is_meta.__get__: lambda self: -1,
		Tensor.is_mps.__get__: lambda self: -1,
		Tensor.is_mtia.__get__: lambda self: -1,
		Tensor.is_nested.__get__: lambda self: -1,
		Tensor.is_ort.__get__: lambda self: -1,
		Tensor.is_mkldnn.__get__: lambda self: -1,
		Tensor.is_quantized.__get__: lambda self: -1,
		Tensor.is_sparse.__get__: lambda self: -1,
		Tensor.is_sparse_csr.__get__: lambda self: -1,
		Tensor.is_vulkan.__get__: lambda self: -1,
		Tensor.itemsize.__get__: lambda self: -1,
		Tensor.layout.__get__: lambda self: -1,
		Tensor.name.__get__: lambda self: -1,
		Tensor.names.__get__: lambda self: -1,
		Tensor.nbytes.__get__: lambda self: -1,
		Tensor.ndim.__get__: lambda self: -1,
		Tensor.output_nr.__get__: lambda self: -1,
		Tensor.requires_grad.__get__: lambda self: -1,
		Tensor.shape.__get__: lambda self: -1,
		Tensor.volatile.__get__: lambda self: -1,
		Tensor.real.__get__: lambda self: -1,
		Tensor.imag.__get__: lambda self: -1,
		Tensor.__cuda_array_interface__.__get__: lambda self: -1,
		Tensor.type: lambda self, dtype=None, non_blocking=False, **kwargs: -1,
		Tensor._dimI: lambda self: -1,
		Tensor._dimV: lambda self: -1,
		Tensor._indices: lambda self: -1,
		Tensor._is_view: lambda self: -1,
		Tensor._nnz: lambda self: -1,
		Tensor.crow_indices: lambda self: -1,
		Tensor.col_indices: lambda self: -1,
		Tensor.ccol_indices: lambda self: -1,
		Tensor.row_indices: lambda self: -1,
		Tensor._update_names: lambda self, names, inplace: -1,
		Tensor._values: lambda self: -1,
		Tensor.adjoint: lambda self: -1,
		Tensor.align_as: lambda self, other: -1,
		Tensor.align_to: lambda self, order, ellipsis_idx: -1,
		Tensor.apply_: lambda self, callable: -1,
		Tensor.as_strided: lambda self, size, stride: -1,
		Tensor.as_strided_: lambda self, size, stride: -1,
		Tensor.backward: lambda self, gradient=None, retain_graph=None, create_graph=False, inputs=None: -1,
		Tensor.bfloat16: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.bool: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.byte: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.char: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.cauchy_: lambda self, median=0, sigma=1, *, generator=None: -1,
		Tensor.coalesce: lambda self: -1,
		Tensor._coalesced_: lambda self, coalesced: -1,
		Tensor.contiguous: lambda self, memory_format=torch.contiguous_format: -1,
		Tensor.copy_: lambda self, src, non_blocking=False: -1,
		Tensor.cpu: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.cuda: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.xpu: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.ipu: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.data_ptr: lambda self: -1,
		Tensor.dense_dim: lambda self: -1,
		Tensor.diagonal_scatter: lambda self, src, offset=0, dim1=0, dim2=1: -1,
		Tensor.dim: lambda self: -1,
		Tensor.dim_order: lambda self: -1,
		Tensor.double: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.cdouble: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.element_size: lambda self: -1,
		Tensor.expand: lambda self, size: -1,
		Tensor.expand_as: lambda self, other: -1,
		Tensor.exponential_: lambda self, lambd=1, *, generator=None: -1,
		Tensor.fill_: lambda self, value: -1,
		Tensor.fill_diagonal_: lambda self, value: -1,
		Tensor.float: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.cfloat: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.geometric_: lambda self, p, *, generator=None: -1,
		Tensor.get_device: lambda self: -1,
		Tensor.half: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.chalf: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.has_names: lambda self: -1,
		Tensor.indices: lambda self: -1,
		Tensor.int: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.is_coalesced: lambda self: -1,
		Tensor.is_contiguous: lambda self: -1,
		Tensor.is_inference: lambda self: -1,
		Tensor.is_pinned: lambda self: -1,
		Tensor.is_set_to: lambda self, tensor: -1,
		Tensor.is_shared: lambda self: -1,
		Tensor.item: lambda self: -1,
		Tensor.log_normal_: lambda self, mean=1, std=2, *, generator=None: -1,
		Tensor.log_softmax: lambda self, dim: -1,
		Tensor.long: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.map_: lambda self, tensor, callable: -1,
		Tensor.map2_: lambda self, x, y, callable: -1,
		Tensor.mm: lambda self, mat2: -1,
		Tensor.narrow_copy: lambda self, dimension, start, length: -1,
		Tensor.ndimension: lambda self: -1,
		Tensor.nelement: lambda self: -1,
		Tensor._nested_tensor_size: lambda self: -1,
		Tensor._nested_tensor_storage_offsets: lambda self: -1,
		Tensor._nested_tensor_strides: lambda self: -1,
		Tensor.normal_: lambda self: -1,
		Tensor.numpy: lambda self: -1,
		Tensor.permute: lambda self, dim: -1,
		Tensor.pin_memory: lambda self: -1,
		Tensor.put_: lambda self, indices, tensor, accumulate=False: -1,
		Tensor.qscheme: lambda self: -1,
		Tensor.random_: lambda self, from_=0, to=None, *, generator=None: -1,
		Tensor.record_stream: lambda self, stream: -1,
		Tensor.refine_names: lambda self, names: -1,
		Tensor.register_hook: lambda self, hook: -1,
		Tensor.register_post_accumulate_grad_hook: lambda self, hook: -1,
		Tensor.rename: lambda self, name: -1,
		Tensor.repeat: lambda self, *size: -1,
		Tensor.requires_grad_: lambda self, requires_grad=True: -1,
		Tensor.reshape_as: lambda self, other: -1,
		Tensor.resize: lambda self, *size: -1,
		Tensor.resize_: lambda self, size: -1,
		Tensor.resize_as: lambda self, other: -1,
		Tensor.resize_as_sparse_: lambda self, other: -1,
		Tensor.retain_grad: lambda self: -1,
		Tensor.set_: lambda self, source=None, storage_offset=0, size=None, stride=None: -1,
		Tensor.select_scatter: lambda self, src, dim, index: -1,
		Tensor.share_memory_: lambda self: -1,
		Tensor.short: lambda self, memory_format=torch.preserve_format: -1,
		Tensor.size: lambda self: -1,
		Tensor.slice_scatter: lambda self, src, dim=0, start=None, end=None, step=1: -1,
		Tensor.sparse_dim: lambda self: -1,
		Tensor.sparse_mask: lambda self, mask: -1,
		Tensor._sparse_mask_projection: lambda self, mask, accumulate_matches=False: -1,
		Tensor.sparse_resize_: lambda self, size1, size2, dense_dim: -1,
		Tensor.sparse_resize_and_clear_: lambda self, size1, size2, dense_dim: -1,
		Tensor.sspaddmm: lambda self, mat1, mat2, beta=1, alpha=1, out=None: -1,
		Tensor.storage: lambda self: -1,
		Tensor.untyped_storage: lambda self: -1,
		Tensor.storage_offset: lambda self: -1,
		Tensor.storage_type: lambda self: -1,
		Tensor.sum_to_size: lambda self, size: -1,
		Tensor.tile: lambda self, *reps: -1,
		Tensor.to: lambda self, dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format: -1,
		Tensor.to_dense: lambda self, dtype=None, *, masked_grad=None: -1,
		Tensor._to_dense: lambda self, dtype=None, masked_grad=None: -1,
		Tensor.to_sparse: lambda self: -1,
		Tensor.tolist: lambda self: -1,
		Tensor.to_mkldnn: lambda self: -1,
		Tensor.type_as: lambda self, other: -1,
		Tensor.unfold: lambda self, dimension, size, step: -1,
		Tensor.uniform_: lambda self, from_=0, to=1: -1,
		Tensor.values: lambda self: -1,
		Tensor.view: lambda self, shape: -1,
		Tensor.view_as: lambda self, other: -1,
		Tensor.zero_: lambda self: -1,
		Tensor.__dlpack__: lambda self, stream=None: -1,
		Tensor.__dlpack_device__: lambda self: -1,
		torch.linalg.lstsq: lambda self, b, cond=None, driver=None: -1,
	}

	ret2 = {}
	ignored = get_ignored_functions()

	for k, v in ret.items():
		names = [
			k.__name__,  # Default method
			k.__name__ + "_",  # Inplace variant
			"__" + k.__name__ + "__",  # Dunder method
			"__i" + k.__name__ + "__",  # Inplace dunder method
			"__r" + k.__name__ + "__",  # Reverse dunder method
		]

		if k.__name__.startswith("bitwise_"):
			subname = k.__name__[len("bitwise_"):]
			names.extend([
				"__" + subname + "__",
				"__i" + subname + "__",
				"__r" + subname + "__"
			])

		for name in names:
			func = getattr(Tensor, name, None)
			if callable(func) and func not in ret and func not in ignored:
				ret2[func] = v

	ret.update(ret2)
	return ret

def wrap_torch_function(dispatcher: Callable):
	def inner(func):
		@functools.wraps(func)
		def wrapped(*args, **kwargs):
			relevant_args = dispatcher(*args, **kwargs)
			if has_torch_function(relevant_args):
				return handle_torch_function(wrapped, relevant_args, *args, **kwargs)

			return func(*args, **kwargs)

		return wrapped

	return inner

def _get_overloaded_args(relevant_args: Iterable[Any], get_type_fn: Callable[[Any], Type] = None) -> List[Any]:
	if get_type_fn is None:
		get_type_fn = type

	if not torch._C._is_torch_function_enabled():
		return []
	overloaded_types: Set[Type] = set()
	overloaded_args: List[Any] = []
	for arg in relevant_args:
		arg_type = get_type_fn(arg)
		if (arg_type not in overloaded_types and hasattr(arg_type, '__torch_function__') and
				arg_type.__torch_function__ != torch._C._disabled_torch_function_impl):
			if overloaded_types:
				overloaded_types.add(arg_type)
				index = len(overloaded_args)
				for i, old_arg in enumerate(overloaded_args):
					if issubclass(arg_type, get_type_fn(old_arg)):
						index = i
						break
				overloaded_args.insert(index, arg)
			else:
				overloaded_types = {arg_type}
				overloaded_args = [arg]
	return overloaded_args


def handle_torch_function(
		public_api: Callable, relevant_args: Iterable[Any], *args, **kwargs) -> Any:
	overloaded_args = _get_overloaded_args(relevant_args)
	types = tuple(map(type, overloaded_args))

	if _is_torch_function_mode_enabled():
		with _pop_mode_temporarily() as mode:
			result = mode.__torch_function__(public_api, types, args, kwargs)
		if result is not NotImplemented:
			return result

	for overloaded_arg in overloaded_args:
		torch_func_method = overloaded_arg.__torch_function__
		if hasattr(torch_func_method, "__self__") and torch_func_method.__self__ is overloaded_arg and \
				torch_func_method is not torch._C._disabled_torch_function_impl:
			warnings.warn("Defining your `__torch_function__ as a plain method is deprecated and "
						  "will be an error in future, please define it as a classmethod.",
						  DeprecationWarning)

		result = torch_func_method(public_api, types, args, kwargs)

		if result is not NotImplemented:
			return result

	func_name = f'{public_api.__module__}.{public_api.__name__}'
	msg = (
		f"no implementation found for '{func_name}' on types that implement "
		f'__torch_function__: {[type(arg) for arg in overloaded_args]}'
	)
	if _is_torch_function_mode_enabled():
		msg += f" nor in mode {_get_current_function_mode()}"
	raise TypeError(msg)

has_torch_function = _add_docstr(
	_has_torch_function,
)

has_torch_function_unary = _add_docstr(
	_has_torch_function_unary,
)

has_torch_function_variadic = _add_docstr(
	_has_torch_function_variadic,
)

@functools.lru_cache(None)
def _get_overridable_functions() -> Tuple[Dict[Any, List[Callable]], Dict[Callable, str]]:
	overridable_funcs = collections.defaultdict(list)
	index = {}
	tested_namespaces = [
		("torch", torch, torch.__all__),
		("torch.functional", torch.functional, torch.functional.__all__),
		("torch.nn.functional", torch.nn.functional, dir(torch.nn.functional)),
		("torch.nn.init", torch.nn.init, dir(torch.nn.init)),
		("torch.Tensor", torch.Tensor, dir(torch.Tensor)),
		("torch.linalg", torch.linalg, dir(torch.linalg)),
		("torch.fft", torch.fft, dir(torch.fft)),
		("torch.special", torch.special, dir(torch.special)),
	]
	for namespace_str, namespace, ns_funcs in tested_namespaces:
		for func_name in ns_funcs:
			ignore = False
			if namespace is not torch.Tensor:
				if func_name.startswith('__'):
					continue
				elif func_name.startswith('_'):
					ignore = True
				elif func_name.endswith('_'):
					ignore = True
				elif not func_name[0].islower():
					ignore = True
				elif func_name == 'unique_dim':
					continue
			else:
				func = getattr(namespace, func_name)
				if getattr(object, func_name, None) == func:
					continue
				if func_name == '__weakref__':
					continue
			func = getattr(namespace, func_name)
			if namespace is torch.Tensor and getattr(object, func_name, None) == func:
				continue
			if isinstance(func, types.ModuleType):
				continue
			if isinstance(func, __future__._Feature):
				continue

			if not callable(func) and hasattr(func, "__get__"):
				index[func.__get__] = f"{namespace_str}.{func_name}.__get__"
				index[func.__set__] = f"{namespace_str}.{func_name}.__set__"
				if ignore:
					continue
				if func.__get__ in get_ignored_functions():
					msg = ("{}.{} is in the tuple returned by torch._overrides.get_ignored_functions "
						   "but still has an explicit override")
					assert func.__get__ not in get_testing_overrides(), msg.format(namespace, func.__name__)
					continue
				else:
					overridable_funcs[func].append(func.__get__)
					continue

			if not callable(func):
				continue

			index[func] = f"{namespace_str}.{func_name}"

			if ignore:
				continue

			if func in get_ignored_functions():
				msg = ("{}.{} is in the tuple returned by torch._overrides.get_ignored_functions "
					   "but still has an explicit override")
				assert func not in get_testing_overrides(), msg.format(namespace, func.__name__)
				continue
			overridable_funcs[namespace].append(func)
	return overridable_funcs, index

@_disable_user_warnings
def get_overridable_functions() -> Dict[Any, List[Callable]]:
	return _get_overridable_functions()[0]

@_disable_user_warnings
def resolve_name(f):
	if isinstance(f, (torch._ops.OpOverload, torch._ops.OpOverloadPacket)):
		return str(f)
	return _get_overridable_functions()[1].get(f)

@functools.lru_cache(None)
def _get_tensor_methods() -> Set[Callable]:
	Returns True if the function passed in is a handler for a
	method or property belonging to ``torch.Tensor``, as passed
	into ``__torch_function__``.

	.. note::
	   For properties, their ``__get__`` method must be passed in.

	This may be needed, in particular, for the following reasons:

	1. Methods/properties sometimes don't contain a `__module__` slot.
	2. They require that the first passed-in argument is an instance
	   of ``torch.Tensor``.

	Examples
	--------
	>>> is_tensor_method_or_property(torch.Tensor.add)
	True
	>>> is_tensor_method_or_property(torch.add)
	False
	Returns ``True`` if the passed-in input is a Tensor-like.

	Currently, this occurs whenever there's a ``__torch_function__``
	attribute on the type of the input.

	Examples
	--------
	A subclass of tensor is generally a Tensor-like.

	>>> class SubTensor(torch.Tensor): ...
	>>> is_tensor_like(SubTensor([0]))
	True

	Built-in or user types aren't usually Tensor-like.

	>>> is_tensor_like(6)
	False
	>>> is_tensor_like(None)
	False
	>>> class NotATensor: ...
	>>> is_tensor_like(NotATensor())
	False

	But, they can be made Tensor-like by implementing __torch_function__.

	>>> class TensorLike:
	...	 @classmethod
	...	 def __torch_function__(cls, func, types, args, kwargs):
	...		 return -1
	>>> is_tensor_like(TensorLike())
	True
	A ``TorchFunctionMode`` allows you to override the meaning of all
	``__torch_function__`` overrideable functions within a dynamic scope,
	without having to actually create a tensor subclass or manually
	monkey-patch functions in the PyTorch API.  Some common situations
	where you should use a mode:

		* You want to override the meaning of factory functions, or other
		  functions that do not otherwise take a tensor as an argument
		  (these cannot be overridden with tensor subclasses).

		* You want to override the behavior of all functions without needing
		  to wrap your inputs in tensor subclasses; e.g., if you are just
		  interested in logging intermediate computations.

		* You want to control the order of execution of various tensor
		  subclasses explicitly, rather than implicitly via the return of
		  ``NotImplemented``.

	Independent subclasses of :class:`TorchFunctionMode` are compositional:
	modes can be pushed onto a stack using ``with MyMode():``.
	When you call functions in the PyTorch API inside your
	``__torch_function__`` implementation, by default, they will forward on to
	the next mode on the mode stack.  If you want recursively call back into
	your current ``__torch_function__`` implementation, either explicitly
	invoke ``self.__torch_function__(...)``, or use the context manager
	``enable_torch_function_mode(self, replace=self.inner)`` to make PyTorch
	API self-referential (beware of infinite loops, in this case!)

<END>

<START>
from typing import Dict

import torch
from torch.distributions import Categorical, constraints
from torch.distributions.distribution import Distribution

__all__ = ["MixtureSameFamily"]


class MixtureSameFamily(Distribution):
	arg_constraints: Dict[str, constraints.Constraint] = {}
	has_rsample = False

	def __init__(
		self, mixture_distribution, component_distribution, validate_args=None
	):
		self._mixture_distribution = mixture_distribution
		self._component_distribution = component_distribution

		if not isinstance(self._mixture_distribution, Categorical):
			raise ValueError(
				" The Mixture distribution needs to be an "
				" instance of torch.distributions.Categorical"
			)

		if not isinstance(self._component_distribution, Distribution):
			raise ValueError(
				"The Component distribution need to be an "
				"instance of torch.distributions.Distribution"
			)

		mdbs = self._mixture_distribution.batch_shape
		cdbs = self._component_distribution.batch_shape[:-1]
		for size1, size2 in zip(reversed(mdbs), reversed(cdbs)):
			if size1 != 1 and size2 != 1 and size1 != size2:
				raise ValueError(
					f"`mixture_distribution.batch_shape` ({mdbs}) is not "
					"compatible with `component_distribution."
					f"batch_shape`({cdbs})"
				)

		km = self._mixture_distribution.logits.shape[-1]
		kc = self._component_distribution.batch_shape[-1]
		if km is not None and kc is not None and km != kc:
			raise ValueError(
				f"`mixture_distribution component` ({km}) does not"
				" equal `component_distribution.batch_shape[-1]`"
				f" ({kc})"
			)
		self._num_component = km

		event_shape = self._component_distribution.event_shape
		self._event_ndims = len(event_shape)
		super().__init__(
			batch_shape=cdbs, event_shape=event_shape, validate_args=validate_args
		)

	def expand(self, batch_shape, _instance=None):
		batch_shape = torch.Size(batch_shape)
		batch_shape_comp = batch_shape + (self._num_component,)
		new = self._get_checked_instance(MixtureSameFamily, _instance)
		new._component_distribution = self._component_distribution.expand(
			batch_shape_comp
		)
		new._mixture_distribution = self._mixture_distribution.expand(batch_shape)
		new._num_component = self._num_component
		new._event_ndims = self._event_ndims
		event_shape = new._component_distribution.event_shape
		super(MixtureSameFamily, new).__init__(
			batch_shape=batch_shape, event_shape=event_shape, validate_args=False
		)
		new._validate_args = self._validate_args
		return new

	@constraints.dependent_property
	def support(self):
		return self._component_distribution.support

	@property
	def mixture_distribution(self):
		return self._mixture_distribution

	@property
	def component_distribution(self):
		return self._component_distribution

	@property
	def mean(self):
		probs = self._pad_mixture_dimensions(self.mixture_distribution.probs)
		return torch.sum(
			probs * self.component_distribution.mean, dim=-1 - self._event_ndims
		)  # [B, E]

	@property
	def variance(self):
		probs = self._pad_mixture_dimensions(self.mixture_distribution.probs)
		mean_cond_var = torch.sum(
			probs * self.component_distribution.variance, dim=-1 - self._event_ndims
		)
		var_cond_mean = torch.sum(
			probs * (self.component_distribution.mean - self._pad(self.mean)).pow(2.0),
			dim=-1 - self._event_ndims,
		)
		return mean_cond_var + var_cond_mean

	def cdf(self, x):
		x = self._pad(x)
		cdf_x = self.component_distribution.cdf(x)
		mix_prob = self.mixture_distribution.probs

		return torch.sum(cdf_x * mix_prob, dim=-1)

	def log_prob(self, x):
		if self._validate_args:
			self._validate_sample(x)
		x = self._pad(x)
		log_prob_x = self.component_distribution.log_prob(x)  # [S, B, k]
		log_mix_prob = torch.log_softmax(
			self.mixture_distribution.logits, dim=-1
		)  # [B, k]
		return torch.logsumexp(log_prob_x + log_mix_prob, dim=-1)  # [S, B]

	def sample(self, sample_shape=torch.Size()):
		with torch.no_grad():
			sample_len = len(sample_shape)
			batch_len = len(self.batch_shape)
			gather_dim = sample_len + batch_len
			es = self.event_shape

			mix_sample = self.mixture_distribution.sample(sample_shape)
			mix_shape = mix_sample.shape

			comp_samples = self.component_distribution.sample(sample_shape)

			mix_sample_r = mix_sample.reshape(
				mix_shape + torch.Size([1] * (len(es) + 1))
			)
			mix_sample_r = mix_sample_r.repeat(
				torch.Size([1] * len(mix_shape)) + torch.Size([1]) + es
			)

			samples = torch.gather(comp_samples, gather_dim, mix_sample_r)
			return samples.squeeze(gather_dim)

	def _pad(self, x):
		return x.unsqueeze(-1 - self._event_ndims)

	def _pad_mixture_dimensions(self, x):
		dist_batch_ndims = self.batch_shape.numel()
		cat_batch_ndims = self.mixture_distribution.batch_shape.numel()
		pad_ndims = 0 if cat_batch_ndims == 1 else dist_batch_ndims - cat_batch_ndims
		xs = x.shape
		x = x.reshape(
			xs[:-1]
			+ torch.Size(pad_ndims * [1])
			+ xs[-1:]
			+ torch.Size(self._event_ndims * [1])
		)
		return x

	def __repr__(self):
		args_string = (
			f"\n  {self.mixture_distribution},\n  {self.component_distribution}"
		)
		return "MixtureSameFamily" + "(" + args_string + ")"

<END>

<START>
import warnings
from enum import auto, Enum
from functools import partial
from typing import Any, Callable, Dict, Iterator, Optional, Tuple

import torch
import torch.nn as nn
from torch.autograd.graph import save_on_cpu
from torch.distributed.utils import _pack_kwargs, _replace_by_prefix, _unpack_kwargs
from torch.utils.checkpoint import checkpoint as torch_utils_checkpoint

_CHECKPOINT_WRAPPED_MODULE = "_checkpoint_wrapped_module"
_CHECKPOINT_PREFIX = _CHECKPOINT_WRAPPED_MODULE + "."


class CheckpointImpl(Enum):
	REENTRANT = auto()
	NO_REENTRANT = auto()


class ActivationWrapper(torch.nn.Module):

	def __init__(self, mod):
		super().__init__()
		self._checkpoint_wrapped_module = mod
		self._register_state_dict_hook(self._post_state_dict_hook)
		self._register_load_state_dict_pre_hook(
			self._pre_load_state_dict_hook, with_module=True
		)

	def forward(self, *args, **kwargs):
		raise ValueError("Subclasses should implement forward().")

	def __getattr__(self, name: str) -> Any:
		return self._checkpoint_wrapped_module.__getitem__(key)  # type: ignore[operator]

	def named_parameters(
		self,
		*args,
		**kwargs,
	) -> Iterator[Tuple[str, torch.nn.Parameter]]:
		for param_name, param in super().named_parameters(*args, **kwargs):
			yield param_name.replace(_CHECKPOINT_PREFIX, ""), param

	@staticmethod
	def _post_state_dict_hook(
		module: nn.Module,
		state_dict: Dict[str, Any],
		prefix: str,
		*args: Any,
	) -> Dict[str, Any]:
		_replace_by_prefix(state_dict, f"{prefix}{_CHECKPOINT_PREFIX}", prefix)
		return state_dict

	@staticmethod
	def _pre_load_state_dict_hook(
		module: nn.Module,
		state_dict: Dict[str, Any],
		prefix: str,
		*args: Any,
	) -> None:
		_replace_by_prefix(state_dict, prefix, prefix + f"{_CHECKPOINT_PREFIX}")


class OffloadWrapper(ActivationWrapper):
	def __init__(self, mod):
		super().__init__(mod)

	def forward(self, *args, **kwargs):
		with save_on_cpu(pin_memory=True):
			return self._checkpoint_wrapped_module(*args, **kwargs)


class CheckpointWrapper(ActivationWrapper):

	def __init__(
		self,
		mod: torch.nn.Module,
		checkpoint_impl: CheckpointImpl = CheckpointImpl.NO_REENTRANT,
		checkpoint_fn=None,
		**checkpoint_fn_kwargs,
	):
		super().__init__(mod)
		self.checkpoint_impl = checkpoint_impl
		if checkpoint_fn is None:
			self.checkpoint_fn = partial(
				torch_utils_checkpoint,
				use_reentrant=(self.checkpoint_impl == CheckpointImpl.REENTRANT),
				**checkpoint_fn_kwargs,
			)
		else:
			self.checkpoint_fn = partial(
				checkpoint_fn,
				**checkpoint_fn_kwargs,
			)

	def forward(self, *args, **kwargs):
		if self.checkpoint_impl == CheckpointImpl.REENTRANT and kwargs != {}:
			flat_args, kwarg_keys = _pack_kwargs(*args, **kwargs)

			def my_function(*inputs):
				unpacked_args, unpacked_kwargs = _unpack_kwargs(inputs, kwarg_keys)
				return self._checkpoint_wrapped_module(
					*unpacked_args, **unpacked_kwargs
				)

			return self.checkpoint_fn(  # type: ignore[misc]
				my_function,
				*flat_args,
			)
		else:
			return self.checkpoint_fn(  # type: ignore[misc]
				self._checkpoint_wrapped_module, *args, **kwargs
			)


def offload_wrapper(module: torch.nn.Module) -> torch.nn.Module:
	return OffloadWrapper(module)


def checkpoint_wrapper(
	module: torch.nn.Module,
	checkpoint_impl: CheckpointImpl = CheckpointImpl.NO_REENTRANT,
	checkpoint_fn=None,
	**checkpoint_fn_kwargs,
) -> torch.nn.Module:

	if checkpoint_impl == CheckpointImpl.REENTRANT:
		warnings.warn(
			f"Please specify {CheckpointImpl.NO_REENTRANT} as "
			f"{CheckpointImpl.REENTRANT} will soon be removed as "
			"the default and eventually deprecated.",
			stacklevel=1,
		)
	return CheckpointWrapper(
		module,
		checkpoint_impl,
		checkpoint_fn,
		**checkpoint_fn_kwargs,
	)


def apply_activation_checkpointing(
	model,
	checkpoint_wrapper_fn=checkpoint_wrapper,
	check_fn=lambda _: True,
	auto_wrap_policy: Optional[Callable[[nn.Module, bool, int], bool]] = None,
):
	from torch.distributed.fsdp.wrap import _recursive_wrap, lambda_auto_wrap_policy, _Policy
	from torch.distributed.fsdp._wrap_utils import _construct_wrap_fn, _post_order_apply

	policy = (
		auto_wrap_policy
		if auto_wrap_policy is not None
		else partial(lambda_auto_wrap_policy, lambda_fn=check_fn)
	)
	if not callable(policy):
		if not isinstance(policy, _Policy):
			raise ValueError(
				f"Expected {policy} to be callable or be a pre-defined wrap policy"
			)
		target_module_to_kwargs = policy._run_policy(
			model, ignored_modules=set(), root_kwargs={}
		)
		wrap_fn = _construct_wrap_fn(model, target_module_to_kwargs, checkpoint_wrapper_fn)
		_post_order_apply(model, wrap_fn)
		return

	_recursive_wrap(
		module=model,
		auto_wrap_policy=policy,  # type: ignore[arg-type]
		wrapper_cls=checkpoint_wrapper_fn,
		ignored_modules=set(),
		ignored_params=set(),
		only_wrap_children=True,
	)

<END>

<START>
import torch
from torch.distributions import constraints
from torch.distributions.categorical import Categorical
from torch.distributions.distribution import Distribution
from torch.distributions.transformed_distribution import TransformedDistribution
from torch.distributions.transforms import ExpTransform
from torch.distributions.utils import broadcast_all, clamp_probs

__all__ = ["ExpRelaxedCategorical", "RelaxedOneHotCategorical"]


class ExpRelaxedCategorical(Distribution):
	arg_constraints = {"probs": constraints.simplex, "logits": constraints.real_vector}
	support = (
		constraints.real_vector
	)  # The true support is actually a submanifold of this.
	has_rsample = True

	def __init__(self, temperature, probs=None, logits=None, validate_args=None):
		self._categorical = Categorical(probs, logits)
		self.temperature = temperature
		batch_shape = self._categorical.batch_shape
		event_shape = self._categorical.param_shape[-1:]
		super().__init__(batch_shape, event_shape, validate_args=validate_args)

	def expand(self, batch_shape, _instance=None):
		new = self._get_checked_instance(ExpRelaxedCategorical, _instance)
		batch_shape = torch.Size(batch_shape)
		new.temperature = self.temperature
		new._categorical = self._categorical.expand(batch_shape)
		super(ExpRelaxedCategorical, new).__init__(
			batch_shape, self.event_shape, validate_args=False
		)
		new._validate_args = self._validate_args
		return new

	def _new(self, *args, **kwargs):
		return self._categorical._new(*args, **kwargs)

	@property
	def param_shape(self):
		return self._categorical.param_shape

	@property
	def logits(self):
		return self._categorical.logits

	@property
	def probs(self):
		return self._categorical.probs

	def rsample(self, sample_shape=torch.Size()):
		shape = self._extended_shape(sample_shape)
		uniforms = clamp_probs(
			torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device)
		)
		gumbels = -((-(uniforms.log())).log())
		scores = (self.logits + gumbels) / self.temperature
		return scores - scores.logsumexp(dim=-1, keepdim=True)

	def log_prob(self, value):
		K = self._categorical._num_events
		if self._validate_args:
			self._validate_sample(value)
		logits, value = broadcast_all(self.logits, value)
		log_scale = torch.full_like(
			self.temperature, float(K)
		).lgamma() - self.temperature.log().mul(-(K - 1))
		score = logits - value.mul(self.temperature)
		score = (score - score.logsumexp(dim=-1, keepdim=True)).sum(-1)
		return score + log_scale


class RelaxedOneHotCategorical(TransformedDistribution):
	arg_constraints = {"probs": constraints.simplex, "logits": constraints.real_vector}
	support = constraints.simplex
	has_rsample = True

	def __init__(self, temperature, probs=None, logits=None, validate_args=None):
		base_dist = ExpRelaxedCategorical(
			temperature, probs, logits, validate_args=validate_args
		)
		super().__init__(base_dist, ExpTransform(), validate_args=validate_args)

	def expand(self, batch_shape, _instance=None):
		new = self._get_checked_instance(RelaxedOneHotCategorical, _instance)
		return super().expand(batch_shape, _instance=new)

	@property
	def temperature(self):
		return self.base_dist.temperature

	@property
	def logits(self):
		return self.base_dist.logits

	@property
	def probs(self):
		return self.base_dist.probs

<END>

<START>
from __future__ import annotations

import copy
from typing import List, Set

import torch
import torch.nn.functional as F
from torch.ao.quantization.observer import PerChannelMinMaxObserver
from torch.ao.quantization.quantizer.quantizer import (
	QuantizationAnnotation,
	QuantizationSpec,
	Quantizer,
)
from torch.ao.quantization.quantizer.xnnpack_quantizer_utils import (
	OperatorConfig,
	OperatorPatternType,
	QuantizationConfig,
)

__all__ = [
	"get_embedding_operators_config",
	"EmbeddingQuantizer",
]


def get_embedding_operators_config() -> OperatorConfig:
	weight_quantization_spec = QuantizationSpec(
		dtype=torch.uint8,
		qscheme=torch.per_channel_affine_float_qparams,
		ch_axis=0,
		observer_or_fake_quant_ctr=PerChannelMinMaxObserver.with_args(eps=2**-12),
	)
	quantization_config = QuantizationConfig(None, None, weight_quantization_spec, None)
	ops: List[OperatorPatternType] = [[torch.nn.Embedding]]
	ops.append([F.embedding])
	supported_config_and_operators = OperatorConfig(
		config=quantization_config, operators=ops
	)
	return copy.deepcopy(supported_config_and_operators)


class EmbeddingQuantizer(Quantizer):
	def __init__(self):
		super().__init__()

	@classmethod
	def get_supported_quantization_configs(cls) -> List[QuantizationConfig]:
		op_configs: Set[QuantizationConfig] = set({})
		for spec, _ in cls.get_supported_operators():
			op_configs.add(spec)
		return list(op_configs)

	@classmethod
	def get_supported_operator_for_quantization_config(
		cls, quantization_config: QuantizationConfig
	) -> List[OperatorPatternType]:
		for config, ops in cls.get_supported_operators():
			if config == quantization_config:
				return ops
		return []

	def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:

<END>

<START>
import contextlib
import warnings
import weakref
from typing import ContextManager, List, Optional, Tuple, TYPE_CHECKING

import torch
from torch._C._functorch import (
	_add_batch_dim,
	_unwrap_functional_tensor,
	_wrap_functional_tensor,
	current_level,
	get_unwrapped,
	is_batchedtensor,
	maybe_get_bdim,
	maybe_get_level,
	peek_interpreter_stack,
	TransformType,
)
from torch._guards import Source

from torch.multiprocessing.reductions import StorageWeakRef
from torch.utils._python_dispatch import (
	is_traceable_wrapper_subclass,
	transform_subclass,
)
from torch.utils.weak import WeakIdRef

if TYPE_CHECKING:
	from torch.fx.experimental.symbolic_shapes import SymbolicContext

DimList = List


def safe_is_leaf(t):
	try:
		return t.is_leaf
	except RuntimeError:
		return False


def safe_grad(t):
	with warnings.catch_warnings():
		warnings.filterwarnings("ignore", "The .grad attribute of a Tensor")
		return t.grad


def assert_eq(a, b):
	assert a == b, f"{a} != {b}"


def assert_metadata_eq(assert_eq, m1, m2, *, skip_symbolic=False):
	def go(m1, m2):
		assert_eq(m1.dtype, m2.dtype)
		if not skip_symbolic:
			assert_eq(m1.shape, m2.shape)
		assert_eq(m1.requires_grad, m2.requires_grad)
		assert_eq(m1.is_leaf, m2.is_leaf)
		assert_eq(m1.grad_fn is None, m2.grad_fn is None)
		assert_eq(m1.is_sparse, m2.is_sparse)
		assert_eq(m1.is_inference(), m2.is_inference())
		assert_eq(m1.is_conj(), m2.is_conj())
		assert_eq(m1.is_neg(), m2.is_neg())
		assert_eq(safe_grad(m1) is not None, safe_grad(m2) is not None)
		if safe_grad(m1) is not None:
			go(safe_grad(m1), safe_grad(m2))
		if m1.is_sparse:
			assert_eq(m1.dense_dim(), m2.dense_dim())
			assert_eq(m1.sparse_dim(), m2.sparse_dim())
			assert_eq(m1.is_coalesced(), m2.is_coalesced())
		else:
			if not skip_symbolic:
				assert_eq(m1.stride(), m2.stride())
				assert_eq(m1.storage_offset(), m2.storage_offset())
			assert_eq(m1._is_view(), m2._is_view())
			if m1._is_view():
				go(m1._base, m2._base)

	return go(m1, m2)


class MetaConverter:
	def __init__(self):
		self.storage_memo = {}
		self.tensor_memo: weakref.WeakValueDictionary = weakref.WeakValueDictionary()
		self.maybe_storages_to_delete = []
		self.check_expired_frequency = 128
		self.check_expired_count = 0
		self.hit = 0
		self.miss = 0
		self.del_hook = None
		self.arg_cnt = 0

	def successful(self):
		return self.hit > 0 and self.miss == 0

	def check_for_expired_weak_storages(self):
		new_li = []
		stor_to_delete = []
		for obj in self.maybe_storages_to_delete:
			if not obj.expired():
				new_li.append(obj)
			else:
				stor_to_delete.append(obj)
		for obj in stor_to_delete:
			self.storage_memo.pop(obj, None)
		self.maybe_storages_to_delete = new_li

		self.check_expired_frequency = max(
			self.check_expired_frequency, len(self.maybe_storages_to_delete)
		)

	def get_tensor_memo(self, t):
		return self.tensor_memo.get(WeakIdRef(t), None)

	def set_tensor_memo(self, t, v):
		self_weak_ref = weakref.ref(self)
		if t.is_sparse or t.is_mkldnn or is_batchedtensor(t):
			weak_st = None
		else:
			weak_st = StorageWeakRef(t._typed_storage())
		tensor_ref_key = WeakIdRef(t)

		def del_ten():
			self_ref = self_weak_ref()
			if self_ref is None:
				return
			self_ref.tensor_memo.pop(tensor_ref_key, None)
			if weak_st and weak_st.expired():
				self_ref.storage_memo.pop(weak_st, None)
			elif weak_st is not None:
				self_ref.maybe_storages_to_delete.append(weak_st)

		weakref.finalize(t, del_ten)
		self.tensor_memo[tensor_ref_key] = v

	def meta_storage(self, s, callback):

		swr = StorageWeakRef(s)
		if swr not in self.storage_memo:
			self.storage_memo[swr] = callback(
				lambda: torch.empty(s.size(), dtype=torch.uint8, device="meta")
			).untyped_storage()
		return self.storage_memo[swr]

	def meta_tensor(
		self,
		t,
		shape_env=None,
		callback=lambda t: t(),
		source: Optional[Source] = None,
		symbolic_context: Optional["SymbolicContext"] = None,
	):
		if source is None:
			from torch._dynamo.source import ConstantSource

			source = ConstantSource(
				f"__meta_utils_unknown_tensor{len(self.tensor_memo)}"
			)

		assert not torch._C._dispatch_tls_local_exclude_set().has(
			torch._C.DispatchKey.Python
		)
		arg_cnt = self.arg_cnt
		self.arg_cnt += 1

		maybe_suppress = contextlib.nullcontext
		if shape_env is not None:
			maybe_suppress = shape_env.suppress_guards

		def sym_sizes_strides_storage_offset(
			t, src, symbolic_context=symbolic_context
		) -> Tuple[Tuple[int, ...], Tuple[int, ...], int]:
			if shape_env is not None:
				fake_mode = torch._subclasses.fake_tensor.maybe_get_fake_mode(t)
				if fake_mode is not None and fake_mode.shape_env is shape_env:
					return (t.size(), t.stride(), t.storage_offset())
				else:
					return shape_env.create_symbolic_sizes_strides_storage_offset(
						t,
						src,
						symbolic_context=symbolic_context,
					)
			else:
				assert symbolic_context is None
			return (t.size(), t.stride(), t.storage_offset())

		def empty_create(inner_t, inner_src, symbolic_context=symbolic_context):
			(
				inner_sizes,
				inner_strides,
				inner_storage_offset,
			) = sym_sizes_strides_storage_offset(inner_t, inner_src, symbolic_context)
			return torch.empty_strided(
				inner_sizes,
				inner_strides,
				dtype=inner_t.dtype,
				device="meta",
			)

		self.check_expired_count += 1
		if self.check_expired_count >= self.check_expired_frequency:
			self.check_for_expired_weak_storages()
			self.check_expired_count = 0

		if self.get_tensor_memo(t) is None:
			with torch.inference_mode(t.is_inference()):
				if t.is_sparse:
					is_leaf = safe_is_leaf(t)
					r = callback(
						lambda: torch.ops.aten._sparse_coo_tensor_with_dims(
							t.sparse_dim(),
							t.dense_dim(),
							t.shape,
							dtype=t.dtype,
							layout=torch.sparse_coo,
							device="meta",
						)
					)
					assert safe_is_leaf(r), "the callback you passed in doesn't detach"
					r._coalesced_(t.is_coalesced())
					if t.requires_grad:
						r.requires_grad = True
					if t.requires_grad and not is_leaf:
						with torch.enable_grad():
							r = r.clone()
							r._coalesced_(t.is_coalesced())
				elif t.is_nested and not is_traceable_wrapper_subclass(t):
					from torch._dynamo.exc import unimplemented

					unimplemented(
						"strided nested tensors are not supported by meta conversion"
					)
				elif t.is_mkldnn:
					is_leaf = safe_is_leaf(t)
					sizes, strides, _storage_offset = sym_sizes_strides_storage_offset(
						t, source
					)
					r = callback(
						lambda: torch.empty_strided(
							sizes, strides, dtype=t.dtype, device="meta"
						)
					)
					assert safe_is_leaf(r), "the callback you passed in doesn't detach"
					if t.requires_grad:
						r.requires_grad = True
					if t.requires_grad and not is_leaf:
						with torch.enable_grad():
							r = r.clone()
				elif is_batchedtensor(t):
					def _to_fake_tensor(t):
						if is_batchedtensor(t):
							ft = _to_fake_tensor(get_unwrapped(t))
							lvl = maybe_get_level(t)
							bdim = maybe_get_bdim(t)
							r = _add_batch_dim(ft, bdim, lvl)
						else:
							sizes = t.size()
							strides = t.stride()
							r = callback(
								lambda: torch.empty_strided(
									sizes,
									strides,
									dtype=t.dtype,
									device="meta",
								)
							)
						return r

					r = _to_fake_tensor(t)

				elif t._is_view():
					assert t._is_view()

					from torch._dynamo.source import AttrSource
					from torch.fx.experimental.symbolic_shapes import (
						DimDynamic,
						StatelessSymbolicContext,
					)

					if shape_env and not t.is_nested and not t._base.is_nested:
						base_symbolic_context = StatelessSymbolicContext(
							dynamic_sizes=[DimDynamic.STATIC] * t._base.dim(),
							constraint_sizes=[None] * t._base.dim(),
						)
					else:
						base_symbolic_context = None
					base = self.meta_tensor(
						t._base,
						shape_env,
						callback,
						source=AttrSource(source, "_base"),
						symbolic_context=base_symbolic_context,
					)

					def is_c_of_r(complex_dtype, real_dtype):
						return (
							utils.is_complex_dtype(complex_dtype)
							and utils.corresponding_real_dtype(complex_dtype)
							== real_dtype
						)

					old_exclude = torch._C._dispatch_tls_is_dispatch_key_excluded(
						torch._C.DispatchKey.ADInplaceOrView
					)
					torch._C._dispatch_tls_set_dispatch_key_excluded(
						torch._C.DispatchKey.ADInplaceOrView, False
					)
					try:
						if base.dtype == t.dtype:
							pass
						elif is_c_of_r(base.dtype, t.dtype):
							base = torch.view_as_real(base)
						elif is_c_of_r(t.dtype, base.dtype):
							base = torch.view_as_complex(base)
						else:
							base = base.view(t.dtype)

						def _view_from_base(base, t):
							if t.is_nested:
								return t._view_func_unsafe(base)
							else:
								(
									sizes,
									strides,
									storage_offset,
								) = sym_sizes_strides_storage_offset(t, source)
								return base.as_strided(sizes, strides, storage_offset)

						if safe_is_leaf(t):
							with torch.no_grad(), maybe_suppress():
								r = _view_from_base(base, t)
							r.requires_grad = t.requires_grad
						else:
							if t._base.requires_grad == t.requires_grad:
								with torch.enable_grad(), maybe_suppress():
									r = _view_from_base(base, t)

							else:
								assert t.requires_grad
								with torch.no_grad():
									mid = base.view(base.shape)
								mid.requires_grad = t.requires_grad
								with torch.enable_grad(), maybe_suppress():
									r = _view_from_base(mid, t)
						torch._C._autograd._set_creation_meta(
							r, torch._C._autograd._get_creation_meta(t)
						)
					finally:
						torch._C._dispatch_tls_set_dispatch_key_excluded(
							torch._C.DispatchKey.ADInplaceOrView, old_exclude
						)

				else:
					is_leaf = safe_is_leaf(t)

					from torch.fx.experimental.symbolic_shapes import (
						SubclassSymbolicContext,
					)

					(
						sizes,
						strides,
						storage_offset,
					) = sym_sizes_strides_storage_offset(t, source, symbolic_context)

					if is_traceable_wrapper_subclass(t):
						from torch._dynamo.source import AttrSource

						assert symbolic_context is None or isinstance(
							symbolic_context, SubclassSymbolicContext
						)
						r = transform_subclass(
							t,
							lambda attr, inner_t: callback(
								lambda: empty_create(
									inner_t,
									AttrSource(source, attr),
									symbolic_context=(
										None
										if symbolic_context is None
										else symbolic_context.inner_contexts[attr]
									),
								)
							),
							outer_size=sizes,
							outer_stride=strides,
						)
					else:
						r = callback(
							lambda: torch.empty_strided(
								sizes,
								strides,
								dtype=t.dtype,
								device="meta",
							)
						)
					assert safe_is_leaf(r), "the callback you passed in doesn't detach"
					if t.requires_grad:
						r.requires_grad = t.requires_grad
						if not is_leaf:
							with torch.enable_grad():
								r = r.clone(memory_format=torch.preserve_format)

					if not is_batchedtensor(
						t
					) and torch._C._functorch.is_functorch_wrapped_tensor(t):
						return NotImplemented

					s = t.untyped_storage()
					swr = StorageWeakRef(s)
					if swr not in self.storage_memo and (
						r.is_nested
						or (
							r.stride() == strides
							and r.storage_offset() == storage_offset
						)
					):
						self.storage_memo[swr] = r.untyped_storage()
					else:
						r_s = self.meta_storage(s, callback=callback)
						maybe_fake_mgr: ContextManager[None] = contextlib.nullcontext()
						from torch._subclasses.fake_tensor import (
							in_kernel_invocation_manager,
							maybe_get_fake_mode,
						)

						mb_fake_mode = maybe_get_fake_mode(r)
						if mb_fake_mode is not None:
							maybe_fake_mgr = in_kernel_invocation_manager(mb_fake_mode)
						with maybe_fake_mgr, torch.no_grad():
							r.set_(r_s, storage_offset, sizes, strides)

				if safe_grad(t) is not None:
					from torch._dynamo.source import AttrSource

					r.grad = self.meta_tensor(
						safe_grad(t),
						shape_env,
						callback,
						source=AttrSource(source, "grad"),
						symbolic_context=symbolic_context,
					)
				torch._C._set_conj(r, t.is_conj())
				torch._C._set_neg(r, t.is_neg())
			assert_metadata_eq(assert_eq, t, r, skip_symbolic=True)
			self.set_tensor_memo(t, r)

		return self.get_tensor_memo(t)

	def __call__(
		self,
		t,
		shape_env=None,
		*,
		callback=lambda t: t(),
		source=None,
		symbolic_context=None,
	):

		if isinstance(t, torch.Tensor) or is_traceable_wrapper_subclass(t):
			if t.device.type != "xla" and any(
				[
					t.is_sparse_csr,
					t.layout in [torch.sparse_csc, torch.sparse_bsr, torch.sparse_bsc],
					t.is_quantized,
					t._is_view() and t._base is not None and t._base.is_sparse,
					torch._is_functional_tensor(t),
					t.device.type in ("lazy"),
				]
			):

				if torch._is_functional_tensor(t) and t.device.type != "lazy":
					if t._is_view():
						raise RuntimeError(
							"Cannot safely fakify a view because this process drops the view information right now."
						)

					st = peek_interpreter_stack()
					assert (
						st is None or st.key() == TransformType.Functionalize
					), "Expect st to be either None or have Functionalize transform key."
					if st is None:
						torch._sync(t)
						unwrap_t = torch._from_functional_tensor(t)
						with torch._dispatch.python.suspend_functionalization():
							fake_t = self.meta_tensor(
								unwrap_t,
								shape_env=shape_env,
								callback=callback,
								source=source,
								symbolic_context=symbolic_context,
							)
						out = torch._to_functional_tensor(fake_t)
						torch._mirror_autograd_meta_to(fake_t, out)
						return out
					else:
						reapply_views = torch._C._functionalization_reapply_views_tls()
						unwrap_t = _unwrap_functional_tensor(t, reapply_views)
						pop_st_ctx = (
							torch._functorch.pyfunctorch.temporarily_pop_interpreter_stack()
						)
						with pop_st_ctx:
							fake_t = self.meta_tensor(
								unwrap_t,
								shape_env=shape_env,
								callback=callback,
								source=source,
								symbolic_context=symbolic_context,
							)
						return _wrap_functional_tensor(fake_t, current_level())
				self.miss += 1
				return NotImplemented
			else:
				self.hit += 1
				r = self.meta_tensor(
					t,
					shape_env=shape_env,
					callback=callback,
					source=source,
					symbolic_context=symbolic_context,
				)
				if type(t) is torch.nn.Parameter:
					r._is_param = True
				return r
		elif torch.overrides.is_tensor_like(t):
			self.miss += 1
			return NotImplemented
		else:
			return t


import torch._prims_common as utils

<END>

<START>
import torch.library
from torch import Tensor
from torch.autograd import Function

_test_lib_def = torch.library.Library("_inductor_test", "DEF")
_test_lib_def.define("realize(Tensor self) -> Tensor", tags=torch.Tag.pt2_compliant_tag)

_test_lib_impl = torch.library.Library("_inductor_test", "IMPL")
for dispatch_key in ("CPU", "CUDA", "Meta"):
	_test_lib_impl.impl("realize", lambda x: x.clone(), dispatch_key)


class Realize(Function):
	@staticmethod
	def forward(ctx, x):
		return torch.ops._inductor_test.realize(x)

	@staticmethod
	def backward(ctx, grad_output):
		return grad_output


def realize(x: Tensor) -> Tensor:
	return Realize.apply(x)

<END>

<START>
import inspect
from functools import wraps
from typing import Any, Callable, Optional, Type, Union, get_type_hints
from torch.utils.data.datapipes.datapipe import IterDataPipe, MapDataPipe
from torch.utils.data.datapipes._typing import _DataPipeMeta


class functional_datapipe:
	name: str

	def __init__(self, name: str, enable_df_api_tracing=False) -> None:
		self.name = name
		self.enable_df_api_tracing = enable_df_api_tracing

	def __call__(self, cls):
		if issubclass(cls, IterDataPipe):
			if isinstance(cls, Type):  # type: ignore[arg-type]
				if not isinstance(cls, _DataPipeMeta):
					raise TypeError('`functional_datapipe` can only decorate IterDataPipe')
			else:
				if not isinstance(cls, non_deterministic) and \
					not (hasattr(cls, '__self__') and
						 isinstance(cls.__self__, non_deterministic)):
					raise TypeError('`functional_datapipe` can only decorate IterDataPipe')
			IterDataPipe.register_datapipe_as_function(self.name, cls, enable_df_api_tracing=self.enable_df_api_tracing)
		elif issubclass(cls, MapDataPipe):
			MapDataPipe.register_datapipe_as_function(self.name, cls)

		return cls


_determinism: bool = False


class guaranteed_datapipes_determinism:
	prev: bool

	def __init__(self) -> None:
		global _determinism
		self.prev = _determinism
		_determinism = True

	def __enter__(self) -> None:
		pass

	def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
		global _determinism
		_determinism = self.prev


class non_deterministic:
	cls: Optional[Type[IterDataPipe]] = None
	deterministic_fn: Callable[[], bool]

	def __init__(self, arg: Union[Type[IterDataPipe], Callable[[], bool]]) -> None:
		if isinstance(arg, Type):  # type: ignore[arg-type]
			if not issubclass(arg, IterDataPipe):  # type: ignore[arg-type]
				raise TypeError("Only `IterDataPipe` can be decorated with `non_deterministic`"
								f", but {arg.__name__} is found")
			self.cls = arg  # type: ignore[assignment]
		elif isinstance(arg, Callable):  # type:ignore[arg-type]
			self.deterministic_fn = arg  # type: ignore[assignment, misc]
		else:
			raise TypeError(f"{arg} can not be decorated by non_deterministic")

	def __call__(self, *args, **kwargs):
		global _determinism
		if self.cls is not None:
			if _determinism:
				raise TypeError("{} is non-deterministic, but you set 'guaranteed_datapipes_determinism'. "
								"You can turn off determinism for this DataPipe if that is acceptable "
								"for your application".format(self.cls.__name__))
			return self.cls(*args, **kwargs)  # type: ignore[call-arg]

		if not (isinstance(args[0], Type) and  # type: ignore[arg-type]
				issubclass(args[0], IterDataPipe)):
			raise TypeError(f"Only `IterDataPipe` can be decorated, but {args[0].__name__} is found")
		self.cls = args[0]
		return self.deterministic_wrapper_fn

	def deterministic_wrapper_fn(self, *args, **kwargs) -> IterDataPipe:
		res = self.deterministic_fn(*args, **kwargs)  # type: ignore[call-arg, misc]
		if not isinstance(res, bool):
			raise TypeError("deterministic_fn of `non_deterministic` decorator is required "
							f"to return a boolean value, but {type(res)} is found")
		global _determinism
		if _determinism and res:
			raise TypeError(f"{self.cls.__name__} is non-deterministic with the inputs, but you set "  # type: ignore[union-attr]
							"'guaranteed_datapipes_determinism'. You can turn off determinism "
							"for this DataPipe if that is acceptable for your application"
							)
		return self.cls(*args, **kwargs)  # type: ignore[call-arg, misc]


def argument_validation(f):
	signature = inspect.signature(f)
	hints = get_type_hints(f)

	@wraps(f)
	def wrapper(*args, **kwargs):
		bound = signature.bind(*args, **kwargs)
		for argument_name, value in bound.arguments.items():
			if argument_name in hints and isinstance(hints[argument_name], _DataPipeMeta):
				hint = hints[argument_name]
				if not isinstance(value, IterDataPipe):
					raise TypeError(f"Expected argument '{argument_name}' as a IterDataPipe, but found {type(value)}")
				if not value.type.issubtype(hint.type):
					raise TypeError(f"Expected type of argument '{argument_name}' as a subtype of "
									f"hint {hint.type}, but found {value.type}"
									)

		return f(*args, **kwargs)

	return wrapper


_runtime_validation_enabled: bool = True


class runtime_validation_disabled:
	prev: bool

	def __init__(self) -> None:
		global _runtime_validation_enabled
		self.prev = _runtime_validation_enabled
		_runtime_validation_enabled = False

	def __enter__(self) -> None:
		pass

	def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
		global _runtime_validation_enabled
		_runtime_validation_enabled = self.prev


def runtime_validation(f):
	if f.__name__ != '__iter__':
		raise TypeError(f"Can not decorate function {f.__name__} with 'runtime_validation'")

	@wraps(f)
	def wrapper(self):
		global _runtime_validation_enabled
		if not _runtime_validation_enabled:
			yield from f(self)
		else:
			it = f(self)
			for d in it:
				if not self.type.issubtype_of_instance(d):
					raise RuntimeError(f"Expected an instance as subtype of {self.type}, but found {d}({type(d)})")
				yield d

	return wrapper

<END>

<START>
import numpy as np
import torch
from contextlib import contextmanager
from torch.testing._internal.common_utils import TEST_WITH_ASAN, TEST_WITH_TSAN, TEST_WITH_UBSAN, IS_PPC, IS_MACOS, IS_WINDOWS

supported_qengines = torch.backends.quantized.supported_engines
supported_qengines.remove('none')
if 'qnnpack' in supported_qengines and any([IS_PPC, TEST_WITH_ASAN, TEST_WITH_TSAN, TEST_WITH_UBSAN, IS_MACOS, IS_WINDOWS]):
	supported_qengines.remove('qnnpack')

def _conv_output_shape(input_size, kernel_size, padding, stride, dilation,
					   output_padding=0):
	if qmin is None:
		qmin = np.iinfo(dtype).min
	if qmax is None:
		qmax = np.iinfo(dtype).max
	qx = np.round(x / scale + zero_point).astype(np.int64)
	qx = np.clip(qx, qmin, qmax)
	qx = qx.astype(dtype)
	return qx


def _dequantize(qx, scale, zero_point):
	power, as well as the SNR in dB.
	If the input is a list/tuple this function is called recursively on each
	element. The result will have the same nested structure as the inputs.

	Args:
		x, x_hat: Either a tensor or a nested list/tuple of tensors.
	Returns:
		signal, noise, SNR(in dB): Either floats or a nested list of floats

<END>

<START>
import copy
import dataclasses
import dis
import itertools
import sys
import types
from typing import Any, Callable, cast, Dict, Iterator, List, Optional, Tuple

from .bytecode_analysis import (
	get_indexof,
	propagate_line_nums,
	remove_extra_line_nums,
	stacksize_analysis,
)


@dataclasses.dataclass
class InstructionExnTabEntry:
	start: "Instruction"
	end: "Instruction"
	target: "Instruction"
	depth: int
	lasti: bool

	def __repr__(self) -> str:
		return (
			f"InstructionExnTabEntry(start={self.start.short_inst_repr()}, "
			f"end={self.end.short_inst_repr()}, "
			f"target={self.target.short_inst_repr()}, "
			f"depth={self.depth}, lasti={self.lasti})"
		)

	def __eq__(self, o) -> bool:
		return (
			self.start is o.start
			and self.end is o.end
			and self.target is o.target
			and self.depth == o.depth
			and self.lasti == o.lasti
		)


@dataclasses.dataclass
class Instruction:
	At most one of `arg`, `argval`, and `target` can be not None/_NotProvided.
	This is to prevent ambiguity, e.g. does
		create_instruction("LOAD_CONST", 5)
	mean load the constant at co_consts[5], or load the constant 5?

	If `arg` is not provided, it will be computed during assembly from
	`argval` or `target`.

	Do not use for LOAD_GLOBAL - use create_load_global instead.
	`name` is the name of the global to be loaded.
	`push_null` specifies whether or not a NULL should be pushed to the stack
	before the global (Python 3.11+ only).

	Python 3.11 changed the LOAD_GLOBAL instruction in that the first bit of
	the instruction arg specifies whether a NULL should be pushed to the stack
	before the global. The remaining bits of the instruction arg contain the
	name index. See `create_call_function` for why this NULL is needed.

	The instruction's `arg` is actually computed when assembling the bytecode.
	For Python 3.11, push_null information is propagated through the arg.

	NOTE: we don't use create_instruction since LOAD_GLOBAL is the only instruction
	where both arg and argval need to be specified.
	Returns a "simple" sequence of instructions that rotates TOS to the n-th
	position in the stack. For Python < 3.11, returns a single ROT_*
	instruction. If no such instruction exists, an error is raised and the
	caller is expected to generate an equivalent sequence of instructions.
	For Python >= 3.11, any rotation can be expressed as a simple sequence of
	swaps.
	Creates a sequence of instructions that makes a function call.

	`push_null` is used in Python 3.11+ only. It is used in codegen when
	a function call is intended to be made with the NULL + fn convention,
	and we know that the NULL has not been pushed yet. We will push a
	NULL and rotate it to the correct position immediately before making
	the function call.
	push_null should default to True unless you know you are calling a function
	that you codegen'd with a null already pushed, for example
	(assume `math` is available in the global scope),

	create_load_global("math", True)  # pushes a null
	create_instruction("LOAD_ATTR", argval="sqrt")
	create_instruction("LOAD_CONST", argval=25)
	create_call_function(1, False)
	Used to create typing.CodeType.co_lnotab
	See https://github.com/python/cpython/blob/main/Objects/lnotab_notes.txt
	This is the internal format of the line number table if Python < 3.10
	Used to create typing.CodeType.co_linetable
	See https://github.com/python/cpython/blob/main/Objects/lnotab_notes.txt
	This is the internal format of the line number table for Python 3.10
	6-bit chunk encoding of an unsigned integer
	See https://github.com/python/cpython/blob/3.11/Objects/locations.md
	Used to create typing.CodeType.co_linetable
	See https://github.com/python/cpython/blob/3.11/Objects/locations.md
	This is the internal format of the line number table for Python 3.11
	Similar to `encode_varint`, but the 6-bit chunks are ordered in reverse.
	Inverse of `encode_exception_table_varint`.
	Verifies that a list of ExceptionTableEntries will make a well-formed
	jump table: entries are non-empty, sorted, and do not overlap.
	Parse the exception table according to
	https://github.com/python/cpython/blob/3.11/Objects/exception_handling_notes.txt
	Inverse of parse_exception_table - encodes list of exception
	table entries into bytes.
	code: List[int] = []
	if sys.version_info >= (3, 11):
		lnotab, update_lineno = linetable_311_writer(firstlineno)
		num_ext = 0
		for i, inst in enumerate(instructions):
			if inst.opname == "EXTENDED_ARG":
				inst_size = 1
				num_ext += 1
				for j in (1, 2, 3):
					if instructions[i + j].opname != "EXTENDED_ARG":
						inst.positions = instructions[i + j].positions
						break
			else:
				inst_size = instruction_size(inst) // 2 + num_ext
				num_ext = 0
			update_lineno(inst.positions, inst_size)
			num_ext = 0
			arg = inst.arg or 0
			code.extend((inst.opcode, arg & 0xFF))
			for _ in range(instruction_size(inst) // 2 - 1):
				code.extend((0, 0))
	else:
		if sys.version_info < (3, 10):
			lnotab, update_lineno = lnotab_writer(firstlineno)
		else:
			lnotab, update_lineno, end = linetable_310_writer(firstlineno)

		for inst in instructions:
			if inst.starts_line is not None:
				update_lineno(inst.starts_line, len(code))
			arg = inst.arg or 0
			code.extend((inst.opcode, arg & 0xFF))

		if sys.version_info >= (3, 10):
			end(len(code))

	return bytes(code), bytes(lnotab)


def _get_instruction_by_offset(offset_to_inst: Dict[int, Instruction], offset: int):
	for n in (0, 2, 4, 6):
		if offset_to_inst[offset + n].opcode != dis.EXTENDED_ARG:
			return offset_to_inst[offset + n]
	return None


def virtualize_jumps(instructions) -> None:
	i.e. get the first EXTENDED_ARG instruction (if any) when targeting
	instructions[idx] with a jump.
	indexof = get_indexof(instructions)
	jumps = set(dis.hasjabs).union(set(dis.hasjrel))

	for inst in instructions:
		if inst.opcode in jumps:
			target = _get_instruction_front(instructions, indexof[inst.target])
			if inst.opcode in dis.hasjabs:
				if sys.version_info < (3, 10):
					inst.arg = target.offset
				elif sys.version_info < (3, 11):
					inst.arg = int(target.offset / 2)
				else:
					raise RuntimeError("Python 3.11+ should not have absolute jumps")
			else:  # relative jump
				inst.arg = int(target.offset - inst.offset - instruction_size(inst))
				if inst.arg < 0:
					if sys.version_info < (3, 11):
						raise RuntimeError("Got negative jump offset for Python < 3.11")
					inst.arg = -inst.arg
					if "FORWARD" in inst.opname:
						flip_jump_direction(inst)
				elif inst.arg > 0:
					if sys.version_info >= (3, 11) and "BACKWARD" in inst.opname:
						flip_jump_direction(inst)
				if sys.version_info >= (3, 10):
					inst.arg //= 2
			inst.argval = target.offset
			inst.argrepr = f"to {target.offset}"


def virtualize_exception_table(exn_tab_bytes: bytes, instructions: List[Instruction]):
	exn_dict: Dict[Tuple[int, int], Tuple[int, int, bool]] = {}
	indexof = get_indexof(instructions)

	for inst in instructions:
		if inst.exn_tab_entry:
			start = _get_instruction_front(
				instructions, indexof[inst.exn_tab_entry.start]
			).offset
			end = (
				cast(int, inst.exn_tab_entry.end.offset)
				+ instruction_size(inst.exn_tab_entry.end)
				- 2
			)
			target = _get_instruction_front(
				instructions, indexof[inst.exn_tab_entry.target]
			).offset
			key = (start, end)
			val = (target, inst.exn_tab_entry.depth, inst.exn_tab_entry.lasti)
			if key in exn_dict:
				assert exn_dict[key] == val
			exn_dict[key] = val


	keys_sorted = sorted(exn_dict.keys(), key=lambda t: (t[0], -t[1]))
	nexti = 0
	key_stack: List[Tuple[int, int]] = []
	exn_tab: List[ExceptionTableEntry] = []

	def pop():
		nonlocal nexti
		if key_stack:
			key = key_stack.pop()
			if nexti <= key[1]:
				exn_tab.append(
					ExceptionTableEntry(max(key[0], nexti), key[1], *exn_dict[key])
				)
				nexti = key[1] + 2

	for key in keys_sorted:
		while key_stack and key_stack[-1][1] < key[0]:
			pop()
		if key_stack:
			assert key_stack[-1][0] <= key[0] <= key[1] <= key_stack[-1][1]
			left = max(nexti, key_stack[-1][0])
			if left < key[0]:
				exn_tab.append(
					ExceptionTableEntry(left, key[0] - 2, *exn_dict[key_stack[-1]])
				)
			nexti = key[0]
		key_stack.append(key)
	while key_stack:
		pop()
	check_exception_table(exn_tab)
	return exn_tab


def check_inst_exn_tab_entries_nested(
	tab: List[InstructionExnTabEntry], indexof
) -> None:
	entry_stack: List[Tuple[int, int]] = []
	for entry in tab:
		key = (indexof[entry.start], indexof[entry.end])
		while entry_stack and entry_stack[-1][1] < key[0]:
			entry_stack.pop()
		if entry_stack:
			assert entry_stack[-1][0] <= key[0] <= key[1] <= entry_stack[-1][1]
		entry_stack.append(key)


def propagate_inst_exn_table_entries(instructions: List[Instruction]) -> None:
	indexof = get_indexof(instructions)
	entries: Dict[Tuple[int, int], InstructionExnTabEntry] = {}
	for inst in instructions:
		if inst.exn_tab_entry:
			key = (
				indexof[inst.exn_tab_entry.start],
				indexof[inst.exn_tab_entry.end],
			)
			if key in entries:
				assert inst.exn_tab_entry == entries[key]
			entries[key] = inst.exn_tab_entry
	sorted_entries = [
		entries[key] for key in sorted(entries.keys(), key=lambda t: (t[0], -t[1]))
	]
	check_inst_exn_tab_entries_nested(sorted_entries, indexof)
	for entry in sorted_entries:
		for i in range(indexof[entry.start], indexof[entry.end] + 1):
			instructions[i].exn_tab_entry = copy.copy(entry)


def check_inst_exn_tab_entries_valid(instructions: List[Instruction]):
	indexof = get_indexof(instructions)
	exn_tab_entry_set = set()
	for i, inst in enumerate(instructions):
		if inst.exn_tab_entry:
			assert sys.version_info >= (3, 11)
			assert id(inst.exn_tab_entry) not in exn_tab_entry_set
			exn_tab_entry_set.add(id(inst.exn_tab_entry))
			entry = inst.exn_tab_entry
			assert entry.start in indexof
			assert entry.end in indexof
			assert entry.target in indexof
			assert indexof[entry.start] <= i <= indexof[entry.end]


def strip_extended_args(instructions: List[Instruction]) -> None:
	instructions[:] = [i for i in instructions if i.opcode != dis.EXTENDED_ARG]


def remove_load_call_method(instructions: List[Instruction]) -> List[Instruction]:
	cell_and_free = (code.co_cellvars or tuple()) + (code.co_freevars or tuple())
	output = []
	for idx, inst in enumerate(instructions):
		output.append(inst)
		if inst.opname == "LOAD_GLOBAL" and inst.argval == "super":
			nexti = instructions[idx + 1]
			if nexti.opname in ("CALL_FUNCTION", "PRECALL") and nexti.arg == 0:
				assert "__class__" in cell_and_free
				output.append(create_instruction("LOAD_DEREF", argval="__class__"))
				first_var = code.co_varnames[0]
				if first_var in cell_and_free:
					output.append(create_instruction("LOAD_DEREF", argval=first_var))
				else:
					output.append(create_instruction("LOAD_FAST", argval=first_var))
				nexti.arg = 2
				nexti.argval = 2
				if nexti.opname == "PRECALL":
					call_inst = instructions[idx + 2]
					call_inst.arg = 2
					call_inst.argval = 2

	instructions[:] = output


def fix_extended_args(instructions: List[Instruction]) -> int:
	dode = transform_code_object(code, lambda x, y: None, safe=True)
	assert code.co_code == dode.co_code, debug_bytes(code.co_code, dode.co_code)
	assert code.co_lnotab == dode.co_lnotab, debug_bytes(code.co_lnotab, dode.co_lnotab)


HAS_LOCAL = set(dis.haslocal)
HAS_NAME = set(dis.hasname)
HAS_FREE = set(dis.hasfree)
HAS_CONST = set(dis.hasconst)


def get_const_index(code_options, val) -> int:
	for i, v in enumerate(code_options["co_consts"]):
		if val is v:
			return i
	code_options["co_consts"] += (val,)
	return len(code_options["co_consts"]) - 1


def fix_vars(instructions: List[Instruction], code_options, varname_from_oparg=None):
	names = {name: idx for idx, name in enumerate(code_options["co_names"])}
	if sys.version_info < (3, 11):
		assert varname_from_oparg is None
		varnames = {name: idx for idx, name in enumerate(code_options["co_varnames"])}
		freenames = {
			name: idx
			for idx, name in enumerate(
				code_options["co_cellvars"] + code_options["co_freevars"]
			)
		}
	else:
		assert callable(varname_from_oparg)
		allnames = {}
		for idx in itertools.count():
			try:
				name = varname_from_oparg(idx)
				allnames[name] = idx
			except IndexError:
				break
		varnames = {name: allnames[name] for name in code_options["co_varnames"]}
		freenames = {
			name: allnames[name]
			for name in code_options["co_cellvars"] + code_options["co_freevars"]
		}
	for i in range(len(instructions)):

		def should_compute_arg():
			return instructions[i].argval is not _NotProvided

		if instructions[i].opname == "LOAD_GLOBAL":
			assert instructions[i].arg is not None
			assert instructions[i].argval is not _NotProvided
			if sys.version_info >= (3, 11):
				instructions[i].arg = (names[instructions[i].argval] << 1) + (
					cast(int, instructions[i].arg) % 2
				)
			else:
				instructions[i].arg = names[instructions[i].argval]
		elif instructions[i].opcode in HAS_LOCAL:
			if should_compute_arg():
				instructions[i].arg = varnames[instructions[i].argval]
		elif instructions[i].opcode in HAS_NAME:
			if should_compute_arg():
				instructions[i].arg = names[instructions[i].argval]
		elif instructions[i].opcode in HAS_FREE:
			if should_compute_arg():
				instructions[i].arg = freenames[instructions[i].argval]
		elif instructions[i].opcode in HAS_CONST:
			if instructions[i].arg is None:
				idx = get_const_index(code_options, instructions[i].argval)
				assert idx >= 0
				instructions[i].arg = idx


def get_code_keys() -> List[str]:
	keys = ["co_argcount"]
	keys.append("co_posonlyargcount")
	keys.extend(
		[
			"co_kwonlyargcount",
			"co_nlocals",
			"co_stacksize",
			"co_flags",
			"co_code",
			"co_consts",
			"co_names",
			"co_varnames",
			"co_filename",
			"co_name",
		]
	)
	if sys.version_info >= (3, 11):
		keys.append("co_qualname")
	keys.append("co_firstlineno")
	if sys.version_info >= (3, 10):
		keys.append("co_linetable")
	else:
		keys.append("co_lnotab")
	if sys.version_info >= (3, 11):
		keys.append("co_exceptiontable")
	keys.extend(
		[
			"co_freevars",
			"co_cellvars",
		]
	)
	return keys


def transform_code_object(code, transformations, safe=False) -> types.CodeType:
	keys = get_code_keys()
	code_options = {k: getattr(code, k) for k in keys}
	assert len(code_options["co_varnames"]) == code_options["co_nlocals"]

	instructions = cleaned_instructions(code, safe)
	propagate_line_nums(instructions)

	transformations(instructions, code_options)
	return clean_and_assemble_instructions(instructions, keys, code_options)[1]


def clean_and_assemble_instructions(
	instructions: List[Instruction], keys: List[str], code_options: Dict[str, Any]
) -> Tuple[List[Instruction], types.CodeType]:
	check_inst_exn_tab_entries_valid(instructions)

	code_options["co_nlocals"] = len(code_options["co_varnames"])
	varname_from_oparg = None
	if sys.version_info >= (3, 11):
		tmp_code = types.CodeType(*[code_options[k] for k in keys])
		varname_from_oparg = tmp_code._varname_from_oparg  # type: ignore[attr-defined]
	fix_vars(instructions, code_options, varname_from_oparg=varname_from_oparg)

	dirty = True
	while dirty:
		update_offsets(instructions)
		devirtualize_jumps(instructions)
		dirty = bool(fix_extended_args(instructions))

	remove_extra_line_nums(instructions)
	bytecode, lnotab = assemble(instructions, code_options["co_firstlineno"])
	if sys.version_info < (3, 10):
		code_options["co_lnotab"] = lnotab
	else:
		code_options["co_linetable"] = lnotab

	code_options["co_code"] = bytecode
	code_options["co_stacksize"] = stacksize_analysis(instructions)
	assert set(keys) - {"co_posonlyargcount"} == set(code_options.keys()) - {
		"co_posonlyargcount"
	}
	if sys.version_info >= (3, 11):
		code_options["co_exceptiontable"] = assemble_exception_table(
			compute_exception_table(instructions)
		)
	return instructions, types.CodeType(*[code_options[k] for k in keys])


def populate_kw_names_argval(instructions, consts):
	for inst in instructions:
		if inst.opname == "KW_NAMES":
			inst.argval = consts[inst.arg]


def cleaned_instructions(code, safe=False) -> List[Instruction]:
	instructions = list(map(convert_instruction, dis.get_instructions(code)))
	check_offsets(instructions)
	if sys.version_info >= (3, 11):
		populate_kw_names_argval(instructions, code.co_consts)
		virtualize_exception_table(code.co_exceptiontable, instructions)
	virtualize_jumps(instructions)
	strip_extended_args(instructions)
	if not safe:
		if sys.version_info < (3, 11):
			remove_load_call_method(instructions)
		else:
			remove_jump_if_none(instructions)
			update_offsets(instructions)
			devirtualize_jumps(instructions)
		explicit_super(code, instructions)
	return instructions


_unique_id_counter = itertools.count()


def unique_id(name) -> str:
	return f"{name}_{next(_unique_id_counter)}"


def is_generator(code: types.CodeType) -> bool:
	co_generator = 0x20
	return (code.co_flags & co_generator) > 0

<END>

<START>
import torch

from torch._export.db.case import export_case, SupportLevel


@export_case(
	example_inputs=(torch.ones(3, 2),),
	tags={"python.object-model"},
	support_level=SupportLevel.NOT_SUPPORTED_YET,
)
class ModelAttrMutation(torch.nn.Module):

	def __init__(self):
		super().__init__()
		self.attr_list = [torch.ones(3, 2), torch.ones(3, 2)]

	def recreate_list(self):
		return [torch.zeros(3, 2), torch.zeros(3, 2)]

	def forward(self, x):
		self.attr_list = self.recreate_list()
		return x.sum() + self.attr_list[0].sum()

<END>

<START>

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.ao.nn.intrinsic.quantized.dynamic as nniqd
import torch.ao.nn.quantized as nnq
import torch.ao.nn.quantized.dynamic as nnqd
from torch.ao.nn.intrinsic import _FusedModule
import torch.distributed as dist
from torch.testing._internal.common_utils import TestCase, TEST_WITH_ROCM

from torch.ao.quantization import (
	QuantType,
	default_dynamic_qat_qconfig,
	default_embedding_qat_qconfig,
	default_symmetric_qnnpack_qat_qconfig,
)
from torch.ao.quantization import QuantWrapper, QuantStub, DeQuantStub, \
	default_qconfig, default_dynamic_qconfig, default_per_channel_qconfig, QConfig, default_observer, default_weight_observer, \
	propagate_qconfig_, convert, get_default_qconfig, quantize_dynamic_jit, quantize_jit, float_qparams_weight_only_qconfig, \
	get_default_qat_qconfig, PerChannelMinMaxObserver, default_dynamic_quant_observer, quantize, \
	QConfigMapping, get_default_qconfig_mapping, get_default_qat_qconfig_mapping
from torch.ao.quantization.quantization_mappings import (
	get_default_dynamic_quant_module_mappings,
	get_default_qconfig_propagation_list,
	get_default_qat_module_mappings,
)
from torch.testing._internal.common_quantized import (
	override_quantized_engine,
)
from torch.jit.mobile import _load_for_lite_interpreter

try:
	from torch.ao.quantization.quantize_fx import (
		prepare_fx,
		prepare_qat_fx,
		convert_fx,
		convert_to_reference_fx,
	)
	from torch.ao.ns.fx.ns_types import NSSingleResultValuesType, NSSubgraph
	from torch.fx.graph import Node
	from torch.fx import GraphModule
	HAS_FX = True
except ImportError:
	HAS_FX = False

import copy
import io
import functools
import time
import os

import unittest
import numpy as np
from torch.testing import FileCheck
from typing import Callable, Tuple, Dict, Any, Union, Type, Optional
import torch._dynamo as torchdynamo

class NodeSpec:
	def __init__(self, op, target):
		self.op = op
		self.target = target

	@classmethod
	def call_function(cls, target):
		return NodeSpec('call_function', target)

	@classmethod
	def call_method(cls, target):
		return NodeSpec('call_method', target)

	@classmethod
	def call_module(cls, target):
		return NodeSpec('call_module', target)

	def __hash__(self):
		return hash((self.op, self.target))

	def __eq__(self, other):
		if not isinstance(other, NodeSpec):
			return NotImplemented

		return self.op == other.op and self.target == other.target

	def __repr__(self):
		return repr(self.op) + " " + repr(self.target)

def get_supported_device_types():
	return ['cpu', 'cuda'] if torch.cuda.is_available() and not TEST_WITH_ROCM else ['cpu']

def test_only_eval_fn(model, calib_data):
	for inp in calib_data:
		output = model(*inp)

_default_loss_fn = torch.nn.CrossEntropyLoss()
def test_only_train_fn(model, train_data, loss_fn=_default_loss_fn):
	optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
	train_loss, correct, total = 0, 0, 0
	for i in range(10):
		model.train()

		for data, target in train_data:
			optimizer.zero_grad()
			output = model(data)
			loss = loss_fn(output, target)
			loss.backward()
			optimizer.step()
			train_loss += loss.item()
			_, predicted = torch.max(output, 1)
			total += target.size(0)
			correct += (predicted == target).sum().item()
	return train_loss, correct, total

class AverageMeter:
	with torch.no_grad():
		maxk = max(topk)
		batch_size = target.size(0)

		_, pred = output.topk(maxk, 1, True, True)
		pred = pred.t()
		correct = pred.eq(target.view(1, -1).expand_as(pred))

		res = []
		for k in topk:
			correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
			res.append(correct_k.mul_(100.0 / batch_size))
		return res

def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):
	model.train()
	cnt = 0
	for image, target in data_loader:
		start_time = time.time()
	Convert lengths to offsets for embedding_bag
			modules for quantization preparation, e.g.
			quant, dequant and observer
			modules for quantization preparation, e.g.
			quant, dequant and observer
			have observers in preparation for quantization
			nn.DeQuantize submodules inserted
			module, the bias is qint32, and that the module
			has Quantize and DeQuantize submodules
			module, the bias is float.
			module, the bias is float.
			module, the bias is float.
			module, the bias is float.

		if eval_mode:
			module = module.eval()
		if dynamic:
		Args:
			graph_module: the GraphModule instance we want to check
			expected_node, expected_node_occurrence, expected_node_list:
			   see docs for checkGraphModeFxOp
			Verifies that the types specified in expected_types match
			the underlying objects pointed to by the nodes in matched_subgraph_pairs.

			An example successful test case:

			  matched_subgraph_pairs = {'x0': (graph_a_conv_0_node, graph_b_conv_0_node)}
			  expected_types = {'x0': (nn.Conv2d, nnq.Conv2d)}

			The function tests for key equivalence, and verifies types with
			instance checks.
			Verifies that the act_compare_dict (output of Numeric Suite APIs) is valid:
			1. for each layer, results are recorded for two models
			2. number of seen tensors match
			3. shapes of each pair of seen tensors match
				quantized model contains the quantized_node

				Args:
					model: floating point torch.nn.Module
					inputs: one positional sample input arguments for model
					expected_node: NodeSpec
						e.g. NodeSpec.call_function(torch.quantize_per_tensor)
					expected_node_occurrence: a dict from NodeSpec to
						expected number of occurrences (int)
						e.g. {NodeSpec.call_function(torch.quantize_per_tensor) : 1,
								NodeSpec.call_method('dequantize'): 1}
					expected_node_list: a list of NodeSpec, used to check the order
						of the occurrence of Node
						e.g. [NodeSpec.call_function(torch.quantize_per_tensor),
								NodeSpec.call_module(nnq.Conv2d),
								NodeSpec.call_function(F.hardtanh_),
								NodeSpec.call_method('dequantize')]
					is_reference: if True, enables reference mode
					print_debug_info: if True, prints debug info
					custom_qconfig_dict: overrides default qconfig_dict
					prepare_expected_node: same as expected_node, but for prepare
					prepare_expected_node_occurrence: same as
						expected_node_occurrence, but for prepare
					prepare_expected_node_list: same as expected_node_list, but
						for prepare

				Returns:
					A dictionary with the following structure:
				   {
					   "prepared": ...,  # the prepared model
					   "quantized": ...,  # the quantized non-reference model
					   "quantized_reference": ...,  # the quantized reference model
					   "result": ...,  # the result for either quantized or
				   }
	setting qconfig of a submodule to None
	setting qconfig of a submodule to None
	and contains both linear and conv modules
	Supported only with qnnpack.
	this module uses a separate embedding and bagging op, similar
	to that which is described in the EmbeddingBag documentation.

	https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html

<END>

<START>
from collections import defaultdict
from copy import deepcopy
import torch
from typing import Any, Optional, Dict
import pytorch_lightning as pl  # type: ignore[import]

from ._data_sparstity_utils import (
	_attach_model_to_data_sparsifier,
	_log_sparsified_level,
	_get_valid_name
)


class PostTrainingDataSparsity(pl.callbacks.Callback):
	def __init__(self, data_sparsifier_class, data_sparsifier_args):
		super().__init__()
		self.data_sparsifier_class = data_sparsifier_class
		self.data_sparsifier_args = data_sparsifier_args
		self.data_sparsifier: Any = None
		self.sparsified: Optional[torch.nn.Module] = None

	def on_fit_end(self, trainer, pl_module) -> None:
		self.sparsified = deepcopy(pl_module.model).eval()
		self.data_sparsifier = self.data_sparsifier_class(**self.data_sparsifier_args)

		_attach_model_to_data_sparsifier(self.sparsified, self.data_sparsifier)

		self.data_sparsifier.step()

		self.data_sparsifier.squash_mask()  # currently squashes params for all mask

		_log_sparsified_level(self.sparsified, self.data_sparsifier)


class TrainingAwareDataSparsity(pl.callbacks.Callback):
	def __init__(self, data_sparsifier_class, data_sparsifier_args,
				 data_scheduler_class, data_scheduler_args):
		super().__init__()
		self.data_sparsifier_class = data_sparsifier_class
		self.data_sparsifier_args = data_sparsifier_args

		self.data_scheduler_class = data_scheduler_class
		self.data_scheduler_args = data_scheduler_args

		self.data_sparsifier: Any = None
		self.data_scheduler: Any = None
		self.sparsified: Optional[torch.nn.Module] = None

		self.data_sparsifier_state_dict: Any = None

	def on_train_start(self, trainer, pl_module) -> None:
		self.data_sparsifier = self.data_sparsifier_class(**self.data_sparsifier_args)
		self.sparsified = deepcopy(pl_module.model)

		_attach_model_to_data_sparsifier(self.sparsified, self.data_sparsifier)  # just to populate the base_sl in the scheduler

		args = deepcopy(self.data_scheduler_args)
		args['data_sparsifier'] = self.data_sparsifier
		self.data_scheduler = self.data_scheduler_class(**args)

	def on_train_epoch_start(self, trainer, pl_module):
		if self.data_sparsifier_state_dict is None:
			return  # probably first epoch

		self.data_sparsifier.load_state_dict(self.data_sparsifier_state_dict)

	def __create_config_based_on_state(self, pl_module):
		config: Dict = defaultdict()
		if self.data_sparsifier_state_dict is None:
			return config
		for name, _ in pl_module.model.named_parameters():
			valid_name = _get_valid_name(name)
			config[valid_name] = self.data_sparsifier.data_groups[valid_name]

		return config

	def on_train_epoch_end(self, trainer, pl_module):
		self.sparsified = deepcopy(pl_module.model)
		config = self.__create_config_based_on_state(pl_module)

		_attach_model_to_data_sparsifier(self.sparsified, self.data_sparsifier, config=config)
		self.data_sparsifier.step()
		self.data_scheduler.step()

		self.data_sparsifier_state_dict = self.data_sparsifier.state_dict()

	def on_train_end(self, trainer, pl_module):
		self.data_sparsifier.squash_mask()

<END>

<START>
import warnings

from abc import ABC, abstractmethod
from collections import deque
import copy as copymodule
from typing import Any, Callable, Iterator, List, Literal, Optional, Sized, Tuple, TypeVar, Deque

from torch.utils.data.datapipes._decorator import functional_datapipe
from torch.utils.data.datapipes._hook_iterator import _SnapshotState
from torch.utils.data.datapipes.datapipe import IterDataPipe
from torch.utils.data.datapipes.utils.common import StreamWrapper, _check_unpickable_fn

__all__ = [
	"ConcaterIterDataPipe",
	"DemultiplexerIterDataPipe",
	"ForkerIterDataPipe",
	"MultiplexerIterDataPipe",
	"ZipperIterDataPipe",
]

T_co = TypeVar('T_co', covariant=True)


@functional_datapipe('concat')
class ConcaterIterDataPipe(IterDataPipe):

	datapipes: Tuple[IterDataPipe]

	def __init__(self, *datapipes: IterDataPipe):
		if len(datapipes) == 0:
			raise ValueError("Expected at least one DataPipe, but got nothing")
		if not all(isinstance(dp, IterDataPipe) for dp in datapipes):
			raise TypeError("Expected all inputs to be `IterDataPipe`")
		self.datapipes = datapipes  # type: ignore[assignment]

	def __iter__(self) -> Iterator:
		for dp in self.datapipes:
			yield from dp

	def __len__(self) -> int:
		if all(isinstance(dp, Sized) for dp in self.datapipes):
			return sum(len(dp) for dp in self.datapipes)
		else:
			raise TypeError(f"{type(self).__name__} instance doesn't have valid length")


@functional_datapipe('fork')
class ForkerIterDataPipe(IterDataPipe):

	def __new__(
		cls,
		datapipe: IterDataPipe,
		num_instances: int,
		buffer_size: int = 1000,
		copy: Optional[Literal["shallow", "deep"]] = None
	):
		if num_instances < 1:
			raise ValueError(f"Expected `num_instances` larger than 0, but {num_instances} is found")
		if num_instances == 1:
			return datapipe
		container = _ForkerIterDataPipe(datapipe, num_instances, buffer_size, copy)
		return [_ChildDataPipe(container, i) for i in range(num_instances)]


class _ContainerTemplate(ABC):


def _no_op(x):
	return x


class _ForkerIterDataPipe(IterDataPipe, _ContainerTemplate):

	def __init__(
		self,
		datapipe: IterDataPipe,
		num_instances: int,
		buffer_size: int = 1000,
		copy: Optional[Literal["shallow", "deep"]] = None
	):
		self.main_datapipe = datapipe
		self._datapipe_iterator: Optional[Iterator[Any]] = None
		self.num_instances = num_instances
		self.buffer: Deque = deque()
		self.buffer_size = buffer_size
		if self.buffer_size < 0:
			warnings.warn(
				"Unlimited buffer size is set for `fork`, "
				"please be aware of OOM at random places",
				UserWarning
			)
		if copy is None:
			self.copy_fn = _no_op
		elif copy == "shallow":
			self.copy_fn = copymodule.copy
		elif copy == "deep":
			self.copy_fn = copymodule.deepcopy
		else:
			raise ValueError(f"Unknown copy method `{copy}` requested, choose one of None, `shallow` or `deep`.")

		self.child_pointers: List[int] = [0] * num_instances  # Indicate the indices of the next element to get
		self.slowest_ptr = 0  # The index to read by the slowest child
		self.leading_ptr = 0  # The index to read by the fastest child
		self.end_ptr: Optional[int] = None  # The index to stop child
		self._child_stop: List[bool] = [True for _ in range(num_instances)]

	def __len__(self):
		return len(self.main_datapipe)

	def get_next_element_by_instance(self, instance_id: int):
		if self._datapipe_iterator is None and self._child_stop[instance_id]:
			self._datapipe_iterator = iter(self.main_datapipe)
			self._snapshot_state = _SnapshotState.Iterating
			for i in range(self.num_instances):
				self._child_stop[i] = False
		try:
			while not self._child_stop[instance_id]:
				self.child_pointers[instance_id] += 1
				if self.end_ptr is not None and self.child_pointers[instance_id] == self.end_ptr:
					self._child_stop[instance_id] = True
					break
				if self.buffer and self.child_pointers[instance_id] <= self.leading_ptr:
					idx = self.child_pointers[instance_id] - self.slowest_ptr - 1
					return_val = self.buffer[idx]
				else:  # Retrieve one element from main datapipe
					self.leading_ptr = self.child_pointers[instance_id]
					try:
						return_val = next(self._datapipe_iterator)  # type: ignore[arg-type]
						self.buffer.append(return_val)
					except StopIteration:
						self._child_stop[instance_id] = True
						self._datapipe_iterator = None
						self.end_ptr = self.leading_ptr
						continue
				if self.child_pointers[instance_id] == self.slowest_ptr + 1:
					new_min = min(self.child_pointers)  # Can optimize by avoiding the call to min()
					if self.slowest_ptr < new_min:
						self.slowest_ptr = new_min
						self.buffer.popleft()
				if self.buffer_size >= 0 and self.leading_ptr > self.buffer_size + self.slowest_ptr:
					raise BufferError("ForkerIterDataPipe buffer overflow," +
									  f"buffer size {self.buffer_size} is insufficient.")

				yield self.copy_fn(return_val)
		finally:
			self._child_stop[instance_id] = True
			if all(self._child_stop):
				self._datapipe_iterator = None
				self._cleanup()

	def is_every_instance_exhausted(self) -> bool:
		return self.end_ptr is not None and all(self._child_stop)

	def get_length_by_instance(self, instance_id: int) -> int:
		return len(self.main_datapipe)

	def reset(self) -> None:
		self._datapipe_iterator = None
		self.buffer = deque()
		self.child_pointers = [0] * self.num_instances
		self.slowest_ptr = 0
		self.leading_ptr = 0
		self.end_ptr = None
		self._child_stop = [True for _ in range(self.num_instances)]

	def __getstate__(self):
		state = (
			self.main_datapipe,
			self.num_instances,
			self.buffer_size,
			self.copy_fn,
			self._valid_iterator_id,
			self._number_of_samples_yielded,
		)
		if IterDataPipe.getstate_hook is not None:
			return IterDataPipe.getstate_hook(state)
		return state

	def __setstate__(self, state):
		(
			self.main_datapipe,
			self.num_instances,
			self.buffer_size,
			self.copy_fn,
			self._valid_iterator_id,
			self._number_of_samples_yielded,
		) = state
		self._datapipe_iterator = None
		self.buffer = deque()
		self.child_pointers = [0] * self.num_instances
		self.slowest_ptr = 0
		self.leading_ptr = 0
		self.end_ptr = None
		self._child_stop = [True for _ in range(self.num_instances)]

	def _cleanup(self):
		while self.buffer:
			d = self.buffer.popleft()
			StreamWrapper.close_streams(d)

	def __del__(self):
		self._cleanup()


class _ChildDataPipe(IterDataPipe):

	_is_child_datapipe: bool = True

	def __init__(self, main_datapipe: IterDataPipe, instance_id: int):
		assert isinstance(main_datapipe, _ContainerTemplate)

		self.main_datapipe: IterDataPipe = main_datapipe
		self.instance_id = instance_id

	def __iter__(self):
		return self.main_datapipe.get_next_element_by_instance(self.instance_id)

	def __len__(self):
		return self.main_datapipe.get_length_by_instance(self.instance_id)

	def _set_main_datapipe_valid_iterator_id(self) -> int:
		if self.main_datapipe._valid_iterator_id is None:
			self.main_datapipe._valid_iterator_id = 0  # type: ignore[attr-defined]
		elif self.main_datapipe._valid_iterator_id == self._valid_iterator_id:  # type: ignore[has-type]
			self.main_datapipe._valid_iterator_id += 1  # type: ignore[attr-defined]
			if not self.main_datapipe.is_every_instance_exhausted():
				warnings.warn("Some child DataPipes are not exhausted when __iter__ is called. We are resetting "
							  "the buffer and each child DataPipe will read from the start again.", UserWarning)
			self.main_datapipe.reset()
		self._valid_iterator_id = self.main_datapipe._valid_iterator_id
		return self._valid_iterator_id

	def _check_valid_iterator_id(self, iterator_id) -> bool:
	Splits the input DataPipe into multiple child DataPipes, using the given classification function (functional name: ``demux``).

	A list of the child DataPipes is returned from this operation.

	Args:
		datapipe: Iterable DataPipe being filtered
		num_instances: number of instances of the DataPipe to create
		classifier_fn: a function that maps values to an integer within the range ``[0, num_instances - 1]`` or ``None``
		drop_none: defaults to ``False``, if ``True``, the function will skip over elements classified as ``None``
		buffer_size: this defines the maximum number of inputs that the buffer can hold across all child
			DataPipes while waiting for their values to be yielded.
			Defaults to ``1000``. Use ``-1`` for the unlimited buffer.

	Examples:
		>>> # xdoctest: +REQUIRES(module:torchdata)
		>>> from torchdata.datapipes.iter import IterableWrapper
		>>> def odd_or_even(n):
		...	 return n % 2
		>>> source_dp = IterableWrapper(range(5))
		>>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even)
		>>> list(dp1)
		[0, 2, 4]
		>>> list(dp2)
		[1, 3]
		>>> # It can also filter out any element that gets `None` from the `classifier_fn`
		>>> def odd_or_even_no_zero(n):
		...	 return n % 2 if n != 0 else None
		>>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even_no_zero, drop_none=True)
		>>> list(dp1)
		[2, 4]
		>>> list(dp2)
		[1, 3]
	Container to hold instance-specific information on behalf of DemultiplexerIterDataPipe.

	It tracks the state of its child DataPipes, maintains the buffer, classifies and yields the next correct value
	as requested by the child DataPipes.
	Yields one element at a time from each of the input Iterable DataPipes (functional name: ``mux``).

	As in, one element from the 1st input DataPipe, then one element from the 2nd DataPipe in the next iteration,
	and so on. It ends when the shortest input DataPipe is exhausted.

	Args:
		datapipes: Iterable DataPipes that will take turn to yield their elements, until the shortest DataPipe is exhausted

	Example:
		>>> # xdoctest: +REQUIRES(module:torchdata)
		>>> from torchdata.datapipes.iter import IterableWrapper
		>>> dp1, dp2, dp3 = IterableWrapper(range(3)), IterableWrapper(range(10, 15)), IterableWrapper(range(20, 25))
		>>> list(dp1.mux(dp2, dp3))
		[0, 10, 20, 1, 11, 21, 2, 12, 22]
	Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``).

	The output is stopped as soon as the shortest input DataPipe is exhausted.

	Args:
		*datapipes: Iterable DataPipes being aggregated

	Example:
		>>> # xdoctest: +REQUIRES(module:torchdata)
		>>> from torchdata.datapipes.iter import IterableWrapper
		>>> dp1, dp2, dp3 = IterableWrapper(range(5)), IterableWrapper(range(10, 15)), IterableWrapper(range(20, 25))
		>>> list(dp1.zip(dp2, dp3))
		[(0, 10, 20), (1, 11, 21), (2, 12, 22), (3, 13, 23), (4, 14, 24)]

<END>

<START>
import random

from torch.utils.data.datapipes._decorator import functional_datapipe
from torch.utils.data.datapipes.datapipe import DFIterDataPipe, IterDataPipe

from torch.utils.data.datapipes.dataframe import dataframe_wrapper as df_wrapper

__all__ = [
	"ConcatDataFramesPipe",
	"DataFramesAsTuplesPipe",
	"ExampleAggregateAsDataFrames",
	"FilterDataFramesPipe",
	"PerRowDataFramesPipe",
	"ShuffleDataFramesPipe",
]


@functional_datapipe('_dataframes_as_tuples')
class DataFramesAsTuplesPipe(IterDataPipe):
	def __init__(self, source_datapipe):
		self.source_datapipe = source_datapipe

	def __iter__(self):
		for df in self.source_datapipe:
			yield from df_wrapper.iterate(df)


@functional_datapipe('_dataframes_per_row', enable_df_api_tracing=True)
class PerRowDataFramesPipe(DFIterDataPipe):
	def __init__(self, source_datapipe):
		self.source_datapipe = source_datapipe

	def __iter__(self):
		for df in self.source_datapipe:
			for i in range(len(df)):
				yield df[i:i + 1]


@functional_datapipe('_dataframes_concat', enable_df_api_tracing=True)
class ConcatDataFramesPipe(DFIterDataPipe):
	def __init__(self, source_datapipe, batch=3):
		self.source_datapipe = source_datapipe
		self.n_batch = batch

	def __iter__(self):
		buffer = []
		for df in self.source_datapipe:
			buffer.append(df)
			if len(buffer) == self.n_batch:
				yield df_wrapper.concat(buffer)
				buffer = []
		if len(buffer):
			yield df_wrapper.concat(buffer)


@functional_datapipe('_dataframes_shuffle', enable_df_api_tracing=True)
class ShuffleDataFramesPipe(DFIterDataPipe):
	def __init__(self, source_datapipe):
		self.source_datapipe = source_datapipe

	def __iter__(self):
		size = None
		all_buffer = []
		for df in self.source_datapipe:
			if size is None:
				size = df_wrapper.get_len(df)
			for i in range(df_wrapper.get_len(df)):
				all_buffer.append(df_wrapper.get_item(df, i))
		random.shuffle(all_buffer)
		buffer = []
		for df in all_buffer:
			buffer.append(df)
			if len(buffer) == size:
				yield df_wrapper.concat(buffer)
				buffer = []
		if len(buffer):
			yield df_wrapper.concat(buffer)


@functional_datapipe('_dataframes_filter', enable_df_api_tracing=True)
class FilterDataFramesPipe(DFIterDataPipe):
	def __init__(self, source_datapipe, filter_fn):
		self.source_datapipe = source_datapipe
		self.filter_fn = filter_fn

	def __iter__(self):
		size = None
		all_buffer = []
		filter_res = []
		for df in self.source_datapipe:
			if size is None:
				size = len(df.index)
			for i in range(len(df.index)):
				all_buffer.append(df[i:i + 1])
				filter_res.append(self.filter_fn(df.iloc[i]))

		buffer = []
		for df, res in zip(all_buffer, filter_res):
			if res:
				buffer.append(df)
				if len(buffer) == size:
					yield df_wrapper.concat(buffer)
					buffer = []
		if len(buffer):
			yield df_wrapper.concat(buffer)


@functional_datapipe('_to_dataframes_pipe', enable_df_api_tracing=True)
class ExampleAggregateAsDataFrames(DFIterDataPipe):
	def __init__(self, source_datapipe, dataframe_size=10, columns=None):
		self.source_datapipe = source_datapipe
		self.columns = columns
		self.dataframe_size = dataframe_size

	def _as_list(self, item):
		try:
			return list(item)
		except Exception:  # TODO(VitalyFedyunin): Replace with better iterable exception
			return [item]

	def __iter__(self):
		aggregate = []
		for item in self.source_datapipe:
			aggregate.append(self._as_list(item))
			if len(aggregate) == self.dataframe_size:
				yield df_wrapper.create_dataframe(aggregate, columns=self.columns)
				aggregate = []
		if len(aggregate) > 0:
			yield df_wrapper.create_dataframe(aggregate, columns=self.columns)

<END>

<START>
from typing import List

import torch

from torch._export.db.case import export_case


@export_case(
	example_inputs=([torch.ones(3, 2), torch.tensor(4), torch.tensor(5)],),
	tags={"python.control-flow", "python.data-structure"},
)
def list_unpack(args: List[torch.Tensor]):
	x, *y = args
	return x + y[0]

<END>

<START>
from abc import ABC, abstractmethod
from typing import Optional, Union, Tuple
from functools import partial

import torch.nn as nn
from torch.distributed._tensor import DeviceMesh, DTensor, Placement, Replicate, Shard, distribute_tensor, distribute_module


__all__ = [
	"ParallelStyle",
	"RowwiseParallel",
	"ColwiseParallel",
	"PrepareModuleInput",
	"PrepareModuleOutput",
]


class ParallelStyle(ABC):

	@abstractmethod
	def _apply(self, module: nn.Module, device_mesh: DeviceMesh) -> nn.Module:
		...


class ColwiseParallel(ParallelStyle):

	def __init__(
		self,
		*,
		input_layouts: Optional[Placement] = None,
		output_layouts: Optional[Placement] = None,
		use_local_output: bool = True
	):
		super().__init__()
		self.input_layouts = (input_layouts or Replicate(), )
		self.output_layouts = (output_layouts or Shard(-1), )
		self.desired_input_layouts = (Replicate(), )
		self.use_local_output = use_local_output

	@staticmethod
	def _prepare_input_fn(input_layouts, desired_input_layouts, inputs, device_mesh):

		input_tensor = inputs[0]
		if not isinstance(input_tensor, DTensor):
			input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)

		if input_layouts != desired_input_layouts:
			input_tensor = input_tensor.redistribute(placements=desired_input_layouts)
		return input_tensor

	def _partition_fn(self, name, module, device_mesh):
		if isinstance(module, nn.Linear):
			for name, param in module.named_parameters():
				dist_param = nn.Parameter(
					distribute_tensor(param, device_mesh, [Shard(0)])
				)
				module.register_parameter(name, dist_param)
		elif isinstance(module, nn.Embedding):
			for name, param in module.named_parameters():
				dist_param = nn.Parameter(
					distribute_tensor(param, device_mesh, [Shard(1)])
				)
				module.register_parameter(name, dist_param)
		else:
			raise NotImplementedError(
				"ColwiseParallel only supports nn.Linear"
				f"and nn.Embedding for now, but found {type(module)}!"
			)

	@staticmethod
	def _prepare_output_fn(output_layouts, use_local_output, outputs, device_mesh):
		outputs = outputs.redistribute(placements=output_layouts)
		return outputs.to_local() if use_local_output else outputs

	def _apply(self, module: nn.Module, device_mesh: DeviceMesh) -> nn.Module:
		return distribute_module(
			module,
			device_mesh,
			self._partition_fn,
			partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),
			partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),
		)


class RowwiseParallel(ParallelStyle):

	def __init__(
		self,
		*,
		input_layouts: Optional[Placement] = None,
		output_layouts: Optional[Placement] = None,
		use_local_output: bool = True
	):
		super().__init__()
		self.input_layouts = (input_layouts or Shard(-1), )
		self.output_layouts = (output_layouts or Replicate(), )
		self.desired_input_layouts = (Shard(-1), )
		self.use_local_output = use_local_output

	@staticmethod
	def _prepare_input_fn(input_layouts, desired_input_layouts, inputs, device_mesh):
		input_tensor = inputs[0]
		if not isinstance(input_tensor, DTensor):
			input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)

		if input_layouts != desired_input_layouts:
			input_tensor = input_tensor.redistribute(placements=desired_input_layouts)
		return input_tensor

	def _partition_fn(self, name, module, device_mesh):
		if isinstance(module, nn.Linear):
			module.register_parameter("weight", nn.Parameter(
				distribute_tensor(module.weight, device_mesh, [Shard(1)])
			))
			if module.bias is not None:
				module.register_parameter("bias", nn.Parameter(
					distribute_tensor(module.bias, device_mesh, [Replicate()])
				))
		else:
			raise NotImplementedError("RowwiseParallel currently only support nn.Linear!")

	@staticmethod
	def _prepare_output_fn(output_layouts, use_local_output, outputs, device_mesh):
		outputs = outputs.redistribute(placements=output_layouts)
		return outputs.to_local() if use_local_output else outputs

	def _apply(self, module: nn.Module, device_mesh: DeviceMesh) -> nn.Module:
		return distribute_module(
			module,
			device_mesh,
			self._partition_fn,
			partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),
			partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),
		)


class PrepareModuleInput(ParallelStyle):

	def __init__(
		self,
		*,
		input_layouts: Union[Placement, Tuple[Placement]],
		desired_input_layouts: Union[Placement, Tuple[Placement]],
		use_local_output: bool = False
	):
		self.input_layouts = (input_layouts,) if isinstance(input_layouts, Placement) else input_layouts
		self.desired_input_layouts = \
			(desired_input_layouts,) if isinstance(desired_input_layouts, Placement) else desired_input_layouts
		self.use_local_output = use_local_output
		assert len(self.input_layouts) == len(self.desired_input_layouts), \
			"input_layouts and desired_input_layouts should have same length!"

	def _prepare_input_fn(self, inputs, device_mesh):
		prepared_inputs = []
		if not isinstance(inputs, tuple):
			inputs = (inputs,)
		assert len(inputs) == len(self.input_layouts), \
			"module inputs and input_layouts should have same length!"
		for inp, input_layout, desired_layout in zip(inputs, self.input_layouts, self.desired_input_layouts):
			if input_layout is not None:
				if isinstance(inp, DTensor):
					assert inp.placements[0] == input_layout
					dt_inp = inp
				else:
					dt_inp = DTensor.from_local(inp, device_mesh, (input_layout,), run_check=False)
				if input_layout != desired_layout:
					dt_inp = dt_inp.redistribute(placements=(desired_layout,))
				prepared_inputs.append(dt_inp.to_local() if self.use_local_output else dt_inp)
			else:
				prepared_inputs.append(inp)
		return tuple(prepared_inputs)

	def _apply(self, module: nn.Module, device_mesh: DeviceMesh) -> nn.Module:
		module.register_forward_pre_hook(lambda _, inputs: self._prepare_input_fn(inputs, device_mesh))  # type: ignore[misc, call-arg]
		return module


class PrepareModuleOutput(ParallelStyle):
	def __init__(
		self,
		*,
		output_layouts: Union[Placement, Tuple[Placement]],
		desired_output_layouts: Union[Placement, Tuple[Placement]],
		use_local_output: bool = True
	):
		self.output_layouts = (output_layouts,) if isinstance(output_layouts, Placement) else output_layouts
		self.desired_output_layouts = \
			(desired_output_layouts,) if isinstance(desired_output_layouts, Placement) else desired_output_layouts
		self.use_local_output = use_local_output
		assert len(self.output_layouts) == len(self.desired_output_layouts), \
			"output_layouts and desired_output_layouts should have same length!"

	def _prepare_out_fn(self, outputs, device_mesh):
		prepared_outputs = []
		if not isinstance(outputs, tuple):
			outputs = (outputs,)
		assert len(outputs) == len(self.output_layouts), \
			"module outputs and output_layouts should have same length!"
		for out, out_layout, desired_out_layout in zip(outputs, self.output_layouts, self.desired_output_layouts):
			if out_layout is not None:
				if isinstance(out, DTensor):
					assert out.placements[0] == out_layout
					dt_out = out
				else:
					dt_out = DTensor.from_local(out, device_mesh, (out_layout,), run_check=False)

				if out_layout != desired_out_layout:
					dt_out = dt_out.redistribute(placements=(desired_out_layout,))
				prepared_outputs.append(dt_out.to_local() if self.use_local_output else dt_out)
			else:
				prepared_outputs.append(out)
		if len(prepared_outputs) == 1:
			return prepared_outputs[0]
		else:
			return tuple(prepared_outputs)

	def _apply(self, module: nn.Module, device_mesh: DeviceMesh) -> nn.Module:
		module.register_forward_hook(lambda _, inputs, outputs: self._prepare_out_fn(outputs, device_mesh))  # type: ignore[misc, call-arg]
		return module

<END>

<START>
import torch
import torch.fx
from torch.fx import (
	Node,
	GraphModule,
	Graph,
)

from torch.ao.ns.fx.utils import (
	get_target_type_str,
	get_normalized_nth_input,
)
from torch.ao.ns.fx.ns_types import (
	NSSingleResultValuesType,
	NSResultsType,
)
from torch.ao.ns.fx.graph_passes import _maybe_get_fqn
from torch.ao.quantization import QConfigMapping
from torch.ao.quantization.qconfig import QConfigAny
from torch.ao.quantization.utils import getattr_from_fqn
from torch.ao.quantization.fx.match_utils import _MatchResult
from torch.utils._pytree import tree_map

import collections
import copy
from typing import List, Dict, Set, Tuple, Callable, Any, Optional
import operator

SHADOW_NODE_NAME_PREFIX = 'shadow'
SHADOW_WRAPPER_NODE_NAME_PREFIX = 'shadow_wrapper'

BINARY_FUNCTIONS = {
	torch.add,
	torch.Tensor.add,
	operator.add,
	torch.mul,
	torch.Tensor.mul,
	operator.mul,
}

def _get_attr_name(subgraph_idx, subgraph_candidate_idx):
	return f"{SHADOW_NODE_NAME_PREFIX}_{subgraph_idx}_{subgraph_candidate_idx}"

def _get_attr_wrapper_name(subgraph_idx, subgraph_candidate_idx):
	return f"{SHADOW_WRAPPER_NODE_NAME_PREFIX}_{subgraph_idx}_{subgraph_candidate_idx}"


class OutputProp:
	def __init__(self, mod):
		self.mod = mod
		self.graph = mod.graph
		self.modules = dict(self.mod.named_modules())

	def propagate(self, *args):
		args_iter = iter(args)
		env : Dict[str, Node] = {}

		def load_arg(a):
			return torch.fx.graph.map_arg(a, lambda n: env[n.name])

		def fetch_attr(target : str):
			target_atoms = target.split('.')
			attr_itr = self.mod
			for i, atom in enumerate(target_atoms):
				if not hasattr(attr_itr, atom):
					raise RuntimeError(f"Node referenced nonexistent target {'.'.join(target_atoms[:i])}")
				attr_itr = getattr(attr_itr, atom)
			return attr_itr

		for node in self.graph.nodes:
			if node.op == 'placeholder':
				result = next(args_iter)
			elif node.op == 'get_attr':
				result = fetch_attr(node.target)
			elif node.op == 'call_function':
				result = node.target(*load_arg(node.args), **load_arg(node.kwargs))
			elif node.op == 'call_method':
				self_obj, *args = load_arg(node.args)
				kwargs = load_arg(node.kwargs)
				result = getattr(self_obj, node.target)(*args, **kwargs)
			elif node.op == 'call_module':
				result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))

			if isinstance(result, torch.Tensor):
				node.traced_result = result

			env[node.name] = result

		return None

def _get_dedup_subgraphs(
	matches: Dict[str, _MatchResult]
) -> Dict[str, List[Node]]:
	seen_nodes = set()
	subgraphs_dedup = {}

	matches_items_reversed: List[Tuple[str, _MatchResult]] = []
	for name, cur_match in matches.items():
		matches_items_reversed.insert(0, (name, cur_match))

	for name, cur_match in matches_items_reversed:  # type: ignore[call-overload]
		was_seen = False
		for node_or_tuple in cur_match[1]:


			if isinstance(node_or_tuple, Node):
				if node_or_tuple in seen_nodes:
					was_seen = True
				seen_nodes.add(node_or_tuple)

			else:
				assert isinstance(node_or_tuple, tuple)
				for node in node_or_tuple:
					assert isinstance(node, Node)
					if node in seen_nodes:
						was_seen = True
					seen_nodes.add(node)

		if was_seen:
			continue

		list_of_nodes = []

		if len(cur_match[1]) == 1:
			list_of_nodes = cur_match[1]
		else:
			assert len(cur_match[1]) == 2

			def _order_nodes(node_a, node_b, node_c) -> List[Node]:
				nodes = [node_a, node_b, node_c]
				first_node = None
				mid_node = None
				last_node = None
				for n in nodes:
					prev_n = n.args[0]
					next_n = next(iter(n.users))
					if prev_n not in nodes:
						first_node = n
					elif next_n not in nodes:
						last_node = n
					else:
						mid_node = n
				assert first_node is not None and mid_node is not None and \
					last_node is not None
				assert mid_node.args[0] is first_node
				assert last_node.args[0] is mid_node
				return [last_node, mid_node, first_node]

			if isinstance(cur_match[1][0], Node) and isinstance(cur_match[1][1], Node):
				list_of_nodes = cur_match[1]
			elif isinstance(cur_match[1][0], tuple):
				node_a, node_b = cur_match[1][0]
				node_c = cur_match[1][1]
				list_of_nodes = _order_nodes(node_a, node_b, node_c)
			elif isinstance(cur_match[1][1], tuple):
				node_a, node_b = cur_match[1][1]
				node_c = cur_match[1][0]
				list_of_nodes = _order_nodes(node_a, node_b, node_c)

		list_of_nodes.reverse()
		subgraphs_dedup[name] = list_of_nodes

	return subgraphs_dedup

def _get_logger_for_subgraph(
	model: GraphModule,
	first_node: Node,
	last_node: Node,
	subgraph_idx: int,
	subgraph_candidate_idx: int,
	qconfig_str: str,
	logger_cls: Callable,
	fqn: Optional[str],
) -> torch.nn.Module:
	if fqn is None:
	Input: a model, and a linear subgraph within the model from first_node to
	  last_node.

	Output: a new submodule containing a copy of the subgraph, with the inputs
	  to the first node becoming the inputs to the submodule, and all other
	  nodes in the subgraph being copied.

	Example inputs:

	`model`: a module with graph

	  x0 -> op1 -> x1 -> op2 -> x2
			 |
			arg1

	`first_node`: op1
	`last_node`: op2

	Example output: a new module with graph

	  input1 -> op1_copy -> x1 -> op2_copy -> output1
				   |
				  arg1
	Given a subgraph in `mt` and a subgraph candidate idx, inserts the
	subgraph candidate copy and instruments it with loggers.

	If subgraph_candidate_idx is 0, this is the baseline fp32 subgraph and we just
	add a logger to the end.

	If subgraph_candidate_idx is not 0, we create a copy of the subgraph and
	prepare it with `prepare_fx`.
		logger_mod_orig = _get_logger_for_subgraph(
			mt, first_node, last_node, subgraph_idx, subgraph_candidate_idx,
			qconfig_str, OutputLogger, fqn)

		attr_name = _get_attr_name(subgraph_idx, subgraph_candidate_idx)
		assert not hasattr(mt, attr_name)
		setattr(mt, attr_name, logger_mod_orig)
		with mt.graph.inserting_after(last_node):
			new_node = mt.graph.call_module(attr_name, args=(last_node,), kwargs={})
			last_added_shadow_node_list[0] = new_node

	else:

		node_name_to_qconfig = \
			list_of_node_name_to_qconfig[subgraph_candidate_idx - 1]
		qconfig = node_name_to_qconfig[first_node.name]

		if qconfig is None:
			return

		qconfig_mapping = QConfigMapping().set_global(qconfig)

		orig_mod_copy_wrapped = create_submodule_from_subgraph(
			mt, first_node, last_node)

		if custom_prepare_fn is None:
			orig_mod_copy_wrapped = torch.ao.quantization.quantize_fx.prepare_fx(
				orig_mod_copy_wrapped, qconfig_mapping, example_inputs=example_inputs)
		else:
			if custom_prepare_kwargs is None:
				custom_prepare_kwargs = {}
			for kwarg_name in ["example_inputs", "prepare_custom_config", "qconfig_mapping"]:
				assert kwarg_name not in custom_prepare_kwargs, f"cannot specify {kwarg_name} in custom_prepare_kwargs"
			prepare_kwargs: Dict[str, Any] = {
				"example_inputs": example_inputs,
				"qconfig_mapping": qconfig_mapping
			}
			prepare_kwargs.update(custom_prepare_kwargs)
			orig_mod_copy_wrapped = custom_prepare_fn(
				orig_mod_copy_wrapped,
				**prepare_kwargs)

		attr_name = _get_attr_wrapper_name(subgraph_idx, subgraph_candidate_idx)
		assert not hasattr(mt, attr_name)
		setattr(mt, attr_name, orig_mod_copy_wrapped)

		insert_after_node = last_added_shadow_node_list[0]
		with mt.graph.inserting_after(insert_after_node):


			new_args = []
			for arg in first_node.args:
				if isinstance(arg, Node):
					new_args.append(arg)
				elif isinstance(arg, (list, tuple)) and len(arg) and isinstance(arg[0], Node):
					for inner_arg in arg:
						if isinstance(inner_arg, Node):
							new_args.append(inner_arg)

			new_kwargs = {}
			for name, old_kwarg in first_node.kwargs.items():
				if isinstance(old_kwarg, Node):
					new_kwargs[name] = old_kwarg
				elif isinstance(old_kwarg, (list, tuple)) and len(old_kwarg):
					new_args.extend(old_kwarg)

			new_args = tuple(new_args)  # type: ignore[assignment]

			new_node = mt.graph.call_module(
				attr_name, args=new_args, kwargs=new_kwargs)

		logger_mod_orig = _get_logger_for_subgraph(
			mt, first_node, last_node, subgraph_idx, subgraph_candidate_idx,
			str(qconfig), OutputComparisonLogger, fqn)

		attr_name = _get_attr_name(subgraph_idx, subgraph_candidate_idx)
		assert not hasattr(mt, attr_name)
		setattr(mt, attr_name, logger_mod_orig)
		with mt.graph.inserting_after(new_node):
			logger = mt.graph.call_module(attr_name, args=(new_node, last_node), kwargs={})
			last_added_shadow_node_list[0] = logger

	mt.recompile()

def create_n_transformed_and_logged_copies_of_subgraph(
	mt: GraphModule,
	subgraph_idx: int,
	match_name: str,
	nodes_in_this_subgraph: List[Any],
	qconfig_mappings: List[QConfigMapping],
	list_of_node_name_to_qconfig: List[Dict[str, QConfigAny]],
	custom_prepare_fn: Optional[Callable] = None,
	custom_prepare_kwargs: Optional[Dict[str, Any]] = None,
) -> None:

	if any(
		not isinstance(node, Node)
		for node in nodes_in_this_subgraph
	):
		return

	first_node = nodes_in_this_subgraph[0]
	last_node = nodes_in_this_subgraph[-1]
	prev_node = get_normalized_nth_input(first_node, mt, 0)
	if isinstance(prev_node, list):
		example_inputs = [x.traced_result for x in prev_node]
	elif isinstance(prev_node, tuple):
		example_inputs = (x.traced_result for x in prev_node)  # type: ignore[assignment]
	else:
		if hasattr(prev_node, 'traced_result'):
			example_inputs = (prev_node.traced_result,)  # type: ignore[attr-defined, assignment]
		else:
			print(
				'unable to get example input for node ' +
				f'{first_node.format_node()}, skipping')
			return

	found_at_least_one_qconfig = False
	for subgraph_candidate_idx in range(len(qconfig_mappings) + 1):

		if subgraph_candidate_idx == 0:
			continue

		node_name_to_qconfig = \
			list_of_node_name_to_qconfig[subgraph_candidate_idx - 1]
		qconfig = node_name_to_qconfig[first_node.name]
		if qconfig is not None:
			found_at_least_one_qconfig = True
			break
	if not found_at_least_one_qconfig:
		print('unable to find at least one qconfig for node ' +
			  f'{first_node.format_node()}, skipping')
		return

	fqn = _maybe_get_fqn(first_node, mt)

	last_added_shadow_node_list: List[Optional[Node]] = [None]
	for subgraph_candidate_idx in range(len(qconfig_mappings) + 1):

		create_one_transformed_and_logged_copy_of_subgraph(
			mt, subgraph_idx, subgraph_candidate_idx, first_node,
			last_node, fqn, list_of_node_name_to_qconfig,
			example_inputs, last_added_shadow_node_list, custom_prepare_fn,
			custom_prepare_kwargs)

def create_add_loggers_graph(
	model: GraphModule,
	subgraphs_dedup: Dict[str, List[Node]],
	qconfig_mapping: QConfigMapping,
	node_name_to_qconfig: Dict[str, QConfigAny],
) -> None:
	from torch.ao.ns._numeric_suite_fx import OutputLogger, OutputComparisonLogger

	def _get_subgraph_containing_node(node, subgraphs_dedup):
		for subgraph in subgraphs_dedup.values():
			if node in subgraph:
				return subgraph
		return None



	nodes_to_skip = set()
	orig_first_node_to_shadow_in_node = {}
	orig_first_node_to_shadow_out_node = {}
	orig_nodes = list(model.graph.nodes)  # type: ignore[union-attr, arg-type]
	cur_subgraph_idx = 0
	for n in orig_nodes:
		if n.op in ('placeholder', 'get_attr', 'output') or n in nodes_to_skip:
			continue

		maybe_subgraph = _get_subgraph_containing_node(n, subgraphs_dedup)
		insert_submodule_copy = False
		if maybe_subgraph is not None:
			first_node, last_node = maybe_subgraph[0], maybe_subgraph[-1]
			for node_to_skip in maybe_subgraph:
				nodes_to_skip.add(node_to_skip)
			qconfig = node_name_to_qconfig[first_node.name]
			if qconfig is not None:
				insert_submodule_copy = True
		else:
			first_node, last_node = n, n

		if insert_submodule_copy:
			match_name = first_node.name
			create_n_transformed_and_logged_copies_of_subgraph(
				model, cur_subgraph_idx, match_name, maybe_subgraph,
				[qconfig_mapping], [node_name_to_qconfig],
				None, None  # type: ignore[arg-type]
			)
			expected_shadow_target = f"shadow_wrapper_{cur_subgraph_idx}_1"
			new_shadow_mod = None
			for maybe_shadow_mod in model.graph.nodes:
				if maybe_shadow_mod.op == 'call_module' and \
						maybe_shadow_mod.target == expected_shadow_target:
					new_shadow_mod = maybe_shadow_mod
					break
			assert new_shadow_mod is not None
			orig_first_node_to_shadow_in_node[first_node] = new_shadow_mod
			orig_first_node_to_shadow_out_node[first_node] = new_shadow_mod

		else:
			subgraph_to_use = maybe_subgraph if maybe_subgraph is not None \
				else [first_node]

			If unshadowed `node` has a shadow version, return that. If not,
			return `node`.
			'comparisons': [comparison],
			'comparison_fn_name': 'sqnr',
		}
		result_q = {
			'res_type': NSSingleResultValuesType.WEIGHT.value,
			'values': [w_obj_q],
			'prev_node_name': prev_node_name,
			'prev_node_target_type': prev_node_type,
			'ref_node_name': ref_node_name,
			'ref_node_target_type': ref_node_type,
			'index_within_arg': 0,
			'index_of_arg': 0,
			'fqn': fqn,
	Creates a comparison of results

	Input:

	{
	  'model': {
		'node_output': {
		  'subgraph_0_0': [
			'values': [torch.tensor(...), ...], ...
			'ref_node_name': ...,
			'ref_node_target_type': ...,
			'qconfig_str': ...,
			'comparisons': [], ...
	subgraph_name_to_subgraph_results: Any = collections.defaultdict(dict)

	key_to_use = next(iter(results['model'].keys()))

	for subgraph_name_with_idx, subgraph_candidate_results in \
			results['model'][key_to_use].items():

		subgraph_str, subgraph_idx, subgraph_candidate_idx = \
			subgraph_name_with_idx.split('_')
		subgraph_name = f'{subgraph_str}_{subgraph_idx}'

		subgraph_results = {
			'ref_node_name': subgraph_candidate_results[0]['ref_node_name'],
			'ref_node_target_type': subgraph_candidate_results[0]['ref_node_target_type'],
			'fqn': subgraph_candidate_results[0]['fqn'],
			'values': subgraph_candidate_results[0]['values'],
			'qconfig_str': subgraph_candidate_results[0]['qconfig_str'],
			'comparisons': subgraph_candidate_results[0]['comparisons'],
			'comparison_fn_name': subgraph_candidate_results[0]['comparison_fn_name'],
		}

		subgraph_name_to_subgraph_results[subgraph_name][subgraph_candidate_idx] = \
			subgraph_results

	return dict(subgraph_name_to_subgraph_results)

def create_results_comparison(
	results_grouped,
) -> Any:
		  'comparisons': [],

	results_comparison = {}

	for subgraph_name, subgraph_results in results_grouped.items():

		candidates = {}
		for subgraph_inner_name, subgraph_inner_result in subgraph_results.items():
			if subgraph_inner_name == '0':
				continue

			cmp_raw = subgraph_inner_result['comparisons']
			cmp_raw_tensor = torch.stack(cmp_raw)

			candidates[subgraph_inner_name] = {
				'qconfig_str': subgraph_inner_result['qconfig_str'],
				'comparison_fn_name': subgraph_inner_result['comparison_fn_name'],
				'cmp_raw': cmp_raw_tensor,
				'cmp_mean': torch.mean(cmp_raw_tensor),
			}

		results_comparison[subgraph_name] = {
			'ref_node_name': subgraph_results['0']['ref_node_name'],
			'ref_node_target_type': subgraph_results['0']['ref_node_target_type'],
			'fqn': subgraph_results['0']['fqn'],
			'candidates': candidates,
		}

	return results_comparison

def print_n_shadows_summary(
	results_comparison,
) -> None:

	try:
		from tabulate import tabulate
	except ImportError:
		print("`print_tabular` relies on the library `tabulate`, "
			  "which could not be found on this machine. Run `pip "
			  "install tabulate` to install the library.")
		return

	results = []
	for subgraph_data in results_comparison.values():
		mean_all_candidates = [
			candidate['cmp_mean']
			for candidate_name, candidate in subgraph_data['candidates'].items()
		]

		data_row = [
			subgraph_data['ref_node_name'],
			subgraph_data['ref_node_target_type'],
			subgraph_data['fqn'],
			*mean_all_candidates,
		]
		results.append(data_row)

	max_candidate_idx_len = -1
	for data_row in results:
		max_candidate_idx_len = max(max_candidate_idx_len, len(data_row[1]))
	candidate_idx_headers = [str(x) for x in range(max_candidate_idx_len)]

	headers = ['node_name', 'node_type', 'fqn', *candidate_idx_headers]
	print(tabulate(results, headers=headers))

<END>

<START>
import warnings
from collections import defaultdict
from typing import Any, Callable, DefaultDict, Iterator, List, Optional, Sized, TypeVar

import torch.utils.data.datapipes.iter.sharding

from torch.utils.data.datapipes._decorator import functional_datapipe
from torch.utils.data.datapipes.datapipe import DataChunk, IterDataPipe
from torch.utils.data.datapipes.utils.common import _check_unpickable_fn

__all__ = [
	"BatcherIterDataPipe",
	"GrouperIterDataPipe",
	"UnBatcherIterDataPipe",
]

T_co = TypeVar("T_co", covariant=True)

def __getattr__(name: str):
	if name in ["SHARDING_PRIORITIES", "ShardingFilterIterDataPipe"]:
		warnings.warn(f"`{name}` from `torch.utils.data.datapipes.iter.grouping` is going to be removed in PyTorch 2.1"
					  f"Please use `{name}` from the `torch.utils.data.datapipes.iter.sharding`",
					  category=FutureWarning, stacklevel=2)

		return getattr(torch.utils.data.datapipes.iter.sharding, name)

	raise AttributeError(f"module {__name__} has no attribute {name}")

@functional_datapipe('batch')
class BatcherIterDataPipe(IterDataPipe[DataChunk]):

	datapipe: IterDataPipe
	batch_size: int
	drop_last: bool

	def __init__(self,
				 datapipe: IterDataPipe,
				 batch_size: int,
				 drop_last: bool = False,
				 wrapper_class=DataChunk,
				 ) -> None:
		assert batch_size > 0, "Batch size is required to be larger than 0!"
		super().__init__()
		self.datapipe = datapipe
		self.batch_size = batch_size
		self.drop_last = drop_last
		self.wrapper_class = wrapper_class

	def __iter__(self) -> Iterator[DataChunk]:
		batch: List = []
		for x in self.datapipe:
			batch.append(x)
			if len(batch) == self.batch_size:
				yield self.wrapper_class(batch)
				batch = []
		if len(batch) > 0:
			if not self.drop_last:
				yield self.wrapper_class(batch)

	def __len__(self) -> int:
		if isinstance(self.datapipe, Sized):
			if self.drop_last:
				return len(self.datapipe) // self.batch_size
			else:
				return (len(self.datapipe) + self.batch_size - 1) // self.batch_size
		else:
			raise TypeError(f"{type(self).__name__} instance doesn't have valid length")


@functional_datapipe('unbatch')
class UnBatcherIterDataPipe(IterDataPipe):

	def __init__(self,
				 datapipe: IterDataPipe,
				 unbatch_level: int = 1):
		self.datapipe = datapipe
		self.unbatch_level = unbatch_level

	def __iter__(self):
		for element in self.datapipe:
			yield from self._dive(element, unbatch_level=self.unbatch_level)

	def _dive(self, element, unbatch_level):
		if unbatch_level < -1:
			raise ValueError("unbatch_level must be -1 or >= 0")
		if unbatch_level == -1:
			if isinstance(element, (list, DataChunk)):
				for item in element:
					yield from self._dive(item, unbatch_level=-1)
			else:
				yield element
		elif unbatch_level == 0:
			yield element
		else:
			if isinstance(element, (list, DataChunk)):
				for item in element:
					yield from self._dive(item, unbatch_level=unbatch_level - 1)
			else:
				raise IndexError(f"unbatch_level {self.unbatch_level} exceeds the depth of the DataPipe")


@functional_datapipe('groupby')
class GrouperIterDataPipe(IterDataPipe[DataChunk]):

	def __init__(self,
				 datapipe: IterDataPipe[T_co],
				 group_key_fn: Callable[[T_co], Any],
				 *,
				 keep_key: bool = False,
				 buffer_size: int = 10000,
				 group_size: Optional[int] = None,
				 guaranteed_group_size: Optional[int] = None,
				 drop_remaining: bool = False):
		_check_unpickable_fn(group_key_fn)
		self.datapipe = datapipe
		self.group_key_fn = group_key_fn

		self.keep_key = keep_key
		self.max_buffer_size = buffer_size
		self.buffer_elements: DefaultDict[Any, List] = defaultdict(list)
		self.curr_buffer_size = 0
		self.group_size = group_size
		self.guaranteed_group_size = None
		if group_size is not None and buffer_size is not None:
			assert 0 < group_size <= buffer_size
			self.guaranteed_group_size = group_size
		if guaranteed_group_size is not None:
			assert group_size is not None and 0 < guaranteed_group_size <= group_size
			self.guaranteed_group_size = guaranteed_group_size
		self.drop_remaining = drop_remaining
		self.wrapper_class = DataChunk

	def _remove_biggest_key(self):
		biggest_key = None
		biggest_size = 0
		result_to_yield = None
		for findkey in self.buffer_elements.keys():
			if len(self.buffer_elements[findkey]) > biggest_size:
				biggest_size = len(self.buffer_elements[findkey])
				biggest_key = findkey

		if self.guaranteed_group_size is not None and biggest_size < self.guaranteed_group_size and not self.drop_remaining:
			raise RuntimeError('Failed to group items', str(self.buffer_elements[biggest_key]))

		if self.guaranteed_group_size is None or biggest_size >= self.guaranteed_group_size:
			result_to_yield = self.buffer_elements[biggest_key]

		self.curr_buffer_size -= biggest_size
		del self.buffer_elements[biggest_key]

		return result_to_yield

	def __iter__(self):
		for x in self.datapipe:
			key = self.group_key_fn(x)

			self.buffer_elements[key].append(x)
			self.curr_buffer_size += 1

			if self.group_size is not None and self.group_size == len(self.buffer_elements[key]):
				result: DataChunk[Any] = self.wrapper_class(self.buffer_elements[key])
				yield (key, result) if self.keep_key else result
				self.curr_buffer_size -= len(self.buffer_elements[key])
				del self.buffer_elements[key]

			if self.curr_buffer_size == self.max_buffer_size:
				result_to_yield = self._remove_biggest_key()
				if result_to_yield is not None:
					result = self.wrapper_class(result_to_yield)
					yield (key, result) if self.keep_key else result

		for key in tuple(self.buffer_elements.keys()):
			result = self.wrapper_class(self.buffer_elements.pop(key))
			self.curr_buffer_size -= len(result)
			yield (key, result) if self.keep_key else result

	def reset(self) -> None:
		self.curr_buffer_size = 0
		self.buffer_elements = defaultdict(list)

	def __getstate__(self):
		state = (
			self.datapipe,
			self.group_key_fn,
			self.keep_key,
			self.max_buffer_size,
			self.group_size,
			self.guaranteed_group_size,
			self.drop_remaining,
			self.wrapper_class,
			self._valid_iterator_id,
			self._number_of_samples_yielded,
		)
		if IterDataPipe.getstate_hook is not None:
			return IterDataPipe.getstate_hook(state)
		return state

	def __setstate__(self, state):
		(
			self.datapipe,
			self.group_key_fn,
			self.keep_key,
			self.max_buffer_size,
			self.group_size,
			self.guaranteed_group_size,
			self.drop_remaining,
			self.wrapper_class,
			self._valid_iterator_id,
			self._number_of_samples_yielded,
		) = state
		self.curr_buffer_size = 0
		self.buffer_elements = defaultdict(list)

	def __del__(self):
		self.buffer_elements.clear()

<END>

<START>



import json
import os
import shutil
import signal
import socket
from string import Template
import tempfile
import uuid
from typing import Any, Dict, Optional, Tuple

import torch.distributed.elastic.timer as timer
from torch.distributed.elastic import events

from torch.distributed.elastic.agent.server.api import (
	RunResult,
	SimpleElasticAgent,
	WorkerGroup,
	WorkerSpec,
	WorkerState,
)
from torch.distributed.elastic.events.api import EventMetadataValue
from torch.distributed.elastic.metrics.api import prof
from torch.distributed.elastic.multiprocessing import PContext, start_processes
from torch.distributed.elastic.utils import macros
from torch.distributed.elastic.utils.logging import get_logger

log = get_logger(__name__)

__all__ = [
	"LocalElasticAgent",
	"TORCHELASTIC_ENABLE_FILE_TIMER",
	"TORCHELASTIC_TIMER_FILE",
]

TORCHELASTIC_ENABLE_FILE_TIMER = "TORCHELASTIC_ENABLE_FILE_TIMER"
TORCHELASTIC_TIMER_FILE = "TORCHELASTIC_TIMER_FILE"

class LocalElasticAgent(SimpleElasticAgent):

	def __init__(
		self,
		spec: WorkerSpec,
		start_method="spawn",
		exit_barrier_timeout: float = 300,
		log_dir: Optional[str] = None,
		log_line_prefix_template: Optional[str] = None,
	):
		super().__init__(spec, exit_barrier_timeout)
		self._start_method = start_method
		self._pcontext: Optional[PContext] = None
		rdzv_run_id = spec.rdzv_handler.get_run_id()
		self._log_dir = self._make_log_dir(log_dir, rdzv_run_id)
		self._log_line_prefix_template = log_line_prefix_template
		self._worker_watchdog: Optional[timer.FileTimerServer] = None

	def _make_log_dir(self, log_dir: Optional[str], rdzv_run_id: str):
		base_log_dir = log_dir or tempfile.mkdtemp(prefix="torchelastic_")
		os.makedirs(base_log_dir, exist_ok=True)
		dir = tempfile.mkdtemp(prefix=f"{rdzv_run_id}_", dir=base_log_dir)
		log.info("log directory set to: %s", dir)
		return dir

	def _setup_local_watchdog(self, envs: Dict[int, Dict[str, str]]) -> None:
		enable_watchdog_env_name = TORCHELASTIC_ENABLE_FILE_TIMER
		watchdog_enabled = os.getenv(enable_watchdog_env_name)
		watchdog_file_env_name = TORCHELASTIC_TIMER_FILE
		watchdog_file_path = os.getenv(watchdog_file_env_name)
		if watchdog_enabled is not None and str(watchdog_enabled) == "1":
			if watchdog_file_path is None:
				watchdog_file_path = "/tmp/watchdog_timer_" + str(uuid.uuid4())
			log.info("Starting a FileTimerServer with %s ...", watchdog_file_path)
			self._worker_watchdog = timer.FileTimerServer(
				file_path=watchdog_file_path,
				max_interval=0.1,
				daemon=True,
				log_event=self._log_watchdog_event)
			self._worker_watchdog.start()
			log.info("FileTimerServer started")
		else:
			log.info("Environment variable '%s' not found. Do not start FileTimerServer.", enable_watchdog_env_name)
		if watchdog_file_path is not None:
			for worker_env in envs.values():
				worker_env[watchdog_file_env_name] = watchdog_file_path


	def _get_fq_hostname(self) -> str:
		return socket.getfqdn(socket.gethostname())

	def _log_watchdog_event(
		self,
		name: str,
		request: Optional[timer.FileTimerRequest],
	) -> None:
		wg = self._worker_group
		spec = wg.spec
		md = {
			"watchdog_event": name
		}
		if request is not None:
			md["worker_pid"] = str(request.worker_pid)
			md["scope_id"] = request.scope_id
			md["expiration_time"] = str(request.expiration_time)
			md["signal"] = str(request.signal)
		md_str = json.dumps(md)
		state = "RUNNING"
		metadata: Dict[str, EventMetadataValue] = {
			"run_id": spec.rdzv_handler.get_run_id(),
			"global_rank": None,
			"group_rank": wg.group_rank,
			"worker_id": None,
			"role": spec.role,
			"hostname": self._get_fq_hostname(),
			"state": state,
			"total_run_time": self._total_execution_time,
			"rdzv_backend": spec.rdzv_handler.get_backend(),
			"raw_error": None,
			"metadata": md_str,
			"agent_restarts": spec.max_restarts - self._remaining_restarts,
		}
		event = events.Event(
			name=name, source=events.EventSource.AGENT, metadata=metadata
		)
		events.record(event)

	@prof
	def _stop_workers(self, worker_group: WorkerGroup) -> None:
		self._shutdown()

	@prof
	def _start_workers(self, worker_group: WorkerGroup) -> Dict[int, Any]:
		spec = worker_group.spec
		store = worker_group.store
		assert store is not None
		master_addr, master_port = super()._get_master_addr_port(store)
		restart_count = spec.max_restarts - self._remaining_restarts

		use_agent_store = spec.rdzv_handler.get_backend() == "static"

		args: Dict[int, Tuple] = {}
		envs: Dict[int, Dict[str, str]] = {}
		log_line_prefixes: Optional[Dict[int, str]] = {} if self._log_line_prefix_template else None
		for worker in worker_group.workers:
			local_rank = worker.local_rank
			worker_env = {
				"LOCAL_RANK": str(local_rank),
				"RANK": str(worker.global_rank),
				"GROUP_RANK": str(worker_group.group_rank),
				"ROLE_RANK": str(worker.role_rank),
				"ROLE_NAME": spec.role,
				"LOCAL_WORLD_SIZE": str(spec.local_world_size),
				"WORLD_SIZE": str(worker.world_size),
				"GROUP_WORLD_SIZE": str(worker_group.group_world_size),
				"ROLE_WORLD_SIZE": str(worker.role_world_size),
				"MASTER_ADDR": master_addr,
				"MASTER_PORT": str(master_port),
				"TORCHELASTIC_RESTART_COUNT": str(restart_count),
				"TORCHELASTIC_MAX_RESTARTS": str(spec.max_restarts),
				"TORCHELASTIC_RUN_ID": spec.rdzv_handler.get_run_id(),
				"TORCHELASTIC_USE_AGENT_STORE": str(use_agent_store),
				"TORCH_NCCL_ASYNC_ERROR_HANDLING": os.getenv(
					"TORCH_NCCL_ASYNC_ERROR_HANDLING", str(1)
				),
			}
			if "OMP_NUM_THREADS" in os.environ:
				worker_env["OMP_NUM_THREADS"] = os.environ["OMP_NUM_THREADS"]


			if self._log_line_prefix_template:
				log_line_prefix = Template(self._log_line_prefix_template).safe_substitute(
					role_name=spec.role,
					rank=worker.global_rank,
					local_rank=local_rank,)
				log_line_prefixes[local_rank] = log_line_prefix

			envs[local_rank] = worker_env
			worker_args = list(spec.args)
			worker_args = macros.substitute(worker_args, str(local_rank))
			args[local_rank] = tuple(worker_args)

		attempt_log_dir = os.path.join(self._log_dir, f"attempt_{restart_count}")
		shutil.rmtree(attempt_log_dir, ignore_errors=True)
		os.makedirs(attempt_log_dir)

		self._setup_local_watchdog(envs=envs)

		assert spec.entrypoint is not None
		self._pcontext = start_processes(
			name=spec.role,
			entrypoint=spec.entrypoint,
			args=args,
			envs=envs,
			log_dir=attempt_log_dir,
			log_line_prefixes=log_line_prefixes,
			start_method=self._start_method,
			redirects=spec.redirects,
			tee=spec.tee,
		)

		return self._pcontext.pids()

	def _shutdown(self, death_sig: signal.Signals = signal.SIGTERM) -> None:
		if self._worker_watchdog is not None:
			self._worker_watchdog.stop()
			self._worker_watchdog = None
		if self._pcontext:
			self._pcontext.close(death_sig)

	@prof
	def _monitor_workers(self, worker_group: WorkerGroup) -> RunResult:
		role = worker_group.spec.role
		worker_pids = {w.id for w in worker_group.workers}
		assert self._pcontext is not None
		pc_pids = set(self._pcontext.pids().values())
		if worker_pids != pc_pids:
			log.error(
				"[%s] worker pids do not match process_context pids."
				" Expected: %s, actual: %s",
				role, worker_pids, pc_pids
			)
			return RunResult(state=WorkerState.UNKNOWN)

		result = self._pcontext.wait(0)
		if result:
			if result.is_failed():
				worker_failures = {}
				for local_rank, failure in result.failures.items():
					worker = worker_group.workers[local_rank]
					worker_failures[worker.global_rank] = failure
				return RunResult(
					state=WorkerState.FAILED,
					failures=worker_failures,
				)
			else:
				workers_ret_vals = {}
				for local_rank, ret_val in result.return_values.items():
					worker = worker_group.workers[local_rank]
					workers_ret_vals[worker.global_rank] = ret_val
				return RunResult(
					state=WorkerState.SUCCEEDED,
					return_values=workers_ret_vals,
				)
		else:
			return RunResult(state=WorkerState.HEALTHY)

<END>

<START>
import logging
import warnings
import weakref
from typing import cast, List, Optional

import torch
import torch.distributed as dist
import torch.distributed.distributed_c10d as c10d


logger = logging.getLogger(__name__)

data_ptr_to_work = dict()
work_version = 0


class _WaitRegistration:
	def __init__(self, work):
		global work_version
		self.work = work
		self.version = work_version
		self.ptrs = set()
		self.ptr_alias_count = {}
		self.cleanup_count = 0
		work_version += 1

	def _register_tensor_ptr(self, data_ptr):
		global data_ptr_to_work
		data_ptr_to_work[data_ptr] = self
		self.ptrs.add(data_ptr)

	def _record_wrapper(self, ptr):
		self._register_tensor_ptr(ptr)
		self.ptr_alias_count.setdefault(ptr, 0)
		self.ptr_alias_count[ptr] += 1
		self.cleanup_count += 1

	def wait(self):
		if self.work is not None:
			self.work.wait()
			self.work = None
		self.cleanup()

	def decrement_live_tensor(self, ptr):
		self.cleanup_count -= 1
		if self.cleanup_count == 0:
			self.wait()
		else:
			self.ptr_alias_count[ptr] -= 1
			if (
				self.ptr_alias_count[ptr] < 1
				and data_ptr_to_work.get(ptr, None) == self
			):
				del data_ptr_to_work[ptr]

	def cleanup(self):
		for ptr in self.ptrs:
			if data_ptr_to_work.get(ptr, None) == self:
				del data_ptr_to_work[ptr]


def _register_tensor_work(tensor_or_list, work_or_list):
	if not isinstance(tensor_or_list, list):
		tensor_or_list = [tensor_or_list]
	if not isinstance(work_or_list, list):
		reg = _WaitRegistration(work_or_list)
		for tensor in tensor_or_list:
			reg._register_tensor_ptr(tensor.data_ptr())
	else:
		for tensor, work in zip(tensor_or_list, work_or_list):
			reg = _WaitRegistration(work)
			reg._register_tensor_ptr(tensor.data_ptr())


def _wait_reg_dec(ptr, wait_reg):
	wait_reg.decrement_live_tensor(ptr)


def _register_tensor_wrapper(tensor) -> None:
	global data_ptr_to_work
	data_ptr = tensor.elem.data_ptr()
	wait_reg = data_ptr_to_work.get(data_ptr, None)
	if wait_reg is None:
		warnings.warn(
			"Trying to register finalizer to AsyncCollectiveTensor but the inner tensor is already gone"
		)
	else:
		wait_reg._record_wrapper(data_ptr)
		weakref.finalize(tensor, _wait_reg_dec, data_ptr, wait_reg)


def _wait_tensor(tensor: torch.Tensor) -> torch.Tensor:
	global data_ptr_to_work
	data_ptr = tensor.data_ptr()
	wait_reg = data_ptr_to_work.get(data_ptr)
	if wait_reg is not None:
		wait_reg.wait()
	return tensor


def _tensor_needs_wait(tensor: torch.Tensor) -> bool:
	return len(data_ptr_to_work)


def _wait_all() -> None:
Kernel implementations (for eager runtime only) - should never be traced by torch.compile

These functions should all be bound to dispatcher ops.  During tracing, the op itself should be
captured in the graph and the backend should implement the op however it prefers.

<END>

<START>
from torch.ao.nn.intrinsic.quantized.dynamic import LinearReLU

__all__ = [
	'LinearReLU',
]

<END>

<START>
from torch.utils.data.datapipes._hook_iterator import _SnapshotState
from torch.utils.data.datapipes.datapipe import IterDataPipe
from torch.utils.data.graph_settings import apply_random_seed


def _simple_graph_snapshot_restoration(datapipe: IterDataPipe, n_iterations: int, rng=None) -> None:
	if datapipe._snapshot_state == _SnapshotState.Restored:
		raise RuntimeError(
			"Snapshot restoration cannot be applied. You can only restore simple snapshot to the graph "
			"if your graph has not been restored.")

	datapipe.reset()  # This ensures `SnapshotState` is `Iterating` by this point, even if it was `Restored`.
	apply_random_seed(datapipe, rng)

	remainder = n_iterations
	it = iter(datapipe)  # This always reset the DataPipe if it hasn't already.
	while remainder > 0:
		try:
			next(it)
			remainder -= 1
		except StopIteration as e:
			raise RuntimeError(f"Fast-forward {datapipe} by {n_iterations} iterations "
							   "exceeds the number of samples available.") from e
	datapipe._fast_forward_iterator = it

	datapipe._snapshot_state = _SnapshotState.Restored

<END>

<START>
from __future__ import annotations

import copy
from typing import Any, Callable, Dict, List, Union

import torch
from torch.ao.quantization import QConfigMapping
from torch.ao.quantization.qconfig_mapping import _QCONFIG_STYLE_ORDER
from torch.ao.quantization.qconfig import QConfigAny

__all__ = ["QConfigMultiMapping"]

_QCONFIG_STYLE_TO_METHOD: Dict[str, str] = {
	"global_qconfig": "set_global",
	"object_type_qconfigs": "set_object_type",
	"module_name_regex_qconfigs": "set_module_name_regex",
	"module_name_qconfigs": "set_module_name",
	"module_name_object_type_order_qconfigs": "set_module_name_object_type_order",
}

def _remove_duplicates_and_none(qconfig_list: List[QConfigAny]) -> None:
	to_remove = []
	for index, cur_qconfig in enumerate(qconfig_list):
		if cur_qconfig is None:
			to_remove.append(index)
			break
		for checked_qconfig in qconfig_list[:index]:
			if torch.ao.quantization.qconfig_equals(cur_qconfig, checked_qconfig):
				to_remove.append(index)
				break
	for index in to_remove[::-1]:
		qconfig_list.pop(index)

class QConfigMultiMapping:

	def __init__(self):
		self.qconfig_mappings_list: List[QConfigMapping] = [QConfigMapping()]

	def _handle_list_size_mismatch(
		self, qconfig_list: List[QConfigAny], style: str
	) -> None:


		if len(qconfig_list) > len(self.qconfig_mappings_list):


			new_qconfig_mapping = QConfigMapping()
			for qconfig_mapping in self.qconfig_mappings_list:

				for check_style in _QCONFIG_STYLE_ORDER[1:]:
					qconfigs_dict = getattr(qconfig_mapping, check_style)
					target_qconfigs_dict = getattr(new_qconfig_mapping, check_style)
					for key in qconfigs_dict:
						target_qconfigs_dict[key] = None
				break

			while len(qconfig_list) > len(self.qconfig_mappings_list):
				self.qconfig_mappings_list.append(copy.deepcopy(new_qconfig_mapping))
		else:

			while len(qconfig_list) < len(self.qconfig_mappings_list):
				qconfig_list.append(None)

	def _insert_qconfig_list(
		self,
		style: str,
		args: List[Union[str, int, Callable]],
		qconfig_list: List[QConfigAny],
	) -> None:

		_remove_duplicates_and_none(qconfig_list)

		self._handle_list_size_mismatch(qconfig_list, style)
		method_name = _QCONFIG_STYLE_TO_METHOD[style]
		for qconfig_mapping, qconfig in zip(self.qconfig_mappings_list, qconfig_list):
			set_method = getattr(qconfig_mapping, method_name)
			set_method(*args, qconfig)

	def set_global(self, global_qconfig_list: List[QConfigAny]) -> QConfigMultiMapping:
		self._insert_qconfig_list("global_qconfig", [], global_qconfig_list)
		return self

	def set_object_type(
		self, object_type: Union[Callable, str], qconfig_list: List[QConfigAny]
	) -> QConfigMultiMapping:
		self._insert_qconfig_list("object_type_qconfigs", [object_type], qconfig_list)
		return self

	def set_module_name_regex(
		self, module_name_regex: str, qconfig_list: List[QConfigAny]
	) -> QConfigMultiMapping:
		self._insert_qconfig_list(
			"module_name_regex_qconfigs", [module_name_regex], qconfig_list
		)
		return self

	def set_module_name(
		self, module_name: str, qconfig_list: List[QConfigAny]
	) -> QConfigMultiMapping:
		self._insert_qconfig_list("module_name_qconfigs", [module_name], qconfig_list)
		return self

	def set_module_name_object_type_order(
		self,
		module_name: str,
		object_type: Callable,
		index: int,
		qconfig_list: List[QConfigAny],
	) -> QConfigMultiMapping:
		self._insert_qconfig_list(
			"module_name_object_type_order_qconfigs",
			[module_name, object_type, index],
			qconfig_list,
		)
		return self

	def __repr__(self):
		return (
			self.__class__.__name__ +
			" [" +
			"".join(f"\n{qconfig_mapping.__repr__()}," for qconfig_mapping in self.qconfig_mappings_list) +
			"\n]"
		)

	@classmethod
	def from_list_qconfig_mapping(
		cls, qconfig_mapping_list: List[QConfigMapping]
	) -> QConfigMultiMapping:
		new_qconfig_multi_mapping = cls()

		new_qconfig_multi_mapping.qconfig_mappings_list = copy.deepcopy(
			qconfig_mapping_list
		)


		for style in _QCONFIG_STYLE_ORDER[1:]:

			qconfig_dict_list: Dict[Any, List[QConfigAny]] = {}
			for qconfig_mapping in qconfig_mapping_list:
				qconfig_dict = getattr(qconfig_mapping, style)
				for key, qconfig in qconfig_dict.items():
					if key not in qconfig_dict_list:
						qconfig_dict_list[key] = []
					qconfig_dict_list[key].append(qconfig)

			set_method_name = _QCONFIG_STYLE_TO_METHOD[style]
			set_method = getattr(new_qconfig_multi_mapping, set_method_name)
			for key, qconfig_list in qconfig_dict_list.items():
				if isinstance(key, tuple):
					set_method(*key, qconfig_list)
				else:
					set_method(key, qconfig_list)

		return new_qconfig_multi_mapping

<END>

<START>
import torch

from torch._export.db.case import export_case, SupportLevel


@export_case(
	example_inputs=(torch.ones(3, 2),),
	tags={"torch.mutation"},
	support_level=SupportLevel.SUPPORTED,
)
class UserInputMutation(torch.nn.Module):

	def forward(self, x):
		x.mul_(2)
		return x.cos()

<END>

<START>
import os
import time


class FileBaton:
		Create a new :class:`FileBaton`.

		Args:
			lock_file_path: The path to the file used for locking.
			wait_seconds: The seconds to periodically sleep (spin) when
				calling ``wait()``.
		Try to atomically create a file under exclusive access.

		Returns:
			True if the file could be created, else False.
		Periodically sleeps for a certain amount until the baton is released.

		The amount of time slept depends on the ``wait_seconds`` parameter
		passed to the constructor.
		if self.fd is not None:
			os.close(self.fd)

		os.remove(self.lock_file_path)

<END>

<START>
from __future__ import annotations

import base64
import copyreg
import dataclasses
import functools
import hashlib
import importlib
import io
import json
import logging
import multiprocessing
import os
import pathlib
import pickle
import pkgutil
import platform
import re
import shlex
import shutil
import signal
import subprocess
import sys
import sysconfig
import tempfile
import threading
import warnings
import weakref
from bisect import bisect_right
from concurrent.futures import Future, ProcessPoolExecutor, ThreadPoolExecutor
from copy import copy
from ctypes import c_void_p, cdll, CDLL
from dataclasses import field
from functools import partial
from importlib import abc
from pathlib import Path
from threading import Thread
from time import sleep, time
from types import ModuleType
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, TYPE_CHECKING, Union

import torch

from torch._dynamo.device_interface import (
	get_interface_for_device,
	get_registered_device_interfaces,
)
from torch._dynamo.utils import counters
from torch._inductor import config, exc
from torch._inductor.codegen.cuda import cuda_env
from torch._inductor.utils import cache_dir, developer_warning, is_linux
from torch._prims_common import suggest_memory_format
from torch.fx.experimental.symbolic_shapes import has_hint, hint_int, ShapeEnv

if TYPE_CHECKING:
	from torch._inductor.graph import GraphLowering
	from torch._inductor.select_algorithm import ChoiceCaller

from torch.hub import _Faketqdm, tqdm

_HERE = os.path.abspath(__file__)
_TORCH_PATH = os.path.dirname(os.path.dirname(_HERE))

if config.is_fbcode():
	from triton.fb import build_paths
	from triton.fb.build import _run_build_command

	from torch._inductor.fb.utils import (
		log_global_cache_errors,
		log_global_cache_stats,
		log_global_cache_vals,
		use_global_cache,
	)
else:

	def log_global_cache_errors(*args, **kwargs):
		pass

	def log_global_cache_stats(*args, **kwargs):
		pass

	def log_global_cache_vals(*args, **kwargs):
		pass

	def use_global_cache() -> bool:
		return False


LOCK_TIMEOUT = 600

_cumulative_compile_time = 0.0
_t0 = None


def _compile_start() -> None:
	global _t0
	if _t0 is None:
		_t0 = time()


def _compile_end() -> None:
	global _cumulative_compile_time, _t0
	if _t0 is not None:
		t1 = time()
		_cumulative_compile_time += t1 - _t0
		_t0 = None


log = logging.getLogger(__name__)


def cpp_wrapper_cache_dir(name: str) -> str:
	cu_str = (
		"cpu"
		if torch.version.cuda is None
		else f'cu{torch.version.cuda.replace(".", "")}'
	)
	python_version = f"py{sys.version_info.major}{sys.version_info.minor}"
	build_folder = f"{python_version}_{cu_str}"

	cpp_wrapper_dir = os.path.join(cache_dir(), build_folder)
	cpp_wrapper_build_directory = os.path.join(cpp_wrapper_dir, name)
	os.makedirs(cpp_wrapper_build_directory, exist_ok=True)
	return cpp_wrapper_build_directory


def get_cpp_wrapper_cubin_path_name():
	return "cubin_path" if torch.version.hip is None else "hsaco_path"


class CacheBase:
	@staticmethod
	@functools.lru_cache(None)
	def get_system() -> Dict[str, Any]:
		try:
			import triton

			triton_version = triton.__version__
		except ModuleNotFoundError:
			triton_version = None

		try:
			system: Dict[str, Any] = {
				"device": {
					"name": torch.cuda.get_device_properties(
						torch.cuda.current_device()
					).name,
				},
				"version": {
					"cuda": torch.version.cuda,
					"triton": triton_version,
				},
			}
		except (AssertionError, RuntimeError):
			system = {}

		system["hash"] = hashlib.sha256(
			json.dumps(system, sort_keys=True).encode("utf-8")
		).hexdigest()

		return system

	@staticmethod
	@functools.lru_cache(None)
	def get_local_cache_path() -> Path:
		return Path(os.path.join(cache_dir(), "cache", CacheBase.get_system()["hash"]))

	@staticmethod
	@functools.lru_cache(None)
	def get_global_cache_path() -> Optional[Path]:
		return (
			Path(os.path.join(config.global_cache_dir, CacheBase.get_system()["hash"]))
			if config.global_cache_dir is not None
			else None
		)

	def __init__(self) -> None:
		if not torch.cuda.is_available():
			return

		self.system = CacheBase.get_system()

		self.local_cache_path = CacheBase.get_local_cache_path()
		self.global_cache_path = CacheBase.get_global_cache_path()

	def get_local_cache(self) -> Dict[str, Any]:
		if not self.local_cache_path.is_file():
			return {}
		with open(self.local_cache_path) as local_cache_fp:
			local_cache = json.load(local_cache_fp)
		return local_cache["cache"]

	def update_local_cache(self, local_cache: Dict[str, Any]) -> None:
		if not os.path.exists(self.local_cache_path.parent):
			os.makedirs(self.local_cache_path.parent, exist_ok=True)
		write_atomic(
			str(self.local_cache_path),
			json.dumps({"system": self.system, "cache": local_cache}, indent=4),
		)


class LocalCache(CacheBase):
	def lookup(self, *keys: str) -> Optional[Dict[str, Any]]:
		cache = self.get_local_cache()

		sub_cache = cache
		for key in keys:
			if key in cache:
				sub_cache = cache[key]
			else:
				return None

		return sub_cache

	def set_value(self, *keys: str, value: Any) -> None:
		cache = self.get_local_cache()

		sub_cache = cache
		for key in keys[0:-1]:
			sub_cache.setdefault(key, {})
			sub_cache = sub_cache[key]
		sub_cache[keys[-1]] = value

		self.update_local_cache(cache)


class PersistentCache(CacheBase):
	@functools.lru_cache(None)
	def get_global_cache(self):
		if self.global_cache_path is None or not self.global_cache_path.is_file():
			return {}
		with open(self.global_cache_path) as global_cache_fp:
			global_cache = json.load(global_cache_fp)
		return global_cache["cache"]

	def lookup(
		self,
		choices: List[ChoiceCaller],
		op: str,
		inputs: str,
		benchmark: Callable[[Any], Dict[ChoiceCaller, float]],
	) -> Dict[ChoiceCaller, float]:
		precision = torch.get_float32_matmul_precision()

		log_stats = partial(log_global_cache_stats, self.system, op, inputs, precision)
		log_vals = partial(log_global_cache_vals, self.system, op, inputs, precision)
		log_errors = partial(
			log_global_cache_errors, self.system, op, inputs, precision
		)
		timings = {}

		def check_cache(cache, callback=None) -> bool:
	The Tensor metadata relevant when hashing FxGraph cache keys.
	TensorMetadata plus the elements as a list of raw values.
	Used for hashing inlined constants.
	Extract the TensorMetadata of a tensor.
	See FxGraphCachePickler. Custom reducer to pickle FakeTensors.
	See FxGraphCachePickler. Custom reducer to pickle Tensors.
	See FxGraphCachePickler. Custom reducer to pickle SymInts.
	Custom pickler to customize the pickling of some objects (Tensors), only for the
	purpose of computing a hash for keying into the FxGraphCache. Tensors contain
	objects that don't pickle and/or vary between runs, and we want to capture the
	data that allow us to compute a stable, but safe hash.
		Pickle an object using the FxGraphCachePickler.
		Serialize an object using the FxGraphCachePickler and return a hash
		of the pickled object.
	Compute a hash of all inductor code modules. Used by the FxGraph cache
	so any inductor code changes would result in new cache keys.
	See FxGraphHashDetails. Holds a sorted list to support stable hashing
	of set kwargs.
	Object to capture all the details for a compiled FX graph relevant to computing
	a safe and stable cache key.
		Get a printable string describing in more detail all the attributes
		comprising this object. Useful for debugging when one graph hashes
		to a different value than another.
	Generate a unique hash of the FX graph for caching.
	Supports caching and reusing compiled Fx graphs.

	The overall strategy is as follows:
	- This cache stores entries on disk. When saving an entry, we can't
	  serialize callables (that could be C++, Triton, etc.), so we serialize
	  their own disk cache location. We then recreate the compiled artifact
	  after fetching from disk.
	- For indexing the cache, we gather the fields relevant to identifying an
	  FxGraph (the graph module, graph inputs, system settings etc.) into an
	  FxGraphCacheDetails object, pickle it, and compute a hash for the key.
	  See FxGraphCachePickler.
	- Among the metadata we store, we also include a guards expression that's
	  appropriate for validating any symbols for Tensor arguments that have
	  symbolic bounds. On cache lookup then, we evaluate those guards in the
	  current context to validate that a cached entry can be served.
	- A given graph could have multiple compiled versions, corresponding to
	  different sets of guards. Therefore, we store cache entries in the form:
		  <temp dir>/<fx graph hash>/<serialized metatdata>
	- On lookup, we compute the key from the graph details, iterate over all
	  leaf files in the corresponding subdirectory, deserialize the entry, and
	  evaluate its guards expression. If the evaluation succeeds, we have a
	  cache hit. If it fails, we compile the graph and store a new entry.
	- Finally, on a cache hit, we need to make sure any guards that would
	  have been created during compilation are added to the current context.
		Get the toplevel temporary directory for storing compiled graphs.
		Return the disk location for a given cache key.
		Get the SymInt objects from the input list.
		Helper to get the shape env from the tracing context.
		Lookup a compiled graph in the cache by key. On a hit, return the
		deserialized CompiledFxGraph object. On a miss, return None.
		Store a serialized CompiledFxGraph on disk.
		Load a compiled graph from the cache. If a cached entry does not exist,
		compile the graph and save it to the cache.
		Clear out the on-disk cache.
	Class holding a compiled FX graph. This is the object serialized on disk
	to support FxGraph caching.
	prefix = os.path.join(cache_dir(), "gcc")
	cxx_path = os.path.join(prefix, "bin", "g++")
	if not os.path.exists(cxx_path):
		log.info("Downloading GCC via conda")
		conda = os.environ.get("CONDA_EXE", "conda")
		if conda is None:
			conda = shutil.which("conda")
		if conda is not None:
			subprocess.check_call(
				[
					conda,
					"create",
					f"--prefix={prefix}",
					"--channel=conda-forge",
					"--quiet",
					"-y",
					"python=3.8",
					"gxx",
				],
				stdout=subprocess.PIPE,
			)
	return cxx_path


def is_gcc() -> bool:
	return bool(re.search(r"(gcc|g\+\+)", cpp_compiler()))


def is_clang() -> bool:
	return bool(re.search(r"(clang|clang\+\+)", cpp_compiler()))


@functools.lru_cache(None)
def is_apple_clang() -> bool:
	cxx = cpp_compiler()
	version_string = subprocess.check_output([cxx, "--version"]).decode("utf8")
	return "Apple" in version_string.splitlines()[0]


class VecISA:
	_bit_width: int
	_macro: str
	_arch_flags: str
	_dtype_nelements: Dict[torch.dtype, int]



	def bit_width(self) -> int:
		return self._bit_width

	def nelements(self, dtype: torch.dtype = torch.float) -> int:
		return self._dtype_nelements[dtype]

	def build_macro(self) -> str:
		return self._macro

	def build_arch_flags(self) -> str:
		return self._arch_flags

	def __hash__(self) -> int:
		return hash(str(self))

	@functools.lru_cache(None)
	def __bool__(self) -> bool:
		if config.cpp.vec_isa_ok is not None:
			return config.cpp.vec_isa_ok

		if config.is_fbcode():
			return True

		key, input_path = write(VecISA._avx_code, "cpp")
		from filelock import FileLock

		lock_dir = get_lock_dir()
		lock = FileLock(os.path.join(lock_dir, key + ".lock"), timeout=LOCK_TIMEOUT)
		with lock:
			output_path = input_path[:-3] + "so"
			build_cmd = shlex.split(
				cpp_compile_command(
					input_path, output_path, warning_all=False, vec_isa=self
				)
			)
			try:
				compile_file(input_path, output_path, build_cmd)
				subprocess.check_call(
					[
						sys.executable,
						"-c",
						VecISA._avx_py_load.replace("__lib_path__", output_path),
					],
					stderr=subprocess.DEVNULL,
					env={**os.environ, "PYTHONPATH": ":".join(sys.path)},
				)
			except Exception as e:
				return False

			return True


@dataclasses.dataclass
class VecAVX512(VecISA):
	_bit_width = 512
	_macro = "-DCPU_CAPABILITY_AVX512"
	_arch_flags = "-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma"
	_dtype_nelements = {torch.float: 16, torch.bfloat16: 32, torch.float16: 32}

	def __str__(self) -> str:
		return "avx512"

	__hash__: Callable[[VecISA], Any] = VecISA.__hash__


@dataclasses.dataclass
class VecAVX2(VecISA):
	_bit_width = 256
	_macro = "-DCPU_CAPABILITY_AVX2"
	_arch_flags = "-mavx2 -mfma"
	_dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}

	def __str__(self) -> str:
		return "avx2"

	__hash__: Callable[[VecISA], Any] = VecISA.__hash__


@dataclasses.dataclass
class VecZVECTOR(VecISA):
	_bit_width = 256
	_macro = "-DCPU_CAPABILITY_ZVECTOR -DCPU_CAPABILITY=ZVECTOR -DHAVE_ZVECTOR_CPU_DEFINITION"
	_arch_flags = "-mvx -mzvector"
	_dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}

	def __str__(self) -> str:
		return "zvector"

	__hash__: Callable[[VecISA], Any] = VecISA.__hash__


class InvalidVecISA(VecISA):
	_bit_width = 0
	_macro = ""
	_arch_flags = ""
	_dtype_nelements = {}

	def __str__(self) -> str:
		return "INVALID_VEC_ISA"

	def __bool__(self) -> bool:  # type: ignore[override]
		return False

	__hash__: Callable[[VecISA], Any] = VecISA.__hash__


invalid_vec_isa = InvalidVecISA()
supported_vec_isa_list = [VecAVX512(), VecAVX2()]


@functools.lru_cache(None)
def valid_vec_isa_list() -> List[VecISA]:
	if sys.platform != "linux":
		return []

	if platform.machine() == "s390x":
		return [VecZVECTOR()]

	isa_list = []
	with open("/proc/cpuinfo") as _cpu_info:
		_cpu_info_content = _cpu_info.read()
		for isa in supported_vec_isa_list:
			if str(isa) in _cpu_info_content and isa:
				isa_list.append(isa)
		return isa_list


def pick_vec_isa() -> VecISA:
	if config.is_fbcode():
		return VecAVX2()

	_valid_vec_isa_list: List[VecISA] = valid_vec_isa_list()
	if not _valid_vec_isa_list:
		return invalid_vec_isa

	if config.cpp.simdlen is None:
		assert _valid_vec_isa_list
		return _valid_vec_isa_list[0]

	for isa in _valid_vec_isa_list:
		if config.cpp.simdlen == isa.bit_width():
			return isa

	return invalid_vec_isa


def get_compile_only(compile_only: bool = True) -> str:
	return "-c" if compile_only else ""


def get_shared(shared: bool = True) -> str:
	return "-shared -fPIC" if shared else ""


def get_warning_all_flag(warning_all: bool = True) -> str:
	return "-Wall" if warning_all else ""


def get_glibcxx_abi_build_flags() -> str:
	return "-D_GLIBCXX_USE_CXX11_ABI=" + str(int(torch._C._GLIBCXX_USE_CXX11_ABI))


def cpp_flags() -> str:
	flags = ["-std=c++17", "-Wno-unused-variable", "-Wno-unknown-pragmas"]
	if is_clang():
		flags.append("-Werror=ignored-optimization-argument")
	return " ".join(flags)


def cpp_wrapper_flags() -> str:
	return "-DTORCH_INDUCTOR_CPP_WRAPPER"


def optimization_flags() -> str:
	base_flags = "-O0 -g" if config.aot_inductor.debug_compile else "-O3 -DNDEBUG"
	base_flags += " -ffast-math -fno-finite-math-only"
	if not config.cpp.enable_unsafe_math_opt_flag:
		base_flags += " -fno-unsafe-math-optimizations"

	if config.is_fbcode():
		return base_flags

	if sys.platform == "darwin":
		base_flags += " -Xclang"
	else:
		if platform.machine() == "ppc64le":
			base_flags += " -mcpu=native"
		else:
			base_flags += " -march=native"

	if not config.is_fbcode():
		base_flags += " -fopenmp"
	return base_flags


def use_custom_generated_macros() -> str:
	return "-D C10_USING_CUSTOM_GENERATED_MACROS"


def use_fb_internal_macros() -> str:
	if config.is_fbcode():
		openmp_lib = build_paths.openmp_lib()
		preprocessor_flags = " ".join(
			(
				"-D C10_USE_GLOG",
				"-D C10_USE_MINIMAL_GLOG",
				"-D C10_DISABLE_TENSORIMPL_EXTENSIBILITY",
			)
		)
		return f"-Wp,-fopenmp {openmp_lib} {preprocessor_flags}"
	else:
		return ""


def use_standard_sys_dir_headers() -> str:
	if config.is_fbcode():
		return "-nostdinc"
	else:
		return ""


@functools.lru_cache(None)
def is_conda_llvm_openmp_installed() -> bool:
	try:
		command = "conda list llvm-openmp --json"
		output = subprocess.check_output(command.split()).decode("utf8")
		return len(json.loads(output)) > 0
	except subprocess.SubprocessError:
		return False


@functools.lru_cache(None)
def homebrew_libomp() -> Tuple[bool, str]:
	try:
		subprocess.check_output(["which", "brew"])
		libomp_path = (
			subprocess.check_output(["brew", "--prefix", "libomp"])
			.decode("utf8")
			.strip()
		)
		omp_available = os.path.exists(libomp_path)
		return omp_available, libomp_path
	except subprocess.SubprocessError:
		return False, ""


def get_include_and_linking_paths(
	include_pytorch: bool = False,
	vec_isa: VecISA = invalid_vec_isa,
	cuda: bool = False,
	aot_mode: bool = False,
) -> Tuple[List[str], str, str, str, str]:
	if (
		config.is_fbcode()
		and "CUDA_HOME" not in os.environ
		and "CUDA_PATH" not in os.environ
	):
		os.environ["CUDA_HOME"] = os.path.dirname(build_paths.cuda())
	from torch.utils import cpp_extension

	macros = ""
	build_arch_flags = ""
	if sys.platform == "linux" and (
		include_pytorch
		or vec_isa != invalid_vec_isa
		or cuda
		or config.cpp.enable_kernel_profile
	):
		ipaths = cpp_extension.include_paths(cuda) + [sysconfig.get_path("include")]
		lpaths = cpp_extension.library_paths(cuda) + [
			sysconfig.get_config_var("LIBDIR")
		]

		libs = []

		if not config.is_fbcode():
			libs += ["torch", "torch_cpu"]
			libs += ["gomp"]
			if not aot_mode:
				libs += ["torch_python"]
		else:
			libs += ["omp"]
			if aot_mode:
				ipaths += [os.path.dirname(cpp_prefix_path())]
				if cuda:
					for i, path in enumerate(lpaths):
						if path.startswith(
							os.environ["CUDA_HOME"]
						) and not os.path.exists(f"{path}/libcudart_static.a"):
							for root, dirs, files in os.walk(path):
								if "libcudart_static.a" in files:
									lpaths[i] = os.path.join(path, root)
									lpaths.append(os.path.join(lpaths[i], "stubs"))
									break
		macros = vec_isa.build_macro()
		if macros:
			if config.is_fbcode() and vec_isa != invalid_vec_isa:
				cap = str(vec_isa).upper()
				macros = " ".join(
					[
						vec_isa.build_arch_flags(),
						f"-D CPU_CAPABILITY={cap}",
						f"-D CPU_CAPABILITY_{cap}",
						f"-D HAVE_{cap}_CPU_DEFINITION",
					]
				)

		if aot_mode and cuda:
			if macros is None:
				macros = ""
			macros += " -D USE_CUDA"

		if cuda:
			if torch.version.hip is not None:
				libs += ["c10_hip", "torch_hip"]
			else:
				if config.is_fbcode():
					libs += ["cuda"]
				else:
					libs += ["c10_cuda", "cuda", "torch_cuda"]
		build_arch_flags = vec_isa.build_arch_flags()
	else:
		ipaths = cpp_extension.include_paths(cuda) + [sysconfig.get_path("include")]
		if aot_mode:
			ipaths += [os.path.dirname(cpp_prefix_path())]
		lpaths = []
		if sys.platform == "darwin":
			omp_available = not is_apple_clang()

			if os.getenv("OMP_PREFIX") is not None:
				header_path = os.path.join(os.getenv("OMP_PREFIX"), "include", "omp.h")
				valid_env = os.path.exists(header_path)
				if valid_env:
					ipaths.append(os.path.join(os.getenv("OMP_PREFIX"), "include"))
					lpaths.append(os.path.join(os.getenv("OMP_PREFIX"), "lib"))
				else:
					warnings.warn("environment variable `OMP_PREFIX` is invalid.")
				omp_available = omp_available or valid_env

			libs = [] if omp_available else ["omp"]

			if not omp_available and os.getenv("CONDA_PREFIX") is not None:
				omp_available = is_conda_llvm_openmp_installed()
				if omp_available:
					conda_lib_path = os.path.join(os.getenv("CONDA_PREFIX"), "lib")
					ipaths.append(os.path.join(os.getenv("CONDA_PREFIX"), "include"))
					lpaths.append(conda_lib_path)
					if os.uname().machine == "x86_64" and os.path.exists(
						os.path.join(conda_lib_path, "libiomp5.dylib")
					):
						libs = ["iomp5"]

			if not omp_available:
				omp_available, libomp_path = homebrew_libomp()
				if omp_available:
					ipaths.append(os.path.join(libomp_path, "include"))
					lpaths.append(os.path.join(libomp_path, "lib"))

		else:
			libs = ["omp"] if config.is_fbcode() else ["gomp"]

	if not config.aot_inductor.abi_compatible:
		libs += ["c10"]
		lpaths += [cpp_extension.TORCH_LIB_PATH]

	if config.is_fbcode():
		ipaths.append(build_paths.sleef())
		ipaths.append(build_paths.openmp())
		ipaths.append(build_paths.cc_include())
		ipaths.append(build_paths.libgcc())
		ipaths.append(build_paths.libgcc_arch())
		ipaths.append(build_paths.libgcc_backward())
		ipaths.append(build_paths.glibc())
		ipaths.append(build_paths.linux_kernel())
		ipaths.append(build_paths.cuda())
		ipaths.append("include")

	static_link_libs = []
	if aot_mode and cuda and config.is_fbcode():
		static_link_libs = ["-Wl,-Bstatic", "-lcudart_static", "-Wl,-Bdynamic"]

	lpaths_str = " ".join(["-L" + p for p in lpaths])
	libs_str = " ".join(static_link_libs + ["-l" + p for p in libs])
	return ipaths, lpaths_str, libs_str, macros, build_arch_flags


def cpp_compile_command(
	input: Union[str, List[str]],
	output: str,
	warning_all: bool = True,
	shared: bool = True,
	include_pytorch: bool = False,
	vec_isa: VecISA = invalid_vec_isa,
	cuda: bool = False,
	aot_mode: bool = False,
	compile_only: bool = False,
	use_absolute_path: bool = False,
) -> str:
	ipaths, lpaths, libs, macros, build_arch_flags = get_include_and_linking_paths(
		include_pytorch, vec_isa, cuda, aot_mode
	)
	if isinstance(input, str):
		input = [input]
	ipaths_str = " ".join(["-I" + p for p in ipaths])
	clang_flags = ""
	if config.is_fbcode():
		if aot_mode and not use_absolute_path:
			inp_name = input
			out_name = output
		else:
			inp_name = [os.path.basename(i) for i in input]
			out_name = os.path.basename(output)
		assert is_clang()
		clang_flags += " --rtlib=compiler-rt"
		clang_flags += " -fuse-ld=lld"
		linker_paths = "-B" + build_paths.glibc_lib()
		linker_paths += " -L" + build_paths.glibc_lib()
	else:
		inp_name = input
		out_name = output
		linker_paths = ""  # let the compiler pick
	inp_name_str = " ".join(inp_name)
	return re.sub(
		r"[ \n]+",
		" ",
	).strip()


def run_command_and_check(cmd: str):
	cmd = shlex.split(cmd)
	try:
		subprocess.check_call(cmd)
	except subprocess.CalledProcessError as e:
		raise exc.CppCompileError(cmd, e.output) from e


@functools.lru_cache(None)
def split_aot_inductor_output_path(path: str) -> Tuple[str, str]:

	def __init__(
		self,
		lib_path: str,
	):
		self.lib_path = lib_path
		self.DLL = cdll.LoadLibrary(lib_path)
		self.is_open = True

	def close(self):
		if self.is_open:
			self._dlclose()
			self.is_open = False

	def _dlclose(self):
		f_dlclose = None

		if is_linux():
			syms = CDLL(None)
			if not hasattr(syms, "dlclose"):
				syms = CDLL("libc.so")

			if hasattr(syms, "dlclose"):
				f_dlclose = syms.dlclose
		else:
			raise NotImplementedError("Unsupported env, failed to do dlclose!")

		if f_dlclose is not None:
			f_dlclose.argtypes = [c_void_p]
			f_dlclose(self.DLL._handle)
		else:
			log.warning(
				"dll unloading function was not found, library may not be unloaded properly!"
			)

	def __getattr__(self, name):
		if not self.is_open:
			raise RuntimeError(f"Cannot use closed DLL library: {self.lib_path}")

		method = getattr(self.DLL, name)

		def _wrapped_func(*args):
			err = method(*args)
			if err:
				raise RuntimeError(f"Error in function: {method.__name__}")

		return _wrapped_func

	def __enter__(self):
		return self

	def __exit__(self, *args):
		self.close()

	def __del__(self):
		self.close()


class CUDACodeCache:
	@dataclasses.dataclass
	class CacheEntry:
		input_path: str
		output_path: str

	cache: Dict[str, CacheEntry] = dict()
	clear = staticmethod(cache.clear)
	_SOURCE_CODE_SUFFIX = "cu"

	@classmethod
	def write(cls, source_code, dst_file_ext) -> Tuple[str, str]:

		cuda_command = repr(
			cuda_compile_command(["dummy_input"], "dummy_output", dst_file_ext)
		)
		key, input_path = write(
			source_code, cls._SOURCE_CODE_SUFFIX, extra=cuda_command
		)
		return key, input_path

	@classmethod
	def compile(cls, source_code, dst_file_ext) -> Tuple[str, str, str]:

		key, input_path = cls.write(source_code, dst_file_ext)
		if key not in cls.cache:
			from filelock import FileLock

			lock_dir = get_lock_dir()
			lock = FileLock(os.path.join(lock_dir, key + ".lock"), timeout=LOCK_TIMEOUT)
			with lock:
				output_path = input_path[: -len(cls._SOURCE_CODE_SUFFIX)] + dst_file_ext
				if not os.path.exists(output_path):
					cmd = cuda_compile_command(
						[input_path], output_path, dst_file_ext
					).split(" ")
					try:
						subprocess.check_output(
							cmd, stderr=subprocess.STDOUT, env=os.environ
						)
					except subprocess.CalledProcessError as error:
						raise exc.CUDACompileError(cmd, error.output) from error
				cls.cache[key] = CUDACodeCache.CacheEntry(input_path, output_path)

		return (cls.cache[key].output_path, key, input_path)

	@classmethod
	def load(cls, source_code, dst_file_ext) -> Tuple[DLLWrapper, str, str]:

		if dst_file_ext != "so":
			raise RuntimeError(
				f"Only support loading a .so file for now. "
				f"Requested file extension: {dst_file_ext}. Source code: {source_code}"
			)
		dst_file_path, hash_key, source_code_path = cls.compile(
			source_code, dst_file_ext
		)
		return (DLLWrapper(dst_file_path), hash_key, source_code_path)


def caching_device_properties():
	for _, device_interface in get_registered_device_interfaces():
		if device_interface.is_available():
			device_interface.Worker.get_device_properties()


def _worker_compile(
	kernel_name: str, source_code: str, cc: int, device: torch.device
) -> None:
	device_interface = get_interface_for_device(device.type)
	device_interface.Worker.set_device(device.index)
	kernel = TritonCodeCache.load(kernel_name, source_code)
	kernel.precompile(warm_cache_only_with_cc=cc)


def _load_kernel(kernel_name: str, source_code: str) -> ModuleType:
	kernel = TritonCodeCache.load(kernel_name, source_code)
	kernel.precompile()
	return kernel


class TritonFuture:
	kernel: ModuleType

	def __init__(
		self,
		kernel_name: str,
		source_code: str,
		future: Future[Any],
	) -> None:
		self.kernel_name = kernel_name
		self.source_code = source_code
		self.future = future

	def result(self) -> ModuleType:
		t0 = time()
		if hasattr(self, "kernel"):
			return self.kernel
		self.future.result()
		kernel = self.kernel = _load_kernel(self.kernel_name, self.source_code)
		latency = time() - t0
		if latency > 50:
			developer_warning(
				f"Detected long compilation time of {latency} seconds for kernel name {self.kernel_name}"
			)
			developer_warning(self.source_code)
		del self.kernel_name, self.source_code, self.future
		return kernel


def _async_compile_initializer(orig_ppid) -> None:
	def run() -> None:
		while True:
			sleep(1)
			if orig_ppid != os.getppid():
				os.kill(os.getpid(), signal.SIGKILL)

	global _watchdog_thread
	_watchdog_thread = Thread(target=run, daemon=True)
	_watchdog_thread.start()
	signal.signal(signal.SIGINT, signal.SIG_IGN)


_watchdog_thread: Optional[Thread] = None


class AsyncCompile:
	def __init__(self) -> None:
		pass

	@staticmethod
	@functools.lru_cache(1)
	def pool() -> ThreadPoolExecutor:
		assert config.compile_threads > 1
		return ThreadPoolExecutor(config.compile_threads)

	@staticmethod
	@functools.lru_cache(1)
	def process_pool() -> ProcessPoolExecutor:
		caching_device_properties()
		assert config.compile_threads > 1
		orig_ppid = os.getpid()

		ctx = multiprocessing.get_context(config.worker_start_method)
		pool = ProcessPoolExecutor(
			config.compile_threads,
			mp_context=ctx,
			initializer=partial(_async_compile_initializer, orig_ppid),
		)
		multiprocessing.util.Finalize(None, pool.shutdown, exitpriority=sys.maxsize)
		return pool

	@classmethod
	def warm_pool(cls) -> None:
		if config.compile_threads <= 1:
			return
		_compile_start()
		pool = cls.process_pool()





		if hasattr(pool, "_start_queue_management_thread"):
			pool._start_queue_management_thread()
		else:
			for _ in range(config.compile_threads):
				pool._adjust_process_count()
			if hasattr(pool, "_start_executor_manager_thread"):
				pool._start_executor_manager_thread()
		_compile_end()

	@classmethod
	def submit(cls, task: Callable[..., Any]) -> Any:
		if config.compile_threads <= 1:
			return task()
		return cls.pool().submit(task)

	@classmethod
	def map(cls, fn: Callable[..., Any], seq: List[Any]) -> List[Any]:
		if config.compile_threads <= 1 or len(seq) <= 1:
			return list(map(fn, seq))
		return [t.result() for t in [cls.pool().submit(fn, x) for x in seq]]

	def triton(
		self, kernel_name: str, source_code: str, device_str: str = "cuda"
	) -> Union[TritonFuture, ModuleType]:
		_compile_start()

		if config.compile_threads > 1:
			device_interface = get_interface_for_device(device_str)
			device = torch.device(device_str, device_interface.current_device())
			cc = device_interface.get_compute_capability(device)
			future = self.process_pool().submit(
				_worker_compile, kernel_name, source_code, cc, device
			)
			return TritonFuture(kernel_name, source_code, future)
		else:
			return _load_kernel(kernel_name, source_code)

	def cpp(self, source_code: str) -> ModuleType:
		def task():
			return CppCodeCache.load(source_code).kernel

		return self.submit(task)

	def cuda(self, source_code, dst_file_ext):
		def task():
			return CUDACodeCache.load(source_code, dst_file_ext)[0]

		return self.submit(task)

	def wait(self, scope: Dict[str, Any]) -> None:
		num_kernels = len(
			[
				value
				for key, value in scope.items()
				if isinstance(value, (Future, TritonFuture))
			]
		)
		pbar = tqdm(
			total=num_kernels,
			desc="Inductor Compilation",
			disable=config.disable_progress,
			delay=0,
		)
		if config.compile_threads > 1:
			for key, result in scope.items():
				if config.verbose_progress and not isinstance(pbar, _Faketqdm):
					pbar.set_postfix_str(key)
				if isinstance(result, (Future, TritonFuture)):
					scope[key] = result.result()
					pbar.update(1)

		_compile_end()


AsyncCompile.warm_pool()

<END>

<START>
from torch._functorch.vmap import (vmap_impl, _check_randomness_arg,
								   Callable, in_dims_t, out_dims_t, _check_out_dims_is_int_or_int_pytree,
								   _process_batched_inputs, _chunked_vmap)
from torch._functorch.utils import exposed_in, argnums_t
import functools



@exposed_in('torch.func')
def vmap(
		func: Callable,
		in_dims: in_dims_t = 0,
		out_dims: out_dims_t = 0,
		randomness: str = 'error',
		*,
		chunk_size=None) -> Callable:
	_check_randomness_arg(randomness)
	if not (chunk_size is None or chunk_size > 0):
		raise ValueError(f"vmap: chunk_size should be None or greater than 0. (got {chunk_size})")

	def wrapped(*args, **kwargs):
		return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)

	return wrapped


def chunk_vmap(
		func: Callable,
		in_dims: in_dims_t = 0,
		out_dims: out_dims_t = 0,
		randomness: str = 'error',
		chunks=2) -> Callable:
	_check_randomness_arg(randomness)

	if chunks == 1:
		return vmap(func, in_dims=in_dims, out_dims=out_dims, randomness=randomness)

	def _get_chunk_flat_args(flat_args_, flat_in_dims_, chunks_):
		flat_args_chunks = tuple(
			t.chunk(chunks_, dim=in_dim) if in_dim is not None else [t, ] * chunks_
			for t, in_dim in zip(flat_args_, flat_in_dims_)
		)
		chunks_flat_args = zip(*flat_args_chunks)
		return chunks_flat_args

	@functools.wraps(func)
	def wrapped_with_chunks(*args, **kwargs):
		_check_out_dims_is_int_or_int_pytree(out_dims, func)
		_, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
		chunks_flat_args = _get_chunk_flat_args(flat_args, flat_in_dims, chunks)

		return _chunked_vmap(func, flat_in_dims, chunks_flat_args, args_spec, out_dims, randomness, **kwargs)

	return wrapped_with_chunks


@exposed_in("torch.func")
def grad(func: Callable, argnums: argnums_t = 0, has_aux: bool = False) -> Callable:
	import torch._functorch.eager_transforms as eager_transforms

	@functools.wraps(func)
	def wrapper(*args, **kwargs):
		return eager_transforms.grad_impl(func, argnums, has_aux, args, kwargs)
	return wrapper

<END>

<START>
import os
import sys

translation_validation = (
	os.environ.get("TORCHDYNAMO_TRANSLATION_VALIDATION", "0") == "1"
)
translation_validation_timeout = int(
	os.environ.get("TORCHDYNAMO_TRANSLATION_VALIDATION_TIMEOUT", "600000")
)
translation_validation_no_bisect = (
	os.environ.get("TORCHDYNAMO_TRANSLATION_NO_BISECT", "0") == "1"
)
check_shape_env_recorded_events = False


print_specializations = False

inject_EVALUATE_EXPR_flip_equality_TESTING_ONLY = False

validate_shape_env_verison_key = False

from torch.utils._config_module import install_config_module

install_config_module(sys.modules[__name__])

<END>

<START>

<END>

<START>
from typing import Dict, List, Optional

import torch
import torch.optim._functional as F

from torch import Tensor

__all__: List[str] = []

@torch.jit.script
class _FunctionalAdagrad:
	def __init__(
		self,
		params: List[Tensor],
		lr: float = 1e-2,
		lr_decay: float = 0.0,
		weight_decay: float = 0.0,
		initial_accumulator_value: float = 0.0,
		warmup_lr_multiplier: float = 1.0,
		warmup_num_iters: float = 0.0,
		eps: float = 1e-10,
		coalesce_grad: bool = True,
		foreach: bool = False,
		maximize: bool = False,
		_allow_empty_param_list: bool = False,
	):
		self.defaults = {
			"lr": lr,
			"lr_decay": lr_decay,
			"eps": eps,
			"weight_decay": weight_decay,
			"initial_accumulator_value": initial_accumulator_value,
			"warmup_lr_multiplier": warmup_lr_multiplier,
			"warmup_num_iters": warmup_num_iters,
		}
		self.coalesce_grad = coalesce_grad
		self.foreach = foreach
		self.maximize = maximize
		self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})

		if len(params) == 0 and not _allow_empty_param_list:
			raise ValueError("optimizer got an empty parameter list")

		self.param_group = {"params": params}

		for p in self.param_group["params"]:
			self.state[p] = {
				"sum": torch.full_like(p.data, initial_accumulator_value),
				"step": torch.tensor(0.0),
			}

	def step(self, gradients: List[Optional[Tensor]]):
		params = self.param_group["params"]
		params_with_grad = []
		grads = []
		state_sums = []
		state_steps: List[Tensor] = []

		if len(params) != len(gradients):
			raise ValueError(
				"the gradients passed in does not equal to the size of the parameters!"
				+ f"Params length: {len(params)}. "
				+ f"Gradients length: {len(gradients)}"
			)

		has_sparse_grad, has_complex = False, False
		for param, gradient in zip(self.param_group["params"], gradients):
			if gradient is not None:
				has_sparse_grad |= gradient.is_sparse
				has_complex |= torch.is_complex(param)
				params_with_grad.append(param)
				grads.append(gradient)
				state = self.state[param]
				state_sums.append(state["sum"])
				state_steps.append(state["step"])

		with torch.no_grad():
			F.adagrad(
				params,
				grads,
				state_sums,
				state_steps,
				lr=self.defaults["lr"],
				weight_decay=self.defaults["weight_decay"],
				lr_decay=self.defaults["lr_decay"],
				eps=self.defaults["eps"],
				has_sparse_grad=has_sparse_grad,
				foreach=self.foreach,
				maximize=self.maximize,
				has_complex=has_complex,
			)

<END>

<START>
import functools
import itertools
import logging
from typing import List, Optional
from unittest.mock import patch

import sympy

import torch
from ...autotune_process import CUDABenchmarkRequest, TensorMeta
from ...ir import Buffer, CUDATemplateBuffer, IRNode, Layout

from ...utils import IndentedBuffer, unique
from ...virtualized import V
from ..common import KernelTemplate
from .cuda_kernel import CUDATemplateCaller, CUDATemplateKernel

log = logging.getLogger(__name__)


class CUDATemplate(KernelTemplate):
	index_counter = itertools.count()

	def __init__(
		self,
		name: str,
		input_nodes: List[Buffer],
		layout: Layout,
		input_reorder: Optional[List[int]] = None,
	):
		super().__init__(name)
		self.input_nodes = input_nodes
		self.output_node: Buffer = Buffer("buf_out", layout)
		self.input_reorder = input_reorder
		self.layout = layout

	def generate(  # type: ignore[override]
		self,
		**kwargs,
	) -> CUDATemplateCaller:
		kernel_name = f"cuda_{self.name}"
		with patch.object(
			V.graph, "get_dtype", self._fake_get_dtype(self.output_node)
		), CUDATemplateKernel(
			kernel_name=kernel_name,
		) as kernel:
			code = self.render(kernel=kernel, **kwargs)
			_, call_args, _ = kernel.args.python_argdefs()
			log.debug("Generated Code:\n%s", code)
			log.debug(
				"Args: cpp_argdefs: %s, python_argdefs: %s",
				kernel.args.cpp_argdefs(),
				kernel.args.python_argdefs(),
			)

		input_reorder = (
			self.input_reorder
			if self.input_reorder is not None
			else list(range(len(self.input_nodes)))
		)
		expected_args = list(
			unique(self.input_nodes[idx].get_name() for idx in input_reorder)
		)
		expected_args.extend([self.output_node.get_name()])
		assert list(call_args)[: len(expected_args)] == expected_args, (
			call_args,
			expected_args,
		)
		extra_args = V.graph.sizevars.size_hints(
			map(sympy.expand, call_args[len(expected_args) :])
		)

		kernel_hash_name = f"cuda_{self.name}_{next(self.index_counter)}"

		bmreq = CUDABenchmarkRequest(
			kernel_name=kernel_name,
			input_tensor_meta=TensorMeta.from_irnodes(self.input_nodes),
			output_tensor_meta=TensorMeta.from_irnodes(self.output_node),
			extra_args=extra_args,
			source_code=code,
		)

		def make_kernel_render(
			template_node: CUDATemplateBuffer,
			epilogue_nodes: Optional[List[IRNode]] = None,
		):
			kernel = CUDATemplateKernel(
				kernel_name="KERNEL_NAME",
			)
			render = functools.partial(
				self.render,
				kernel=kernel,
				template_buffer_node=template_node,
				epilogue_nodes=epilogue_nodes,
				**kwargs,  # includes "op" argument in case of CUTLASSGemmTemplate
			)
			return kernel, render

		return CUDATemplateCaller(
			kernel_hash_name,
			self.name,
			self.input_nodes,
			self.output_node.get_layout(),
			make_kernel_render,
			bmreq,
			self,
		)

	def header(self) -> IndentedBuffer:
		res = IndentedBuffer()
		res.splice(
		)
		return res

	def globals(self) -> IndentedBuffer:
		res = IndentedBuffer()
		res.splice(
		)
		return res

	def render(self, **kwargs) -> str:
		raise NotImplementedError


class CUTLASSTemplate(CUDATemplate):

	def header(self) -> IndentedBuffer:
		res = super().header()
		res.splice(
		)
		return res

	def globals(self) -> IndentedBuffer:
		res = super().globals()
		res.splice(
		)
		return res

	def cute_int(self, int_str: str, var_name: str) -> str:
		res = ""
		if int_str in {"1", "1L"}:
			res = "cute::Int<1>{}"
		else:
			res = int_str

		return f"{res} /* {var_name} */"

	_DTYPE_TO_CUTLASS = {
		torch.float32: "float",
		torch.float64: "double",
		torch.float16: "cutlass::half_t",
		torch.int32: "int",
		torch.int8: "int8_t",
		torch.uint8: "uint8_t",
		torch.bool: "bool",
		torch.bfloat16: "cutlass::bfloat16_t",
	}

	def cutlass_type_cast(self, node: IRNode, ptr: str) -> str:
		if node is None:
			return ptr
		else:
			return f"({self._DTYPE_TO_CUTLASS.get(node.get_dtype())}*)({ptr})"

<END>

<START>
from typing import Optional, Tuple, Union
from numbers import Number
import torch
from torch.utils.benchmark import FuzzedTensor
import math

class FuzzedSparseTensor(FuzzedTensor):
	def __init__(
		self,
		name: str,
		size: Tuple[Union[str, int], ...],
		min_elements: Optional[int] = None,
		max_elements: Optional[int] = None,
		dim_parameter: Optional[str] = None,
		sparse_dim: Optional[str] = None,
		nnz: Optional[str] = None,
		density: Optional[str] = None,
		coalesced: Optional[str] = None,
		dtype=torch.float32,
		cuda=False
	):
		super().__init__(name=name, size=size, min_elements=min_elements,
						 max_elements=max_elements, dim_parameter=dim_parameter, dtype=dtype, cuda=cuda)
		self._density = density
		self._coalesced = coalesced
		self._sparse_dim = sparse_dim

	@staticmethod
	def sparse_tensor_constructor(size, dtype, sparse_dim, nnz, is_coalesced):
		if isinstance(size, Number):
			size = [size] * sparse_dim
		assert all(size[d] > 0 for d in range(sparse_dim)) or nnz == 0, 'invalid arguments'
		v_size = [nnz] + list(size[sparse_dim:])
		if dtype.is_floating_point:
			v = torch.rand(size=v_size, dtype=dtype, device="cpu")
		else:
			v = torch.randint(1, 127, size=v_size, dtype=dtype, device="cpu")

		i = torch.rand(sparse_dim, nnz, device="cpu")
		i.mul_(torch.tensor(size[:sparse_dim]).unsqueeze(1).to(i))
		i = i.to(torch.long)

		if not is_coalesced:
			v = torch.cat([v, torch.randn_like(v)], 0)
			i = torch.cat([i, i], 1)

		x = torch.sparse_coo_tensor(i, v, torch.Size(size))
		if is_coalesced:
			x = x.coalesce()
		return x

	def _make_tensor(self, params, state):
		size, _, _ = self._get_size_and_steps(params)
		density = params['density']
		nnz = math.ceil(sum(size) * density)
		assert nnz <= sum(size)

		is_coalesced = params['coalesced']
		sparse_dim = params['sparse_dim'] if self._sparse_dim else len(size)
		sparse_dim = min(sparse_dim, len(size))
		tensor = self.sparse_tensor_constructor(size, self._dtype, sparse_dim, nnz, is_coalesced)

		if self._cuda:
			tensor = tensor.cuda()
		sparse_dim = tensor.sparse_dim()
		dense_dim = tensor.dense_dim()
		is_hybrid = len(size[sparse_dim:]) > 0

		properties = {
			"numel": int(tensor.numel()),
			"shape": tensor.size(),
			"is_coalesced": tensor.is_coalesced(),
			"density": density,
			"sparsity": 1.0 - density,
			"sparse_dim": sparse_dim,
			"dense_dim": dense_dim,
			"is_hybrid": is_hybrid,
			"dtype": str(self._dtype),
		}
		return tensor, properties

<END>

<START>

import sys


def is_stdlib_module(module: str) -> bool:
	base_module = module.partition(".")[0]
	return base_module in _get_stdlib_modules()


def _get_stdlib_modules():
	if sys.version_info.major == 3:
		if sys.version_info.minor == 8:
			return stdlib3_8
		if sys.version_info.minor == 9:
			return stdlib3_9
		if sys.version_info.minor >= 10:
			return sys.stdlib_module_names  # type: ignore[attr-defined]
	elif sys.version_info.major > 3:
		return sys.stdlib_module_names  # type: ignore[attr-defined]

	raise RuntimeError(f"Unsupported Python version: {sys.version_info}")


stdlib3_8 = {
	"_dummy_thread",
	"_thread",
	"abc",
	"aifc",
	"argparse",
	"array",
	"ast",
	"asynchat",
	"asyncio",
	"asyncore",
	"atexit",
	"audioop",
	"base64",
	"bdb",
	"binascii",
	"binhex",
	"bisect",
	"builtins",
	"bz2",
	"cProfile",
	"calendar",
	"cgi",
	"cgitb",
	"chunk",
	"cmath",
	"cmd",
	"code",
	"codecs",
	"codeop",
	"collections",
	"colorsys",
	"compileall",
	"concurrent",
	"configparser",
	"contextlib",
	"contextvars",
	"copy",
	"copyreg",
	"crypt",
	"csv",
	"ctypes",
	"curses",
	"dataclasses",
	"datetime",
	"dbm",
	"decimal",
	"difflib",
	"dis",
	"distutils",
	"doctest",
	"dummy_threading",
	"email",
	"encodings",
	"ensurepip",
	"enum",
	"errno",
	"faulthandler",
	"fcntl",
	"filecmp",
	"fileinput",
	"fnmatch",
	"formatter",
	"fractions",
	"ftplib",
	"functools",
	"gc",
	"getopt",
	"getpass",
	"gettext",
	"glob",
	"grp",
	"gzip",
	"hashlib",
	"heapq",
	"hmac",
	"html",
	"http",
	"imaplib",
	"imghdr",
	"imp",
	"importlib",
	"inspect",
	"io",
	"ipaddress",
	"itertools",
	"json",
	"keyword",
	"lib2to3",
	"linecache",
	"locale",
	"logging",
	"lzma",
	"mailbox",
	"mailcap",
	"marshal",
	"math",
	"mimetypes",
	"mmap",
	"modulefinder",
	"msilib",
	"msvcrt",
	"multiprocessing",
	"netrc",
	"nis",
	"nntplib",
	"ntpath",
	"numbers",
	"operator",
	"optparse",
	"os",
	"ossaudiodev",
	"parser",
	"pathlib",
	"pdb",
	"pickle",
	"pickletools",
	"pipes",
	"pkgutil",
	"platform",
	"plistlib",
	"poplib",
	"posix",
	"posixpath",
	"pprint",
	"profile",
	"pstats",
	"pty",
	"pwd",
	"py_compile",
	"pyclbr",
	"pydoc",
	"queue",
	"quopri",
	"random",
	"re",
	"readline",
	"reprlib",
	"resource",
	"rlcompleter",
	"runpy",
	"sched",
	"secrets",
	"select",
	"selectors",
	"shelve",
	"shlex",
	"shutil",
	"signal",
	"site",
	"smtpd",
	"smtplib",
	"sndhdr",
	"socket",
	"socketserver",
	"spwd",
	"sqlite3",
	"sre",
	"sre_compile",
	"sre_constants",
	"sre_parse",
	"ssl",
	"stat",
	"statistics",
	"string",
	"stringprep",
	"struct",
	"subprocess",
	"sunau",
	"symbol",
	"symtable",
	"sys",
	"sysconfig",
	"syslog",
	"tabnanny",
	"tarfile",
	"telnetlib",
	"tempfile",
	"termios",
	"test",
	"textwrap",
	"threading",
	"time",
	"timeit",
	"tkinter",
	"token",
	"tokenize",
	"trace",
	"traceback",
	"tracemalloc",
	"tty",
	"turtle",
	"turtledemo",
	"types",
	"typing",
	"unicodedata",
	"unittest",
	"urllib",
	"uu",
	"uuid",
	"venv",
	"warnings",
	"wave",
	"weakref",
	"webbrowser",
	"winreg",
	"winsound",
	"wsgiref",
	"xdrlib",
	"xml",
	"xmlrpc",
	"zipapp",
	"zipfile",
	"zipimport",
	"zlib",
}

stdlib3_9 = {
	"_thread",
	"abc",
	"aifc",
	"argparse",
	"array",
	"ast",
	"asynchat",
	"asyncio",
	"asyncore",
	"atexit",
	"audioop",
	"base64",
	"bdb",
	"binascii",
	"binhex",
	"bisect",
	"builtins",
	"bz2",
	"cProfile",
	"calendar",
	"cgi",
	"cgitb",
	"chunk",
	"cmath",
	"cmd",
	"code",
	"codecs",
	"codeop",
	"collections",
	"colorsys",
	"compileall",
	"concurrent",
	"configparser",
	"contextlib",
	"contextvars",
	"copy",
	"copyreg",
	"crypt",
	"csv",
	"ctypes",
	"curses",
	"dataclasses",
	"datetime",
	"dbm",
	"decimal",
	"difflib",
	"dis",
	"distutils",
	"doctest",
	"email",
	"encodings",
	"ensurepip",
	"enum",
	"errno",
	"faulthandler",
	"fcntl",
	"filecmp",
	"fileinput",
	"fnmatch",
	"formatter",
	"fractions",
	"ftplib",
	"functools",
	"gc",
	"getopt",
	"getpass",
	"gettext",
	"glob",
	"graphlib",
	"grp",
	"gzip",
	"hashlib",
	"heapq",
	"hmac",
	"html",
	"http",
	"imaplib",
	"imghdr",
	"imp",
	"importlib",
	"inspect",
	"io",
	"ipaddress",
	"itertools",
	"json",
	"keyword",
	"lib2to3",
	"linecache",
	"locale",
	"logging",
	"lzma",
	"mailbox",
	"mailcap",
	"marshal",
	"math",
	"mimetypes",
	"mmap",
	"modulefinder",
	"msilib",
	"msvcrt",
	"multiprocessing",
	"netrc",
	"nis",
	"nntplib",
	"ntpath",
	"numbers",
	"operator",
	"optparse",
	"os",
	"ossaudiodev",
	"parser",
	"pathlib",
	"pdb",
	"pickle",
	"pickletools",
	"pipes",
	"pkgutil",
	"platform",
	"plistlib",
	"poplib",
	"posix",
	"posixpath",
	"pprint",
	"profile",
	"pstats",
	"pty",
	"pwd",
	"py_compile",
	"pyclbr",
	"pydoc",
	"queue",
	"quopri",
	"random",
	"re",
	"readline",
	"reprlib",
	"resource",
	"rlcompleter",
	"runpy",
	"sched",
	"secrets",
	"select",
	"selectors",
	"shelve",
	"shlex",
	"shutil",
	"signal",
	"site",
	"smtpd",
	"smtplib",
	"sndhdr",
	"socket",
	"socketserver",
	"spwd",
	"sqlite3",
	"sre",
	"sre_compile",
	"sre_constants",
	"sre_parse",
	"ssl",
	"stat",
	"statistics",
	"string",
	"stringprep",
	"struct",
	"subprocess",
	"sunau",
	"symbol",
	"symtable",
	"sys",
	"sysconfig",
	"syslog",
	"tabnanny",
	"tarfile",
	"telnetlib",
	"tempfile",
	"termios",
	"test",
	"textwrap",
	"threading",
	"time",
	"timeit",
	"tkinter",
	"token",
	"tokenize",
	"trace",
	"traceback",
	"tracemalloc",
	"tty",
	"turtle",
	"turtledemo",
	"types",
	"typing",
	"unicodedata",
	"unittest",
	"urllib",
	"uu",
	"uuid",
	"venv",
	"warnings",
	"wave",
	"weakref",
	"webbrowser",
	"winreg",
	"winsound",
	"wsgiref",
	"xdrlib",
	"xml",
	"xmlrpc",
	"zipapp",
	"zipfile",
	"zipimport",
	"zlib",
	"zoneinfo",
}

<END>

<START>
import torch
from torch.distributed._tensor.op_schema import OpSchema, OpStrategy, OutputSharding
from torch.distributed._tensor.ops.basic_strategy import gen_einsum_strategies
from torch.distributed._tensor.ops.common_rules import einop_rule
from torch.distributed._tensor.ops.utils import (
	generate_redistribute_costs,
	infer_broadcast_dims_map,
	is_tensor_shardable,
	map_placements_after_broadcast,
	register_op_strategy,
	register_prop_rule,
)
from torch.distributed._tensor.placement_types import DTensorSpec

from torch.distributed.device_mesh import DeviceMesh

aten = torch.ops.aten


@register_prop_rule(aten.t.default)
def transpose_rule(op_schema: OpSchema) -> OutputSharding:
	return einop_rule("ij->ji", op_schema, linearity=True)


def _mm_like_strategy(
	mm_equation: str, mesh: DeviceMesh, op_schema: OpSchema
) -> OpStrategy:
	self_strategy, mat2_strategy = op_schema.args_schema
	assert isinstance(self_strategy, OpStrategy)
	assert isinstance(mat2_strategy, OpStrategy)
	mm_strategy = gen_einsum_strategies(mm_equation, mesh)
	strategies = mm_strategy.strategies
	filtered_strategies = []
	for strtg in strategies:
		assert strtg.input_specs is not None
		self_spec = strtg.input_specs[0]
		mat2_spec = strtg.input_specs[1]
		if is_tensor_shardable(
			self_strategy.output_shape, self_spec
		) and is_tensor_shardable(mat2_strategy.output_shape, mat2_spec):
			redistribute_cost = [
				generate_redistribute_costs(self_strategy, self_spec),
				generate_redistribute_costs(mat2_strategy, mat2_spec),
			]
			strtg.redistribute_cost = redistribute_cost
			filtered_strategies.append(strtg)

	mm_strategy.strategies = filtered_strategies

	return mm_strategy


def _addmm_like_strategy(
	mm_equation: str, mesh: DeviceMesh, op_schema: OpSchema
) -> OpStrategy:
	self_strategy, mat1_strategy, mat2_strategy = op_schema.args_schema
	assert isinstance(self_strategy, OpStrategy)
	assert isinstance(mat1_strategy, OpStrategy)
	assert isinstance(mat2_strategy, OpStrategy)
	self_shape = self_strategy.output_shape
	mm_out_shape = torch.Size(
		[
			mat2_strategy.output_shape[-1]
			if i == len(mat1_strategy.output_shape) - 1
			else dim_size
			for i, dim_size in enumerate(mat1_strategy.output_shape)
		]
	)
	mm_strategy = gen_einsum_strategies(mm_equation, mesh)
	strategies = mm_strategy.strategies
	filtered_strategies = []
	for strtg in strategies:
		assert strtg.input_specs is not None
		mat1_spec = strtg.input_specs[0]
		mat2_spec = strtg.input_specs[1]
		out_spec = strtg.out_spec

		broadcast_dims_map = infer_broadcast_dims_map(mm_out_shape, self_shape)
		self_placements = map_placements_after_broadcast(
			out_spec.placements, mm_out_shape, broadcast_dims_map
		)
		self_spec = DTensorSpec(mesh=mesh, placements=self_placements)

		if is_tensor_shardable(
			mat1_strategy.output_shape, mat1_spec
		) and is_tensor_shardable(mat2_strategy.output_shape, mat2_spec):
			strtg.input_specs = (self_spec, mat1_spec, mat2_spec)

			redistribute_cost = [
				generate_redistribute_costs(self_strategy, self_spec),
				generate_redistribute_costs(mat1_strategy, mat1_spec),
				generate_redistribute_costs(mat2_strategy, mat2_spec),
			]
			strtg.redistribute_cost = redistribute_cost
			filtered_strategies.append(strtg)

	mm_strategy.strategies = filtered_strategies

	return mm_strategy


@register_op_strategy(aten.mm.default)
def mm_strategy(mesh: DeviceMesh, op_schema: OpSchema) -> OpStrategy:
	return _mm_like_strategy("mk,kn->mn", mesh, op_schema)


@register_op_strategy(aten.addmm.default)
def addmm_strategy(mesh: DeviceMesh, op_schema: OpSchema) -> OpStrategy:
	return _addmm_like_strategy("mk,kn->mn", mesh, op_schema)


@register_op_strategy(aten.bmm.default)
def bmm_strategy(mesh: DeviceMesh, op_schema: OpSchema) -> OpStrategy:
	return _mm_like_strategy("bmk,bkn->bmn", mesh, op_schema)


@register_op_strategy(aten.baddbmm.default)
def baddmm_strategy(mesh: DeviceMesh, op_schema: OpSchema) -> OpStrategy:
	return _addmm_like_strategy("bmk,bkn->bmn", mesh, op_schema)

<END>

<START>
import argparse
import io
import os
import random
import shlex
import subprocess
import time

import numpy as np
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.distributed.autograd as dist_autograd
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.optim as optim
from torch.distributed.optim import DistributedOptimizer
from torch.distributed.rpc import RRef, TensorPipeRpcBackendOptions
from torch.distributed.rpc.backend_registry import BackendType
from torch.nn.parallel import DistributedDataParallel as DDP


NUM_TRAINERS = 8
NUM_PS = 8

NUM_EMBEDDINGS = 300
EMBEDDING_DIM = 64

WARMUP_CYCLES = 5


class HybridModel(torch.nn.Module):

	def __init__(self, emb_rref_list, device):
		super().__init__()
		self.emb_rref_list = emb_rref_list
		fc1 = torch.nn.Linear(512, 256)
		fc2 = torch.nn.Linear(256, 128)
		relu = torch.nn.ReLU()
		fc3 = torch.nn.Linear(128, 64)
		fc4 = torch.nn.Linear(64, 32)
		fc5 = torch.nn.Linear(32, 8)
		sec = nn.Sequential(fc1, fc2, relu, fc3, fc4, fc5)
		self.ddp = DDP(sec.to(device), device_ids=[device])
		self.device = device

	def forward(self, indices, offsets):
		emb_lookups = []

		for emb_rref in self.emb_rref_list:
			emb_lookups.append(
				emb_rref.rpc_sync().forward(
					indices, offsets
				)  # embedding_sum(input, offsets)
			)
			emb_lookups_cat = torch.cat(emb_lookups, dim=1)

		assert NUM_PS * EMBEDDING_DIM >= 512
		dim_normalizer = int(NUM_PS * EMBEDDING_DIM / 512)
		emb_lookups_reshaped = emb_lookups_cat.reshape(
			[emb_lookups_cat.shape[0] * dim_normalizer, 512]
		)

		return self.ddp(emb_lookups_reshaped)


def _retrieve_embedding_parameters(emb_rref):
	return [RRef(p) for p in emb_rref.local_value().parameters()]


def _print_header():
	_print_cont("\n")
	_print_cont("%10s" % "")
	for p in [50, 75, 90, 95]:
		_print_cont("%14s%10s" % ("sec/epoch", "epoch/sec"))
	_print_cont("\n")


def _print_benchmark(prefix, nelem, measurements):
	measurements = sorted(measurements)
	_print_cont("%8s:" % prefix)
	for p in [50, 75, 90, 95]:
		v = np.percentile(measurements, p)
		_print_cont("  p%02d:  %1.3fs  %6d/s" % (p, v, nelem / v))
	_print_cont("\n")


def _print_cont(msg):
	print(msg, end="", flush=True)


def _run_printable(cmd):
	proc = subprocess.run(shlex.split(cmd), capture_output=True, check=False)  # type: ignore[call-overload]
	assert proc.returncode == 0

	buffer = io.BytesIO()
	torch.save(proc.stdout.decode("utf-8"), buffer)
	input_tensor = torch.ByteTensor(list(buffer.getvalue()))
	input_length = torch.IntTensor([input_tensor.size(0)])

	output = []
	buffer = io.BytesIO(np.asarray(input_tensor).tobytes())
	output.append(torch.load(buffer))
	return output


def _run_trainer(emb_rref_list, rank):
	model = HybridModel(emb_rref_list, rank)


	model_parameter_rrefs = []
	for ind, emb_rref in enumerate(emb_rref_list):
		ps_name = f"ps{ind}"
		model_parameter_rrefs.extend(
			rpc.rpc_sync(ps_name, _retrieve_embedding_parameters, args=(emb_rref,))
		)

	for param in model.parameters():
		model_parameter_rrefs.append(RRef(param))

	opt = DistributedOptimizer(optim.SGD, model_parameter_rrefs, lr=0.05)

	criterion = torch.nn.CrossEntropyLoss()

	def get_next_batch(rank):
		for _ in range(10):
			num_indices = random.randint(20, 50)
			indices = torch.LongTensor(num_indices).random_(0, NUM_EMBEDDINGS)

			offsets = []
			start = 0
			batch_size = 0

			while start < num_indices:
				offsets.append(start)
				start += random.randint(1, 10)
				batch_size += 1

			offsets_tensor = torch.LongTensor(offsets)
			target = torch.LongTensor(batch_size).random_(8).cuda(rank)

			yield indices, offsets_tensor, target

	measurements = []
	for epoch in range(100 + WARMUP_CYCLES):
		start = time.time()
		batch_size = 0

		for indices, offsets, target in get_next_batch(rank):
			batch_size += len(target)

			with dist_autograd.context() as context_id:
				output = model(indices, offsets)
				loss = criterion(output, target)

				dist_autograd.backward(context_id, [loss])

				opt.step(context_id)


		measurements.append(time.time() - start)

	measurements = measurements[WARMUP_CYCLES:]
	return rank, measurements, batch_size


def run_worker(rank, world_size):
	rpc_backend_options = TensorPipeRpcBackendOptions()
	rpc_backend_options.init_method = "tcp://localhost:29500"

	if rank == (NUM_TRAINERS + NUM_PS):

		rpc.init_rpc(
			"master", rank=rank,
			backend=BackendType.TENSORPIPE,  # type: ignore[attr-defined]
			world_size=world_size
		)


		emb_rref_list = []
		index = 0
		while index < NUM_PS:
			ps_name = f"ps{index}"
			emb_rref = rpc.remote(
				ps_name,
				torch.nn.EmbeddingBag,
				args=(NUM_EMBEDDINGS, EMBEDDING_DIM),
				kwargs={"mode": "sum"},
			)
			emb_rref_list.append(emb_rref)
			index += 1

		futs = []
		for trainer_rank in range(NUM_TRAINERS):
			trainer_name = f"trainer{trainer_rank}"
			fut = rpc.rpc_async(
				trainer_name, _run_trainer, args=(emb_rref_list, trainer_rank)
			)
			futs.append(fut)

		_print_header()

		measurements_all_trainers = []
		batch_size_all_trainers = 0
		for fut in futs:
			rank, measurements, batch_size = fut.wait()
			_print_benchmark(f"Trainer{rank}", batch_size, measurements)
			batch_size_all_trainers += batch_size
			measurements_all_trainers.append(measurements)

		_print_benchmark("All", batch_size_all_trainers, measurements_all_trainers)

	elif rank >= 0 and rank < NUM_PS:

		dist.init_process_group(
			backend=dist.Backend.GLOO,
			rank=rank,
			world_size=NUM_TRAINERS,
			init_method="tcp://localhost:29501",
		)

		trainer_name = f"trainer{rank}"
		rpc.init_rpc(
			trainer_name,
			rank=rank,
			world_size=world_size,
			rpc_backend_options=rpc_backend_options,
		)

	elif rank >= NUM_TRAINERS and rank < NUM_TRAINERS + NUM_PS:
		ps_name = f"ps{rank - NUM_TRAINERS}"
		rpc.init_rpc(
			ps_name,
			rank=rank,
			world_size=world_size,
			backend=BackendType.TENSORPIPE,  # type: ignore[attr-defined]
			rpc_backend_options=rpc_backend_options,
		)
		pass

	rpc.shutdown()


if __name__ == "__main__":

<END>

<START>

from torch.ao.quantization.fx.graph_module import ObservedGraphModule
from torch.ao.quantization.quantize_fx import (
	_check_is_graph_module,
	_convert_fx,
	_convert_standalone_module_fx,
	_fuse_fx,
	_prepare_fx,
	_prepare_standalone_module_fx,
	_swap_ff_with_fxff,
	convert_fx,
	fuse_fx,
	prepare_fx,
	prepare_qat_fx,
	QuantizationTracer,
	Scope,
	ScopeContextManager,
)

<END>

<START>
from .module import Module
from .utils import _pair, _quadruple, _ntuple
from .. import functional as F

from torch import Tensor
from ..common_types import _size_2_t, _size_4_t, _size_6_t
from typing import Sequence, Tuple



__all__ = ['CircularPad1d', 'CircularPad2d', 'CircularPad3d', 'ConstantPad1d', 'ConstantPad2d',
		   'ConstantPad3d', 'ReflectionPad1d', 'ReflectionPad2d', 'ReflectionPad3d',
		   'ReplicationPad1d', 'ReplicationPad2d', 'ReplicationPad3d', 'ZeroPad1d', 'ZeroPad2d', 'ZeroPad3d']


class _CircularPadNd(Module):
	__constants__ = ['padding']
	padding: Sequence[int]

	def _check_input_dim(self, input):
		raise NotImplementedError

	def forward(self, input: Tensor) -> Tensor:
		self._check_input_dim(input)
		return F.pad(input, self.padding, 'circular')

	def extra_repr(self) -> str:
		return f'{self.padding}'


class CircularPad1d(_CircularPadNd):

	padding: Tuple[int, int]

	def __init__(self, padding: _size_2_t) -> None:
		super().__init__()
		self.padding = _pair(padding)

	def _check_input_dim(self, input):
		if input.dim() != 2 and input.dim() != 3:
			raise ValueError(
				f"expected 2D or 3D input (got {input.dim()}D input)"
			)


class CircularPad2d(_CircularPadNd):

	padding: Tuple[int, int, int, int]

	def __init__(self, padding: _size_4_t) -> None:
		super().__init__()
		self.padding = _quadruple(padding)

	def _check_input_dim(self, input):
		if input.dim() != 3 and input.dim() != 4:
			raise ValueError(
				f"expected 3D or 4D input (got {input.dim()}D input)"
			)


class CircularPad3d(_CircularPadNd):

	padding: Tuple[int, int, int, int, int, int]

	def __init__(self, padding: _size_6_t) -> None:
		super().__init__()
		self.padding = _ntuple(6)(padding)

	def _check_input_dim(self, input):
		if input.dim() != 4 and input.dim() != 5:
			raise ValueError(
				f"expected 4D or 5D input (got {input.dim()}D input)"
			)


class _ConstantPadNd(Module):
	__constants__ = ['padding', 'value']
	value: float
	padding: Sequence[int]

	def __init__(self, value: float) -> None:
		super().__init__()
		self.value = value

	def forward(self, input: Tensor) -> Tensor:
		return F.pad(input, self.padding, 'constant', self.value)

	def extra_repr(self) -> str:
		return f'padding={self.padding}, value={self.value}'


class ConstantPad1d(_ConstantPadNd):

	padding: Tuple[int, int]

	def __init__(self, padding: _size_2_t, value: float):
		super().__init__(value)
		self.padding = _pair(padding)


class ConstantPad2d(_ConstantPadNd):

	__constants__ = ['padding', 'value']
	padding: Tuple[int, int, int, int]

	def __init__(self, padding: _size_4_t, value: float) -> None:
		super().__init__(value)
		self.padding = _quadruple(padding)


class ConstantPad3d(_ConstantPadNd):

	padding: Tuple[int, int, int, int, int, int]

	def __init__(self, padding: _size_6_t, value: float) -> None:
		super().__init__(value)
		self.padding = _ntuple(6)(padding)


class _ReflectionPadNd(Module):
	__constants__ = ['padding']
	padding: Sequence[int]

	def forward(self, input: Tensor) -> Tensor:
		return F.pad(input, self.padding, 'reflect')

	def extra_repr(self) -> str:
		return f'{self.padding}'


class ReflectionPad1d(_ReflectionPadNd):

	padding: Tuple[int, int]

	def __init__(self, padding: _size_2_t) -> None:
		super().__init__()
		self.padding = _pair(padding)


class ReflectionPad2d(_ReflectionPadNd):

	padding: Tuple[int, int, int, int]

	def __init__(self, padding: _size_4_t) -> None:
		super().__init__()
		self.padding = _quadruple(padding)


class ReflectionPad3d(_ReflectionPadNd):

	padding: Tuple[int, int, int, int, int, int]

	def __init__(self, padding: _size_6_t) -> None:
		super().__init__()
		self.padding = _ntuple(6)(padding)


class _ReplicationPadNd(Module):
	__constants__ = ['padding']
	padding: Sequence[int]

	def forward(self, input: Tensor) -> Tensor:
		return F.pad(input, self.padding, 'replicate')

	def extra_repr(self) -> str:
		return f'{self.padding}'


class ReplicationPad1d(_ReplicationPadNd):

	padding: Tuple[int, int]

	def __init__(self, padding: _size_2_t) -> None:
		super().__init__()
		self.padding = _pair(padding)


class ReplicationPad2d(_ReplicationPadNd):

	padding: Tuple[int, int, int, int]

	def __init__(self, padding: _size_4_t) -> None:
		super().__init__()
		self.padding = _quadruple(padding)


class ReplicationPad3d(_ReplicationPadNd):

	padding: Tuple[int, int, int, int, int, int]

	def __init__(self, padding: _size_6_t) -> None:
		super().__init__()
		self.padding = _ntuple(6)(padding)


class ZeroPad1d(ConstantPad1d):

	padding: Tuple[int, int]

	def __init__(self, padding: _size_2_t) -> None:
		super().__init__(padding, 0.)

	def extra_repr(self) -> str:
		return f'{self.padding}'

class ZeroPad2d(ConstantPad2d):

	padding: Tuple[int, int, int, int]

	def __init__(self, padding: _size_4_t) -> None:
		super().__init__(padding, 0.)

	def extra_repr(self) -> str:
		return f'{self.padding}'

class ZeroPad3d(ConstantPad3d):

	padding: Tuple[int, int, int, int, int, int]

	def __init__(self, padding: _size_6_t) -> None:
		super().__init__(padding, 0.)

	def extra_repr(self) -> str:
		return f'{self.padding}'

<END>

<START>
import os
import pathlib
from collections import defaultdict
from typing import Any, Dict, List, Set, Tuple, Union


def materialize_lines(lines: List[str], indentation: int) -> str:
	output = ""
	new_line_with_indent = "\n" + " " * indentation
	for i, line in enumerate(lines):
		if i != 0:
			output += new_line_with_indent
		output += line.replace('\n', new_line_with_indent)
	return output


def gen_from_template(dir: str, template_name: str, output_name: str, replacements: List[Tuple[str, Any, int]]):

	template_path = os.path.join(dir, template_name)
	output_path = os.path.join(dir, output_name)

	with open(template_path) as f:
		content = f.read()
	for placeholder, lines, indentation in replacements:
		with open(output_path, "w") as f:
			content = content.replace(placeholder, materialize_lines(lines, indentation))
			f.write(content)


def find_file_paths(dir_paths: List[str], files_to_exclude: Set[str]) -> Set[str]:
	paths: Set[str] = set()
	for dir_path in dir_paths:
		all_files = os.listdir(dir_path)
		python_files = {fname for fname in all_files if ".py" == fname[-3:]}
		filter_files = {fname for fname in python_files if fname not in files_to_exclude}
		paths.update({os.path.join(dir_path, fname) for fname in filter_files})
	return paths


def extract_method_name(line: str) -> str:
	start_token = "class "
	end_token = "("
	start, end = line.find(start_token) + len(start_token), line.find(end_token)
	return line[start:end]


def parse_datapipe_file(file_path: str) -> Tuple[Dict[str, str], Dict[str, str], Set[str], Dict[str, List[str]]]:
	bracket_count = 0
	curr_token = ""
	res = []
	for char in line:
		if char == "[":
			bracket_count += 1
		elif char == "]":
			bracket_count -= 1
		elif char == delimiter and bracket_count == 0:
			res.append(curr_token)
			curr_token = ""
			continue
		curr_token += char
	res.append(curr_token)
	return res


def process_signature(line: str) -> str:
	tokens: List[str] = split_outside_bracket(line)
	for i, token in enumerate(tokens):
		tokens[i] = token.strip(' ')
		if token == "cls":
			tokens[i] = "self"
		elif i > 0 and ("self" == tokens[i - 1]) and (tokens[i][0] != "*"):
			tokens[i] = ""
		elif "Callable =" in token:  # Remove default argument if it is a function
			head, default_arg = token.rsplit("=", 2)
			tokens[i] = head.strip(' ') + "= ..."
	tokens = [t for t in tokens if t != ""]
	line = ', '.join(tokens)
	return line


def get_method_definitions(file_path: Union[str, List[str]],
						   files_to_exclude: Set[str],
						   deprecated_files: Set[str],
						   default_output_type: str,
						   method_to_special_output_type: Dict[str, str],
						   root: str = "") -> List[str]:
	if root == "":
		root = str(pathlib.Path(__file__).parent.resolve())
	file_path = [file_path] if isinstance(file_path, str) else file_path
	file_path = [os.path.join(root, path) for path in file_path]
	file_paths = find_file_paths(file_path,
								 files_to_exclude=files_to_exclude.union(deprecated_files))
	methods_and_signatures, methods_and_class_names, methods_w_special_output_types, methods_and_doc_strings = \
		parse_datapipe_files(file_paths)

	for fn_name in method_to_special_output_type:
		if fn_name not in methods_w_special_output_types:
			methods_w_special_output_types.add(fn_name)

	method_definitions = []
	for method_name, arguments in methods_and_signatures.items():
		class_name = methods_and_class_names[method_name]
		if method_name in methods_w_special_output_types:
			output_type = method_to_special_output_type[method_name]
		else:
			output_type = default_output_type
		doc_string = "".join(methods_and_doc_strings[method_name])
		if doc_string == "":
			doc_string = "	...\n"
		method_definitions.append(f"# Functional form of '{class_name}'\n"
								  f"def {method_name}({arguments}) -> {output_type}:\n"
								  f"{doc_string}")
	method_definitions.sort(key=lambda s: s.split('\n')[1])  # sorting based on method_name

	return method_definitions


iterDP_file_path: str = "iter"
iterDP_files_to_exclude: Set[str] = {"__init__.py", "utils.py"}
iterDP_deprecated_files: Set[str] = set()
iterDP_method_to_special_output_type: Dict[str, str] = {"demux": "List[IterDataPipe]", "fork": "List[IterDataPipe]"}

mapDP_file_path: str = "map"
mapDP_files_to_exclude: Set[str] = {"__init__.py", "utils.py"}
mapDP_deprecated_files: Set[str] = set()
mapDP_method_to_special_output_type: Dict[str, str] = {"shuffle": "IterDataPipe"}


def main() -> None:
	iter_method_definitions = get_method_definitions(iterDP_file_path, iterDP_files_to_exclude, iterDP_deprecated_files,
													 "IterDataPipe", iterDP_method_to_special_output_type)

	map_method_definitions = get_method_definitions(mapDP_file_path, mapDP_files_to_exclude, mapDP_deprecated_files,
													"MapDataPipe", mapDP_method_to_special_output_type)

	path = pathlib.Path(__file__).parent.resolve()
	replacements = [('${IterDataPipeMethods}', iter_method_definitions, 4),
					('${MapDataPipeMethods}', map_method_definitions, 4)]
	gen_from_template(dir=str(path),
					  template_name="datapipe.pyi.in",
					  output_name="datapipe.pyi",
					  replacements=replacements)


if __name__ == '__main__':
	main()

<END>

<START>
import operator

import torch
import torch.nn as nn
import torch.nn.functional as F
toq = torch.ops.quantized

import torch.ao.nn.quantized as nnq
import torch.ao.nn.quantized.dynamic as nnqd
import torch.ao.nn.intrinsic.quantized as nniq
import torch.ao.nn.intrinsic.quantized.dynamic as nniqd
import torch.ao.nn.intrinsic.qat as nniqat
import torch.ao.nn.intrinsic as nni
import torch.ao.nn.qat as nnqat
import torch.ao.nn.qat.dynamic as nnqatd
from torch.ao.quantization.backend_config import get_native_backend_config
import torch.ao.quantization.fx._lower_to_native_backend as \
	_lower_to_native_backend
import torch.ao.quantization.quantization_mappings as quantization_mappings

from .ns_types import NSNodeTargetType

from typing import Callable, Dict, List, Optional, Set, Tuple


def get_base_name_to_sets_of_related_ops() -> Dict[str, Set[NSNodeTargetType]]:
	sets_of_related_ops: List[Set[NSNodeTargetType]] = [
		{
			nn.Conv1d,
		},
		{
			nn.Conv2d,
		},
		{
			nn.Conv3d,
		},
		{
			F.conv1d,
		},
		{
			F.conv2d,
		},
		{
			F.conv3d,
		},
		{
			nn.Linear,
		},
		{
			F.linear,
		},
		{
			nn.AvgPool1d,
			torch.avg_pool1d,
		},
		{
			nn.AvgPool2d,
			torch._C._nn.avg_pool2d,
		},
		{
			nn.AvgPool3d,
			torch._C._nn.avg_pool3d,
		},
		{
			nn.AdaptiveAvgPool1d,
			F.adaptive_avg_pool1d,
		},
		{
			nn.AdaptiveAvgPool2d,
			F.adaptive_avg_pool2d,
		},
		{
			nn.AdaptiveAvgPool3d,
			F.adaptive_avg_pool3d,
		},
		{
			nn.LSTM,
		},
		{
			torch.add,
			operator.add,  # x + y
		},
		{
			torch.cat,
		},
		{
			torch.mul,
			operator.mul,
		},
		{
			F.relu,
			nn.ReLU,
			'relu',
			'relu_',
			torch.relu,
		},
		{
			nn.MaxPool1d,
			F.max_pool1d,
		},
		{
			nn.MaxPool2d,
			F.max_pool2d,
		},
		{
			nn.MaxPool3d,
			F.max_pool3d,
		},
		{
			torch.sigmoid,
			'sigmoid',
			'sigmoid_',
			nn.Sigmoid,
			F.sigmoid,
		},
		{
			nn.BatchNorm2d,
		},
		{
			nn.BatchNorm3d,
		},
		{
			nn.ConvTranspose1d,
		},
		{
			nn.ConvTranspose2d,
		},
		{
			nn.ConvTranspose3d,
		},
		{
			F.conv_transpose1d,
		},
		{
			F.conv_transpose2d,
		},
		{
			F.conv_transpose3d,
		},
		{
			nn.ELU,
		},
		{
			nn.Embedding,
		},
		{
			nn.EmbeddingBag,
		},
		{
			nn.GroupNorm,
		},
		{
			nn.Hardswish,
		},
		{
			nn.InstanceNorm1d,
		},
		{
			nn.InstanceNorm2d,
		},
		{
			nn.InstanceNorm3d,
		},
		{
			nn.LayerNorm,
		},
		{
			nn.LeakyReLU,
		},
		{
			nn.ReLU6,
			F.relu6,
		},
		{
			F.elu,
		},
		{
			F.hardswish,
		},
		{
			F.group_norm,
		},
		{
			F.instance_norm,
		},
		{
			F.layer_norm,
		},
		{
			F.leaky_relu,
		},
		{
			nn.SiLU,
			F.silu,
		},
		{
			nn.Mish,
			F.mish,
		},
		{
			nn.Tanh,
			F.tanh,
			torch.tanh,
			'tanh_',
			'tanh',
		},
		{
			'hardsigmoid_',
			'hardsigmoid',
			F.hardsigmoid,
			nn.Hardsigmoid,
		},
		{
			nn.Hardtanh,
			F.hardtanh,
			F.hardtanh_,
		},
		{
			operator.floordiv,
		},
		{
			torch.unsqueeze,
		},
		{
			torch.stack,
		},
		{
			torch.squeeze,
		},
		{
			torch.sort,
		},
		{
			torch.repeat_interleave,
		},
		{
			torch.min,
		},
		{
			torch.mean,
		},
		{
			torch.max,
		},
		{
			torch.transpose,
		},
		{
			torch.flatten,
		},
		{
			torch.clamp,
		},
		{
			torch.chunk,
		},
		{
			torch.nn.functional.interpolate,
		},
		{
			nn.Dropout,
		},
		{
			F.dropout,
		},
		{
			torch.matmul,
		},
		{
			nn.Softmax,
		},
		{
			nn.PReLU,
			nnq.PReLU,
		},
		{
			F.prelu,
			toq.prelu,
		},
		{
			nn.PixelShuffle,
		},
		{
			F.pixel_shuffle,
		},
		{
			nn.PixelUnshuffle,
		},
		{
			F.pixel_unshuffle,
		},
		{
			torch.narrow,
		},
	]

	backend_config = get_native_backend_config()

	new_connections: List[Tuple[Callable, Callable]] = [
		(nn.Linear, nn.modules.linear.NonDynamicallyQuantizableLinear),
	]

	for pattern, config in backend_config._pattern_complex_format_to_config.items():

		first_element = pattern
		while isinstance(first_element, (list, tuple)):
			first_element = first_element[-1]

		if config.fused_module is not None:
			new_connections.append((first_element, config.fused_module))

		if config.qat_module is not None:
			new_connections.append((first_element, config.qat_module))

		if config.reference_quantized_module is not None:
			new_connections.append((first_element, config.reference_quantized_module))


	for source_to_target in (
		_lower_to_native_backend.STATIC_LOWER_MODULE_MAP,
		_lower_to_native_backend.DYNAMIC_LOWER_MODULE_MAP,
		_lower_to_native_backend.WEIGHT_ONLY_LOWER_MODULE_MAP,
		_lower_to_native_backend.SPECIAL_PATTERN_LOWER_MODULE_MAP,
	):
		for source, target in source_to_target.items():  # type: ignore[attr-defined]
			new_connections.append((source, target))

	for source_to_double_target in (
		_lower_to_native_backend.STATIC_LOWER_FUSED_MODULE_MAP,
		_lower_to_native_backend.STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP,
		_lower_to_native_backend.DYNAMIC_LOWER_FUSED_MODULE_MAP,
	):
		for source, (target1, target2) in source_to_double_target.items():  # type: ignore[attr-defined]
			new_connections.append((source, target1))
			new_connections.append((source, target2))


	for source, (target1, target2) in \
			_lower_to_native_backend.STATIC_LOWER_FUNCTIONAL_MAP.items():
		new_connections.append((source, target1))
		new_connections.append((source, target2))

	for source_to_target in (
		_lower_to_native_backend.QBIN_OP_MAPPING,
		_lower_to_native_backend.QBIN_RELU_OP_MAPPING,
		quantization_mappings.DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS,
	):
		for source, target in source_to_target.items():
			new_connections.append((source, target))

	for source_to_target in (
		quantization_mappings.DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS,
	):
		for source, target in source_to_target.items():
			new_connections.append((source, target))


	for item1, item2 in new_connections:
		for set_of_related_ops in sets_of_related_ops:
			if item1 in set_of_related_ops or item2 in set_of_related_ops:
				set_of_related_ops.add(item1)
				set_of_related_ops.add(item2)
				break

	base_name_to_sets_of_related_ops: Dict[str, Set[NSNodeTargetType]] = {}

	counter = 0
	for set_of_related_ops in sets_of_related_ops:
		base_name = str(counter)
		counter += 1
		base_name_to_sets_of_related_ops[base_name] = set_of_related_ops

	return base_name_to_sets_of_related_ops


def get_base_name_for_op(
	base_name_to_sets_of_related_ops: Dict[str, Set[NSNodeTargetType]],
	op: NSNodeTargetType,
) -> Optional[str]:
	for base_name, set_of_related_ops in base_name_to_sets_of_related_ops.items():
		if op in set_of_related_ops:
			return base_name
	return None


def add_op_to_sets_of_related_ops(
	base_name_to_sets_of_related_ops: Dict[str, Set[NSNodeTargetType]],
	op: NSNodeTargetType,
	related_op: Optional[NSNodeTargetType],
) -> None:
	if related_op is not None:
		for set_of_related_ops in base_name_to_sets_of_related_ops.values():
			if related_op in set_of_related_ops:
				set_of_related_ops.add(op)
				return
		raise AssertionError(f"{related_op} was not found")
	else:
		counter = 0
		while str(counter) in base_name_to_sets_of_related_ops:
			counter += 1
		base_name_to_sets_of_related_ops[str(counter)] = {op}


def get_node_type_to_io_type_map() -> Dict[str, Set[NSNodeTargetType]]:
	FUNS_IO_TYPE_FP32: Set[NSNodeTargetType] = {
		F.linear,
		F.conv1d,
		F.conv2d,
		F.conv3d,
		torch.cat,
		F.elu,
		F.hardswish,
		F.instance_norm,
		F.layer_norm,
		F.leaky_relu,
		F.dropout,
		F.silu,
		F.mish,
		operator.add,
		torch.add,
		operator.mul,
		torch.mul,
		torch.sum,
		F.prelu,
	}

	FUNS_IO_TYPE_FP16: Set[NSNodeTargetType] = set()

	FUNS_IO_TYPE_INT8: Set[NSNodeTargetType] = {
		toq.linear,
		toq.linear_relu,
		toq.conv1d,
		toq.conv1d_relu,
		toq.conv2d,
		toq.conv2d_relu,
		toq.conv3d,
		toq.conv3d_relu,
		toq.cat,
		toq.elu,
		toq.hardswish,
		toq.instance_norm,
		toq.layer_norm,
		toq.leaky_relu,
		toq.dropout,
		toq.prelu,
	}

	FUNS_IO_TYPE_FP32_OR_INT8: Set[NSNodeTargetType] = {
		F.relu,
		F.tanh,
		torch.tanh,
		F.sigmoid,
		torch.sigmoid,
		F.hardsigmoid,
		operator.floordiv,
		torch.adaptive_avg_pool1d,
		F.adaptive_avg_pool2d,
		F.adaptive_avg_pool3d,
		F.dropout,
		F.hardtanh,
		F.hardtanh_,
		F.interpolate,
		F.max_pool1d,
		F.max_pool2d,
		F.max_pool3d,
		F.relu6,
		F.pixel_shuffle,
		F.pixel_unshuffle,
		torch.avg_pool1d,
		torch._C._nn.avg_pool2d,
		torch._C._nn.avg_pool3d,
		torch.cat,
		torch.chunk,
		torch.clamp,
		torch.flatten,
		torch.transpose,
		torch.max,
		torch.mean,
		torch.min,
		torch.narrow,
		torch.repeat_interleave,
		torch.sort,
		torch.squeeze,
		torch.stack,
		torch.unsqueeze,
		operator.add,
	}

	MODS_IO_TYPE_FP32: Set[NSNodeTargetType] = {
		nn.Linear,
		nnqat.Linear,
		nnqatd.Linear,
		nnqd.Linear,
		torch.nn.modules.linear.NonDynamicallyQuantizableLinear,
		nn.Conv1d,
		nn.Conv2d,
		nn.Conv3d,
		nnqat.Conv1d,
		nnqat.Conv2d,
		nnqat.Conv3d,
		nnqat.Embedding,
		nnqat.EmbeddingBag,
		nn.LSTM,
		nnqd.LSTM,
		nn.BatchNorm2d,
		nn.BatchNorm3d,
		nn.Dropout,
		nn.ConvTranspose1d,
		nn.ConvTranspose2d,
		nn.ConvTranspose3d,
		nn.ELU,
		nn.GroupNorm,
		nn.InstanceNorm1d,
		nn.InstanceNorm2d,
		nn.InstanceNorm3d,
		nn.LayerNorm,
		nn.Hardswish,
		nn.LeakyReLU,
		nn.ReLU6,
		nn.SiLU,
		nn.Mish,
		nn.Softmax,
		nn.PReLU,
		nni.BNReLU2d,
		nni.BNReLU3d,
		nni.ConvReLU1d,
		nni.ConvReLU2d,
		nni.ConvReLU3d,
		nni.LinearReLU,
		nni.LinearBn1d,
		nni.ConvBn1d,
		nni.ConvBn2d,
		nni.ConvBn3d,
		nniqat.ConvBn1d,
		nniqat.ConvBn2d,
		nniqat.ConvBn3d,
		nniqat.ConvBnReLU1d,
		nniqat.ConvBnReLU2d,
		nniqat.ConvBnReLU3d,
		nniqat.ConvReLU1d,
		nniqat.ConvReLU2d,
		nniqat.ConvReLU3d,
		nniqat.LinearReLU,
		nniqat.LinearBn1d,
		nniqd.LinearReLU,
		nni.LinearLeakyReLU,
		nni.LinearTanh,
		nni.ConvAdd2d,
		nni.ConvAddReLU2d,
	}

	MODS_IO_TYPE_INT8: Set[NSNodeTargetType] = {
		nnq.Linear,
		nnq.Conv1d,
		nnq.Conv2d,
		nnq.Conv3d,
		nnq.BatchNorm2d,
		nnq.BatchNorm3d,
		nnq.Dropout,
		nnq.ConvTranspose1d,
		nnq.ConvTranspose2d,
		nnq.ELU,
		nnq.InstanceNorm1d,
		nnq.InstanceNorm2d,
		nnq.InstanceNorm3d,
		nnq.LayerNorm,
		nnq.Hardswish,
		nnq.LeakyReLU,
		nnq.Embedding,
		nnq.EmbeddingBag,
		nnq.Dropout,
		nnq.Softmax,
		nnq.PReLU,
		nniq.BNReLU2d,
		nniq.BNReLU3d,
		nniq.ConvReLU1d,
		nniq.ConvReLU2d,
		nniq.ConvReLU3d,
		nniq.LinearReLU,
		nniq.LinearLeakyReLU,
		nniq.LinearTanh,
		nniq.ConvAdd2d,
		nniq.ConvAddReLU2d,
	}

	MODS_IO_TYPE_FP32_OR_INT8: Set[NSNodeTargetType] = {
		nn.ReLU,
		nn.Tanh,
		nn.Sigmoid,
		nn.Hardsigmoid,
		nn.AdaptiveAvgPool1d,
		nn.AdaptiveAvgPool2d,
		nn.AdaptiveAvgPool3d,
		nn.AvgPool1d,
		nn.AvgPool2d,
		nn.AvgPool3d,
		nn.Dropout,
		nn.Hardtanh,
		nn.Identity,
		nn.MaxPool1d,
		nn.MaxPool2d,
		nn.MaxPool3d,
		nn.PixelShuffle,
		nn.PixelUnshuffle,
		nn.ReLU6,
	}

	METHS_IO_TYPE_FP32_OR_INT8: Set[NSNodeTargetType] = {
		'sigmoid_',
		'sigmoid',
		'tanh_',
		'tanh',
		'hardsigmoid_',
		'hardsigmoid',
		'relu_',
		'relu',
	}

	return {
		'funs_io_type_fp32': FUNS_IO_TYPE_FP32,
		'funs_io_type_fp16': FUNS_IO_TYPE_FP16,
		'funs_io_type_int8': FUNS_IO_TYPE_INT8,
		'funs_io_type_fp32_or_int8': FUNS_IO_TYPE_FP32_OR_INT8,
		'mods_io_type_fp32': MODS_IO_TYPE_FP32,
		'mods_io_type_int8': MODS_IO_TYPE_INT8,
		'mods_io_type_fp32_or_int8': MODS_IO_TYPE_FP32_OR_INT8,
		'meths_io_type_fp32_or_int8': METHS_IO_TYPE_FP32_OR_INT8,
	}


def get_unmatchable_types_map() -> Dict[str, Set[NSNodeTargetType]]:

	FUNS_UNMATCHABLE: Set[NSNodeTargetType] = {
		torch.quantize_per_tensor,
		operator.getitem,
	}

	MODS_UNMATCHABLE: Set[NSNodeTargetType] = {
		nn.Identity,
	}

	METHS_UNMATCHABLE: Set[NSNodeTargetType] = {
		'to',
		'dequantize',
		'reshape',
		'view',
		'unsqueeze_',
		'unsqueeze',
		'transpose',
		'squeeze_',
		'squeeze',
		'size',
		'shape',
		'resize_',
		'repeat_interleave',
		'repeat',
		'permute',
		'numel',
		'mean',
		'detach_',
		'detach',
		'contiguous',
		'clamp',
		'chunk',
	}

	return {
		'funs_unmatchable': FUNS_UNMATCHABLE,
		'mods_unmatchable': MODS_UNMATCHABLE,
		'meths_unmatchable': METHS_UNMATCHABLE,
	}

<END>

<START>
import copy
import logging
import os
import re

from tensorboard.compat.proto.graph_pb2 import GraphDef
from tensorboard.compat.proto.node_def_pb2 import NodeDef
from tensorboard.compat.proto.tensor_shape_pb2 import TensorShapeProto

from caffe2.proto import caffe2_pb2
from caffe2.python import core, workspace

from typing import Set, Dict, Tuple, List

log = logging.getLogger(__name__)

def _make_unique_name(seen: Set[str], name: str, min_version: int = 0):
	assert name is not None
	i = min_version
	x = "%s_%d" % (name, i) if i else name
	while x in seen:
		i += 1
		x = "%s_%d" % (name, i)
	seen.add(x)
	return x


def _rename_tensorflow_style(shapes, blob_name_tracker, ops):
	WEIGHT = re.compile(r"(_w)$")
	WEIGHT_ = re.compile(r"(_w_)")
	BN = re.compile(r"(_bn)$")
	BN_ = re.compile(r"(_bn_)")
	BIAS = re.compile(r"(_b)$")
	BIAS_ = re.compile(r"(_b_)")
	SCALE = re.compile(r"(_s)$")
	SCALE_ = re.compile(r"(_s_)")
	SUM = re.compile(r"(_sum)$")
	SUM_ = re.compile(r"(_sum_)")
	BRANCH = re.compile(r"(_branch)")

	def f(name):
		inter_name = WEIGHT_.sub("/weight_", WEIGHT.sub("/weight", name))
		inter_name = BN_.sub("/batchnorm_", BN.sub("/batchnorm", inter_name))
		inter_name = BIAS_.sub("/bias_", BIAS.sub("/bias", inter_name))
		inter_name = SCALE_.sub("/scale_", SCALE.sub("/scale", inter_name))
		inter_name = SUM_.sub("/sum_", SUM.sub("/sum", inter_name))
		new_name = BRANCH.sub("/branch", inter_name)
		return new_name

	_rename_all(shapes, blob_name_tracker, ops, f)


def _convert_to_ssa(shapes, blob_name_tracker, ops):
	ir = core.IR(ops)
	seen: Set[str] = set()
	versioned: Dict[Tuple[str, int], int] = {}
	new_shapes = {}
	new_blob_name_tracker = {}

	def ssa_name(name: str, versions: Dict[str, int]) -> int:
		assert name in versions
		version = versions[name]
		if (name, version) in versioned:
			return versioned[(name, version)]
		new_name = _make_unique_name(seen, name, min_version=version)
		versioned[(name, version)] = new_name
		if name in shapes:
			new_shapes[new_name] = shapes[name]
		if blob_name_tracker and name in blob_name_tracker:
			new_blob_name_tracker[new_name] = blob_name_tracker[name]
		return new_name

	for (op, ssa) in zip(ops, ir.ssa):
		assert op is ssa.op
		inputs = list(op.input)
		outputs = list(op.output)
		del op.input[:]
		del op.output[:]
		op.input.extend(ssa_name(name, ssa.in_versions) for name in inputs)
		op.output.extend(ssa_name(name, ssa.out_versions) for name in outputs)

	shapes.clear()
	shapes.update(new_shapes)
	if blob_name_tracker:
		blob_name_tracker.clear()
		blob_name_tracker.update(new_blob_name_tracker)


def _get_blob_names(ops):
	names = set()
	for op in ops:
		names.update(op.input)
		names.update(op.output)
	return {name: name for name in names}


def _remap_keys(old_dict, rename_fn):
	new_dict = {rename_fn(key): value for key, value in old_dict.items()}
	old_dict.clear()
	old_dict.update(new_dict)


def _rename_all(shapes, blob_name_tracker, ops, rename_fn):
	seen: Set[str] = set()
	renamed: Dict[Tuple[str, int], int] = {}

	def g(name):
	For all operators or blobs with name containing "_grad", add a "GRADIENTS/" scope.

	Note: breaks graph execution since the blob -> gradient mapping is
	hardcoded.

	Args:
		shapes: Dictionary mapping blob names to their shapes/dimensions.
		blob_name_tracker: Dictionary of all unique blob names (with respect to
			some context).
		ops: List of Caffe2 operators

	Returns:
		None. Modifies shapes, blob_name_tracker and ops in-place by renaming.
	`:i` has a special meaning in Tensorflow. This function replaces all colons with $ to avoid any possible conflicts.

	Args:
		shapes: Dictionary mapping blob names to their shapes/dimensions.
		blob_name_tracker: Dictionary of all unique blob names (with respect to
			some context).
		ops: List of Caffe2 operators
		repl: String representing the text to replace ':' with. Usually this is
			'$'.

	Returns:
		None. Modifies blob_name_tracker in-place.

	Give missing operators a name.

	We expect C2 operators to be generally unnamed. This gives them a scope
	(inferred from their outputs) and a name after their type. Duplicates will
	be postfixed by an index.

	Args:
		ops: List of Caffe2 operators to assign names to.

	Returns:
		None: Modifies 'ops' in-place.
	Handle the devices.

	Args:
		device_option (caffe2_pb2.DeviceOption): DeviceOption protobuf,
			associated to an operator, that contains information such as
			device_type (optional), cuda_gpu_id (optional), node_name (optional,
			tells which node the operator should execute on). See caffe2.proto
			in caffe2/proto for the full list.

	Returns:
		Formatted string representing device information contained in
			device_option.
	Convert a list of ints to a TensorShapeProto representing the dimensions of a blob/object.

	Args:
		attr_dict: Dictionary to update (usually attributes of a Node)
		ints: List of integers representing dimensions of some object.

	Returns:
		None. Modifies attr_dict in-place.
	Add attributes to a node. Key is the arg.name, and values can be shape, floats, strings, ints or an empty list.

	Args:
		attr_dict: Dictionary to update (usually attributes of a Node)
		arg: Object with name and data fields.

	Returns:
		None. Modifies attr_dict in-place.
	Convert an operator to a node in a TF graph.

	Args:
		shapes: Dictionary mapping blob names to their shapes/dimensions.
		op: The Caffe2 operator to convert to a TF graph node.

	Returns:
		n: The TF graph node created from op.
	Convert the operators to nodes.

	Args:
		op: Caffe2 operator to convert to node
		inter_blobs: Set of intermediate blobs
		seen: Names that have already been used and are not unique

	Returns:
		nodes: Nodes representing 'op' and the outputs of 'op'
	Convert a blob (operator input or output) to a node in a TF graph.

	Args:
		producing_ops: Dictionary of blob name to list of
			(producing_op, blob_index within producing_op.output) mapping.
		shapes: Dictionary mapping blob names to their shapes/dimensions.
		name: String representing the name of this blob.

	Returns:
		n: The TF graph node created from this blob.
	Remove debug information from operators, they are copious.

	Args:
		ops: List of Caffe2 operators
		perform_clear: Boolean passed from _operators_to_graph_def specifying
			whether to remove the debug information. This boolean is passed into
			this function to reduce the complexity of _operators_to_graph_def.

	Returns:
		None. Modifies the list of Caffe2 operators in-place and removes the
		'debug_info' field.

	Blobs with names containing '_m' or 'grad' are part of the backward pass.

		This function references facebookresearch/Detectron/detectron/utils/net.py.

	Args:
		blob: The blob to inspect

	Returns:
		Boolean representing whether this blob is part of the forward pass
	Check if the blob's name starts with '_gpu'.

	Args:
		blob: The blob to inspect

	Returns:
		Boolean representing whether this blob is associated with a gpu
	Find the input, intermediate and output nodes of a set of operators.

	Args:
		ops: List of Caffe2 operators to look through

	Returns:
		input_blobs: The input nodes of the set of operators
		inter_blobs: The intermediate nodes of the set of operators
		output_blobs: The output nodes of the set of operators
	Filter unwanted operators based on criteria in 'filter_fn'.

	Args:
		ops: List of Caffe2 operators to filter
		filter_fn: Criteria function for whether inputs/outputs in an operator
			should be filtered.
		perform_filter: Boolean passed from _operators_to_graph_def specifying
			whether to filter operators

	Returns:
		new_ops: Subset of ops containing a subset of their inputs and outputs.
	Convert a set of operators to a graph using the main function.

	Args:
		shapes: Dictionary mapping blob names to their shapes/dimensions.
		ops: List of Caffe2 operators, representing some computation graph
		colon_replacement: Symbol to replace ':' with. ':i' in TF has a special
			meaning, so we need to replace it with a non-conflicting symbol.
		with_ssa: Boolean
		with_gradient_scope: Boolean
		blob_name_tracker: Dictionary tracking names of blobs (inputs/outputs
			from operators)
		show_simplified: Whether to show a simplified version of the model graph
			Sets all of the following values:
				clear_debug_info: Boolean representing whether to silence debug
					info (which can be very verbose)
				show_forward_only: Boolean representing whether to only show
					blobs involved in the forward pass
				show_cpu_only: Boolean representing whether to only show blobs
					that are not associated with a gpu
				use_tensorflow_naming: Boolean representing whether to convert
					some common Caffe2 naming conventions to their Tensorflow
					counterparts
		custom_rename: Function string -> string that defines a custom
			renaming function to use.

	Returns:
		current_graph: GraphDef representing the computation graph formed by the
			set of operators.
	Propagate the device options from net to operators.

	Args:
		net_def: A caffe2_pb2.NetDef representing a computation graph. The graph
			consists of Caffe2 operators.

	Returns:
		None. Iterates through all ops contained within the net. For each op,
			modifies the op device_option in-place to be the net device_option
			if the op has no pre-existing device_option, and leaves the op as-is
			if it already has a device_option.
	Get missing shapes for all blobs contained in the nets.

	Args:
		nets: List of core.Net to extract blob shape information from.

	Returns:
		Dictionary containing blob name to shape/dimensions mapping. The net
			is a computation graph that is composed of operators, and the
			operators have input and output blobs, each with their own dims.
	Convert a Caffe2 model to a Tensorflow graph.

	This function extracts 'param_init_net' and 'net' from the model and passes it to nets_to_graph()
	for further processing.

	Args:
		model (cnn.CNNModelHelper, model_helper.ModelHelper): The model to
			extract the nets (instances of core.Net) from.

	Returns:
		Call to nets_to_graph_def() with extracted 'param_init_net', 'net' and
			**kwargs. See _operators_to_graph_def for detailed **kwargs.
	Convert a set of Caffe2 nets to a Tensorflow graph.

	Args:
		nets: List of core.Nets. core.Net is a wrapper around a NetDef protobuf.
			The corresponding protobuf can be extracted using .Proto().
		shapes: Dictionary mapping blob names to their shapes/dimensions.

	Returns:
		Call to protos_to_graph_def() with the extracted NetDef protobufs and
			**kwargs. See _operators_to_graph_def for detailed **kwargs.
	Convert a set of Caffe2 net definitions to a Tensorflow graph.

	Args:
		net_defs: List of caffe2_pb2.NetDef protobufs representing computation
			graphs.
		shapes: Dictionary mapping blob names to their shapes/dimensions.

	Returns:
		Call to _operators_to_graph_def() with the extracted operators from the
			NetDefs and **kwargs. See _operators_to_graph_def for detailed
			**kwargs.

<END>

<START>

	The function will run the op in ONNX Runtime and PyTorch and compare the
	results. It doesn't break the exporting process, but saves each op validated
	result into SARIF, under the section of `fx_onnx_interpreter`.

	There are three signs can be found:
	1. Blue: Pass
	2. Yellow: Bypass

	Args:
		node (torch.fx.Node): The validated fx.node
		symbolic_fn (Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]): The corresponded ONNX node
		torch_args (list): torch argument inputs
		torch_kwargs (dict): torch keyword argument inputs
		fx_graph_module (torch.fx.GraphModule): The fx.GraphModule that contains the nodes

	Args:
		shape (torch.Size): The shape of a tensor
	Raises:
		ValueError: When SymInt is found in shape
	Returns:
		torch.Size: The shape of a tensor with SymInt converted to int

	wrapped_args: List[fx_type_utils.Argument] = []
	for arg in fx_args:
		if isinstance(arg, torch.fx.Node):
			fake_tensor = arg.meta.get("val")
			if fake_tensor is None and arg.op == "get_attr":
				fake_tensor = getattr(fx_graph_module, arg.target)  # type: ignore[operator]
			if isinstance(fake_tensor, torch.Tensor):
				real_tensor = generate_random_tensors(
					fake_tensor.shape, fake_tensor.dtype
				)
				wrapped_args.append(real_tensor)
			elif isinstance(fake_tensor, (int, float, bool)):
				wrapped_args.append(fake_tensor)
			elif symbolic_shapes.has_hint(fake_tensor):
				wrapped_args.append(symbolic_shapes.hint_int(fake_tensor))
			else:
				raise ValueError(
					f"Unexpected input argument type found inside fx.Node. arg: {arg}; "
					f"arg.meta['val']/get_attr: {fake_tensor}; type(arg.meta['val']/get_attr): "
					f"{type(fake_tensor)}."
				)
		elif isinstance(arg, Sequence):
			wrapped_args.append(_fx_args_to_torch_args(arg, fx_graph_module))
		elif isinstance(arg, (int, float, torch.dtype)) or arg is None:
			wrapped_args.append(arg)
		elif isinstance(arg, torch.device):
			wrapped_args.append(str(arg))
		else:
			raise ValueError(
				f"Unexpected input argument type is found in node arguments. arg: {arg}; "
			)

	return wrapped_args


@_beartype.beartype
def _wrap_fx_args_as_torch_args(
	fx_args: List[fx_type_utils.Argument],
	fx_kwargs: Dict[str, fx_type_utils.Argument],
	fx_graph_module: torch.fx.GraphModule,
) -> Tuple[List[fx_type_utils.Argument], Dict[str, fx_type_utils.Argument]]:

	NOTE: This is different from the param_schema separating in dispatcher, since at this point
	we are already sure that the args and kwargs are in order and matched.

	Args:
		param_schemas: The parameter schemas of an Op or a OnnxFunction.
		args: The Python positional arguments supplied by the caller.
		kwargs: The Python keyword arguments supplied by the caller.
		allow_extra_kwargs: Whether to allow extra keyword arguments.
			When set to True, extra/unknown arguments will be ignored.

	Returns:
		A tuple of two elements:
		- A list of Python positional argument.
		- An ordered dictionary of Python keyword argument names and its values.

	Raises:
		TypeError: When allow_extra_kwargs is False and there are unknown kwargs.
		TypeError: When a required input is not provided.

<END>

<START>
import contextlib
import functools
import warnings
from typing import Callable, Optional

import torch
from torch._library.utils import Kernel, RegistrationHandle


class AbstractImplHolder:

		Returns a RegistrationHandle that one can use to de-register this
		abstract impl.
	Context object for writing abstract implementations for custom operators.

		This is useful for writing the abstract implementation (which is necessary
		for torch.compile) for a CustomOp where an output Tensor has a size
		that depends on the data of the input Tensors.

		Args:
			min (int): A statically known inclusive lower bound for this symint. Default: 0
			max (Optional[int]): A statically known inclusive upper bound for this
				symint. Default: None

		.. warning:

			It is important that the ``min`` and ``max`` (if not None) values are set
			correctly, otherwise, there will be undefined behavior under
			torch.compile. The default value of ``min`` is 2 due to torch.compile
			specializing on 0/1 sizes.

			You must also verify that your implementation on concrete Tensors
			(e.g. CPU/CUDA) only returns Tensors where the size that corresponds
			to the symint also has respects these constraint.
			The easiest way to do this is to add an assertion in the CPU/CUDA/etc
			implementation that the size follows these bounds.

		Example::

			>>> # An operator with data-dependent output shape
			>>> lib = torch.library.Library("mymodule", "FRAGMENT")
			>>> lib.define("mymodule::custom_nonzero(Tensor x) -> Tensor")
			>>>
			>>> @torch.library.impl_abstract("mymodule::custom_nonzero")
			>>> def custom_nonzero_abstract(x):
			>>>	 # Number of nonzero-elements is data-dependent.
			>>>	 # Since we cannot peek at the data in an abstract impl,
			>>>	 # we use the ctx object to construct a new symint that
			>>>	 # represents the data-dependent size.
			>>>	 ctx = torch.library.get_ctx()
			>>>	 nnz = ctx.new_dynamic_size()
			>>>	 shape = [nnz, x.dim()]
			>>>	 result = x.new_empty(shape, dtype=torch.int64)
			>>>	 return result
			>>>
			>>> @torch.library.impl(lib, "custom_nonzero", "CPU")
			>>> def custom_nonzero_cpu(x):
			>>>	 x_np = x.numpy()
			>>>	 res = np.stack(np.nonzero(x_np), axis=1)
			>>>	 return torch.tensor(res, device=x.device)


<END>

<START>
import os

import torch
import torch._lazy
import torch._lazy.metrics
import torch._lazy.ts_backend
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torchvision import datasets, transforms

torch._lazy.ts_backend.init()


class Net(nn.Module):
	def __init__(self):
		super().__init__()
		self.conv1 = nn.Conv2d(1, 32, 3, 1)
		self.conv2 = nn.Conv2d(32, 64, 3, 1)
		self.dropout1 = nn.Dropout(0.25)
		self.dropout2 = nn.Dropout(0.5)
		self.fc1 = nn.Linear(9216, 128)
		self.fc2 = nn.Linear(128, 10)

	def forward(self, x):
		x = self.conv1(x)
		x = F.relu(x)
		x = self.conv2(x)
		x = F.relu(x)
		x = F.max_pool2d(x, 2)
		x = self.dropout1(x)
		x = torch.flatten(x, 1)
		x = self.fc1(x)
		x = F.relu(x)
		x = self.dropout2(x)
		x = self.fc2(x)
		output = F.log_softmax(x, dim=1)
		return output


def train(log_interval, model, device, train_loader, optimizer, epoch):
	model.train()
	for batch_idx, (data, target) in enumerate(train_loader):
		data, target = data.to(device), target.to(device)
		optimizer.zero_grad(set_to_none=True)
		output = model(data)
		loss = F.nll_loss(output, target)
		loss.backward()
		optimizer.step()
		torch._lazy.mark_step()

		if batch_idx % log_interval == 0:
			print(
				"Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}".format(
					epoch,
					batch_idx * len(data),
					len(train_loader.dataset),
					100.0 * batch_idx / len(train_loader),
					loss.item(),
				)
			)


if __name__ == "__main__":
	bsz = 64
	device = "lazy"
	epochs = 14
	log_interval = 10
	lr = 1
	gamma = 0.7
	train_kwargs = {"batch_size": bsz}
	if "LTC_TS_CUDA" in os.environ:
		cuda_kwargs = {
			"num_workers": 1,
			"pin_memory": True,
			"shuffle": True,
			"batch_size": bsz,
		}
		train_kwargs.update(cuda_kwargs)

	transform = transforms.Compose(
		[transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
	)
	dataset1 = datasets.MNIST("./data", train=True, download=True, transform=transform)
	train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)
	model = Net().to(device)
	optimizer = optim.Adadelta(model.parameters(), lr=lr)
	scheduler = StepLR(optimizer, step_size=1, gamma=gamma)
	for epoch in range(1, epochs + 1):
		train(log_interval, model, device, train_loader, optimizer, epoch)
		scheduler.step()

<END>

<START>
from torch.distributions import constraints
from torch.distributions.normal import Normal
from torch.distributions.transformed_distribution import TransformedDistribution
from torch.distributions.transforms import StickBreakingTransform

__all__ = ["LogisticNormal"]


class LogisticNormal(TransformedDistribution):
	arg_constraints = {"loc": constraints.real, "scale": constraints.positive}
	support = constraints.simplex
	has_rsample = True

	def __init__(self, loc, scale, validate_args=None):
		base_dist = Normal(loc, scale, validate_args=validate_args)
		if not base_dist.batch_shape:
			base_dist = base_dist.expand([1])
		super().__init__(
			base_dist, StickBreakingTransform(), validate_args=validate_args
		)

	def expand(self, batch_shape, _instance=None):
		new = self._get_checked_instance(LogisticNormal, _instance)
		return super().expand(batch_shape, _instance=new)

	@property
	def loc(self):
		return self.base_dist.base_dist.loc

	@property
	def scale(self):
		return self.base_dist.base_dist.scale

<END>

<START>
import sys
import warnings
from typing import cast, List, Optional, Tuple, TYPE_CHECKING, Union

import torch
import torch.distributed as dist
import torch.distributed.distributed_c10d as c10d
from torch._custom_ops import impl_abstract
from torch.distributed.device_mesh import DeviceMesh
from torch.fx.experimental.proxy_tensor import get_innermost_proxy_mode

from . import _functional_collectives_impl as fun_col_impl
from ._functional_collectives_impl import _register_tensor_wrapper

try:
	from torch.utils._cxx_pytree import tree_map_only
except ImportError:
	from torch.utils._pytree import tree_map_only  # type: ignore[no-redef]


if torch._running_with_deploy():

	def is_torchdynamo_compiling():
New traceable, functional collectives.
RFC: https://github.com/pytorch/pytorch/issues/93173

  compiler: trace these ops with plain-old-data schemas, then choose how to lower them.
  eager: execute these 'functional' ops which in eager return AsyncCollectiveTensor subclasses,
		 automatically calling .wait() on underlying/hidden async 'work' obj only when fed to
		 a downstream op.

Issues:
* Where should these ops live? Couldn't `import torch` if putting these ops in existing torch.distributed files
* Proper support for eager requires inplace ops. We should explore having it as an option for the API.
Functional collectives are asynchronous only and we perform implicit stream synchronization
on behalf of the user.

We use AsyncCollectiveTensor to wrap the result tensor of a collective and it lets us witness
first usage of the tensor and insert cross stream sync at the right place.

The above are the easy bits, the hard one is how we match the Work object returned by
c10d and the tensor AsyncCollectiveTensor wraps. We alloc the tensor inside the collective
op implementation (see ``clone()`` call in ``_all_reduce``) and then it's handled by the
dispatcher which might call other implementations that are allowed to change the returned
tensor - even return a tensor with a different shape (see ``torch.vmap``).

This means the caller of our ops receives a Tensor that is not guaranteed to be the same
allocated by our implementations and that makes pairing The AsyncTensor to the original
tensor a lot harder. This pairing is needed so we can lookup the Work object to use.

Originally, we tried WeakKeyDictionary to map from Tensor to Work, but because Tensor's
identity is not stable across dispatch, the op caller would end up with a different Tensor
instance that would not match any in the dictionary.

With Tensor identity out of the question, we decided use the tensor data pointer, which
should be stable across all the Tensor changes done during dispatch.

We have a dictionary of tensor::data_ptr -> Work that we insert right after we call into c10d.

We use this dictionary when AsyncCollectiveTensor is used to invoke Work::wait()

Finally, we setup a finalizer against the tensor wrapper to observe it getting collected so we
can clean up stale entries in the dictionary.

To eliminate the possibility of races we have a global version counter that is used by the finalizer.

As a wise man said once: Don't cross the streams (https://www.youtube.com/watch?v=wyKQe_i9yyo)

Functional collectives can accept any of these types to describe the ranks participating in collectives.

The different types will be desugared to a canonical format
User facing APIs for functional collectives
-------------------------------------------

These apis are called by user code and expected to work both in eager execution and compilation,
but there are significant differences to how the two modes are implemented underneath.

Eager execution is 'optimized' using a tensor subclass that schedules the synchronization (via wait_tensor() op)
just before the tensor is first used.  Compiled tracing currently relies on the compiler to perform this optimization,
and cannot yet correctly trace the AsyncTensor wrapper class.  In the future, these paths may be unified
if sufficient subclass support is added in dynamo.

Example: all_reduce is an entrypoint API, and other collectives follow a similar pattern.

Here's how it works under torch.compile/dynamo:
all_reduce(...)
  |--> _expand_group(...)			   - desugars processgroup into canonical/traceable format
  |--> c10d_functional.all_reduce(...)  - dynamo captures this op call, doesn't trace deeper
  |--> _maybe_wrap_tensor(...)		  - wait_tensor() op is immediately called, no AsyncTensor subclass needed

And under eager execution:
all_reduce(...)
  |--> _expand_group(...)			   - same as above, but less critical for eager
  |--> c10d_functional.all_reduce(...)  - dispatches to real kernel OR records op in trace
  |--> _maybe_wrap_tensor(...)		  - AsyncTensor wrapper applied to returned tensor,
										  which issues wait_tensor() at the time of first use
	Wait on a tensor returned by the collectives ops.

	Waiting follows device semantics, which means blocking on CPU and synchronizing streams on CUDA.
	Broadcasts the tensor to all processes in the given process group.

	Args:
		src (int): Source rank
		group (ProcessGroup or List[int]): The process group to work on.
		tag (str, optional): A unique identifier for the collective. Default: empty string
	Reduces the tensor data across all machines in such a way that all get
	the final result.

	The input tensor is left unmodified.

	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

	:: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
	that information and perform collective algebraic optimization. Use other forms of input for that.
	Gather tensor data across from all machines and concatenate over ``gather_dim``.

	Note that it currently only supports gather_dim = 0.

	The input tensor is left unmodified.
	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

	:: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
	that information and perform collective algebraic optimization. Use other forms of input for that.
	Reduces the tensor data across all machines in such a way that all get
	the final result, then scatter the results to corresponding ranks.


	The input tensor is left unmodified.
	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh
	:: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
	that information and perform collective algebraic optimization. Use other forms of input for that.
	Reduces a list of tensors across all machines in such a way that all get
	the final result.

	The all tensors in the input list are left unmodified.

	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

	:: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
	that information and perform collective algebraic optimization. Use other forms of input for that.
	Gather a list of tensors across from all machines.

	Note that it currently only supports gather_dim = 0.

	The input tensor is left unmodified.
	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

	:: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
	that information and perform collective algebraic optimization. Use other forms of input for that.
	Reduces a list of tensors across all machines in such a way that all get
	the final result, then scatter the results to corresponding ranks.

	The input tensors are left unmodified.
	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

	:: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
	that information and perform collective algebraic optimization. Use other forms of input for that.
	Each process splits input tensor and then scatters the split list
	to all processes in a group. Then concatenate the received tensors from all
	the processes in the group and return single output tensor.

	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

	:: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
	that information and perform collective algebraic optimization. Use other forms of input for that.
	Permutes the elements of the tensor according to the given source/destination pairs. `src_dst` should
	be defined such that src_dst[m] == n means m sends to n.

	Group can be one of:
		List[int]: ranks participating in the collective.
		List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
		ProcessGroup: Will perform a collective using the ranks and tag of the PG.
		DeviceMesh: Do a SPMD collective over all ranks of the mesh
		(DeviceMesh, int): Do a MPMD collective over one
	A Tensor wrapper subclass that is used to trigger a call to wait
	prior to first use of the underlying tensor.
	Use it inside functional collective pytorch wrappers like the following:
	def functional_collective(self, group, tag):
		tag, rankset, group_size = _expand_group(group, tag)
		tensor = torch.ops.c10d_functional.{collective}(self, tag, rankset, group_size)
		return _maybe_wrap_tensor(tensor)
		return self.elem

	@classmethod
	def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
		if func == torch.ops.aten.view.default:
			res = func(args[0].elem, args[1])
			wrapper_res = AsyncCollectiveTensor(res)
			_register_tensor_wrapper(wrapper_res)
			return wrapper_res

		is_view_op = _is_view_op(func)

		def unwrap(e: AsyncCollectiveTensor):
			if not is_view_op:
				wait_tensor(e.elem)
			return e.elem

		def wrap(e: torch.Tensor):
			assert not isinstance(e, AsyncCollectiveTensor)
			res = AsyncCollectiveTensor(e)
			_register_tensor_wrapper(res)
			return res

		unwrapped_args = tree_map_only(AsyncCollectiveTensor, unwrap, args)
		unwrapped_kwargs = tree_map_only(AsyncCollectiveTensor, unwrap, kwargs)

		out = func(*unwrapped_args, **unwrapped_kwargs)

		if is_view_op:
			out = tree_map_only(torch.Tensor, wrap, out)

		return out

	def numpy(self):
		return self.wait().numpy()




def _expand_group(group: RANK_TYPES, tag: str = "") -> Tuple[str, List[int], int]:
	if TYPE_CHECKING:

		def cast_listlistint(x):
			return cast(List[List[int]], x)

		def cast_listint(x):
			return cast(List[int], x)

	else:
		def cast_listlistint(x):
			return x

		def cast_listint(x):
			return x

	rankset: List[int]
	if isinstance(group, list):
		if isinstance(group[0], list):
			nested_list = cast_listlistint(group)
			rankset = []
			group_size = -1
			for rs in nested_list:
				rankset.extend(rs)
				if group_size != -1 and group_size != len(rs):
					raise ValueError(
						f"group sizes must be identical found {group_size} and {len(rs)}"
					)
				group_size = len(rs)
		else:
			rankset = cast_listint(group)
			group_size = len(rankset)
	elif isinstance(group, dist.ProcessGroup):
		rankset = dist.get_process_group_ranks(group)
		group_size = len(rankset)
		tag = tag or c10d._get_group_tag(group)
	elif isinstance(group, DeviceMesh):
		assert (
			group.ndim == 1
		), "Only 1D mesh is supported, pass in (DeviceMesh, int) together if mesh > 1D"
		tag, rankset = group._dim_group_infos[0]
		group_size = len(rankset)
	elif isinstance(group, tuple):
		if (
			len(group) == 2
			and isinstance(group[0], DeviceMesh)
			and isinstance(group[1], int)
		):
			dmesh = group[0]
			dim = group[1]
			tag, rankset = dmesh._dim_group_infos[dim]
			group_size = len(rankset)
		else:
			raise ValueError("Invalid tuple for group must be (DeviceMesh, int)")
	else:
		raise ValueError(
			"Invalid type for group, must be one of List, Processgroup, DeviceMesh or (DeviceMesh, int)."
		)

	return (tag, rankset, group_size)


def _are_we_tracing() -> bool:
	if is_torchdynamo_compiling():
		return True
	if (
		torch._C._get_dispatch_mode(torch._C._TorchDispatchModeKey.FUNCTIONAL)
		is not None
	):
		return True
	mode = get_innermost_proxy_mode()
	if mode is None:
		return False
	return mode.tracer is not None


def _maybe_wrap_tensor(self) -> torch.Tensor:
	if _are_we_tracing():
		return wait_tensor(self)
	res = AsyncCollectiveTensor(self)
	_register_tensor_wrapper(res)
	return cast(torch.Tensor, res)


def _all_gather_into_tensor_coalesced_meta(self, tag, rankset, group_size):
	def mk_out_tensor(shard):
		out_size = list(shard.size())
		out_size[0] *= group_size
		out_tensor = shard.new_empty(out_size)
		return out_tensor

	return [mk_out_tensor(t) for t in self]


def _broadcast_meta(self, *args):
	return torch.empty_like(self)


def _all_reduce_meta(self, *args):
	return torch.empty_like(self)


def _wait_tensor_meta(self, *args):
	return torch.empty_like(self)


def _all_gather_into_tensor_meta(shard, tag, rankset, group_size):
	out_size = list(shard.size())
	out_size[0] *= group_size
	return shard.new_empty(out_size)


def _reduce_scatter_tensor_meta(input, reduce_op, tag, rankset, group_size):
	out_size = list(input.size())
	out_size[0] //= group_size
	return input.new_empty(out_size)


def _all_reduce_coalesced_meta(self, *args):
	return [torch.empty_like(t) for t in self]


def _all_reduce__meta(inp, *args):
	return inp


def _all_reduce_coalesced__meta(inputs, *args):
	return inputs


def _reduce_scatter_tensor_coalesced_meta(inputs, reduceOp, tag, rankset, group_size):
	def mk_out_tensor(input):
		out_size = list(input.size())
		out_size[0] //= group_size
		out_tensor = input.new_empty(out_size)
		return out_tensor

	return [mk_out_tensor(t) for t in inputs]


def _all_to_all_single_meta(
	input, output_split_sizes, input_split_sizes, *args, **kwargs
):
	if output_split_sizes is None:
		return input.new_empty(input.size())
	else:
		for s in output_split_sizes:
			torch._check_is_size(s)
		out_size = list(input.size())
		out_size[0] = sum(output_split_sizes)
		return input.new_empty(out_size)


def _all_gather_into_tensor_native_meta(input, group_size, group_name):
	shape = list(input.size())
	shape[0] *= group_size
	return input.new_empty(shape)


def _all_gather_into_tensor_coalesced_native_meta(inputs, group_size, group_name):
	return [
		_all_gather_into_tensor_native_meta(input, group_size, group_name)
		for input in inputs
	]


def _reduce_scatter_tensor_native_meta(inp, reduce_op, group_size, group_name):
	shape = list(inp.size())
	shape[0] //= group_size
	return inp.new_empty(shape)


def _reduce_scatter_tensor_coalesced_native_meta(
	inputs, reduce_op, group_size, group_name
):
	return [
		_reduce_scatter_tensor_native_meta(inp, reduce_op, group_size, group_name)
		for inp in inputs
	]


def _register_ops():
	ops_defs = [
		"broadcast(Tensor self, int src, str tag, int[] ranks, int group_size) -> Tensor",
		"all_reduce(Tensor self, str reduceOp, str tag, int[] ranks, int group_size) -> Tensor",
		"all_reduce_coalesced(Tensor[] self, str reduceOp, str tag, int[] ranks, int group_size) -> Tensor[]",
		"wait_tensor(Tensor self) -> Tensor",
		"all_gather_into_tensor(Tensor shard, str tag, int[] ranks, int group_size) -> Tensor",
		"all_gather_into_tensor_coalesced(Tensor[] input, str tag, int[] ranks, int group_size) -> Tensor[]",
		"reduce_scatter_tensor(Tensor input, str reduceOp, str tag, int[] ranks, int group_size) -> Tensor",
		"reduce_scatter_tensor_coalesced(Tensor[] inputs, str reduceOp, str tag, int[] ranks, int group_size) -> Tensor[]",
		"all_to_all_single(Tensor input, SymInt[]? output_split_sizes, SymInt[]? input_split_sizes, str tag, int[] ranks, int group_size) -> Tensor",  # noqa: B950
	]

	my_module = sys.modules[__name__]
	for op_def in ops_defs:
		op_name = op_def[0 : op_def.index("(")]
		backend_impl = getattr(fun_col_impl, f"_{op_name}")
		meta_impl = getattr(my_module, f"_{op_name}_meta")
		c10_lib.define(op_def, tags=torch.Tag.pt2_compliant_tag)
		c10_lib_impl.impl(op_name, backend_impl, "CompositeExplicitAutograd")
		impl_abstract(f"c10d_functional::{op_name}")(meta_impl)


if not torch._running_with_deploy():
	c10_lib = torch.library.Library("c10d_functional", "DEF")
	c10_lib_impl = torch.library.Library("c10d_functional", "IMPL")
	_register_ops()

	_c10_lib_impl = torch.library.Library("_c10d_functional", "IMPL")
	_c10_lib_impl.impl("all_reduce", _all_reduce_meta, "Meta")
	_c10_lib_impl.impl("all_reduce_", _all_reduce__meta, "Meta")
	_c10_lib_impl.impl("all_reduce_coalesced", _all_reduce_coalesced_meta, "Meta")
	_c10_lib_impl.impl("all_reduce_coalesced_", _all_reduce_coalesced__meta, "Meta")
	_c10_lib_impl.impl("wait_tensor", _wait_tensor_meta, "Meta")
	_c10_lib_impl.impl(
		"all_gather_into_tensor", _all_gather_into_tensor_native_meta, "Meta"
	)
	_c10_lib_impl.impl(
		"all_gather_into_tensor_coalesced",
		_all_gather_into_tensor_coalesced_native_meta,
		"Meta",
	)
	_c10_lib_impl.impl(
		"reduce_scatter_tensor", _reduce_scatter_tensor_native_meta, "Meta"
	)
	_c10_lib_impl.impl(
		"reduce_scatter_tensor_coalesced",
		_reduce_scatter_tensor_coalesced_native_meta,
		"Meta",
	)
	_c10_lib_impl.impl("all_to_all_single", _all_to_all_single_meta, "Meta")
else:
	warnings.warn(
		"PyTorch Distributed functional collectives do not work with torch::deploy."
	)




def all_gather_tensor_inplace(
	output: torch.Tensor,
	input: torch.Tensor,
	group,  # TODO add a type,
	async_op: bool = False,
	tag: str = "",
	gather_dim: int = 0,
):
	assert (
		not async_op
	), "Can't remap async version of inplace op to functional collective"
	return output.copy_(all_gather_tensor(input, gather_dim, group, tag))


def reduce_scatter_tensor_inplace(
	output: torch.Tensor,
	input: torch.Tensor,
	op: str = "sum",  # TODO type is actually c10d ReduceOp. is this ok?
	group=None,  # TODO add a type
	async_op: bool = False,
	scatter_dim: int = 0,
	tag: str = "",
):
	assert (
		not async_op
	), "Can't remap async version of inplace op to functional collective"
	return output.copy_(reduce_scatter_tensor(input, op, scatter_dim, group, tag))


def all_reduce_inplace(
	tensor: torch.Tensor,
	op: str = "sum",
	group=None,
	async_op: bool = False,
	tag: str = "",
):
	assert (
		not async_op
	), "Can't remap async version of inplace op to functional collective"

	return tensor.copy_(all_reduce(tensor, op, group, tag))


from torch.distributed.distributed_c10d import (
	all_gather_into_tensor as legacy_allgather,
	all_reduce as legacy_allreduce,
	reduce_scatter_tensor as legacy_reducescatter,
)

traceable_collective_remaps = {
	legacy_allgather: all_gather_tensor_inplace,
	legacy_reducescatter: reduce_scatter_tensor_inplace,
	legacy_allreduce: all_reduce_inplace,
}

<END>

<START>

from __future__ import annotations

import dataclasses
from typing import Any, List, Optional

from torch.onnx._internal.diagnostics.infra.sarif import (
	_multiformat_message_string,
	_property_bag,
	_reporting_configuration,
	_reporting_descriptor_relationship,
)


@dataclasses.dataclass
class ReportingDescriptor(object):

<END>

<START>
from torch.onnx._internal.onnxruntime import (
	is_onnxrt_backend_supported,
	torch_compile_backend,
)
from .registry import register_backend


def has_onnxruntime():
	return is_onnxrt_backend_supported()


if is_onnxrt_backend_supported():
	register_backend(name="onnxrt", compiler_fn=torch_compile_backend)
else:

	def information_displaying_backend(*args, **kwargs):
		raise ImportError(
			"onnxrt is not registered as a backend. "
			"Please make sure all dependencies such as "
			"numpy, onnx, onnxscript, and onnxruntime-training are installed. "
			"Suggested procedure to fix dependency problem:\n"
			"  (1) pip or conda install numpy onnx onnxscript onnxruntime-training.\n"
			"  (2) Open a new python terminal.\n"
			"  (3) Call the API `torch.onnx.is_onnxrt_backend_supported()`:\n"
			"  (4)   If it returns `True`, then you can use `onnxrt` backend.\n"
			"  (5)   If it returns `False`, please execute the package importing section in "
			"torch/onnx/_internal/onnxruntime.py under pdb line-by-line to see which import fails."
		)

	register_backend(name="onnxrt", compiler_fn=information_displaying_backend)

<END>

<START>
import functools
import operator
from functools import reduce
from typing import Any, Tuple

import torch

from torch.fx.experimental.symbolic_shapes import has_free_symbols

from .. import ir

from ..lowering import lowerings as L
from ..pattern_matcher import (
	Arg,
	CallFunction,
	filter_nodes,
	get_arg_value,
	KeywordArg,
	MULTIPLE,
)
from ..virtualized import ops
from .freezing_patterns import register_freezing_graph_pattern
from .post_grad import register_lowering_pattern
from .quantization import (
	_register_quantization_lowerings,
	_register_quantization_weight_pack_pass,
)

if torch._C._has_mkldnn:
	aten = torch.ops.aten
	mkldnn = torch.ops.mkldnn
	prims = torch.ops.prims

	_conv_args = [Arg() for _ in range(10)]
	_linear_args = [Arg() for _ in range(6)]
	_conv_transpose_args = [Arg() for _ in range(11)]

	def _conv_call(users=1):
		return CallFunction(
			mkldnn._convolution_pointwise.default, *_conv_args, _users=users
		)

	def _linear_call(users=1):
		return CallFunction(
			mkldnn._linear_pointwise.default, *_linear_args, _users=users
		)

	def _conv_transpose_call(users=1):
		return CallFunction(
			mkldnn._convolution_transpose_pointwise.default,
			*_conv_transpose_args,
			_users=users,
		)

	def _to_float(input_call, users=1):
		return CallFunction(
			prims.convert_element_type.default,
			input_call,
			KeywordArg("to_float"),
			_users=users,
		)

	def _to_bf16(input_call):
		return CallFunction(
			prims.convert_element_type.default,
			input_call,
			KeywordArg("to_bf16"),
			_users=1,
		)

	def _unary_fusion_pattern(unary_fusion, call_fn, users, is_bf16):
		computation_call = (
			_to_float(call_fn(), users=users) if is_bf16 else call_fn(users=users)
		)
		out = unary_fusion(computation_call)
		return _to_bf16(out) if is_bf16 else out

	def _gelu_fusion_1(computation_call):
		return CallFunction(
			aten.mul,
			CallFunction(aten.mul, computation_call, 0.5),
			CallFunction(
				aten.add,
				CallFunction(
					aten.erf,
					CallFunction(aten.mul, computation_call, 0.7071067811865476),
				),
				1,
			),
		)

	def _gelu_fusion_2(computation_call):
		return CallFunction(
			aten.mul,
			CallFunction(aten.mul, computation_call, 0.5),
			CallFunction(
				aten.add,
				CallFunction(
					aten.tanh,
					CallFunction(
						aten.mul,
						CallFunction(
							aten.add,
							computation_call,
							CallFunction(
								aten.mul,
								CallFunction(
									aten.mul,
									CallFunction(
										aten.mul, computation_call, computation_call
									),
									computation_call,
								),
								0.044715,
							),
						),
						0.7978845608028654,
					),
				),
				1,
			),
		)

	def _hardswish_fusion(computation_call):
		return CallFunction(
			aten.div,
			CallFunction(
				aten.mul,
				computation_call,
				CallFunction(
					aten.clamp_max,
					CallFunction(
						aten.clamp_min, CallFunction(aten.add, computation_call, 3), 0
					),
					6,
				),
			),
			6,
		)

	def _silu_fusion(computation_call):
		return CallFunction(
			aten.mul, computation_call, CallFunction(aten.sigmoid, computation_call)
		)

	def _hardsigmoid_fusion(computation_call):
		return CallFunction(
			aten.div,
			CallFunction(
				aten.clamp_max,
				CallFunction(
					aten.clamp_min, CallFunction(aten.add, computation_call, 3), 0
				),
				6,
			),
			6,
		)

	def _leaky_relu_fusion(computation_call):
		return CallFunction(
			aten.where,
			CallFunction(aten.gt, computation_call, 0),
			computation_call,
			CallFunction(aten.mul, computation_call, KeywordArg("negative_slope")),
		)

	def _hardtanh_fusion(computation_call):
		return CallFunction(
			aten.clamp_max,
			CallFunction(aten.clamp_min, computation_call, KeywordArg("min_value")),
			KeywordArg("max_value"),
		)

	def _combined_fusion(computation_call, elementwise_op):
		return CallFunction(elementwise_op, computation_call)

	def _binary_fusion_v1(computation_call, binary_fn):
		return CallFunction(binary_fn, KeywordArg("other"), computation_call)

	def _binary_fusion_v2(computation_call, binary_fn):
		return CallFunction(binary_fn, computation_call, KeywordArg("other"))

	def _is_single_computation_op(computation_op):
		def fn(match):
			computation_nodes = filter_nodes(match.nodes, computation_op)
			if len(computation_nodes) < 1:
				return False
			if any(n.args[-3] != "none" for n in computation_nodes):
				return False
			return True

		return fn

	def _is_valid_computation_unary_fusion(computation_op, is_bf16=False):
		def fn(match):
			matched = _is_single_computation_op(computation_op)(match)
			computation_node = filter_nodes(match.nodes, computation_op)[0]
			if is_bf16:
				conversion_dtype_nodes = filter_nodes(
					match.nodes, prims.convert_element_type.default
				)
				if len(conversion_dtype_nodes) != 2:
					return False
				if computation_node == conversion_dtype_nodes[0].args[0]:
					to_float = conversion_dtype_nodes[0].args[1]
					to_bf16 = conversion_dtype_nodes[1].args[1]
				else:
					to_float = conversion_dtype_nodes[1].args[1]
					to_bf16 = conversion_dtype_nodes[0].args[1]
				matched = (
					matched and to_float == torch.float and to_bf16 == torch.bfloat16
				)
			return matched

		return fn

	def _register_unary_fusion_lowering(
		pattern, unary_attr, computation_op, is_bf16=False
	):
		@register_lowering_pattern(
			pattern,
			extra_check=_is_valid_computation_unary_fusion(computation_op, is_bf16),
		)
		def fn(match, *args, **kwargs):
			computation_args = list(args)[:-3] + [
				unary_attr.op_name,
				unary_attr.scalars_attr,
				unary_attr.algorithm_attr,
			]
			return L[computation_op](*computation_args)

		return fn

	def _register_leaky_relu_fusion_lowering(pattern, computation_op, is_bf16=False):
		@register_lowering_pattern(
			pattern, extra_check=_is_single_computation_op(computation_op)
		)
		def fn(match, *args, **kwargs):
			negative_slope = kwargs.get("negative_slope")
			if isinstance(negative_slope, ir.TensorBox):
				matched = False
			else:  # inp is a Number
				matched = True
			if is_bf16:
				dtype1 = kwargs.get("to_float")
				dtype2 = kwargs.get("to_bf16")
				matched = matched and dtype1 == torch.float and dtype2 == torch.bfloat16
			computation_args = list(args)
			if matched:
				computation_args = computation_args[:-3] + [
					"leaky_relu",
					[negative_slope],
					"",
				]
				return L[computation_op](*computation_args)
			else:
				out = L[computation_op](*computation_args)
				if is_bf16:
					out = L[prims.convert_element_type.default](out, dtype=torch.float)
				out = L[aten.where](
					L[aten.gt](out, 0),
					out,
					L[aten.mul](out, negative_slope),
				)
				if is_bf16:
					out = L[prims.convert_element_type.default](
						out, dtype=torch.bfloat16
					)
				return out

		return fn

	def _register_hardtanh_fusion_lowering(pattern, computation_op, is_bf16=False):
		@register_lowering_pattern(
			pattern, extra_check=_is_single_computation_op(computation_op)
		)
		def fn(match, *args, **kwargs):
			min_value = kwargs.get("min_value")
			max_value = kwargs.get("max_value")
			if isinstance(min_value, ir.TensorBox) or isinstance(
				max_value, ir.TensorBox
			):
				matched = False
			else:  # inp is a Number
				assert max_value is not None
				matched = min_value <= max_value
			if is_bf16:
				dtype1 = kwargs.get("to_float")
				dtype2 = kwargs.get("to_bf16")
				matched = matched and dtype1 == torch.float and dtype2 == torch.bfloat16
			computation_args = list(args)
			if matched:
				computation_args = computation_args[:-3] + [
					"hardtanh",
					[min_value, max_value],
					"",
				]
				return L[computation_op](*computation_args)
			else:
				out = L[computation_op](*computation_args)
				if is_bf16:
					out = L[prims.convert_element_type.default](out, dtype=torch.float)
				out = L[aten.clamp_max](L[aten.clamp_min](out, min_value), max_value)
				if is_bf16:
					out = L[prims.convert_element_type.default](
						out, dtype=torch.bfloat16
					)
				return out

		return fn

	_binary_attr = {
		aten.add: "add",
		ops.add: "add",
		aten.sub: "sub",
		ops.sub: "sub",
	}

	def _is_valid_binary(match, fn):
		binary_nodes = filter_nodes(match.nodes, fn)
		if len(binary_nodes) < 1:
			return False
		if any(
			not (
				hasattr(n.args[0], "meta")
				and isinstance(n.args[0].meta.get("val", None), torch.Tensor)
			)
			or not (
				hasattr(n.args[1], "meta")
				and isinstance(n.args[1].meta.get("val", None), torch.Tensor)
			)
			for n in binary_nodes
		):
			return False
		if any(
			get_arg_value(n, 2, kwarg_name="alpha") != 1.0
			and get_arg_value(n, 2, kwarg_name="alpha") is not None
			for n in binary_nodes
		):
			return False
		if any(
			n.args[0].meta["val"].size() != n.args[1].meta["val"].size()
			or n.args[0].meta["val"].device != n.args[1].meta["val"].device
			or n.args[0].meta["val"].dtype != n.args[1].meta["val"].dtype
			for n in binary_nodes
		):
			return False
		if any(n.args[0] == n.args[1] for n in binary_nodes):
			return False
		return True

	def _is_valid_computation_binary(computation_op, binary_op, other_index=None):
		def fn(match):
			if not _is_single_computation_op(computation_op)(match):
				return False
			if not _is_valid_binary(match, binary_op):
				return False
			return True

		return fn

	def _is_valid_computation_binary_inplace(computation_op, binary_op, other_index):
		def fn(match):
			if not _is_valid_computation_binary(computation_op, binary_op)(match):
				return False
			binary_nodes = filter_nodes(match.nodes, binary_op)
			if any(len(n.args[other_index].users) > 1 for n in binary_nodes):
				return False
			if any(
				n.args[other_index].op in ["placeholder", "output"]
				for n in binary_nodes
			):
				return False
			return True

		return fn

	def _register_binary_unary_fusion_lowering(
		pattern,
		computation_op,
		binary_op,
		fusion_op,
		unary_attr=None,
	):
		@register_lowering_pattern(
			pattern, extra_check=_is_valid_computation_binary(computation_op, binary_op)
		)
		def fn(match, *args, **kwargs):
			other = kwargs.get("other")
			assert isinstance(other, ir.TensorBox)
			binary_attr = _binary_attr[binary_op]
			args_list = list(args)
			computation_args = [args_list[0], other] + args_list[1:-3] + [binary_attr]
			if len(args_list) > 6:
				if unary_attr is not None:
					computation_args += [
						1.0,
						unary_attr.op_name,
						unary_attr.scalars_attr,
						unary_attr.algorithm_attr,
					]
				else:
					computation_args += [1.0, None, [], None]
			return L[fusion_op](*computation_args)

		return fn

	def _register_binary_unary_maybe_inplace_fusion_lowering(
		pattern,
		computation_op,
		binary_op,
		inplace_fusion_op,
		outplace_fusion_op,
		unary_attr=None,
		other_index=None,
	):
		@register_lowering_pattern(
			pattern,
			extra_check=_is_valid_computation_binary_inplace(
				computation_op, binary_op, other_index
			),
		)
		def fn(match, *args, **kwargs):
			other = kwargs.get("other")
			assert isinstance(other, ir.TensorBox)
			binary_attr = _binary_attr[binary_op]
			args_list = list(args)
			computation_args = [args_list[0], other] + args_list[1:-3] + [binary_attr]
			if len(args_list) > 6:
				if unary_attr is not None:
					computation_args += [
						1.0,
						unary_attr.op_name,
						unary_attr.scalars_attr,
						unary_attr.algorithm_attr,
					]
				else:
					computation_args += [1.0, None, [], None]
			other.realize()
			can_be_inplace = not (
				isinstance(other.data, ir.ReinterpretView)
				or isinstance(other.get_layout(), (ir.MutationLayout, ir.AliasedLayout))
			)
			if not can_be_inplace:
				return L[outplace_fusion_op](*computation_args)
			return L[inplace_fusion_op](*computation_args)

		return fn

	computation_ops = [
		mkldnn._convolution_pointwise.default,
		mkldnn._linear_pointwise.default,
		mkldnn._convolution_transpose_pointwise.default,
	]

	class UnaryAttr:
		def __init__(self, op_name: str, scalars_attr=None, algorithm_attr=None):
			self.op_name = op_name
			self.scalars_attr = scalars_attr if scalars_attr else []
			self.algorithm_attr = algorithm_attr if algorithm_attr else ""

	def _register_unary_fusion():
		computation_call_fns = [_conv_call, _linear_call, _conv_transpose_call]

		def _unary_fusion_patterns(is_bf16):
			replacement_unary_fusion_patterns = {
				UnaryAttr("gelu", algorithm_attr="tanh"): [
					_unary_fusion_pattern(_gelu_fusion_2, call_fn, 4, is_bf16)
					for call_fn in computation_call_fns
				],
				UnaryAttr("gelu", algorithm_attr="none"): [
					_unary_fusion_pattern(_gelu_fusion_1, call_fn, 2, is_bf16)
					for call_fn in computation_call_fns
				],
				UnaryAttr("hardswish"): [
					_unary_fusion_pattern(_hardswish_fusion, call_fn, 2, is_bf16)
					for call_fn in computation_call_fns
				],
				UnaryAttr("hardsigmoid"): [
					_unary_fusion_pattern(_hardsigmoid_fusion, call_fn, 1, is_bf16)
					for call_fn in computation_call_fns
				],
				UnaryAttr("swish"): [
					_unary_fusion_pattern(_silu_fusion, call_fn, 2, is_bf16)
					for call_fn in computation_call_fns
				],
			}
			if not is_bf16:
				call_user1 = [call_fn(users=1) for call_fn in computation_call_fns]
				replacement_unary_fusion_patterns.update(
					{
						UnaryAttr("relu"): [
							_combined_fusion(u, aten.relu) for u in call_user1
						],
						UnaryAttr("sigmoid"): [
							_combined_fusion(u, aten.sigmoid) for u in call_user1
						],
						UnaryAttr("tanh"): [
							_combined_fusion(u, aten.tanh) for u in call_user1
						],
					}
				)

			return replacement_unary_fusion_patterns

		for is_bf16 in [True, False]:
			replace_patterns = _unary_fusion_patterns(is_bf16)
			for unary_attr, patterns in replace_patterns.items():
				_register_unary_fusion_lowering(
					patterns[0], unary_attr, computation_ops[0], is_bf16
				)
				_register_unary_fusion_lowering(
					patterns[1], unary_attr, computation_ops[1], is_bf16
				)
				_register_unary_fusion_lowering(
					patterns[2], unary_attr, computation_ops[2], is_bf16
				)
			_leaky_relu_patterns = [
				_unary_fusion_pattern(_leaky_relu_fusion, call_fn, 3, is_bf16)
				for call_fn in computation_call_fns
			]
			for pattern, computation_op in zip(_leaky_relu_patterns, computation_ops):
				_register_leaky_relu_fusion_lowering(pattern, computation_op, is_bf16)
			hardtanh_patterns = [
				_unary_fusion_pattern(_hardtanh_fusion, call_fn, 1, is_bf16)
				for call_fn in computation_call_fns
			]
			for pattern, computation_op in zip(hardtanh_patterns, computation_ops):
				_register_hardtanh_fusion_lowering(pattern, computation_op, is_bf16)

	def _register_inplace_fusion():
		binary_ops = [aten.add, ops.add]
		inplace_fusion_op = mkldnn._convolution_pointwise_.binary
		outplace_fusion_op = mkldnn._convolution_pointwise.binary
		conv_call = _conv_call(users=1)
		conv_op = computation_ops[0]
		for binary_op in binary_ops:
			binary_v1 = _binary_fusion_v1(conv_call, binary_op)
			binary_unary_v1 = _combined_fusion(binary_v1, aten.relu)
			_register_binary_unary_maybe_inplace_fusion_lowering(
				binary_unary_v1,
				conv_op,
				binary_op,
				inplace_fusion_op,
				outplace_fusion_op,
				other_index=0,
				unary_attr=UnaryAttr("relu"),
			)
			_register_binary_unary_maybe_inplace_fusion_lowering(
				binary_v1,
				conv_op,
				binary_op,
				inplace_fusion_op,
				outplace_fusion_op,
				other_index=0,
			)
			binary_v2 = _binary_fusion_v2(conv_call, binary_op)
			binary_unary_v2 = _combined_fusion(binary_v2, aten.relu)
			_register_binary_unary_maybe_inplace_fusion_lowering(
				binary_unary_v2,
				conv_op,
				binary_op,
				inplace_fusion_op,
				outplace_fusion_op,
				other_index=1,
				unary_attr=UnaryAttr("relu"),
			)
			_register_binary_unary_maybe_inplace_fusion_lowering(
				binary_v2,
				conv_op,
				binary_op,
				inplace_fusion_op,
				outplace_fusion_op,
				other_index=1,
			)

	def _register_binary_fusion():
		binary_ops = [aten.add, ops.add, aten.sub, ops.sub]
		fusion_ops = [
			mkldnn._convolution_pointwise.binary,
			mkldnn._linear_pointwise.binary,
		]
		_computation_user_1 = [_conv_call(users=1), _linear_call(users=1)]
		for computation_call, computation_op, fusion_op in zip(
			_computation_user_1, computation_ops[:-1], fusion_ops
		):
			for binary_op in binary_ops:
				pattern = _binary_fusion_v2(computation_call, binary_op)
				_register_binary_unary_fusion_lowering(
					pattern, computation_op, binary_op, fusion_op
				)

			for binary_op in [aten.add, ops.add]:
				pattern = _binary_fusion_v1(computation_call, binary_op)
				_register_binary_unary_fusion_lowering(
					pattern, computation_op, binary_op, fusion_op
				)

	def _register_binary_unary_fusion():
		binary_ops = [aten.add, ops.add, aten.sub, ops.sub]
		fusion_ops = [mkldnn._convolution_pointwise.binary]
		_computation_user_1 = [_conv_call(users=1)]
		for computation_call, computation_op, fusion_op in zip(
			_computation_user_1, computation_ops[:-1], fusion_ops
		):
			for binary_op in binary_ops:
				pattern_v1 = _combined_fusion(
					_binary_fusion_v2(computation_call, binary_op), aten.relu
				)
				_register_binary_unary_fusion_lowering(
					pattern_v1,
					computation_op,
					binary_op,
					fusion_op,
					unary_attr=UnaryAttr("relu"),
				)
			for binary_op in [aten.add, ops.add]:
				pattern_v2 = _combined_fusion(
					_binary_fusion_v1(computation_call, binary_op), aten.relu
				)
				_register_binary_unary_fusion_lowering(
					pattern_v2,
					computation_op,
					binary_op,
					fusion_op,
					unary_attr=UnaryAttr("relu"),
				)

	def _recover_linear():
		@register_freezing_graph_pattern(
			CallFunction(
				aten.reshape.default,
				CallFunction(
					mkldnn._linear_pointwise.default,
					CallFunction(
						aten.reshape.default,
						Arg(),
						KeywordArg("reshape_1"),
						_users=MULTIPLE,
					),
					Arg(),
					Arg(),
					Arg(),
					Arg(),
					Arg(),
				),
				KeywordArg("reshape_2"),
			),
			pass_number=1,
		)
		def reshape_linear_reshape_pattern(match, *args, **kwargs):
			reshape_1 = kwargs.get("reshape_1")
			reshape_2 = kwargs.get("reshape_2")
			assert isinstance(reshape_1, list)
			assert isinstance(reshape_2, list)
			assert len(reshape_1) == 2
			dynamic_shapes = not all(
				isinstance(x, int) for x in ([reshape_1[0]] + reshape_2[:-1])
			)

			graph = match.graph
			reshape_2_node = match.output_node()
			linear_input_node = reshape_2_node.args[0].args[0].args[0]
			if dynamic_shapes:
				return
			else:
				can_remove_reshape = linear_input_node.meta.get("val").shape[
					:-1
				] == torch.Size(reshape_2[:-1])
				can_remove_reshape = can_remove_reshape and (
					reduce(operator.mul, reshape_2[:-1]) == reshape_1[0]
				)

			if can_remove_reshape:
				repl = graph.call_function(mkldnn._linear_pointwise.default, args)
				repl.meta.update(reshape_2_node.meta)
				reshape_2_node.replace_all_uses_with(repl)
				old_linear_node = reshape_2_node.args[0]
				reshape_1_node = old_linear_node.args[0]
				graph.erase_node(reshape_2_node)
				graph.erase_node(old_linear_node)
				if len(reshape_1_node.users) == 0:
					graph.erase_node(reshape_1_node)

		def is_linear_add_bias(match):
			add_node = match.output_node()
			linear_node = add_node.args[0]
			weight_meta = linear_node.args[1].meta.get("val")
			bias_meta = add_node.args[1].meta.get("val")
			if weight_meta is None or bias_meta is None:
				return False
			return (
				linear_node.args[2] is None
				and bias_meta.dim() == 1
				and bias_meta.size(0) == weight_meta.size(0)
			)

		@register_freezing_graph_pattern(
			CallFunction(
				aten.add.Tensor,
				CallFunction(mkldnn._linear_pointwise.default, *_linear_args),
				Arg(),
			),
			pass_number=1,
			extra_check=is_linear_add_bias,
		)
		def linear_bias_pattern(match, *args):
			graph = match.graph
			add_node = match.output_node()
			linear_node = add_node.args[0]
			new_args = list(linear_node.args)
			new_args[2] = add_node.args[1]
			repl = graph.call_function(
				mkldnn._linear_pointwise.default, tuple(new_args)
			)
			repl.meta.update(add_node.meta)
			add_node.replace_all_uses_with(repl)
			match.erase_nodes(graph)

	def _is_packable_mkldnn_rnn_layer(match):
		lstm_node = match.output_node()
		POS_WEIGHTS = [1, 2]
		POS_INPUTS = [0, 5, 6]
		POS_ARGS = POS_WEIGHTS + POS_INPUTS
		if any(
			lstm_node.args[POS_WEIGHT].op != "get_attr" for POS_WEIGHT in POS_WEIGHTS
		):
			return False

		if any(lstm_node.args[POS_ARG].meta.get("val") is None for POS_ARG in POS_ARGS):
			return False

		if any(
			lstm_node.args[POS_ARG].meta.get("val").device.type != "cpu"
			for POS_ARG in POS_ARGS
		):
			return False

		if any(
			lstm_node.args[POS_ARG].meta.get("val").dtype == torch.bfloat16
			and not mkldnn._is_mkldnn_bf16_supported()
			for POS_ARG in POS_ARGS
		):
			return False

		return True

	def _is_packable_convolution(match):
		conv_node = match.output_node()
		input_meta_value = conv_node.args[0].meta.get("val")
		weight_meta_value = conv_node.args[1].meta.get("val")
		if input_meta_value is None or weight_meta_value is None:
			return False
		input_size = input_meta_value.shape
		if conv_node.args[1].op != "get_attr":
			return False
		for meta_value in [input_meta_value, weight_meta_value]:
			if (
				meta_value is None
				or meta_value.device.type != "cpu"
				or meta_value.dim() != 4
			):
				return False
		if (
			input_meta_value.dtype == torch.bfloat16
			or weight_meta_value.dtype == torch.bfloat16
		):
			if not mkldnn._is_mkldnn_bf16_supported():
				return False
		is_transposed = conv_node.args[-3]
		if is_transposed:
			if has_free_symbols(input_size):
				return False
			groups = conv_node.args[-1]
			in_channels = weight_meta_value.size(0)
			if groups > 1 and groups == in_channels:
				return False
			output_paddings = conv_node.args[-2]
			strides = conv_node.args[3]
			if any(
				output_padding >= stride
				for output_padding, stride in zip(output_paddings, strides)
			):
				return False
		return True

	def _is_packable_linear(match):
		linear_node = match.output_node()
		weight_idx = 2 if linear_node.target == aten.addmm.default else 1
		if linear_node.args[weight_idx].op != "get_attr":
			return False
		input_meta_value = linear_node.args[weight_idx - 1].meta.get("val")
		weight_meta_value = linear_node.args[weight_idx].meta.get("val")
		if input_meta_value is None or weight_meta_value is None:
			return False
		batch_size = input_meta_value.shape[0]
		is_bf16_weight = weight_meta_value.dtype == torch.bfloat16
		if (
			not is_bf16_weight
			and not mkldnn._is_mkldnn_acl_supported()
			and ((not torch._C.has_mkl) or has_free_symbols(batch_size))
		):
			return False
		for meta_value in [input_meta_value, weight_meta_value]:
			if (
				meta_value is None
				or meta_value.device.type != "cpu"
				or meta_value.dim() != 2
			):
				return False
		if weight_idx == 2:
			bias_meta_value = linear_node.args[0].meta.get("val")
			if (
				bias_meta_value is None
				or meta_value.device.type != "cpu"
				or bias_meta_value.dim() != 1
				or bias_meta_value.size(0) != weight_meta_value.size(1)
			):
				return False

		if (
			input_meta_value.dtype == torch.bfloat16
			or weight_meta_value.dtype == torch.bfloat16
		):
			if not mkldnn._is_mkldnn_bf16_supported():
				return False
		return True

	_aten_conv_args = (
		Arg(),
		Arg(),
		Arg(),
		Arg(),
		Arg(),
		Arg(),
		KeywordArg("is_transposed"),
		Arg(),
		Arg(),
	)

	_aten_mkldnn_rnn_layer_args = (
		Arg(),  # input
		Arg(),  # weight0
		Arg(),  # weight1
		Arg(),  # weight2
		Arg(),  # weight3
		Arg(),  # hx_
		Arg(),  # cx_
		KeywordArg("reverse"),  # reverse
		Arg(),  # batch_sizes
		Arg(),  # mode
		Arg(),  # hidden_size
		Arg(),  # num_layers
		Arg(),  # has_biases
		Arg(),  # bidirectional
		Arg(),  # batch_first
		Arg(),  # train
	)

	def _register_weight_pack_pass():
		@register_freezing_graph_pattern(
			CallFunction(aten.convolution.default, *_aten_conv_args),
			extra_check=_is_packable_convolution,
		)
		def convolution(match, *args, **kwargs):
			is_transposed = kwargs.get("is_transposed")
			assert isinstance(is_transposed, bool)
			graph = match.graph
			conv_node = match.output_node()
			input_size = conv_node.args[0].meta.get("val").shape
			with graph.inserting_before(conv_node):
				constant_args = [args[4], args[3], args[5], args[-1]]
				packed_weight_op = mkldnn._reorder_convolution_weight
				packed_conv_op = mkldnn._convolution_pointwise.default
				if is_transposed:
					constant_args.insert(1, args[-2])  # output_padding
					packed_weight_op = mkldnn._reorder_convolution_transpose_weight
					packed_conv_op = mkldnn._convolution_transpose_pointwise.default
				if not has_free_symbols(input_size):
					packed_weight_inputs = (
						(args[1],) + tuple(constant_args) + (input_size,)
					)
					packed_weight_node = graph.create_node(
						"call_function", packed_weight_op, args=packed_weight_inputs
					)
				else:
					assert not is_transposed
					packed_weight_node = args[1]
				packed_conv_inputs = (
					(args[0], packed_weight_node, args[2])
					+ tuple(constant_args)
					+ ("none", [], "")
				)
				packed_conv_node = graph.create_node(
					"call_function", packed_conv_op, tuple(packed_conv_inputs)
				)
				conv_node.replace_all_uses_with(packed_conv_node)
				packed_conv_node.meta.update(conv_node.meta)
				graph.erase_node(conv_node)

		@register_freezing_graph_pattern(
			CallFunction(aten.mkldnn_rnn_layer.default, *_aten_mkldnn_rnn_layer_args),
			extra_check=_is_packable_mkldnn_rnn_layer,
		)
		def mkldnn_rnn_layer(match, *args, **kwargs):
			def get_item(graph, node, index):
				return graph.call_function(operator.getitem, (node, index))

			graph = match.graph
			lstm_node = match.output_node()
			input = args[0]
			weight0, weight1 = args[1:3]
			reverse = kwargs.get("reverse")
			packed_lstm_op = aten.mkldnn_rnn_layer.default
			hidden_size = args[9]
			has_biases = args[11]
			batch_first = args[13]
			with graph.inserting_before(lstm_node):
				packed_weight_op = mkldnn._reorder_mkldnn_rnn_layer_weight.default
				packed_weight_inputs = (
					weight0,
					weight1,
					hidden_size,
					reverse,
					has_biases,
					batch_first,
				)
				packed_weight_node = graph.create_node(
					"call_function", packed_weight_op, packed_weight_inputs, {}, "name"
				)
				packed_weight_items = [
					get_item(graph, packed_weight_node, i) for i in range(2)
				]
				pack_lstm_inputs = (
					args[0],
					*packed_weight_items,
					args[3],
					args[4],
					args[5],
					args[6],
					reverse,
					*args[7:],
				)

				packed_lstm_node = graph.create_node(
					"call_function", packed_lstm_op, args=pack_lstm_inputs
				)
				lstm_node.replace_all_uses_with(packed_lstm_node)
				packed_lstm_node.meta.update(lstm_node.meta)
				graph.erase_node(lstm_node)

		@register_freezing_graph_pattern(
			CallFunction(aten.addmm.default, Arg(), Arg(), Arg()),
			extra_check=_is_packable_linear,
		)
		@register_freezing_graph_pattern(
			CallFunction(aten.mm.default, Arg(), Arg()),
			extra_check=_is_packable_linear,
		)
		def linear(match, *args, **kwargs):
			graph = match.graph
			linear_node = match.output_node()
			input = args[0] if linear_node.target == aten.mm.default else args[1]
			bias = None if linear_node.target == aten.mm.default else args[0]
			weight = args[1] if linear_node.target == aten.mm.default else args[2]
			with graph.inserting_before(linear_node):
				transpose_weight_node = graph.create_node(
					"call_function", aten.permute.default, (weight, (1, 0))
				)
				weight_dtype = weight.meta.get("val").dtype
				is_bf16_weight = weight_dtype == torch.bfloat16
				batch_size = input.meta.get("val").shape[0]
				if has_free_symbols(batch_size):
					assert (
						is_bf16_weight or mkldnn._is_mkldnn_acl_supported()
					), f"only bf16 weight prepacking supports dynamic shape inputs but got {weight_dtype}"
				packed_weight_inputs = (
					transpose_weight_node,
					batch_size.node.shape_env.size_hint(batch_size.node.expr)
					if has_free_symbols(batch_size)
					else batch_size,
				)
				packed_weight_op = (
					mkldnn._reorder_linear_weight
					if (is_bf16_weight or mkldnn._is_mkldnn_acl_supported())
					else torch.ops.mkl._mkl_reorder_linear_weight
				)
				packed_weight_node = graph.create_node(
					"call_function", packed_weight_op, args=packed_weight_inputs
				)

				packed_linear_inputs: Tuple[Any, ...] = (input, packed_weight_node)
				if is_bf16_weight or mkldnn._is_mkldnn_acl_supported():
					packed_linear_inputs += (bias, "none", [], "")
					packed_linear_op = mkldnn._linear_pointwise.default
				else:
					packed_linear_inputs += (transpose_weight_node, bias, batch_size)
					packed_linear_op = torch.ops.mkl._mkl_linear
				packed_linear_node = graph.create_node(
					"call_function", packed_linear_op, packed_linear_inputs
				)
				linear_node.replace_all_uses_with(packed_linear_node)
				packed_linear_node.meta.update(linear_node.meta)
				graph.erase_node(linear_node)

	def _eliminate_duplicate_packed_nodes(gm):
		if not (torch.backends.mkldnn.enabled and torch.backends.mkldnn.is_available()):
			return gm

		packed_weight_ops = [
			torch._C._nn.mkldnn_reorder_conv2d_weight,
			mkldnn._reorder_convolution_transpose_weight,
			mkldnn._reorder_linear_weight,
			mkldnn._reorder_mkldnn_rnn_layer_weight,
		]
		if torch._C.has_mkl:
			packed_weight_ops.append(torch.ops.mkl._mkl_reorder_linear_weight)

		for node in gm.graph.nodes:
			if node.target in packed_weight_ops and len(node.args[0].users) > 1:
				for user_node in list(node.args[0].users.keys()):
					if (
						user_node.target == node.target
						and user_node != node
						and user_node.args == node.args
					):
						user_node.replace_all_uses_with(node)
						gm.graph.erase_node(user_node)

	@functools.lru_cache(None)
	def _mkldnn_fusion_init():
		if (
			torch.backends.mkldnn.enabled
			and torch.backends.mkldnn.is_available()
			and not torch.ops.mkldnn._is_mkldnn_acl_supported()
		):
			_register_unary_fusion()
			_register_inplace_fusion()
			_register_binary_unary_fusion()
			_register_binary_fusion()
			_register_quantization_lowerings()

	@functools.lru_cache(None)
	def _mkldnn_weight_pack_init():
		if torch.backends.mkldnn.enabled and torch.backends.mkldnn.is_available():
			_register_weight_pack_pass()
			_recover_linear()
			_register_quantization_weight_pack_pass()

<END>

<START>
import functools

import torch
from torch.nn.utils._expanded_weights.expanded_weights_impl import ExpandedWeight

from torch.utils import _pytree as pytree


def call_for_per_sample_grads(module, *, batch_size=None, loss_reduction="sum", batch_first=True):

	def maybe_build_expanded_weight(og_tensor, batch_size):
		if og_tensor.requires_grad:
			return ExpandedWeight(og_tensor, batch_size, loss_reduction)
		else:
			return og_tensor

	def compute_batch_size(*args, **kwargs):
		args_and_kwargs = pytree.arg_tree_leaves(*args, **kwargs)
		batch_size = None
		for arg in args_and_kwargs:
			if not isinstance(arg, torch.Tensor):
				continue

			arg_batch_size = arg.shape[0] if batch_first else arg.shape[1]
			if batch_size is not None and batch_size != arg_batch_size:
				raise RuntimeError("When computing batch size, found at least one input with batch size "
								   f"{batch_size} and one with batch size {arg_batch_size}. Please specify it "
								   "explicitly using the batch size kwarg in call_for_per_sample_grads")
			batch_size = arg_batch_size
		if batch_size is None:
			raise RuntimeError("Unable to find a tensor in the passed args and kwargs. They may not be pytree-able "
							   "and so ExpandedWeights cannot compute the batch size from the inputs. Please specify "
							   "it explicitly")
		return batch_size

	if loss_reduction not in ["sum", "mean"]:
		raise RuntimeError(f"Expected loss_reduction argument to be sum or mean, got {loss_reduction}")

	if not isinstance(module, torch.nn.Module):
		raise RuntimeError(f"Module passed must be nn.Module, got {type(module).__name__}")
	if not (batch_size is None or isinstance(batch_size, int)):
		raise RuntimeError(f"Batch size passed must be None or an integer, got {type(batch_size).__name__}")
	if batch_size is not None and batch_size < 1:
		raise RuntimeError(f"Batch size must be positive, got {batch_size}")
	for weight in module.parameters():
		if hasattr(weight, "grad_sample") and weight.grad_sample is not None:  # type: ignore[attr-defined]
			raise RuntimeError("Current Expanded Weights accumulates the gradients, which will be incorrect for multiple "
							   f"calls without clearing gradients. Please clear out the grad_sample parameter of {weight} or "
							   "post an issue to pytorch/pytorch to prioritize correct behavior")

	@functools.wraps(module.forward)
	def wrapper(*args, **kwargs):
		wrapper_batch_size = batch_size
		if wrapper_batch_size is None:
			wrapper_batch_size = compute_batch_size(*args, **kwargs)

		params = {name: maybe_build_expanded_weight(value, wrapper_batch_size) for (name, value) in module.named_parameters()}
		return torch.func.functional_call(module, params, args, kwargs)
	return wrapper

<END>

<START>
import logging
import typing
from collections import Counter
from typing import Dict, Set

import torch
import torch._guards
from torch._inductor.constant_folding import ConstantFolder
from torch.multiprocessing.reductions import StorageWeakRef

from .. import config
from ..pattern_matcher import (
	CallFunction,
	init_once_fakemode,
	KeywordArg,
	Match,
	PatternMatcherPass,
	register_graph_pattern,
	stable_topological_sort,
)
from .replace_random import replace_random_passes

log = logging.getLogger(__name__)
patterns = PatternMatcherPass()


@init_once_fakemode
def lazy_init():
	from .fuse_attention import _sfdp_init
	from .misc_patterns import _misc_patterns_init
	from .pad_mm import _pad_mm_init

	_pad_mm_init()
	_sfdp_init()
	_misc_patterns_init()


@torch.utils._python_dispatch._disable_current_modes()
def remove_no_ops(
	gm: torch.fx.GraphModule, zeros: Set[torch.fx.Node], ones: Set[torch.fx.Node]
):
	"Removes no-ops: (+ 0, - 0, * 1, / 1)"
	aten = torch.ops.aten
	graph = gm.graph

	def fake_tensors_eq(t1, t2, fields=("shape", "dtype", "device")):
		if any(not isinstance(t, torch.Tensor) for t in (t1, t2)):
			return False
		for field in fields:
			if getattr(t1, field) != getattr(t2, field):
				return False
		return True

	def replace_no_op(node, replace_input_index):
		replacement = node.args[replace_input_index]

		if not all(isinstance(arg, torch.fx.Node) for arg in node.args):
			return

		if not fake_tensors_eq(node.meta["val"], replacement.meta["val"]):
			if fake_tensors_eq(
				node.meta["val"],
				replacement.meta["val"],
				("shape", "device"),
			):
				with graph.inserting_after(node):
					replacement = graph.call_function(
						torch.ops.prims.convert_element_type.default,
						args=(replacement, node.meta["val"].dtype),
					)
			else:
				return

		node.replace_all_uses_with(replacement)
		replacement.meta.update(node.meta)
		graph.erase_node(node)

	for node in graph.nodes:
		if node.op != "call_function":
			continue

		if node.target == aten.add.Tensor and len(node.args) == 2:
			if (
				not any(e in zeros for e in node.args)
				or node.kwargs.get("alpha", 1) != 1
			):
				continue

			replace_index = 1 if node.args[0] in zeros else 0
			replace_no_op(node, replace_index)

		elif node.target == aten.sub.Tensor and len(node.args) == 2:
			if node.args[1] not in zeros or node.kwargs.get("alpha", 1) != 1:
				continue

			replace_no_op(node, 0)

		elif node.target == aten.mul.Tensor and len(node.args) == 2:
			if not any(e in ones for e in node.args):
				continue

			replace_input_index = 1 if node.args[0] in ones else 0
			replace_no_op(node, replace_input_index)

		elif (
			node.target == aten.div.Tensor
			and len(node.args) == 2
			and node.args[1] in ones
		):
			replace_no_op(node, 0)


@torch.utils._python_dispatch._disable_current_modes()
def remove_redundant_views(gm: torch.fx.GraphModule):

	views: Dict[torch.fx.Node, Dict[torch.dtype, torch.fx.Node]] = {}
	graph = gm.graph

	for node in graph.nodes:
		if node.op != "call_function":
			continue

		if node.target != torch.ops.aten.view.dtype:
			continue

		src = node.args[0]
		to_type = node.args[1]
		existing_views = views.get(src)
		is_needed = True

		if existing_views:
			alias = existing_views.get(to_type)
			if alias:
				is_needed = False
				node.replace_all_uses_with(alias)
				alias.meta.update(node.meta)
				graph.erase_node(node)
		else:
			from_type = src.meta["val"].dtype
			existing_views = {from_type: src}
			views[src] = existing_views

		if is_needed:
			existing_views.setdefault(to_type, node)
			views[node] = existing_views

	while True:
		unused_views = []
		for alias in views:
			if not alias.users:
				unused_views.append(alias)
		if len(unused_views) == 0:
			break
		for unused in unused_views:
			views.pop(unused)
			graph.erase_node(unused)


class UniformValueConstantFolder(ConstantFolder):

	def __init__(self, gm, skip_constructors=False):
		super().__init__(gm, skip_constructors)
		self.node_storages_ptrs: Dict[torch.fx.Node, int] = {}
		self.constant_data_ptrs: Dict[torch.fx.Node, StorageWeakRef] = {}

	def insertable_tensor_check(self, t: torch.Tensor) -> bool:
		return (
			t.numel() != 0
			and bool((t == t.flatten()[0]).all())
			and torch._C._has_storage(t)
			and t.layout == torch.strided
		)

	def add_node_replacement(self, node: torch.fx.Node, tensor: torch.Tensor) -> None:
		self.node_replacements[node] = tensor.flatten()[0].item()
		self.constant_data_ptrs[node] = StorageWeakRef(tensor.untyped_storage())


@torch.utils._python_dispatch._disable_current_modes()
def constant_fold_uniform_value(gm: torch.fx.GraphModule):
	"Runs constant folding and replaces constants which can be constructed with a single `full` call. Calls into remove_no_ops."
	aten = torch.ops.aten

	cf = UniformValueConstantFolder(gm)
	cf.run()

	node_replacements = cf.node_replacements

	graph = gm.graph

	zeros = set()
	ones = set()

	constant_data_ptr_count: typing.Counter[StorageWeakRef] = Counter()

	for node in cf.node_replacements:
		constant_data_ptr_count[cf.constant_data_ptrs[node]] += 1

	for node, value in node_replacements.items():
		fake_tensor = node.meta["val"]
		if not fake_tensor.is_contiguous(memory_format=torch.contiguous_format):
			continue

		if constant_data_ptr_count[cf.constant_data_ptrs[node]] > 1:
			continue

		with graph.inserting_after(node):
			if (
				node.op == "call_function"
				and node.target == aten.full.default
				and len(node.args) == 2
			):
				value = node.args[1]

			new_node = graph.call_function(
				aten.full.default,
				args=(list(fake_tensor.shape), value),
				kwargs={
					"dtype": fake_tensor.dtype,
					"layout": torch.strided,
					"device": fake_tensor.device,
					"pin_memory": False,
				},
			)

			new_node.meta.update(node.meta)
			node.replace_all_uses_with(new_node)
			graph.erase_node(node)

			if value == 0:
				zeros.add(new_node)
			elif value == 1:
				ones.add(new_node)

	remove_no_ops(gm, zeros, ones)
	remove_redundant_views(gm)


def joint_graph_passes(graph: torch.fx.GraphModule):
	lazy_init()
	count = 0

	if config.joint_graph_constant_folding:
		constant_fold_uniform_value(graph)

	if config.pattern_matcher:
		count += patterns.apply(graph.graph)

	if not config.fallback_random:
		count += replace_random_passes(graph)

	if count:
		stable_topological_sort(graph.graph)
		graph.graph.lint()
		graph.recompile()
	return graph


@register_graph_pattern(
	CallFunction(
		torch.ops.prims.convert_element_type.default,
		CallFunction(
			torch.ops.prims.convert_element_type.default,
			KeywordArg("arg"),
			KeywordArg("dtype1"),
		),
		KeywordArg("dtype2"),
	),
	pass_dict=patterns,
)
def pointless_convert(match: Match, arg, dtype1: torch.dtype, dtype2: torch.dtype):
	graph = match.graph
	node = match.output_node()
	arg_size = list(node.args[0].meta["val"].shape)
	if size == arg_size:
		node.replace_all_uses_with(node.args[0])
		match.erase_nodes(graph)

<END>

<START>
import tokenize

from typing import Dict, List, Optional

cache: Dict[str, Dict[int, str]] = {}


def clearcache() -> None:
	cache.clear()


def _add_file(filename: str) -> None:
	try:
		with open(filename) as f:
			tokens = list(tokenize.generate_tokens(f.readline))
	except OSError:
		cache[filename] = {}
		return

	result: Dict[int, str] = {}
	cur_name = ""
	cur_indent = 0
	significant_indents: List[int] = []

	for i, token in enumerate(tokens):
		if token.type == tokenize.INDENT:
			cur_indent += 1
		elif token.type == tokenize.DEDENT:
			cur_indent -= 1
			if significant_indents and cur_indent == significant_indents[-1]:
				significant_indents.pop()
				cur_name = cur_name.rpartition(".")[0]
		elif (
			token.type == tokenize.NAME
			and i + 1 < len(tokens)
			and tokens[i + 1].type == tokenize.NAME
			and (token.string == "class" or token.string == "def")
		):
			significant_indents.append(cur_indent)
			if cur_name:
				cur_name += "."
			cur_name += tokens[i + 1].string
		result[token.start[0]] = cur_name

	cache[filename] = result


def get_funcname(filename: str, lineno: int) -> Optional[str]:
	if filename not in cache:
		_add_file(filename)
	return cache[filename].get(lineno, None)

<END>

<START>
from __future__ import annotations
from collections import OrderedDict
from typing import Any, Callable, Dict, Tuple, Union, List

import torch

from .fake_quantize import (
	default_weight_fake_quant,
	FixedQParamsFakeQuantize,
)
from .observer import (
	_PartialWrapper,
	default_fixed_qparams_range_0to1_observer,
	default_fixed_qparams_range_neg1to1_observer,
	default_placeholder_observer,
	default_weight_observer,
)
from .qconfig import (
	default_reuse_input_qconfig,
	default_symmetric_qnnpack_qconfig,
	default_symmetric_qnnpack_qat_qconfig,
	get_default_qconfig,
	get_default_qat_qconfig,
	QConfig,
	QConfigAny,
	default_quint8_weight_qconfig
)


__all__ = [
	"get_default_qconfig_mapping",
	"get_default_qat_qconfig_mapping",
	"QConfigMapping",
]


_GLOBAL_DICT_KEY = ""
_OBJECT_TYPE_DICT_KEY = "object_type"
_MODULE_NAME_REGEX_DICT_KEY = "module_name_regex"
_MODULE_NAME_DICT_KEY = "module_name"
_MODULE_NAME_OBJECT_TYPE_ORDER_DICT_KEY = "module_name_object_type_order"

_FIXED_QPARAMS_OP_TO_OBSERVER: Dict[Union[Callable, str], _PartialWrapper] = {
	torch.nn.Hardsigmoid: default_fixed_qparams_range_0to1_observer,
	torch.nn.functional.hardsigmoid: default_fixed_qparams_range_0to1_observer,
	"hardsigmoid": default_fixed_qparams_range_0to1_observer,
	"hardsigmoid_": default_fixed_qparams_range_0to1_observer,
	torch.nn.Sigmoid: default_fixed_qparams_range_0to1_observer,
	torch.sigmoid: default_fixed_qparams_range_0to1_observer,
	"sigmoid": default_fixed_qparams_range_0to1_observer,
	"sigmoid_": default_fixed_qparams_range_0to1_observer,
	torch.nn.Softmax: default_fixed_qparams_range_0to1_observer,
	torch.nn.Tanh: default_fixed_qparams_range_neg1to1_observer,
	torch.tanh: default_fixed_qparams_range_neg1to1_observer,
	"tanh": default_fixed_qparams_range_neg1to1_observer,
	"tanh_": default_fixed_qparams_range_neg1to1_observer,
}


def _get_default_qconfig_mapping(is_qat: bool, backend: str, version: int) -> QConfigMapping:
	if is_qat:
		qconfig = get_default_qat_qconfig(backend, version)
	else:
		qconfig = get_default_qconfig(backend, version)
	default_weight = default_weight_fake_quant if is_qat else default_weight_observer

	if backend in ("fbgemm", "x86"):
		qconfig_transpose = QConfig(activation=qconfig.activation, weight=default_weight)
	else:
		qconfig_transpose = qconfig

	qconfig_layernorm = QConfig(activation=qconfig.activation, weight=default_placeholder_observer)

	qconfig_mapping = QConfigMapping() \
		.set_global(qconfig) \
		.set_object_type("reshape", default_reuse_input_qconfig) \
		.set_object_type(torch.nn.ConvTranspose1d, qconfig_transpose) \
		.set_object_type(torch.nn.ConvTranspose2d, qconfig_transpose) \
		.set_object_type(torch.nn.ConvTranspose3d, qconfig_transpose) \
		.set_object_type(torch.nn.functional.conv_transpose1d, qconfig_transpose) \
		.set_object_type(torch.nn.functional.conv_transpose2d, qconfig_transpose) \
		.set_object_type(torch.nn.functional.conv_transpose3d, qconfig_transpose) \
		.set_object_type(torch.nn.functional.layer_norm, qconfig_layernorm) \
		.set_object_type(torch.nn.LayerNorm, qconfig_layernorm) \
		.set_object_type(torch.nn.PReLU, default_quint8_weight_qconfig) \

	fixed_qparams_observer_to_qconfig: Dict[Any, QConfigAny] = {}
	for fixed_qparams_op, observer in _FIXED_QPARAMS_OP_TO_OBSERVER.items():
		if observer in fixed_qparams_observer_to_qconfig:
			fixed_qparams_qconfig = fixed_qparams_observer_to_qconfig[observer]
		else:
			if is_qat:
				activation = FixedQParamsFakeQuantize.with_args(observer=observer)
			else:
				activation = observer
			fixed_qparams_qconfig = QConfig(activation=activation, weight=default_weight)
			fixed_qparams_observer_to_qconfig[observer] = fixed_qparams_qconfig
		qconfig_mapping.set_object_type(fixed_qparams_op, fixed_qparams_qconfig)


	return qconfig_mapping

def get_default_qconfig_mapping(backend="x86", version=0) -> QConfigMapping:
	return _get_default_qconfig_mapping(False, backend, version)

def get_default_qat_qconfig_mapping(backend="x86", version=1) -> QConfigMapping:
	return _get_default_qconfig_mapping(True, backend, version)

def _get_symmetric_qnnpack_qconfig_mapping() -> QConfigMapping:
	default_qconfig = default_symmetric_qnnpack_qconfig
	return _get_default_qconfig_mapping_with_default_qconfig(False, "qnnpack", default_qconfig)

def _get_symmetric_qnnpack_qat_qconfig_mapping() -> QConfigMapping:
	default_qconfig = default_symmetric_qnnpack_qat_qconfig
	return _get_default_qconfig_mapping_with_default_qconfig(True, "qnnpack", default_qconfig)

def _get_default_qconfig_mapping_with_default_qconfig(
	is_qat: bool,
	backend: str,
	default_qconfig: QConfig,
) -> QConfigMapping:
	if is_qat:
		qconfig_mapping = get_default_qat_qconfig_mapping(backend)
	else:
		qconfig_mapping = get_default_qconfig_mapping(backend)
	qconfig_mapping.set_global(default_qconfig)
	for pattern in qconfig_mapping.object_type_qconfigs.keys():
		if pattern not in _FIXED_QPARAMS_OP_TO_OBSERVER:
			qconfig_mapping.set_object_type(pattern, default_qconfig)
	return qconfig_mapping

_QCONFIG_STYLE_ORDER: List[str] = [
	"global_qconfig",
	"object_type_qconfigs",
	"module_name_regex_qconfigs",
	"module_name_qconfigs",
	"module_name_object_type_order_qconfigs",
]

class QConfigMapping:

	def __init__(self):
		self.global_qconfig: QConfigAny = None
		self.object_type_qconfigs: OrderedDict[Union[Callable, str], QConfigAny] = OrderedDict()
		self.module_name_regex_qconfigs: OrderedDict[str, QConfigAny] = OrderedDict()
		self.module_name_qconfigs: OrderedDict[str, QConfigAny] = OrderedDict()
		self.module_name_object_type_order_qconfigs: OrderedDict[Tuple[str, Callable, int], QConfigAny] =\
			OrderedDict()

	def set_global(self, global_qconfig: QConfigAny) -> QConfigMapping:
		self.global_qconfig = global_qconfig
		return self

	def set_object_type(self, object_type: Union[Callable, str], qconfig: QConfigAny) -> QConfigMapping:
		self.object_type_qconfigs[object_type] = qconfig
		return self

	def set_module_name_regex(self, module_name_regex: str, qconfig: QConfigAny) -> QConfigMapping:
		self.module_name_regex_qconfigs[module_name_regex] = qconfig
		return self

	def set_module_name(self, module_name: str, qconfig: QConfigAny) -> QConfigMapping:
		self.module_name_qconfigs[module_name] = qconfig
		return self

	def set_module_name_object_type_order(
			self,
			module_name: str,
			object_type: Callable,
			index: int,
			qconfig: QConfigAny) -> QConfigMapping:
		self.module_name_object_type_order_qconfigs[(module_name, object_type, index)] = qconfig
		return self

	def __repr__(self) -> str:
		output = self.__class__.__name__ + " ("
		for style_name in _QCONFIG_STYLE_ORDER:
			output += f"\n {style_name}"
			qconfigs = getattr(self, style_name)
			if isinstance(qconfigs, OrderedDict) and len(qconfigs) > 0:
				for key, qconfig in qconfigs.items():
					output += f"\n  {key}: {qconfig}"
			else:
				output += f"\n  {qconfigs}"
		return output + "\n)"

	def to_dict(self) -> Dict[str, Any]:
		return {
			_GLOBAL_DICT_KEY: self.global_qconfig,
			_OBJECT_TYPE_DICT_KEY: list(self.object_type_qconfigs.items()),
			_MODULE_NAME_REGEX_DICT_KEY: list(self.module_name_regex_qconfigs.items()),
			_MODULE_NAME_DICT_KEY: list(self.module_name_qconfigs.items()),
			_MODULE_NAME_OBJECT_TYPE_ORDER_DICT_KEY: [
				(*k, v) for k, v in self.module_name_object_type_order_qconfigs.items()
			],
		}

	@classmethod
	def from_dict(cls, qconfig_dict: Dict[str, Any]) -> QConfigMapping:
		conf = cls()
		if _GLOBAL_DICT_KEY in qconfig_dict:
			conf.set_global(qconfig_dict[_GLOBAL_DICT_KEY])
		for object_type, qconfig in qconfig_dict.get(_OBJECT_TYPE_DICT_KEY, []):
			conf.set_object_type(object_type, qconfig)
		for module_name_regex, qconfig in qconfig_dict.get(_MODULE_NAME_REGEX_DICT_KEY, []):
			conf.set_module_name_regex(module_name_regex, qconfig)
		for module_name, qconfig in qconfig_dict.get(_MODULE_NAME_DICT_KEY, []):
			conf.set_module_name(module_name, qconfig)
		for module_name, object_type, index, qconfig in qconfig_dict.get(_MODULE_NAME_OBJECT_TYPE_ORDER_DICT_KEY, []):
			conf.set_module_name_object_type_order(module_name, object_type, index, qconfig)
		return conf

<END>

<START>
import torch
import inspect

__all__ = ["pytree_register_structseq"]

return_types = torch._C._return_types  # type: ignore[attr-defined]

def pytree_register_structseq(cls):
	def structseq_flatten(structseq):
		return list(structseq), None

	def structseq_unflatten(values, context):
		return cls(values)

	torch.utils._pytree.register_pytree_node(cls, structseq_flatten, structseq_unflatten)

for name in dir(return_types):
	if name.startswith('__'):
		continue

	_attr = getattr(return_types, name)
	globals()[name] = _attr

	if not name.startswith('_'):
		__all__.append(name)

	if inspect.isclass(_attr) and issubclass(_attr, tuple):
		pytree_register_structseq(_attr)

<END>

<START>

import math

def float_to_apot(x, levels, indices, alpha):
	if x < -alpha:
		return -alpha
	elif x > alpha:
		return alpha

	levels_lst = list(levels)
	indices_lst = list(indices)

	min_delta = math.inf
	best_idx = 0

	for level, idx in zip(levels_lst, indices_lst):
		cur_delta = abs(level - x)
		if cur_delta < min_delta:
			min_delta = cur_delta
			best_idx = idx

	return best_idx

def quant_dequant_util(x, levels, indices):
	levels_lst = list(levels)
	indices_lst = list(indices)

	min_delta = math.inf
	best_fp = 0.0

	for level, idx in zip(levels_lst, indices_lst):
		cur_delta = abs(level - x)
		if cur_delta < min_delta:
			min_delta = cur_delta
			best_fp = level

	return best_fp

def apot_to_float(x_apot, levels, indices):
	idx = list(indices).index(x_apot)
	return levels[idx]

<END>

<START>
__all__ = ["make_fx", "dispatch_trace", "PythonKeyTracer", "pythonkey_decompose"]
from torch.fx.experimental.proxy_tensor import make_fx, dispatch_trace, PythonKeyTracer, decompose

pythonkey_decompose = decompose

<END>

<START>
from typing import Any, Dict, List, Optional
import torch
from collections import defaultdict
from torch import nn
import copy
from ...sparsifier.utils import fqn_to_module, module_to_fqn
import warnings

__all__ = ['ActivationSparsifier']


class ActivationSparsifier:
	def __init__(self, model: nn.Module, aggregate_fn=None, reduce_fn=None, mask_fn=None,
				 features=None, feature_dim=None, **sparse_config):
		self.model = model
		self.defaults: Dict[str, Any] = defaultdict()
		self.defaults['sparse_config'] = sparse_config

		self.defaults['aggregate_fn'] = aggregate_fn
		self.defaults['reduce_fn'] = reduce_fn
		self.defaults['mask_fn'] = mask_fn

		self.defaults['features'] = features
		self.defaults['feature_dim'] = feature_dim

		self.data_groups: Dict[str, Dict] = defaultdict(dict)  # contains all relevant info w.r.t each registered layer

		self.state: Dict[str, Any] = defaultdict(dict)  # layer name -> mask

	@staticmethod
	def _safe_rail_checks(args):

		features, feature_dim = args['features'], args['feature_dim']
		if features is not None:
			assert feature_dim is not None, "need feature dim to select features"

		fn_keys = ['aggregate_fn', 'reduce_fn', 'mask_fn']
		for key in fn_keys:
			fn = args[key]
			assert callable(fn), 'function should be callable'

	def _aggregate_hook(self, name):

		feature_dim = self.data_groups[name]['feature_dim']
		features = self.data_groups[name]['features']
		agg_fn = self.data_groups[name]['aggregate_fn']

		def hook(module, input) -> None:
			input_data = input[0]

			data = self.data_groups[name].get('data')  # aggregated data
			if features is None:
				if data is None:
					data = torch.zeros_like(input_data)
					self.state[name]['mask'] = torch.ones_like(input_data)
				out_data = agg_fn(data, input_data)
			else:
				if data is None:
					out_data = [0 for _ in range(0, len(features))]  # create one incase of 1st forward
					self.state[name]['mask'] = [0 for _ in range(0, len(features))]
				else:
					out_data = data  # a list

				for feature_idx in range(len(features)):
					feature_tensor = torch.Tensor([features[feature_idx]]).long().to(input_data.device)
					data_feature = torch.index_select(input_data, feature_dim, feature_tensor)
					if data is None:
						curr_data = torch.zeros_like(data_feature)
						self.state[name]['mask'][feature_idx] = torch.ones_like(data_feature)
					else:
						curr_data = data[feature_idx]
					out_data[feature_idx] = agg_fn(curr_data, data_feature)
			self.data_groups[name]['data'] = out_data
		return hook

	def register_layer(self, layer: nn.Module, aggregate_fn=None, reduce_fn=None,
					   mask_fn=None, features=None, feature_dim=None, **sparse_config):
		name = module_to_fqn(self.model, layer)
		assert name is not None, "layer not found in the model"  # satisfy mypy

		if name in self.data_groups:  # unregister layer if already present
			warnings.warn("layer already attached to the sparsifier, deregistering the layer and registering with new config")
			self.unregister_layer(name=name)

		local_args = copy.deepcopy(self.defaults)
		update_dict = {
			'aggregate_fn': aggregate_fn,
			'reduce_fn': reduce_fn,
			'mask_fn': mask_fn,
			'features': features,
			'feature_dim': feature_dim,
			'layer': layer
		}
		local_args.update((arg, val) for arg, val in update_dict.items() if val is not None)
		local_args['sparse_config'].update(sparse_config)

		self._safe_rail_checks(local_args)

		self.data_groups[name] = local_args
		agg_hook = layer.register_forward_pre_hook(self._aggregate_hook(name=name))

		self.state[name]['mask'] = None  # mask will be created when model forward is called.

		self.data_groups[name]['hook'] = agg_hook

		self.data_groups[name]['hook_state'] = "aggregate"  # aggregate hook is attached

	def get_mask(self, name: Optional[str] = None, layer: Optional[nn.Module] = None):
		assert name is not None or layer is not None, "Need at least name or layer obj to retrieve mask"

		if name is None:
			assert layer is not None
			name = module_to_fqn(self.model, layer)
			assert name is not None, "layer not found in the specified model"

		if name not in self.state:
			raise ValueError("Error: layer with the given name not found")

		mask = self.state[name].get('mask', None)

		if mask is None:
			raise ValueError("Error: shape unknown, call layer() routine at least once to infer mask")
		return mask

	def unregister_layer(self, name):

		self.data_groups[name]['hook'].remove()

		self.state.pop(name)

		self.data_groups.pop(name)

	def step(self):
		with torch.no_grad():
			for name, configs in self.data_groups.items():
				data = configs['data']
				self.update_mask(name, data, configs)

				self.data_groups[name].pop('data')  # reset the accumulated data

	def update_mask(self, name, data, configs):
		mask = self.get_mask(name)
		sparse_config = configs['sparse_config']
		features = configs['features']
		reduce_fn = configs['reduce_fn']
		mask_fn = configs['mask_fn']
		if features is None:
			data = reduce_fn(data)
			mask.data = mask_fn(data, **sparse_config)
		else:
			for feature_idx in range(len(features)):
				data_feature = reduce_fn(data[feature_idx])
				mask[feature_idx].data = mask_fn(data_feature, **sparse_config)

	def _sparsify_hook(self, name):
		mask = self.get_mask(name)
		features = self.data_groups[name]['features']
		feature_dim = self.data_groups[name]['feature_dim']

		def hook(module, input):
			input_data = input[0]
			if features is None:
				return input_data * mask
			else:
				for feature_idx in range(0, len(features)):
					feature = torch.Tensor([features[feature_idx]]).long().to(input_data.device)
					sparsified = torch.index_select(input_data, feature_dim, feature) * mask[feature_idx]
					input_data.index_copy_(feature_dim, feature, sparsified)
				return input_data
		return hook

	def squash_mask(self, attach_sparsify_hook=True, **kwargs):
		for name, configs in self.data_groups.items():
			configs['hook'].remove()
			configs.pop('hook')
			self.data_groups[name]['hook_state'] = "None"
			if attach_sparsify_hook:
				configs['hook'] = configs['layer'].register_forward_pre_hook(self._sparsify_hook(name))
			configs['hook_state'] = "sparsify"  # signals that sparsify hook is now attached

	def _get_serializable_data_groups(self):
		data_groups: Dict[str, Any] = defaultdict()
		for name, config in self.data_groups.items():
			new_config = {key: value for key, value in config.items() if key not in ['hook', 'layer']}
			data_groups[name] = new_config
		return data_groups

	def _convert_mask(self, states_dict, sparse_coo=True):
		states = copy.deepcopy(states_dict)
		for state in states.values():
			if state['mask'] is not None:
				if isinstance(state['mask'], List):
					for idx in range(len(state['mask'])):
						if sparse_coo:
							state['mask'][idx] = state['mask'][idx].to_sparse_coo()
						else:
							state['mask'][idx] = state['mask'][idx].to_dense()
				else:
					if sparse_coo:
						state['mask'] = state['mask'].to_sparse_coo()
					else:
						state['mask'] = state['mask'].to_dense()
		return states

	def state_dict(self) -> Dict[str, Any]:
		data_groups = self._get_serializable_data_groups()
		state = self._convert_mask(self.state)
		return {
			'state': state,
			'data_groups': data_groups,
			'defaults': self.defaults
		}

	def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
		state = state_dict['state']
		data_groups, defaults = state_dict['data_groups'], state_dict['defaults']

		self.__set_state__({'state': state, 'data_groups': data_groups, 'defaults': defaults})

	def __get_state__(self) -> Dict[str, Any]:

		data_groups = self._get_serializable_data_groups()
		state = self._convert_mask(self.state)
		return {
			'defaults': self.defaults,
			'state': state,
			'data_groups': data_groups,
		}

	def __set_state__(self, state: Dict[str, Any]) -> None:
		state['state'] = self._convert_mask(state['state'], sparse_coo=False)  # convert mask to dense tensor
		self.__dict__.update(state)

		for name, config in self.data_groups.items():
			layer = fqn_to_module(self.model, name)
			assert layer is not None  # satisfy mypy

			if "hook_state" in config and config['hook_state'] == "aggregate":
				hook = layer.register_forward_pre_hook(self._aggregate_hook(name))

			elif "hook_state" in config and config["hook_state"] == "sparsify":
				hook = layer.register_forward_pre_hook(self._sparsify_hook(name))

			config['layer'] = layer
			config['hook'] = hook

	def __repr__(self):
		format_string = self.__class__.__name__ + ' ('
		for name, config in self.data_groups.items():
			format_string += '\n'
			format_string += '\tData Group\n'
			format_string += f'\t	name: {name}\n'
			for key in sorted(config.keys()):
				if key in ['data', 'hook', 'reduce_fn', 'mask_fn', 'aggregate_fn']:
					continue
				format_string += f'\t	{key}: {config[key]}\n'
		format_string += ')'
		return format_string

<END>

<START>

import inspect
import logging
import os
import pickle
import socket
import threading
import time
import weakref
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, cast

from torch.distributed import PrefixStore, Store
from torch.distributed.elastic.events import (
	NodeState,
	construct_and_record_rdzv_event,
)

from .api import (
	RendezvousClosedError,
	RendezvousError,
	RendezvousHandler,
	RendezvousParameters,
	RendezvousStateError,
	RendezvousTimeoutError,
)
from .utils import _delay, _PeriodicTimer

__all__ = ['RendezvousBackend', 'RendezvousTimeout', 'RendezvousSettings', 'DynamicRendezvousHandler', 'create_handler']

log = logging.getLogger(__name__)


def get_method_name(depth=2):
	if len(inspect.stack()) > depth:
		return inspect.stack()[depth].function
	return "no_method_name"


Token = Any

	@property
	@abstractmethod
	def name(self) -> str:

		Returns:
			A tuple of the encoded rendezvous state and its fencing token or
			``None`` if no state is found in the backend.

		Raises:
			RendezvousConnectionError:
				The connection to the backend has failed.
			RendezvousStateError:
				The rendezvous state is corrupt.

		The new rendezvous state is set conditionally:

		  - If the specified ``token`` matches the fencing token stored in the
			backend, the state will be updated. The new state will be returned
			to the caller along with its fencing token.
		  - If the specified ``token`` does not match the fencing token stored
			in the backend, the state won't be updated; instead the existing
			state along with its fencing token will be returned to the caller.
		  - If the specified ``token`` is ``None``, the new state will be set
			only if there is no existing state in the backend. Either the new
			state or the existing state along with its fencing token will be
			returned to the caller.

		Args:
			state:
				The encoded rendezvous state.
			token:
				An optional fencing token that was retrieved by a previous call
				to :py:meth:`get_state` or ``set_state()``.

		Returns:
			A tuple of the serialized rendezvous state, its fencing token, and
			a boolean value indicating whether our set attempt succeeded.

		Raises:
			RendezvousConnectionError:
				The connection to the backend has failed.
			RendezvousStateError:
				The rendezvous state is corrupt.

	Args:
		join:
			The time within which the rendezvous is expected to complete.
		last_call:
			An additional wait amount before completing the rendezvous once the
			rendezvous has the minimum number of required participants.
		close:
			The time within which the rendezvous is expected to close after a
			call to :py:meth:`RendezvousHandler.set_closed` or
			:py:meth:`RendezvousHandler.shutdown`.
		keep_alive:
			The time within which a keep-alive heartbeat is expected to
			complete.
		return self._join

	@property
	def last_call(self) -> timedelta:
		return self._close

	@property
	def heartbeat(self) -> timedelta:

	Attributes:
		run_id:
			The run id of the rendezvous.
		min_nodes:
			The minimum number of nodes to admit to the rendezvous.
		max_nodes:
			The maximum number of nodes to admit to the rendezvous.
		timeout:
			The timeout configuration of the rendezvous.
		keep_alive_interval:
			The amount of time a node waits before sending a heartbeat to keep
			it alive in the rendezvous.
		keep_alive_max_attempt:
			The maximum number of failed heartbeat attempts after which a node
			is considered dead.

	Attributes:
		addr:
			The FQDN of the node or user specified local node address.
		pid:
			The id of the process in which the rendezvous handler runs.
		local_id:
			A process-wide unique id.

	A node descriptor is a combination of an FQDN, a process id, and an auto-
	incremented integer that uniquely identifies a node in the rendezvous.

	Attributes:
		round:
			The current round of the rendezvous.
		complete:
			A boolean value indicating whether the current round of the
			rendezvous is complete.
		deadline:
			The time at which the current round of the rendezvous will be
			considered complete if it is still waiting for nodes to join.
		closed:
			A boolean value indicating whether the rendezvous is closed.
		participants:
			A dictionary of the participants and their corresponding ranks.
		wait_list:
			A set of nodes that are waiting to participate in the next round of
			the rendezvous.
		last_heartbeats:
			A dictionary containing each node's last heartbeat time.

	@property
	@abstractmethod
	def state(self) -> _RendezvousState:

		Returns:
			A boolean value indicating whether the local state, in case marked
			as dirty, was successfully synced with other nodes.


class _BackendRendezvousStateHolder(_RendezvousStateHolder):

	_backend: RendezvousBackend
	_state: _RendezvousState
	_settings: RendezvousSettings
	_cache_duration: int
	_token: Token
	_dirty: bool
	_last_sync_time: float
	_dead_nodes: List[_NodeDesc]

	def __init__(
		self,
		backend: RendezvousBackend,
		settings: RendezvousSettings,
		cache_duration: int = 1,
	) -> None:
		self._backend = backend
		self._state = _RendezvousState()
		self._settings = settings
		self._cache_duration = cache_duration
		self._token = None
		self._dirty = False
		self._last_sync_time = -1
		self._dead_nodes = []

	def _record(self, message: str, node_state: NodeState = NodeState.RUNNING):
		construct_and_record_rdzv_event(
			name=f"{self.__class__.__name__}.{get_method_name()}",
			run_id=self._settings.run_id,
			message=message,
			node_state=node_state,
		)

	@property
	def state(self) -> _RendezvousState:
		state_bits: Optional[bytes] = None

		token = None

		has_set: Optional[bool]

		if self._dirty:
			has_set = False

			state_bits = pickle.dumps(self._state)

			set_response = self._backend.set_state(state_bits, self._token)
			if set_response is not None:
				state_bits, token, has_set = set_response
		else:
			has_set = None

			if self._cache_duration > 0:
				if self._last_sync_time >= max(time.monotonic() - self._cache_duration, 0):
					return None

			get_response = self._backend.get_state()
			if get_response is not None:
				state_bits, token = get_response

		if state_bits is not None:
			try:
				self._state = pickle.loads(state_bits)
			except pickle.PickleError as exc:
				raise RendezvousStateError(
					"The rendezvous state is corrupt. See inner exception for details."
				) from exc
		else:
			self._state = _RendezvousState()

		if has_set and self._dead_nodes and log.isEnabledFor(logging.DEBUG):
			node_list = ", ".join(f"'{dead_node}'" for dead_node in self._dead_nodes)

			msg = (
				f"As part of the sync operation the node(s) {node_list} have been removed from the "
				f"rendezvous '{self._settings.run_id}' since they had no heartbeat."
			)
			self._record(message=msg)
			log.debug(msg)

		self._token = token

		self._dirty = False

		self._last_sync_time = time.monotonic()

		self._sanitize()

		return has_set

	def _sanitize(self) -> None:
		state = self._state

		expire_time = datetime.utcnow() - (
			self._settings.keep_alive_interval * self._settings.keep_alive_max_attempt
		)

		self._dead_nodes = [
			node
			for node, last_heartbeat in state.last_heartbeats.items()
			if last_heartbeat < expire_time
		]

		participant_removed = False

		for dead_node in self._dead_nodes:
			del state.last_heartbeats[dead_node]

			try:
				del state.participants[dead_node]

				participant_removed = True
			except KeyError:
				pass

			try:
				state.wait_list.remove(dead_node)
			except KeyError:
				pass

		if participant_removed:
			_remove_participant_epilogue(state, self._settings)

	def mark_dirty(self) -> None:
		self._dirty = True


class _Action(Enum):

	Attributes:
		node:
			The node descriptor associated with the current rendezvous handler
			instance.
		state:
			The current state of the rendezvous.
		settings:
			The rendezvous settings.

	@abstractmethod
	def run(
		self,
		state_handler: Callable[[_RendezvousContext, float], _Action],
		deadline: float,
	) -> None:


class _DistributedRendezvousOpExecutor(_RendezvousOpExecutor):

	_node: _NodeDesc
	_state: _RendezvousState
	_state_holder: _RendezvousStateHolder
	_settings: RendezvousSettings

	def __init__(
		self,
		node: _NodeDesc,
		state_holder: _RendezvousStateHolder,
		settings: RendezvousSettings,
	) -> None:
		self._node = node
		self._state_holder = state_holder
		self._settings = settings

	def _record(self, message: str, node_state: NodeState = NodeState.RUNNING) -> None:
		construct_and_record_rdzv_event(
			name=f"{self.__class__.__name__}.{get_method_name()}",
			run_id=self._settings.run_id,
			message=message,
			node_state=node_state,
			hostname=self._node.addr,
			pid=self._node.pid,
			local_id=self._node.local_id,
		)

	def run(
		self,
		state_handler: Callable[[_RendezvousContext, float], _Action],
		deadline: float,
	) -> None:
	try:
		last_heartbeat = ctx.state.last_heartbeats[ctx.node]
	except KeyError:
		return False

	return last_heartbeat <= datetime.utcnow() - ctx.settings.keep_alive_interval


class _RendezvousExitOp:

	def __call__(self, ctx: _RendezvousContext, deadline: float) -> _Action:
		state = ctx.state

		if state.closed:
			return _Action.ERROR_CLOSED

		is_participant = ctx.node in state.participants

		if state.complete and is_participant:
			return _Action.FINISH

		now = time.monotonic()
		if now > deadline:
			rollback_period = 5  # 5 seconds

			if now <= deadline + rollback_period:
				if is_participant:
					return _Action.REMOVE_FROM_PARTICIPANTS
				if ctx.node in state.wait_list:
					return _Action.REMOVE_FROM_WAIT_LIST
			return _Action.ERROR_TIMEOUT

		if state.complete:
			if len(state.participants) < ctx.settings.max_nodes:
				if ctx.node not in state.wait_list:
					return _Action.ADD_TO_WAIT_LIST
		elif is_participant:
			if len(state.participants) >= ctx.settings.min_nodes:
				if cast(datetime, state.deadline) < datetime.utcnow():
					return _Action.MARK_RENDEZVOUS_COMPLETE
		else:
			return _Action.ADD_TO_PARTICIPANTS

		if _should_keep_alive(ctx):
			return _Action.KEEP_ALIVE

		return _Action.SYNC


class _RendezvousCloseOp:

	def __call__(self, ctx: _RendezvousContext, deadline: float) -> _Action:
		if _should_keep_alive(ctx):
			if time.monotonic() > deadline:
				return _Action.ERROR_TIMEOUT
			return _Action.KEEP_ALIVE
		return _Action.FINISH


class DynamicRendezvousHandler(RendezvousHandler):

		Args:
			run_id:
				The run id of the rendezvous.
			store:
				The C10d store to return as part of the rendezvous.
			backend:
				The backend to use to hold the rendezvous state.
			min_nodes:
				The minimum number of nodes to admit to the rendezvous.
			max_nodes:
				The maximum number of nodes to admit to the rendezvous.
			local_addr:
				The local node address.
			timeout:
				The timeout configuration of the rendezvous.
		return self._settings

	def get_backend(self) -> str:
		msg = (
			f"The node '{self._this_node}' attempts to join the next round of the rendezvous "
			f"'{self._settings.run_id}'."
		)
		self._record(message=msg)
		log.info(msg)

		try:
			self._stop_heartbeats()

			if self._state_holder.state.round == 0:
				_delay(seconds=(0, 0.3))

			exit_op = _RendezvousExitOp()
			join_op = _RendezvousJoinOp()

			deadline = self._get_deadline(self._settings.timeout.join)

			self._op_executor.run(exit_op, deadline)
			self._op_executor.run(join_op, deadline)

			self._start_heartbeats()

			rank, world_size = self._get_world()
			store = self._get_store()

		except Exception as e:
			self._record(
				message=f"{type(e).__name__}: {str(e)}",
				node_state=NodeState.FAILED,
			)
			raise

		msg = (
			f"The node '{self._this_node}' has joined round {self._state_holder.state.round} of "
			f"the rendezvous '{self._settings.run_id}' as rank {rank} in a world of size "
			f"{world_size}."
		)
		self._record(message=msg, rank=rank)
		log.info(msg)

		return store, rank, world_size

	def is_closed(self) -> bool:
		try:
			with self._heartbeat_lock:
				self._close()
		except Exception as e:
			self._record(
				message=f"{type(e).__name__}: {str(e)}",
				node_state=NodeState.FAILED,
			)
			raise

	def num_nodes_waiting(self) -> int:
		return self._settings.run_id

	def shutdown(self) -> bool:

	Args:
		store:
			The C10d store to return as part of the rendezvous.
		backend:
			The backend to use to hold the rendezvous state.

	+-------------------+------------------------------------------------------+
	| Parameter		 | Description										  |
	+===================+======================================================+
	| join_timeout	  | The total time, in seconds, within which the		 |
	|				   | rendezvous is expected to complete. Defaults to 600  |
	|				   | seconds.											 |
	+-------------------+------------------------------------------------------+
	| last_call_timeout | An additional wait amount, in seconds, before		|
	|				   | completing the rendezvous once the minimum number of |
	|				   | nodes has been reached. Defaults to 30 seconds.	  |
	+-------------------+------------------------------------------------------+
	| close_timeout	 | The time, in seconds, within which the rendezvous is |
	|				   | expected to close after a call to					|
	|				   | :py:meth:`RendezvousHandler.set_closed` or		   |
	|				   | :py:meth:`RendezvousHandler.shutdown`. Defaults to   |
	|				   | 30 seconds.										  |
	+-------------------+------------------------------------------------------+

<END>

<START>
import torch
from torch.fx import GraphModule
from torch.fx import Node

from .pt2e.prepare import prepare
from .pt2e.qat_utils import (
	_fuse_conv_bn_qat,
	_fold_conv_bn_qat,
)
from .pt2e.utils import (
	_get_node_name_to_scope,
	_fuse_conv_bn_,
	_disallow_eval_train,
)
from .pt2e.representation import reference_representation_rewrite
from .quantize_fx import _convert_to_reference_decomposed_fx
from torch.ao.quantization.quantizer import (  # noqa: F401
	Quantizer,
	QuantizationSpecBase,
	QuantizationSpec,
	FixedQParamsQuantizationSpec,
	SharedQuantizationSpec,
	DerivedQuantizationSpec,
	QuantizationAnnotation,
)
from torch.fx.passes.infra.pass_manager import PassManager
from torch.ao.quantization.pt2e.duplicate_dq_pass import DuplicateDQPass
from torch.ao.quantization.pt2e.port_metadata_pass import PortNodeMetaForQDQ
from torch._inductor.constant_folding import constant_fold

__all__ = [
	"prepare_pt2e",
	"prepare_qat_pt2e",
	"convert_pt2e",
]


def prepare_pt2e(
	model: GraphModule,
	quantizer: Quantizer,
) -> GraphModule:
	torch._C._log_api_usage_once("quantization_api.quantize_pt2e.prepare_pt2e")
	original_graph_meta = model.meta
	node_name_to_scope = _get_node_name_to_scope(model)
	_fuse_conv_bn_(model)
	quantizer.transform_for_annotation(model)
	quantizer.annotate(model)
	quantizer.validate(model)
	model = prepare(model, node_name_to_scope, is_qat=False)
	model.meta.update(original_graph_meta)
	model = _disallow_eval_train(model)
	return model

def prepare_qat_pt2e(
	model: GraphModule,
	quantizer: Quantizer,
) -> GraphModule:
	torch._C._log_api_usage_once("quantization_api.quantize_pt2e.prepare_qat_pt2e")
	original_graph_meta = model.meta
	node_name_to_scope = _get_node_name_to_scope(model)
	quantizer.transform_for_annotation(model)
	quantizer.annotate(model)
	quantizer.validate(model)
	_fuse_conv_bn_qat(model)
	model = prepare(model, node_name_to_scope, is_qat=True)
	model.meta.update(original_graph_meta)
	model = _disallow_eval_train(model)
	return model

_QUANT_OPS = [
	torch.ops.quantized_decomposed.quantize_per_tensor.default,
	torch.ops.quantized_decomposed.quantize_per_tensor.tensor,
	torch.ops.quantized_decomposed.quantize_per_channel.default,
]
def _quant_node_constraint(n: Node) -> bool:
	return n.op == "call_function" and n.target in _QUANT_OPS

def convert_pt2e(
	model: GraphModule,
	use_reference_representation: bool = False,
	fold_quantize: bool = False,
) -> GraphModule:
	torch._C._log_api_usage_once("quantization_api.quantize_pt2e.convert_pt2e")
	original_graph_meta = model.meta
	model = _convert_to_reference_decomposed_fx(model)
	model = _fold_conv_bn_qat(model)

	pm = PassManager([DuplicateDQPass()])
	model = pm(model).graph_module

	pm = PassManager([PortNodeMetaForQDQ()])
	model = pm(model).graph_module

	if fold_quantize:
		constant_fold(model, _quant_node_constraint)

	if use_reference_representation:
		model = reference_representation_rewrite(model)

	model.meta.update(original_graph_meta)
	model = _disallow_eval_train(model)
	return model

<END>

<START>
import dataclasses
import itertools
import operator
from typing import Any, Callable, Dict, List, Tuple, TYPE_CHECKING

import torch
from torch.fx import Graph, GraphModule, Node
from torch.fx.subgraph_rewriter import (
	replace_pattern_with_filters,
	ReplacedPatterns,
)
import torch.nn.functional as F
from torch.ao.quantization.fx._decomposed import quantized_decomposed_lib  # noqa: F401
from torch.ao.quantization.quantizer import (
	DerivedQuantizationSpec,
	EdgeOrNode,
	SharedQuantizationSpec,
	QuantizationSpecBase,
)
from .utils import (
	_conv1d_bn_example_inputs,
	_conv2d_bn_example_inputs,
	_is_conv,
	_is_bn_node,
	fold_bn_weights_into_conv_node,
	get_aten_graph_module,
)

if TYPE_CHECKING:
	from torch.fx.passes.utils.matcher_with_name_node_map_utils import InternalMatch

__all__ = []  # type: ignore[var-annotated]


_quantized_conv1d_bn_example_inputs = (
	torch.randn(1, 1, 3),  # x
	torch.randn(1, 1, 1),  # conv_weight
	torch.randn(1),		# bn_weight
	torch.randn(1),		# bn_bias
	torch.randn(1),		# bn_running_mean
	torch.randn(1),		# bn_running_var
)

_quantized_conv2d_bn_example_inputs = (
	torch.randn(1, 1, 3, 3),  # x
	torch.randn(1, 1, 1, 1),  # conv_weight
	torch.randn(1),		   # bn_weight
	torch.randn(1),		   # bn_bias
	torch.randn(1),		   # bn_running_mean
	torch.randn(1),		   # bn_running_var
)


def _get_quantized_conv_bn_example_inputs_kwargs(
	is_per_channel: bool,
	has_bias: bool,
	is_cuda: bool,
) -> Dict[str, Any]:
	kwargs = {}
	if is_per_channel:
		kwargs["scale"] = torch.tensor([1], dtype=torch.float)
		kwargs["zero_point"] = torch.tensor([0], dtype=torch.int)
	if has_bias:
		kwargs["conv_bias"] = torch.randn(1)
	if is_cuda:
		for k, v in kwargs.items():
			if isinstance(v, torch.Tensor):
				kwargs[k] = v.cuda()
	return kwargs

def _get_conv_bn_pattern(conv_fn: Callable) -> Callable:
	def _conv_bn_pattern(
		x: torch.Tensor,
		conv_weight: torch.Tensor,
		conv_bias: torch.Tensor,
		bn_weight: torch.Tensor,
		bn_bias: torch.Tensor,
		bn_running_mean: torch.Tensor,
		bn_running_var: torch.Tensor,
	) -> torch.Tensor:
		x = conv_fn(x, conv_weight, conv_bias)
		x = F.batch_norm(x, bn_running_mean, bn_running_var, bn_weight, bn_bias, training=True)
		return x
	return _conv_bn_pattern

def _get_qat_conv_bn_pattern(conv_fn: Callable) -> Callable:
	def _qat_conv_bn_pattern(
		x: torch.Tensor,
		conv_weight: torch.Tensor,
		conv_bias: torch.Tensor,
		bn_weight: torch.Tensor,
		bn_bias: torch.Tensor,
		bn_running_mean: torch.Tensor,
		bn_running_var: torch.Tensor,
	) -> torch.Tensor:
		bn_eps = 1e-5
		running_std = torch.sqrt(bn_running_var + bn_eps)
		scale_factor = bn_weight / running_std
		weight_shape = [1] * len(conv_weight.shape)
		weight_shape[0] = -1
		bias_shape = [1] * len(conv_weight.shape)
		bias_shape[1] = -1
		scaled_weight = conv_weight * scale_factor.reshape(weight_shape)
		zero_bias = torch.zeros_like(conv_bias, dtype=x.dtype)
		x = conv_fn(x, scaled_weight, zero_bias)
		x = x / scale_factor.reshape(bias_shape)
		x = x + conv_bias.reshape(bias_shape)
		x = F.batch_norm(x, bn_running_mean, bn_running_var, bn_weight, bn_bias, training=True, eps=bn_eps)
		return x
	return _qat_conv_bn_pattern

def _get_qat_conv_bn_pattern_no_conv_bias(conv_fn: Callable) -> Callable:
	def _qat_conv_bn_pattern_no_conv_bias(
		x: torch.Tensor,
		conv_weight: torch.Tensor,
		conv_bias: torch.Tensor,
		bn_weight: torch.Tensor,
		bn_bias: torch.Tensor,
		bn_running_mean: torch.Tensor,
		bn_running_var: torch.Tensor,
	) -> torch.Tensor:
		bn_eps = 1e-5
		running_std = torch.sqrt(bn_running_var + bn_eps)
		scale_factor = bn_weight / running_std
		weight_shape = [1] * len(conv_weight.shape)
		weight_shape[0] = -1
		bias_shape = [1] * len(conv_weight.shape)
		bias_shape[1] = -1
		scaled_weight = conv_weight * scale_factor.reshape(weight_shape)
		x = conv_fn(x, scaled_weight, None)
		x = x / scale_factor.reshape(bias_shape)
		x = F.batch_norm(x, bn_running_mean, bn_running_var, bn_weight, bn_bias, training=True, eps=bn_eps)
		return x
	return _qat_conv_bn_pattern_no_conv_bias

def _append_qdq(x, is_per_channel, kwargs):
	per_channel_axis = 0
	scale = kwargs["scale"] if is_per_channel else 1.0
	zp = kwargs["zero_point"] if is_per_channel else 0
	qmin = -127
	qmax = 127
	dtype = torch.int8

	qd = torch.ops.quantized_decomposed
	if is_per_channel:
		x = qd.quantize_per_channel(x, scale, zp, per_channel_axis, qmin, qmax, dtype)
		x = qd.dequantize_per_channel(x, scale, zp, per_channel_axis, qmin, qmax, dtype)
	else:
		x = qd.quantize_per_tensor(x, scale, zp, qmin, qmax, dtype)
		x = qd.dequantize_per_tensor(x, scale, zp, qmin, qmax, dtype)
	return x

def _get_quantized_qat_conv_bn_pattern(
	is_per_channel: bool,
	has_bias: bool,
	bias_is_quantized: bool,
	conv_fn: Callable,
	bn_is_training: bool,
) -> Callable:
	bn_eps = 1e-5

	def _quantized_qat_conv_bn_pattern(
		x: torch.Tensor,
		conv_weight: torch.Tensor,
		bn_weight: torch.Tensor,
		bn_bias: torch.Tensor,
		bn_running_mean: torch.Tensor,
		bn_running_var: torch.Tensor,
		**kwargs,
	) -> torch.Tensor:
		running_std = torch.sqrt(bn_running_var + bn_eps)
		scale_factor = bn_weight / running_std
		weight_shape = [1] * len(conv_weight.shape)
		weight_shape[0] = -1
		bias_shape = [1] * len(conv_weight.shape)
		bias_shape[1] = -1
		scaled_weight = conv_weight * scale_factor.reshape(weight_shape)
		scaled_weight = _append_qdq(scaled_weight, is_per_channel, kwargs)
		if has_bias:
			zero_bias = torch.zeros_like(kwargs["conv_bias"], dtype=x.dtype)
			if bias_is_quantized:
				zero_bias = _append_qdq(zero_bias, is_per_channel, kwargs)
			x = conv_fn(x, scaled_weight, zero_bias)
		else:
			x = conv_fn(x, scaled_weight, None)
		x = x / scale_factor.reshape(bias_shape)
		if has_bias:
			x = x + kwargs["conv_bias"].reshape(bias_shape)
		x = F.batch_norm(x, bn_running_mean, bn_running_var, bn_weight, bn_bias, training=bn_is_training, eps=bn_eps)
		return x
	return _quantized_qat_conv_bn_pattern

def _get_folded_quantized_qat_conv_bn_pattern(
	is_per_channel: bool,
	has_bias: bool,
	bias_is_quantized: bool,
	conv_fn: Callable,
	bn_is_training: bool,
) -> Callable:
	bn_eps = 1e-5

	def _folded_quantized_qat_conv_bn_pattern(
		x: torch.Tensor,
		conv_weight: torch.Tensor,
		bn_weight: torch.Tensor,
		bn_bias: torch.Tensor,
		bn_running_mean: torch.Tensor,
		bn_running_var: torch.Tensor,
		**kwargs,
	) -> torch.Tensor:
		conv_weight = _append_qdq(conv_weight, is_per_channel, kwargs)
		if has_bias:
			bias = kwargs["conv_bias"]
			if bias_is_quantized:
				bias = _append_qdq(bias, is_per_channel, kwargs)
		else:
			bias = None
		x = conv_fn(x, conv_weight, bias)
		x = F.batch_norm(x, bn_running_mean, bn_running_var, bn_weight, bn_bias, training=bn_is_training, eps=bn_eps)
		return x
	return _folded_quantized_qat_conv_bn_pattern

def _has_conv_bias_filter(
	match: "InternalMatch",
	original_graph: Graph,
	pattern_graph: Graph,
) -> bool:
	for n in match.nodes_map.values():
		if _is_conv(n):
			return len(n.args) > 2 and n.args[2] is not None
	raise ValueError("Could not find conv node in matched conv + bn pattern")

def _no_conv_bias_filter(
	match: "InternalMatch",
	original_graph: Graph,
	pattern_graph: Graph,
) -> bool:
	return not _has_conv_bias_filter(match, original_graph, pattern_graph)

def _is_quantize(n: Node) -> bool:
	return n.target in [
		torch.ops.quantized_decomposed.quantize_per_tensor.default,
		torch.ops.quantized_decomposed.quantize_per_tensor.tensor,
		torch.ops.quantized_decomposed.quantize_per_channel.default,
	]

def _is_dequantize(n: Node) -> bool:
	return n.target in [
		torch.ops.quantized_decomposed.dequantize_per_tensor.default,
		torch.ops.quantized_decomposed.dequantize_per_tensor.tensor,
		torch.ops.quantized_decomposed.dequantize_per_channel.default,
	]

def _get_conv_bn_pattern_nodes(r: ReplacedPatterns) -> Dict[str, Tuple[Node, Node]]:
	def _get_nodes(nodes: List[Node]) -> Tuple[Node, Node, Node]:
		conv_node, bn_node, getitem_node = None, None, None
		for n in nodes:
			if n.op != "call_function":
				continue
			if _is_conv(n):
				assert conv_node is None
				conv_node = n
			if _is_bn_node(n):
				assert bn_node is None
				bn_node = n
			if n.target == operator.getitem:
				assert getitem_node is None
				getitem_node = n
		assert conv_node is not None
		assert bn_node is not None
		assert getitem_node is not None
		return (conv_node, bn_node, getitem_node)

	def _get_q_dq_nodes(n: Node) -> Tuple[Node, Node, Node]:
		assert _is_dequantize(n)
		q_node = n.args[0]
		assert isinstance(q_node, Node)
		assert _is_quantize(q_node)
		orig_node = q_node.args[0]
		assert isinstance(orig_node, Node)
		return (orig_node, q_node, n)

	original_nodes = list(_filter_nodes_map(r.nodes_map).values())
	o_conv, o_bn, o_getitem = _get_nodes(original_nodes)
	r_conv, r_bn, r_getitem = _get_nodes(r.replacements)

	mapping = {
		"conv": (o_conv, r_conv),
		"bn": (o_bn, r_bn),
		"getitem": (o_getitem, r_getitem),
	}

	(p_conv, _, _) = _get_nodes(list(r.nodes_map.keys()))
	(p_conv_input, p_conv_weight, *_) = p_conv.args
	(r_conv_input, r_conv_weight, *_) = r_conv.args
	assert isinstance(p_conv_input, Node)
	assert isinstance(p_conv_weight, Node)
	assert isinstance(r_conv_input, Node)
	assert isinstance(r_conv_weight, Node)
	o_conv_input = r.nodes_map[p_conv_input]
	o_conv_weight = r.nodes_map[p_conv_weight]

	if _is_dequantize(p_conv_weight):
		p_conv_weight, p_conv_weight_q, p_conv_weight_dq = _get_q_dq_nodes(p_conv_weight)
		r_conv_weight, r_conv_weight_q, r_conv_weight_dq = _get_q_dq_nodes(r_conv_weight)
		o_conv_weight = r.nodes_map[p_conv_weight]
		o_conv_weight_q = r.nodes_map[p_conv_weight_q]
		o_conv_weight_dq = r.nodes_map[p_conv_weight_dq]
		mapping["conv_weight_q"] = (o_conv_weight_q, r_conv_weight_q)
		mapping["conv_weight_dq"] = (o_conv_weight_dq, r_conv_weight_dq)
	mapping["conv_input"] = (o_conv_input, r_conv_input)
	mapping["conv_weight"] = (o_conv_weight, r_conv_weight)

	if len(p_conv.args) > 2 and len(r_conv.args) > 2:
		p_conv_bias = p_conv.args[2]
		r_conv_bias = r_conv.args[2]
		assert isinstance(p_conv_bias, Node)
		assert isinstance(r_conv_bias, Node)
		o_conv_bias = r.nodes_map[p_conv_bias]

		if _is_dequantize(p_conv_bias):
			p_conv_bias, p_conv_bias_q, p_conv_bias_dq = _get_q_dq_nodes(p_conv_bias)
			r_conv_bias, r_conv_bias_q, r_conv_bias_dq = _get_q_dq_nodes(r_conv_bias)
			o_conv_bias = r.nodes_map[p_conv_bias]
			o_conv_bias_q = r.nodes_map[p_conv_bias_q]
			o_conv_bias_dq = r.nodes_map[p_conv_bias_dq]
			mapping["conv_bias_q"] = (o_conv_bias_q, r_conv_bias_q)
			mapping["conv_bias_dq"] = (o_conv_bias_dq, r_conv_bias_dq)
		mapping["conv_bias"] = (o_conv_bias, r_conv_bias)
	return mapping

def _filter_nodes_map(nodes_map: Dict[Node, Node]) -> Dict[Node, Node]:
	new_nodes_map: Dict[Node, Node] = {}
	for pattern_node, graph_node in nodes_map.items():
		if graph_node is None:
			continue
		if pattern_node.op == "placeholder":
			continue
		new_nodes_map[pattern_node] = graph_node
	return new_nodes_map

def _copy_over_literal_conv_args(original_node: Node, new_node: Node):
	assert _is_conv(original_node)
	assert _is_conv(new_node)
	new_args = list(new_node.args)
	if len(new_args) < 3:
		new_args.append(None)
	new_node.args = tuple(new_args[:3]) + original_node.args[3:]

def _update_conv_input_qspec_map_after_replacement(original_node: Node, replacement_node: Node):
	assert _is_conv(original_node)
	assert _is_conv(replacement_node)
	if "quantization_annotation" not in original_node.meta:
		return
	original_input_qspec_map = original_node.meta["quantization_annotation"].input_qspec_map
	input_qspec_map = {}
	all_configs = list(original_input_qspec_map.items())
	input_qspec_map[replacement_node.args[0]] = all_configs[0][1]
	input_qspec_map[replacement_node.args[1]] = all_configs[1][1]
	if len(replacement_node.args) > 2 and len(all_configs) > 2:
		input_qspec_map[replacement_node.args[2]] = all_configs[2][1]
	replacement_node.meta["quantization_annotation"].input_qspec_map = input_qspec_map

def _update_special_qspecs_after_replacement(
	node: Node,
	original_to_replacement_node: Dict[Node, Node],
):
	def _get_new_edge_or_node(edge_or_node: EdgeOrNode):
		if isinstance(edge_or_node, Node):
			_node = edge_or_node
			return original_to_replacement_node.get(_node, _node)
		elif isinstance(edge_or_node, tuple) and len(edge_or_node) == 2 and all(isinstance(x, Node) for x in edge_or_node):
			src, dest = edge_or_node
			return (
				original_to_replacement_node.get(src, src),
				original_to_replacement_node.get(dest, dest),
			)
		else:
			raise ValueError("unexpected type for edge_or_node: ", type(edge_or_node))

	def _get_new_qspec(qspec: QuantizationSpecBase):
		if isinstance(qspec, SharedQuantizationSpec):
			new_edge_or_node = _get_new_edge_or_node(qspec.edge_or_node)
			return SharedQuantizationSpec(new_edge_or_node)
		elif isinstance(qspec, DerivedQuantizationSpec):
			new_derived_from = [_get_new_edge_or_node(x) for x in qspec.derived_from]
			return dataclasses.replace(qspec, derived_from=new_derived_from)
		else:
			return qspec

	if "quantization_annotation" not in node.meta:
		return
	annotation = node.meta["quantization_annotation"]
	for input_node, qspec in annotation.input_qspec_map.items():
		annotation.input_qspec_map[input_node] = _get_new_qspec(qspec)
	annotation.output_qspec = _get_new_qspec(annotation.output_qspec)

def _fuse_conv_bn_qat(m: GraphModule) -> GraphModule:
	has_bn = any(_is_bn_node(n) for n in m.graph.nodes)
	if not has_bn:
		return m
	m = _fuse_conv_bn_qat_helper(m, F.conv1d, _conv1d_bn_example_inputs, is_cuda=False)
	m = _fuse_conv_bn_qat_helper(m, F.conv2d, _conv2d_bn_example_inputs, is_cuda=False)
	if torch.cuda.is_available():
		m = _fuse_conv_bn_qat_helper(m, F.conv1d, _conv1d_bn_example_inputs, is_cuda=True)
		m = _fuse_conv_bn_qat_helper(m, F.conv2d, _conv2d_bn_example_inputs, is_cuda=True)
	return m

def _fuse_conv_bn_qat_helper(
	m: GraphModule,
	conv_fn: Callable,
	example_inputs: Tuple[Any, ...],
	is_cuda: bool,
) -> GraphModule:
	m.graph.eliminate_dead_code()
	m.recompile()
	conv_bn_pattern = _get_conv_bn_pattern(conv_fn)
	match_pattern = get_aten_graph_module(conv_bn_pattern, example_inputs, is_cuda)


	qat_conv_bn_pattern = _get_qat_conv_bn_pattern(conv_fn)
	replacement_pattern_with_conv_bias = get_aten_graph_module(
		qat_conv_bn_pattern,
		example_inputs,
		is_cuda,
	)
	replacements_with_conv_bias = replace_pattern_with_filters(
		m,
		match_pattern,
		replacement_pattern_with_conv_bias,
		match_filters=[_has_conv_bias_filter],
		ignore_literals=True,
	)
	m.recompile()


	qat_conv_bn_pattern_no_conv_bias = _get_qat_conv_bn_pattern_no_conv_bias(conv_fn)
	replacement_pattern_no_conv_bias = get_aten_graph_module(
		qat_conv_bn_pattern_no_conv_bias,
		example_inputs,
		is_cuda,
	)
	replacements_no_conv_bias = replace_pattern_with_filters(
		m,
		match_pattern,
		replacement_pattern_no_conv_bias,
		match_filters=[_no_conv_bias_filter],
		ignore_literals=True,
	)
	m.recompile()


	all_original_to_replacement_nodes = {}
	for r in replacements_with_conv_bias + replacements_no_conv_bias:
		for original_node, replacement_node in _get_conv_bn_pattern_nodes(r).values():
			replacement_node.meta = original_node.meta
			if _is_conv(original_node):
				_copy_over_literal_conv_args(original_node, replacement_node)
				_update_conv_input_qspec_map_after_replacement(original_node, replacement_node)
			all_original_to_replacement_nodes[original_node] = replacement_node

	for n in m.graph.nodes:
		_update_special_qspecs_after_replacement(n, all_original_to_replacement_nodes)

	return m

def _duplicate_dequantize_node(m: GraphModule):
	dq_op = torch.ops.quantized_decomposed.dequantize_per_tensor
	for n in m.graph.nodes:
		if n.op != "call_function" or n.target != dq_op or len(n.users) == 1:
			continue
		for user in list(n.users):
			with m.graph.inserting_before(n):
				new_node = m.graph.create_node("call_function", dq_op, n.args, n.kwargs)
			user.replace_input_with(n, new_node)
		m.graph.erase_node(n)
	m.recompile()

def _remove_extra_dequantize(m: GraphModule):
	dq_op = torch.ops.quantized_decomposed.dequantize_per_tensor
	for n in m.graph.nodes:
		dq_users = [user for user in n.users if user.op == "call_function" and user.target == dq_op]
		if len(dq_users) > 1:
			with m.graph.inserting_after(dq_users[0]):
				new_node = m.graph.create_node("call_function", dq_op, dq_users[0].args, {})
			for dq_user in dq_users:
				dq_user.replace_all_uses_with(new_node)
				m.graph.erase_node(dq_user)
	m.recompile()

def _copy_over_q_dq_args(original_node: Node, replacement_node: Node):
	assert original_node.target == replacement_node.target
	if original_node.target in (
		torch.ops.quantized_decomposed.quantize_per_tensor.default,
		torch.ops.quantized_decomposed.dequantize_per_tensor.default,
	):
		start_copy_arg_index = 1
	elif original_node.target in (
		torch.ops.quantized_decomposed.quantize_per_channel.default,
		torch.ops.quantized_decomposed.dequantize_per_channel.default,
	):
		start_copy_arg_index = 3
	else:
		raise ValueError("Expected quantize/dequantize nodes, got '%s'" % original_node.target)
	replacement_node.args = (
		replacement_node.args[:start_copy_arg_index] + original_node.args[start_copy_arg_index:]
	)

def _fold_conv_bn_qat(m: GraphModule) -> GraphModule:
	has_bn = any(_is_bn_node(n) for n in m.graph.nodes)
	if not has_bn:
		return m
	m = _fold_conv_bn_qat_helper(m, F.conv1d, _quantized_conv1d_bn_example_inputs, is_cuda=False)
	m = _fold_conv_bn_qat_helper(m, F.conv2d, _quantized_conv2d_bn_example_inputs, is_cuda=False)
	if torch.cuda.is_available():
		m = _fold_conv_bn_qat_helper(m, F.conv1d, _quantized_conv1d_bn_example_inputs, is_cuda=True)
		m = _fold_conv_bn_qat_helper(m, F.conv2d, _quantized_conv2d_bn_example_inputs, is_cuda=True)
	return m

def _fold_conv_bn_qat_helper(
	m: GraphModule,
	conv_fn: Callable,
	example_inputs: Tuple[Any, ...],
	is_cuda: bool,
) -> GraphModule:
	m.graph.eliminate_dead_code()
	m.recompile()
	_duplicate_dequantize_node(m)

	replacements = []
	replacement_options = itertools.product(
		[True, False],  # is_per_channel
		[True, False],  # has_bias
		[True, False],  # bias_is_quantized
		[True, False],  # bn_is_training
	)
	for is_per_channel, has_bias, bias_is_quantized, bn_is_training in replacement_options:
		if not has_bias and bias_is_quantized:
			continue
		kwargs = _get_quantized_conv_bn_example_inputs_kwargs(is_per_channel, has_bias, is_cuda)
		match_pattern = _get_quantized_qat_conv_bn_pattern(
			is_per_channel, has_bias, bias_is_quantized, conv_fn, bn_is_training
		)
		match_pattern = get_aten_graph_module(match_pattern, example_inputs, is_cuda, **kwargs)
		replacement_pattern = _get_folded_quantized_qat_conv_bn_pattern(
			is_per_channel, has_bias, bias_is_quantized, conv_fn, bn_is_training
		)
		replacement_pattern = get_aten_graph_module(replacement_pattern, example_inputs, is_cuda, **kwargs)
		replacements.extend(
			replace_pattern_with_filters(
				m,
				match_pattern,
				replacement_pattern,
				ignore_literals=True,
			)
		)
	m.recompile()
	_remove_extra_dequantize(m)

	for r in replacements:
		node_map = _get_conv_bn_pattern_nodes(r)

		for original_node, replacement_node in node_map.values():
			replacement_node.meta = original_node.meta

		_copy_over_q_dq_args(*node_map["conv_weight_q"])
		_copy_over_q_dq_args(*node_map["conv_weight_dq"])
		if "conv_bias_q" in node_map:
			assert "conv_bias_dq" in node_map
			_copy_over_q_dq_args(*node_map["conv_bias_q"])
			_copy_over_q_dq_args(*node_map["conv_bias_dq"])

		conv_bias = None
		(_, conv_node) = node_map["conv"]
		(_, bn_node) = node_map["bn"]
		(_, conv_weight) = node_map["conv_weight"]
		if "conv_bias" in node_map:
			(_, conv_bias) = node_map["conv_bias"]
		fold_bn_weights_into_conv_node(conv_node, conv_weight, conv_bias, bn_node, m)

		for original_node in _filter_nodes_map(r.nodes_map).values():
			if _is_conv(original_node):
				_copy_over_literal_conv_args(original_node, conv_node)

	m.graph.eliminate_dead_code()
	m.recompile()
	return m

<END>

<START>
from collections import Counter
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union

import torch
import torch.nn as nn
from torch import Tensor
from torch._functorch.utils import exposed_in


@exposed_in("torch.func")
def functional_call(
	module: "torch.nn.Module",
	parameter_and_buffer_dicts: Union[Dict[str, Tensor], Sequence[Dict[str, Tensor]]],
	args: Union[Any, Tuple],
	kwargs: Optional[Dict[str, Any]] = None,
	*,
	tie_weights: bool = True,
	strict: bool = False,
):
	if isinstance(parameter_and_buffer_dicts, dict):
		parameters_and_buffers = parameter_and_buffer_dicts
	elif isinstance(parameter_and_buffer_dicts, Sequence):
		if not all(isinstance(d, dict) for d in parameter_and_buffer_dicts):
			raise ValueError(
				"Expected all elements of parameter_and_buffer_dicts to be dictionaries"
			)
		all_keys = [k for d in parameter_and_buffer_dicts for k in d.keys()]
		repeated_keys = [key for key, n in Counter(all_keys).items() if n > 1]
		if len(repeated_keys) > 0:
			raise ValueError(
				f"{repeated_keys} appeared in multiple dictionaries; behavior of functional call is ambiguous"
			)
		parameters_and_buffers = {
			k: v for d in parameter_and_buffer_dicts for k, v in d.items()
		}
	else:
		raise ValueError(
			f"Expected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, "
			f"but got {type(parameter_and_buffer_dicts)}"
		)

	return nn.utils.stateless._functional_call(
		module,
		parameters_and_buffers,
		args,
		kwargs,
		tie_weights=tie_weights,
		strict=strict,
	)


@exposed_in("torch.func")
def stack_module_state(
	models: List[nn.Module],
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
	if len(models) == 0:
		raise RuntimeError("stack_module_state: Expected at least one model, got 0.")
	if not (all(m.training for m in models) or all(not m.training for m in models)):
		raise RuntimeError(
			"stack_module_state: Expected all models to have the same training/eval mode."
		)
	model0_typ = type(models[0])
	if not all(type(m) == model0_typ for m in models):
		raise RuntimeError(
			"stack_module_state: Expected all models to be of the same class."
		)
	all_params = [dict(model.named_parameters()) for model in models]
	params = {
		k: construct_stacked_leaf(tuple(params[k] for params in all_params), k)
		for k in all_params[0]
	}
	all_buffers = [dict(model.named_buffers()) for model in models]
	buffers = {
		k: construct_stacked_leaf(tuple(buffers[k] for buffers in all_buffers), k)
		for k in all_buffers[0]
	}

	return params, buffers


def construct_stacked_leaf(
	tensors: Union[Tuple[Tensor, ...], List[Tensor]], name: str
) -> Tensor:
	all_requires_grad = all(t.requires_grad for t in tensors)
	none_requires_grad = all(not t.requires_grad for t in tensors)
	if not all_requires_grad and not none_requires_grad:
		raise RuntimeError(
			f"Expected {name} from each model to have the same .requires_grad"
		)
	result = torch.stack(tensors)
	if all_requires_grad:
		result = result.detach().requires_grad_()
	return result

<END>

<START>
import copy
import operator
import torch
import torch.nn.functional as F
import torch.nn as nn
import torch.ao.nn.intrinsic as nni
import torch.ao.nn.intrinsic.qat as nniqat
import torch.ao.nn.qat as nnqat
import torch.ao.nn.quantized.reference as nnqr
from collections import namedtuple
from typing import Callable, Dict, List, Union
from .backend_config import (
	BackendPatternConfig,
	DTypeConfig,
	DTypeWithConstraints,
	ObservationType,
)
from ..fuser_method_mappings import (
	_sequential_wrapper2,
	fuse_conv_bn,
	fuse_conv_bn_relu,
	fuse_linear_bn,
	fuse_convtranspose_bn,
)

__all__: List[str] = []

_ConvMetadata = namedtuple(
	"_ConvMetadata",
	["root", "transpose", "bn", "reference", "transpose_reference",
	 "fused_conv_relu", "fused_conv_bn", "fused_conv_bn_relu",
	 "qat", "relu_qat", "bn_qat", "bn_relu_qat",
	 "func", "func_transpose"])
_Conv1dMetadata = _ConvMetadata(
	nn.Conv1d, nn.ConvTranspose1d, nn.BatchNorm1d, nnqr.Conv1d, nnqr.ConvTranspose1d,
	nni.ConvReLU1d, nni.ConvBn1d, nni.ConvBnReLU1d,
	nnqat.Conv1d, nniqat.ConvReLU1d, nniqat.ConvBn1d, nniqat.ConvBnReLU1d,
	F.conv1d, F.conv_transpose1d)
_Conv2dMetadata = _ConvMetadata(
	nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d, nnqr.Conv2d, nnqr.ConvTranspose2d,
	nni.ConvReLU2d, nni.ConvBn2d, nni.ConvBnReLU2d,
	nnqat.Conv2d, nniqat.ConvReLU2d, nniqat.ConvBn2d, nniqat.ConvBnReLU2d,
	F.conv2d, F.conv_transpose2d)
_Conv3dMetadata = _ConvMetadata(
	nn.Conv3d, nn.ConvTranspose3d, nn.BatchNorm3d, nnqr.Conv3d, nnqr.ConvTranspose3d,
	nni.ConvReLU3d, nni.ConvBn3d, nni.ConvBnReLU3d,
	nnqat.Conv3d, nniqat.ConvReLU3d, nniqat.ConvBn3d, nniqat.ConvBnReLU3d,
	F.conv3d, F.conv_transpose3d)

_FIXED_QPARAM_OP_0TO1_CONSTRAINTS = DTypeWithConstraints(
	dtype=torch.quint8,
	quant_min_lower_bound=0,
	quant_max_upper_bound=255,
	scale_exact_match=1.0 / 256.0,
	zero_point_exact_match=0,
)
_FIXED_QPARAM_OP_NEG1TO1_CONSTRAINTS = DTypeWithConstraints(
	dtype=torch.quint8,
	quant_min_lower_bound=0,
	quant_max_upper_bound=255,
	scale_exact_match=2.0 / 256.0,
	zero_point_exact_match=128,
)
_FIXED_QPARAMS_OP_TO_CONSTRAINTS: Dict[Union[Callable, str], DTypeWithConstraints] = {
	torch.nn.Hardsigmoid: _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	torch.nn.functional.hardsigmoid: _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	"hardsigmoid": _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	"hardsigmoid_": _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	torch.nn.Sigmoid: _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	torch.sigmoid: _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	"sigmoid": _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	"sigmoid_": _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	torch.nn.Softmax: _FIXED_QPARAM_OP_0TO1_CONSTRAINTS,
	torch.nn.Tanh: _FIXED_QPARAM_OP_NEG1TO1_CONSTRAINTS,
	torch.tanh: _FIXED_QPARAM_OP_NEG1TO1_CONSTRAINTS,
	"tanh": _FIXED_QPARAM_OP_NEG1TO1_CONSTRAINTS,
	"tanh_": _FIXED_QPARAM_OP_NEG1TO1_CONSTRAINTS,
}

def _get_binary_op_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPatternConfig]:
	binary_op_configs: List[BackendPatternConfig] = []
	num_tensor_args_to_observation_type_mapping = {
		0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT,
		1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT,
		2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT,
	}
	for op_with_quantized_bop_scalar_variant in [operator.add, torch.add, operator.mul, torch.mul]:
		bop_patterns = [
			(op_with_quantized_bop_scalar_variant, nn.ReLU),
			(op_with_quantized_bop_scalar_variant, F.relu),
			(op_with_quantized_bop_scalar_variant, torch.relu),
			op_with_quantized_bop_scalar_variant
		]
		for bop_pattern in bop_patterns:
			binary_op_configs.append(
				BackendPatternConfig(bop_pattern)
					.set_dtype_configs(dtype_configs)  # noqa: E131
					._set_num_tensor_args_to_observation_type(num_tensor_args_to_observation_type_mapping))
	binary_op_configs.append(
		BackendPatternConfig(torch.matmul)
		.set_dtype_configs(dtype_configs)  # noqa: E131
	)
	return binary_op_configs

def _get_linear_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPatternConfig]:
	observation_type = ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT
	linear_configs: List[BackendPatternConfig] = []

	linear_configs.append(
		BackendPatternConfig(torch.nn.Linear)
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs)
			.set_root_module(torch.nn.Linear)
			.set_reference_quantized_module(nnqr.Linear)
			.set_qat_module(nnqat.Linear))
	linear_configs.append(
		BackendPatternConfig(nnqat.Linear)
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs)
			.set_root_module(torch.nn.Linear)
			.set_reference_quantized_module(nnqr.Linear))
	linear_configs.append(
		BackendPatternConfig(torch.nn.functional.linear)
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs)
			._set_input_type_to_index({"weight": 1, "bias": 2}))

	linear_configs.append(
		BackendPatternConfig((torch.nn.Linear, torch.nn.ReLU))
			.set_dtype_configs(dtype_configs)  # noqa: E131
			.set_fuser_method(_sequential_wrapper2(nni.LinearReLU))
			.set_fused_module(nni.LinearReLU))
	linear_configs.append(
		BackendPatternConfig((torch.nn.Linear, torch.nn.functional.relu))
			.set_dtype_configs(dtype_configs)  # noqa: E131
			.set_fuser_method(_sequential_wrapper2(nni.LinearReLU))
			.set_fused_module(nni.LinearReLU))

	linear_configs.append(
		BackendPatternConfig(nni.LinearReLU)
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs)
			.set_root_module(torch.nn.Linear)
			.set_reference_quantized_module(nnqr.Linear)
			.set_qat_module(nniqat.LinearReLU))
	linear_configs.append(
		BackendPatternConfig(nniqat.LinearReLU)
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs)
			.set_root_module(torch.nn.Linear)
			.set_reference_quantized_module(nnqr.Linear))
	linear_configs.append(
		BackendPatternConfig((F.linear, torch.nn.ReLU))
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs))
	linear_configs.append(
		BackendPatternConfig((F.linear, F.relu))
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs))

	linear_configs.append(
		BackendPatternConfig((nn.Linear, nn.BatchNorm1d))
			.set_dtype_configs(dtype_configs)  # noqa: E131
			.set_fuser_method(fuse_linear_bn)
			.set_fused_module(nni.LinearBn1d))

	linear_configs.append(
		BackendPatternConfig(nni.LinearBn1d)
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs)
			.set_root_module(torch.nn.Linear)
			.set_reference_quantized_module(nnqr.Linear)
			.set_qat_module(nniqat.LinearBn1d))
	linear_configs.append(
		BackendPatternConfig(nniqat.LinearBn1d)
			.set_observation_type(observation_type)  # noqa: E131
			.set_dtype_configs(dtype_configs)
			.set_root_module(torch.nn.Linear)
			.set_reference_quantized_module(nnqr.Linear))
	return linear_configs

def _get_conv_configs(dtype_configs):
	conv_configs = []
	observation_type = ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT
	for convs in [_Conv1dMetadata, _Conv2dMetadata, _Conv3dMetadata]:

		conv_configs.append(
			BackendPatternConfig(convs.root)
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs)
				.set_root_module(convs.root)
				.set_reference_quantized_module(convs.reference)
				.set_qat_module(convs.qat))
		conv_configs.append(
			BackendPatternConfig(convs.qat)
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs)
				.set_root_module(convs.root)
				.set_reference_quantized_module(convs.reference))
		conv_configs.append(
			BackendPatternConfig(convs.func)
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs)
				._set_input_type_to_index({"weight": 1, "bias": 2}))

		conv_configs.append(
			BackendPatternConfig((convs.root, torch.nn.ReLU))
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_fuser_method(_sequential_wrapper2(convs.fused_conv_relu))
				.set_fused_module(convs.fused_conv_relu))
		conv_configs.append(
			BackendPatternConfig((convs.root, F.relu))
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_fuser_method(_sequential_wrapper2(convs.fused_conv_relu))
				.set_fused_module(convs.fused_conv_relu))
		conv_configs.append(
			BackendPatternConfig(convs.fused_conv_relu)
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs)
				.set_root_module(convs.root)
				.set_reference_quantized_module(convs.reference)
				.set_qat_module(convs.relu_qat))
		conv_configs.append(
			BackendPatternConfig(convs.relu_qat)
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs)
				.set_root_module(convs.root)
				.set_reference_quantized_module(convs.reference))
		conv_configs.append(
			BackendPatternConfig((convs.func, torch.nn.ReLU))
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs))
		conv_configs.append(
			BackendPatternConfig((convs.func, F.relu))
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs))

		conv_configs.append(
			BackendPatternConfig(convs.fused_conv_relu)
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_qat_module(convs.relu_qat))

		conv_configs.append(
			BackendPatternConfig(convs.relu_qat)
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_root_module(convs.root)
				.set_reference_quantized_module(convs.reference))

		conv_configs.append(
			BackendPatternConfig((convs.root, convs.bn))
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_fuser_method(fuse_conv_bn)
				.set_fused_module(convs.fused_conv_bn))
		conv_configs.append(
			BackendPatternConfig((convs.root, convs.bn, nn.ReLU))
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_fuser_method(fuse_conv_bn_relu)
				.set_fused_module(convs.fused_conv_bn_relu))
		conv_configs.append(
			BackendPatternConfig((convs.root, convs.bn, F.relu))
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_root_module(convs.root)
				.set_fuser_method(fuse_conv_bn_relu)
				.set_fused_module(convs.fused_conv_bn_relu))

		conv_configs.append(
			BackendPatternConfig(convs.fused_conv_bn)
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_qat_module(convs.bn_qat))

		conv_configs.append(
			BackendPatternConfig(convs.fused_conv_bn_relu)
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_qat_module(convs.bn_relu_qat))

		conv_configs.append(
			BackendPatternConfig(convs.bn_qat)
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs)
				.set_root_module(convs.root)
				.set_reference_quantized_module(convs.reference))
		conv_configs.append(
			BackendPatternConfig(convs.bn_relu_qat)
				.set_observation_type(observation_type)  # noqa: E131
				.set_dtype_configs(dtype_configs)
				.set_root_module(convs.root)
				.set_reference_quantized_module(convs.reference))

		conv_configs.append(
			BackendPatternConfig(convs.transpose)
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_root_module(convs.transpose)
				.set_reference_quantized_module(convs.transpose_reference))

		conv_configs.append(
			BackendPatternConfig((convs.transpose, convs.bn))
				.set_dtype_configs(dtype_configs)  # noqa: E131
				.set_fuser_method(fuse_convtranspose_bn)
				.set_root_module(convs.transpose)
				.set_reference_quantized_module(convs.transpose_reference))

		conv_configs.append(
			BackendPatternConfig(convs.func_transpose)
				.set_dtype_configs(dtype_configs)  # noqa: E131
				._set_input_type_to_index({"weight": 1, "bias": 2}))

	return conv_configs

def _get_cat_config(dtype_configs: List[DTypeConfig]) -> BackendPatternConfig:
	return BackendPatternConfig(torch.cat) \
		.set_observation_type(ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT) \
		.set_dtype_configs(dtype_configs)

def _get_ln_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPatternConfig]:
	ln_configs = []
	ln_configs.append(
		BackendPatternConfig(torch.nn.LayerNorm)
		.set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)  # noqa: E131
		.set_dtype_configs(dtype_configs)
	)
	ln_configs.append(
		BackendPatternConfig(torch.nn.functional.layer_norm)
		.set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)  # noqa: E131
		.set_dtype_configs(dtype_configs)
		._set_input_type_to_index({"weight": 2, "bias": 3})
	)
	return ln_configs

def _get_default_op_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPatternConfig]:
	configs = []
	default_ops = [
		torch.nn.ELU,
		torch.nn.LeakyReLU,
		torch.nn.Hardswish,
		torch.nn.InstanceNorm1d,
		torch.nn.InstanceNorm2d,
		torch.nn.InstanceNorm3d,
		torch.nn.Dropout,
		torch.nn.PReLU,
		torch.nn.functional.elu,
		torch.nn.functional.hardswish,
		torch.nn.functional.leaky_relu,
		torch.nn.functional.dropout,
	]
	for op in default_ops:
		configs.append(
			BackendPatternConfig(op)
				.set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)  # noqa: E131
				.set_dtype_configs(dtype_configs))

	configs.append(
		BackendPatternConfig(torch.nn.functional.group_norm)
		.set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)  # noqa: E131
		.set_dtype_configs(dtype_configs)
		._set_input_type_to_index({"weight": 2, "bias": 3})
	)

	configs.append(
		BackendPatternConfig(torch.nn.functional.instance_norm)
		.set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)  # noqa: E131
		.set_dtype_configs(dtype_configs)
		._set_input_type_to_index({"weight": 3, "bias": 4})
	)
	return configs

def _add_fixed_qparams_to_dtype_configs(
	dtype_configs: List[DTypeConfig],
	constraints: DTypeWithConstraints,
) -> List[DTypeConfig]:
	new_dtype_configs = []
	for dtype_config in dtype_configs:
		dc = copy.deepcopy(dtype_config)
		for orig_constraints in [dc.input_dtype_with_constraints, dc.output_dtype_with_constraints]:
			if orig_constraints.dtype != constraints.dtype:
				continue
			if orig_constraints.scale_min_lower_bound is not None:
				raise ValueError(f"scale_min_lower_bound is invalid for fixed qparams ops: {dtype_config}")
			if orig_constraints.scale_max_upper_bound is not None:
				raise ValueError(f"scale_max_upper_bound is invalid for fixed qparams ops: {dtype_config}")
			orig_constraints.quant_min_lower_bound = constraints.quant_min_lower_bound
			orig_constraints.quant_max_upper_bound = constraints.quant_max_upper_bound
			orig_constraints.scale_exact_match = constraints.scale_exact_match
			orig_constraints.zero_point_exact_match = constraints.zero_point_exact_match
		new_dtype_configs.append(dc)
	return new_dtype_configs

def _get_fixed_qparams_op_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPatternConfig]:
	fixed_qparams_op_configs = []
	for fixed_qparam_op, constraints in _FIXED_QPARAMS_OP_TO_CONSTRAINTS.items():
		new_dtype_configs = _add_fixed_qparams_to_dtype_configs(dtype_configs, constraints)
		fixed_qparams_op_configs.append(
			BackendPatternConfig(fixed_qparam_op)
				.set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)  # noqa: E131
				.set_dtype_configs(new_dtype_configs))
	return fixed_qparams_op_configs

def _get_share_qparams_op_configs(dtype_configs):

	def _get_share_qprams_op_backend_config(op):
		return BackendPatternConfig(op) \
			.set_observation_type(ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT) \
			.set_dtype_configs(dtype_configs)

	share_qparams_ops = [
		torch.nn.AdaptiveAvgPool1d,
		torch.nn.AdaptiveAvgPool2d,
		torch.nn.AdaptiveAvgPool3d,
		torch.nn.AvgPool1d,
		torch.nn.AvgPool2d,
		torch.nn.AvgPool3d,
		torch.nn.Hardtanh,
		torch.nn.Identity,
		torch.nn.MaxPool1d,
		torch.nn.MaxPool2d,
		torch.nn.MaxPool3d,
		torch.nn.PixelShuffle,
		torch.nn.PixelUnshuffle,
		torch.nn.ReLU,
		torch.nn.ReLU6,
		torch.adaptive_avg_pool1d,
		torch.nn.functional.adaptive_avg_pool2d,
		torch.nn.functional.adaptive_avg_pool3d,
		torch.nn.functional.hardtanh,
		torch.nn.functional.hardtanh_,
		torch.nn.functional.interpolate,
		torch.nn.functional.max_pool1d,
		torch.nn.functional.max_pool2d,
		torch.nn.functional.max_pool3d,
		torch.nn.functional.pixel_shuffle,
		torch.nn.functional.pixel_unshuffle,
		torch.nn.functional.relu,
		torch.nn.functional.relu6,
		torch.avg_pool1d,
		torch._C._nn.avg_pool2d,
		torch._C._nn.avg_pool3d,
		torch.clamp,
		torch.flatten,
		torch.mean,
		torch.narrow,
		torch.repeat_interleave,
		torch.transpose,
		torch.squeeze,
		torch.stack,
		torch.unsqueeze,
		operator.floordiv,
		"contiguous",
		"clamp",
		"detach",
		"detach_",
		"mean",
		"permute",
		"repeat",
		"repeat_interleave",
		"reshape",
		"resize_",
		"relu",
		"relu_",
		"squeeze",
		"squeeze_",
		"transpose",
		"unsqueeze",
		"unsqueeze_",
		"view"
	]
	return [_get_share_qprams_op_backend_config(op) for op in share_qparams_ops]

def _get_bn_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPatternConfig]:
	These ops work on tensors of different dtypes but return non-tensors
	containing information about the input tensor.

<END>

<START>
from abc import ABC, abstractmethod
import contextlib
from typing import Any
import torch
import torch.utils._pytree as pytree
from torch._C._functorch import (
	TransformType,
	RandomnessType,
	CInterpreter,
	CGradInterpreterPtr,
	CFunctionalizeInterpreterPtr,
	CVmapInterpreterPtr,
	CJvpInterpreterPtr,
	pop_dynamic_layer_stack,
	push_dynamic_layer_stack,
)
from torch.autograd.forward_ad import _set_fwd_grad_enabled



class FuncTorchInterpreter(ABC):
	def __init__(self, cptr: Any):
		self._cptr = cptr

	@abstractmethod
	def process(self, op, args, kwargs):
		pass

	def lower(self):
		return temporarily_pop_interpreter_stack()

	def level(self):
		return self._cptr.level()

	def key(self):
		return self._cptr.key()


@contextlib.contextmanager
def temporarily_pop_interpreter_stack():
	try:
		saved = pop_dynamic_layer_stack()
		yield
	finally:
		push_dynamic_layer_stack(saved)


class VmapInterpreter(FuncTorchInterpreter):
	def __init__(self, cdata: CInterpreter):
		assert cdata.key() == TransformType.Vmap
		self._cdata = cdata
		self._cptr = CVmapInterpreterPtr(cdata)

	def process(self, op, args, kwargs):
		kernel = op.functorch_table[TransformType.Vmap]
		return kernel(self, *args, **kwargs)

	def batch_size(self):
		return self._cptr.batchSize()

	def randomness(self):
		typ = self._cptr.randomness()
		if typ == RandomnessType.Error:
			return "error"
		elif typ == RandomnessType.Same:
			return "same"
		elif typ == RandomnessType.Different:
			return "different"
		raise RuntimeError(f"Unknown RandomnessType: {typ}")


@contextlib.contextmanager
def nested(*contexts):
	with contextlib.ExitStack() as stack:
		for ctx in contexts:
			stack.enter_context(ctx)
		yield contexts


class GradInterpreter(FuncTorchInterpreter):
	def __init__(self, cdata: CInterpreter):
		assert cdata.key() == TransformType.Grad
		self._cdata = cdata
		self._cptr = CGradInterpreterPtr(cdata)

	def lift(self, args, kwargs):
		args, kwargs = pytree.tree_map_only(torch.Tensor, self._cptr.lift, [args, kwargs])
		return args, kwargs

	def process(self, op, args, kwargs):
		kernel = op.functorch_table[TransformType.Grad]
		args, kwargs = self.lift(args, kwargs)
		return kernel(self, *args, **kwargs)

	def lower(self):
		prev_grad_mode = self.prev_grad_mode()
		if not prev_grad_mode:
			return nested(torch.no_grad(), super().lower())
		return super().lower()

	def prev_grad_mode(self):
		return self._cptr.prevGradMode()


class JvpInterpreter(FuncTorchInterpreter):
	def __init__(self, cdata: CInterpreter):
		assert cdata.key() == TransformType.Jvp
		self._cdata = cdata
		self._cptr = CJvpInterpreterPtr(cdata)

	def lift(self, args, kwargs):
		args, kwargs = pytree.tree_map_only(torch.Tensor, self._cptr.lift, [args, kwargs])
		return args, kwargs

	def process(self, op, args, kwargs):
		kernel = op.functorch_table[TransformType.Jvp]
		args, kwargs = self.lift(args, kwargs)
		return kernel(self, *args, **kwargs)

	def lower(self):
		prev_fwd_grad_mode = self.prev_fwd_grad_mode()
		if not prev_fwd_grad_mode:
			return nested(_set_fwd_grad_enabled(False), super().lower())
		return super().lower()

	def prev_fwd_grad_mode(self):
		return self._cptr.prevFwdGradMode()


class FunctionalizeInterpreter(FuncTorchInterpreter):
	def __init__(self, cdata: CInterpreter):
		assert cdata.key() == TransformType.Functionalize
		self._cdata = cdata
		self._cptr = CFunctionalizeInterpreterPtr(cdata)

	def process(self, op, args, kwargs):
		kernel = op.functorch_table[TransformType.Functionalize]
		return kernel(self, *args, **kwargs)

	def functionalize_add_back_views(self):
		return self._cptr.functionalizeAddBackViews()


def coerce_cinterpreter(cinterpreter: CInterpreter) -> FuncTorchInterpreter:
	key = cinterpreter.key()
	if key == TransformType.Grad:
		return GradInterpreter(cinterpreter)
	if key == TransformType.Vmap:
		return VmapInterpreter(cinterpreter)
	if key == TransformType.Jvp:
		return JvpInterpreter(cinterpreter)
	if key == TransformType.Functionalize:
		return FunctionalizeInterpreter(cinterpreter)
	raise RuntimeError(f"NYI: PyDispatcher has not implemented support for {key}")


def retrieve_current_functorch_interpreter():
	interpreter = torch._C._functorch.peek_interpreter_stack()
	assert interpreter is not None
	return coerce_cinterpreter(interpreter)


def dispatch_functorch(op, args, kwargs):
	interpreter = retrieve_current_functorch_interpreter()
	args, kwargs = pytree.tree_map_only(
		torch.Tensor, torch._C._functorch.unwrap_if_dead, (args, kwargs))
	return interpreter.process(op, args, kwargs)

<END>

<START>
import contextlib
import copy
import functools
import math
import traceback
import warnings
from contextlib import contextmanager
from enum import auto, Enum
from typing import (
	Any,
	Callable,
	Dict,
	Generator,
	Iterable,
	Iterator,
	List,
	Optional,
	Tuple,
	Union,
)

import torch
import torch.distributed as dist
import torch.distributed.fsdp._traversal_utils as traversal_utils
import torch.nn as nn
from torch.distributed._tensor import DeviceMesh
from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
	_CHECKPOINT_WRAPPED_MODULE,
	ActivationWrapper,
)
from torch.distributed.algorithms._comm_hooks import LOW_PRECISION_HOOKS
from torch.distributed.fsdp._common_utils import (
	_FSDPState,
	_get_param_to_fqns,
	FSDP_PREFIX,
	FSDP_WRAPPED_MODULE,
	TrainingState,
)
from torch.distributed.fsdp._dynamo_utils import _annotate_modules_for_dynamo
from torch.distributed.fsdp._init_utils import (
	_check_orig_params_flattened,
	_init_buffer_state,
	_init_core_state,
	_init_device_handle,
	_init_extension,
	_init_ignored_module_states,
	_init_param_handle_from_module,
	_init_prefetching_state,
	_init_process_group_state,
	_init_runtime_state,
	_init_state_dict_state,
	HYBRID_SHARDING_STRATEGIES,
	ProcessGroupType,
)
from torch.distributed.fsdp._runtime_utils import (
	_get_fsdp_root_states,
	_is_fsdp_root,
	_lazy_init,
	_post_forward,
	_post_forward_reshard,
	_pre_forward,
	_pre_forward_unshard,
	_root_pre_forward,
)
from torch.distributed.fsdp._wrap_utils import _auto_wrap
from torch.distributed.fsdp.api import (
	BackwardPrefetch,
	CPUOffload,
	FullOptimStateDictConfig,
	FullStateDictConfig,
	LocalOptimStateDictConfig,
	LocalStateDictConfig,
	MixedPrecision,
	OptimStateDictConfig,
	ShardedOptimStateDictConfig,
	ShardedStateDictConfig,
	ShardingStrategy,
	StateDictConfig,
	StateDictSettings,
	StateDictType,
)
from torch.distributed.utils import _p_assert
from ._flat_param import FlatParameter

from ._optim_utils import (
	_flatten_optim_state_dict,
	_get_param_id_to_param_from_optim_input,
	_get_param_key_to_param,
	_get_param_to_param_id_from_optim_input,
	_get_param_to_param_key,
	_optim_state_dict,
	_rekey_sharded_optim_state_dict,
	_set_optim_use_dtensor,
)
from ._state_dict_utils import _register_all_state_dict_hooks
from ._unshard_param_utils import (
	_deregister_orig_params,
	_register_flat_param,
	_register_orig_params,
	_unshard_params,
	_unshard_params_recurse,
)
from .wrap import CustomPolicy, ModuleWrapPolicy


__all__ = [
	"FullyShardedDataParallel",
	"OptimStateKeyType",
]


FLAT_PARAM = "_flat_param"


class OptimStateKeyType(Enum):

	This is inspired by `Xu et al.`_ as well as the ZeRO Stage 3 from DeepSpeed_.
	FullyShardedDataParallel is commonly shortened to FSDP.

	.. _`Xu et al.`: https://arxiv.org/abs/2004.13336
	.. _DeepSpeed: https://www.deepspeed.ai/

	Example::

		>>> # xdoctest: +SKIP("undefined variables")
		>>> import torch
		>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
		>>> torch.cuda.set_device(device_id)
		>>> sharded_module = FSDP(my_module)
		>>> optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)
		>>> x = sharded_module(x, y=3, z=torch.Tensor([1]))
		>>> loss = x.sum()
		>>> loss.backward()
		>>> optim.step()

	.. warning::
		The optimizer must be initialized *after* the module has been wrapped
		with FSDP since FSDP will shard and transform the module's parameters
		in a way that may not preserve the original parameter variables. Thus,
		the previously initialized optimizer may have stale references to the
		parameters.

	.. warning::
		If the destination CUDA device has ID ``dev_id``, either (1)
		``module`` should already be placed on that device, (2) the device
		should be set using ``torch.cuda.set_device(dev_id)``, or (3)
		``dev_id`` should be passed into the ``device_id`` constructor
		argument. This FSDP instance's compute device will be that destination
		device. For (1) and (3), the FSDP initialization always occurs on GPU.
		For (2), the FSDP initialization happens on ``module`` 's current
		device, which may be CPU.

	.. warning::
		FSDP currently does not support gradient accumulation outside
		``no_sync()`` when using CPU offloading. Trying to do so yields
		incorrect results since FSDP will use the newly-reduced gradient
		instead of accumulating with any existing gradient.

	.. warning::
		Changing the original parameter variable names after construction will
		lead to undefined behavior.

	.. warning::
		Passing in the ``sync_module_states=True`` flag requires ``module`` to
		be on GPU or to use the ``device_id`` argument to specify a CUDA device
		that FSDP will move ``module`` to in the FSDP constructor. This is
		because ``sync_module_states=True`` requires GPU communication.

	.. warning::
		As of PyTorch 1.12, FSDP only offers limited support for shared parameters
		(for example, setting one ``Linear`` layer's weight to another's). In
		particular, modules that share parameters must be wrapped as part of the
		same FSDP unit. If enhanced shared parameter support is needed for your
		use case, please ping https://github.com/pytorch/pytorch/issues/77724

	.. warning::
		FSDP has some constraints on freezing parameters (i.e. setting
		``param.requires_grad=False``). For ``use_orig_params=False``, each
		FSDP instance must manage parameters that are all frozen or all
		non-frozen. For ``use_orig_params=True``, FSDP supports mixing frozen
		and non-frozen, but we recommend not doing so since then the gradient
		memory usage will be higher than expected (namely, equivalent to not
		freezing those parameters). This means that ideally, frozen parameters
		should be isolated into their own ``nn.Module`` s and wrapped
		separately with FSDP.

	.. note::
		Attempting to run the forward pass of a submodule that is contained in an
		FSDP instance is not supported and will result in errors. This is because the
		submodule's parameters will be sharded, but it itself is not an FSDP instance,
		so its forward pass will not all-gather the full parameters appropriately.
		This could potentially happen when attempting to run only the encoder of a
		encoder-decoder model, and the encoder is not wrapped in its own FSDP instance. To
		resolve this, please wrap the submodule in its own FSDP unit.

	.. note::
		FSDP moves input tensors to the ``forward`` method to the GPU compute
		device, so the user does not need to manually move them from CPU.

	.. warning::
		The user should not modify the parameters between forward and backward
		without using the :meth:`summon_full_params` context since the
		modifications may not persist. Moreover, for ``use_orig_params=False``,
		accessing the original parameters between forward and backward may
		raise an illegal memory access.

	.. warning::
		For ``use_orig_params=True``, ``ShardingStrategy.SHARD_GRAD_OP``
		exposes the unsharded parameters, not the sharded parameters, after
		forward since it does not free the unsharded ones, unlike
		``ShardingStrategy.FULL_SHARD``. One caveat is that, since gradients
		are always sharded or ``None``, ``ShardingStrategy.SHARD_GRAD_OP`` will
		not expose the sharded gradients with the unsharded parameters after
		forward. If you want to inspect the gradients, try
		:meth:`summon_full_params` with ``with_grads=True``.

	.. warning::
		FSDP replaces managed modules' parameters with ``torch.Tensor`` views
		during forward and backward computation for autograd-related reasons.
		If your module's forward relies on saved references to the parameters
		instead of reacquiring the references each iteration, then it will not
		see FSDP's newly created views, and autograd will not work correctly.

	.. note::
		With ``limit_all_gathers=True``, you may see a gap in the FSDP
		pre-forward where the CPU thread is not issuing any kernels. This is
		intentional and shows the rate limiter in effect. Synchronizing the CPU
		thread in that way prevents over-allocating memory for subsequent
		all-gathers, and it should not actually delay GPU kernel execution.

	.. note::
		When using ``sharding_strategy=ShardingStrategy.HYBRID_SHARD`` with the
		sharding process group being intra-node and the replication process
		group being inter-node, setting ``NCCL_CROSS_NIC=1`` can help improve
		the all-reduce times over the replication process group for some
		cluster setups.

	Args:
		module (nn.Module):
			This is the module to be wrapped with FSDP.
		process_group (Optional[Union[ProcessGroup, Tuple[ProcessGroup, ProcessGroup]]]):
			This is the process group over which the model is sharded and thus
			the one used for FSDP's all-gather and reduce-scatter collective
			communications. If ``None``, then FSDP uses the default process
			group. For hybrid sharding strategies such as
			``ShardingStrategy.HYBRID_SHARD``, users can pass in a tuple of
			process groups, representing the groups over which to shard and
			replicate, respectively. If ``None``, then FSDP constructs process
			groups for the user to shard intra-node and replicate inter-node.
			(Default: ``None``)
		sharding_strategy (Optional[ShardingStrategy]):
			This configures the sharding strategy, which may trade off memory
			saving and communication overhead. See :class:`ShardingStrategy`
			for details. (Default: ``FULL_SHARD``)
		cpu_offload (Optional[CPUOffload]):
			This configures CPU offloading. If this is set to ``None``, then
			no CPU offloading happens. See :class:`CPUOffload` for details.
			(Default: ``None``)
		auto_wrap_policy (Optional[Union[Callable[[nn.Module, bool, int], bool], ModuleWrapPolicy, CustomPolicy]]):
			This specifies a policy to apply FSDP to submodules of ``module``,
			which is needed for communication and computation overlap and thus
			affects performance. If ``None``, then FSDP only applies to
			``module``, and users should manually apply FSDP to parent modules
			themselves (proceeding bottom-up). For convenience, this accepts
			``ModuleWrapPolicy`` directly, which allows users to specify the
			module classes to wrap (e.g. the transformer block). Otherwise,
			this should be a callable that takes in three arguments
			``module: nn.Module``, ``recurse: bool``, and
			``nonwrapped_numel: int`` and should return a ``bool`` specifying
			whether the passed-in ``module`` should have FSDP applied if
			``recurse=False`` or if the traversal should continue into the
			module's subtree if ``recurse=True``. Users may add additional
			arguments to the callable. The ``size_based_auto_wrap_policy`` in
			``torch.distributed.fsdp.wrap.py`` gives an example callable that
			applies FSDP to a module if the parameters in its subtree exceed
			100M numel. We recommend printing the model after applying FSDP
			and adjusting as needed.

			Example::

				>>> def custom_auto_wrap_policy(
				>>>	 module: nn.Module,
				>>>	 recurse: bool,
				>>>	 nonwrapped_numel: int,
				>>>	 # Additional custom arguments
				>>>	 min_num_params: int = int(1e8),
				>>> ) -> bool:
				>>>	 return nonwrapped_numel >= min_num_params
				>>> # Configure a custom `min_num_params`
				>>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))

		backward_prefetch (Optional[BackwardPrefetch]):
			This configures explicit backward prefetching of all-gathers. If
			``None``, then FSDP does not backward prefetch, and there is no
			communication and computation overlap in the backward pass. See
			:class:`BackwardPrefetch` for details. (Default: ``BACKWARD_PRE``)
		mixed_precision (Optional[MixedPrecision]):
			This configures native mixed precision for FSDP. If this is set to
			``None``, then no mixed precision is used. Otherwise, parameter,
			buffer, and gradient reduction dtypes can be set. See
			:class:`MixedPrecision` for details. (Default: ``None``)
		ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose
			own parameters and child modules' parameters and buffers are
			ignored by this instance. None of the modules directly in
			``ignored_modules`` should be :class:`FullyShardedDataParallel`
			instances, and any child modules that are already-constructed
			:class:`FullyShardedDataParallel` instances will not be ignored if
			they are nested under this instance. This argument may be used to
			avoid sharding specific parameters at module granularity when using an
			``auto_wrap_policy`` or if parameters' sharding is not managed by
			FSDP. (Default: ``None``)
		param_init_fn (Optional[Callable[[nn.Module], None]]):
			A ``Callable[torch.nn.Module] -> None`` that
			specifies how modules that are currently on the meta device should
			be initialized onto an actual device. As of v1.12, FSDP detects
			modules with parameters or buffers on meta device via ``is_meta``
			and either applies ``param_init_fn`` if specified or calls
			``nn.Module.reset_parameters()`` otherwise. For both cases, the
			implementation should *only* initialize the parameters/buffers of
			the module, not those of its submodules. This is to avoid
			re-initialization. In addition, FSDP also supports deferred
			initialization via torchdistX's (https://github.com/pytorch/torchdistX)
			``deferred_init()`` API, where the deferred modules are initialized
			by calling ``param_init_fn`` if specified or torchdistX's default
			``materialize_module()`` otherwise. If ``param_init_fn`` is
			specified, then it is applied to all meta-device modules, meaning
			that it should probably case on the module type. FSDP calls the
			initialization function before parameter flattening and sharding.

			Example::

				>>> # xdoctest: +SKIP("undefined variables")
				>>> module = MyModule(device="meta")
				>>> def my_init_fn(module: nn.Module):
				>>>	 # E.g. initialize depending on the module type
				>>>	 ...
				>>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)
				>>> print(next(fsdp_model.parameters()).device) # current CUDA device
				>>> # With torchdistX
				>>> module = deferred_init.deferred_init(MyModule, device="cuda")
				>>> # Will initialize via deferred_init.materialize_module().
				>>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)

		device_id (Optional[Union[int, torch.device]]): An ``int`` or
			``torch.device`` giving the CUDA device on which FSDP
			initialization takes place, including the module initialization
			if needed and the parameter sharding. This should be specified to
			improve initialization speed if ``module`` is on CPU. If the
			default CUDA device was set (e.g. via ``torch.cuda.set_device``),
			then the user may pass ``torch.cuda.current_device`` to this.
			(Default: ``None``)
		sync_module_states (bool): If ``True``, then each FSDP module will
			broadcast module parameters and buffers from rank 0 to ensure that
			they are replicated across ranks (adding communication overhead to
			this constructor). This can help load ``state_dict`` checkpoints
			via ``load_state_dict`` in a memory efficient way. See
			:class:`FullStateDictConfig` for an example of this. (Default:
			``False``)
		forward_prefetch (bool): If ``True``, then FSDP *explicitly* prefetches
			the next forward-pass all-gather before the current forward
			computation. This is only useful for CPU-bound workloads, in which
			case issuing the next all-gather earlier may improve overlap. This
			should only be used for static-graph models since the prefetching
			follows the first iteration's execution order. (Default: ``False``)
		limit_all_gathers (bool): If ``True``, then FSDP explicitly
			synchronizes the CPU thread to ensure GPU memory usage from only
			*two* consecutive FSDP instances (the current instance running
			computation and the next instance whose all-gather is prefetched).
			If ``False``, then FSDP allows the CPU thread to issue all-gathers
			without any extra synchronization. (Default: ``True``) We often
			refer to this feature as the "rate limiter". This flag should only
			be set to ``False`` for specific CPU-bound workloads with low
			memory pressure in which case the CPU thread can aggressively issue
			all kernels without concern for the GPU memory usage.
		use_orig_params (bool): Setting this to ``True`` has FSDP use
			``module`` 's original parameters. FSDP exposes those original
			parameters to the user via :meth:`nn.Module.named_parameters`
			instead of FSDP's internal :class:`FlatParameter` s. This means
			that the optimizer step runs on the original parameters, enabling
			per-original-parameter hyperparameters. FSDP preserves the original
			parameter variables and manipulates their data between unsharded
			and sharded forms, where they are always views into the underlying
			unsharded or sharded :class:`FlatParameter`, respectively. With the
			current algorithm, the sharded form is always 1D, losing the
			original tensor structure. An original parameter may have all,
			some, or none of its data present for a given rank. In the none
			case, its data will be like a size-0 empty tensor. Users should not
			author programs relying on what data is present for a given
			original parameter in its sharded form. ``True`` is required to
			use ``torch.compile()``. Setting this to ``False`` exposes FSDP's
			internal :class:`FlatParameter` s to the user via
			:meth:`nn.Module.named_parameters`. (Default: ``False``)
		ignored_states (Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]):
			Ignored parameters or modules that will not be managed by this FSDP
			instance, meaning that the parameters are not sharded and their
			gradients are not reduced across ranks. This argument unifies with
			the existing ``ignored_modules`` argument, and we may deprecate
			``ignored_modules`` soon. For backward compatibility, we keep both
			``ignored_states`` and `ignored_modules``, but FSDP only allows one
			of them to be specified as not ``None``.
		if isinstance(self._fsdp_wrapped_module, ActivationWrapper):
			return getattr(self._fsdp_wrapped_module, _CHECKPOINT_WRAPPED_MODULE)
		return self._fsdp_wrapped_module

	@property
	def _has_params(self) -> bool:
		try:
			return super().__getattr__(name)  # defer to nn.Module's logic
		except AttributeError:
			return getattr(self._fsdp_wrapped_module, name)

	def __getitem__(self, key: int) -> Any:
		return _is_fsdp_root(self, self)

	@staticmethod
	def fsdp_modules(
		module: nn.Module,
		root_only: bool = False,
	) -> List["FullyShardedDataParallel"]:
		if root_only:
			return _get_fsdp_root_states(module)
		return traversal_utils._get_fsdp_states(module)

	def apply(self, fn: Callable[[nn.Module], None]) -> "FullyShardedDataParallel":
		uninitialized = self._is_root is None
		self._assert_state(TrainingState.IDLE)
		with _unshard_params_recurse(
			self,
			self,
			recurse=False,
			writeback=True,
			rank0_only=False,
			offload_to_cpu=False,
			with_grads=False,
		):
			ret = super().apply(fn)

		if uninitialized and self._is_root:
			for module in traversal_utils._get_fsdp_states(self):
				module._reset_lazy_init()

		return ret

	def _mixed_precision_enabled_for_buffers(self) -> bool:
		return self.mixed_precision.buffer_dtype is not None

	def _low_precision_hook_enabled(self) -> bool:
		self._is_root: Optional[bool] = None

	@staticmethod
	def set_state_dict_type(
		module: nn.Module,
		state_dict_type: StateDictType,
		state_dict_config: Optional[StateDictConfig] = None,
		optim_state_dict_config: Optional[OptimStateDictConfig] = None,
	) -> StateDictSettings:
		_state_dict_type_to_config = {
			StateDictType.FULL_STATE_DICT: FullStateDictConfig,
			StateDictType.LOCAL_STATE_DICT: LocalStateDictConfig,
			StateDictType.SHARDED_STATE_DICT: ShardedStateDictConfig,
		}
		_optim_state_dict_type_to_config = {
			StateDictType.FULL_STATE_DICT: FullOptimStateDictConfig,
			StateDictType.LOCAL_STATE_DICT: LocalOptimStateDictConfig,
			StateDictType.SHARDED_STATE_DICT: ShardedOptimStateDictConfig,
		}

		state_dict_config_type = _state_dict_type_to_config[state_dict_type]
		optim_state_dict_config_type = _optim_state_dict_type_to_config[state_dict_type]
		if state_dict_config is None:
			state_dict_config = state_dict_config_type()
		if optim_state_dict_config is None:
			optim_state_dict_config = optim_state_dict_config_type()
		if state_dict_config_type != type(state_dict_config):
			raise RuntimeError(
				f"Expected state_dict_config of type {state_dict_config_type} "
				f"but got {type(state_dict_config)}"
			)
		if optim_state_dict_config_type != type(optim_state_dict_config):
			raise RuntimeError(
				f"Expected optim_state_dict_config of type {optim_state_dict_config_type} "
				f"but got {type(optim_state_dict_config)}"
			)

		prev_state_dict_type = None
		prev_state_dict_config = None
		prev_optim_state_dict_config = None
		for submodule in traversal_utils._get_fsdp_states(module):
			if prev_state_dict_type is None:
				prev_state_dict_type = submodule._state_dict_type
			else:
				assert (
					prev_state_dict_type == submodule._state_dict_type
				), "All FSDP modules should have the same state_dict_type."
			if prev_state_dict_config is None:
				prev_state_dict_config = submodule._state_dict_config
			else:
				assert isinstance(
					submodule._state_dict_config, type(prev_state_dict_config)
				), "All FSDP modules must have the same type of state_dict_config."
			if prev_optim_state_dict_config is None:
				prev_optim_state_dict_config = submodule._optim_state_dict_config
			else:
				assert isinstance(
					submodule._optim_state_dict_config,
					type(prev_optim_state_dict_config),
				), "All FSDP modules must have the same type of optim_state_dict_config."

			submodule._state_dict_type = state_dict_type
			submodule._state_dict_config = state_dict_config
			submodule._optim_state_dict_config = optim_state_dict_config

		return StateDictSettings(
			prev_state_dict_type, prev_state_dict_config, prev_optim_state_dict_config
		)

	@staticmethod
	def get_state_dict_type(module: nn.Module) -> StateDictSettings:
		state_dict_settings: Optional[StateDictSettings] = None
		for submodule in FullyShardedDataParallel.fsdp_modules(module):
			if state_dict_settings is None:
				state_dict_settings = StateDictSettings(
					state_dict_type=submodule._state_dict_type,
					state_dict_config=submodule._state_dict_config,
					optim_state_dict_config=submodule._optim_state_dict_config,
				)
				_set_optim_use_dtensor(submodule, state_dict_settings)
			else:
				submodule_settings = StateDictSettings(
					submodule._state_dict_type,
					submodule._state_dict_config,
					submodule._optim_state_dict_config,
				)
				assert state_dict_settings == submodule_settings, (
					"All FSDP modules must have the same state dict settings."
					f"Got {submodule_settings} and {state_dict_settings}."
				)
				_set_optim_use_dtensor(submodule, submodule_settings)
		return state_dict_settings

	@staticmethod
	@contextlib.contextmanager
	def state_dict_type(
		module: nn.Module,
		state_dict_type: StateDictType,
		state_dict_config: Optional[StateDictConfig] = None,
		optim_state_dict_config: Optional[OptimStateDictConfig] = None,
	) -> Generator:
		prev_state_dict_settings = FullyShardedDataParallel.set_state_dict_type(
			module,
			state_dict_type,
			state_dict_config,
			optim_state_dict_config,
		)
		yield
		FullyShardedDataParallel.set_state_dict_type(
			module,
			prev_state_dict_settings.state_dict_type,
			prev_state_dict_settings.state_dict_config,
			prev_state_dict_settings.optim_state_dict_config,
		)

	def forward(self, *args: Any, **kwargs: Any) -> Any:

		Can be useful *after* forward/backward for a model to get
		the params for additional processing or checking. It can take a non-FSDP
		module and will summon full params for all contained FSDP modules as
		well as their children, depending on the ``recurse`` argument.

		.. note:: This can be used on inner FSDPs.
		.. note:: This can *not* be used within a forward or backward pass. Nor
			can forward and backward be started from within this context.
		.. note:: Parameters will revert to their local shards after the context
			manager exits, storage behavior is the same as forward.
		.. note:: The full parameters can be modified, but only the portion
			corresponding to the local param shard will persist after the
			context manager exits (unless ``writeback=False``, in which case
			changes will be discarded). In the case where FSDP does not shard
			the parameters, currently only when ``world_size == 1``, or ``NO_SHARD``
			config, the modification is persisted regardless of ``writeback``.
		.. note:: This method works on modules which are not FSDP themselves but
			may contain multiple independent FSDP units. In that case, the given
			arguments will apply to all contained FSDP units.

		.. warning:: Note that ``rank0_only=True`` in conjunction with
			``writeback=True`` is not currently supported and will raise an
			error. This is because model parameter shapes would be different
			across ranks within the context, and writing to them can lead to
			inconsistency across ranks when the context is exited.

		.. warning:: Note that ``offload_to_cpu`` and ``rank0_only=False`` will
			result in full parameters being redundantly copied to CPU memory for
			GPUs that reside on the same machine, which may incur the risk of
			CPU OOM. It is recommended to use ``offload_to_cpu`` with
			``rank0_only=True``.

		Args:
			recurse (bool, Optional): recursively summon all params for nested
				FSDP instances (default: True).
			writeback (bool, Optional): if ``False``, modifications to params are
				discarded after the context manager exits;
				disabling this can be slightly more efficient (default: True)
			rank0_only (bool, Optional): if ``True``, full parameters are
				materialized on only global rank 0. This means that within the
				context, only rank 0 will have full parameters and the other
				ranks will have sharded parameters. Note that setting
				``rank0_only=True`` with ``writeback=True`` is not supported,
				as model parameter shapes will be different across ranks
				within the context, and writing to them can lead to
				inconsistency across ranks when the context is exited.
			offload_to_cpu (bool, Optional): If ``True``, full parameters are
				offloaded to CPU. Note that this offloading currently only
				occurs if the parameter is sharded (which is only not the case
				for world_size = 1 or ``NO_SHARD`` config). It is recommended
				to use ``offload_to_cpu`` with ``rank0_only=True`` to avoid
				redundant copies of model parameters being offloaded to the same CPU memory.
			with_grads (bool, Optional): If ``True``, gradients are also
				unsharded with the parameters. Currently, this is only
				supported when passing ``use_orig_params=True`` to the FSDP
				constructor and ``offload_to_cpu=False`` to this method.
				(Default: ``False``)

		If a :class:`FlatParameter` is sharded, then
		this refreshes the sharded views before exiting. This method should
		only be called when using the original parameters.
		context = (
			self._deregister_orig_params_ctx()
			if self._use_orig_params
			else contextlib.nullcontext()
		)
		with context:
			return super()._apply(*args, **kwargs)

	def named_buffers(
		self,
		*args,
		**kwargs,
	) -> Iterator[Tuple[str, torch.Tensor]]:
		should_clean_name = self.training_state == TrainingState.SUMMON_FULL_PARAMS
		for buffer_name, buffer in super().named_buffers(*args, **kwargs):
			if should_clean_name:
				buffer_name = buffer_name.replace(FSDP_PREFIX, "")
			yield (buffer_name, buffer)

	def named_parameters(
		self,
		*args,
		**kwargs,
	) -> Iterator[Tuple[str, torch.nn.Parameter]]:
		should_clean_name = self.training_state == TrainingState.SUMMON_FULL_PARAMS
		for param_name, param in super().named_parameters(*args, **kwargs):
			if should_clean_name:
				param_name = param_name.replace(FSDP_PREFIX, "")
			yield (param_name, param)

	def _assert_state(self, state: Union[TrainingState, List[TrainingState]]) -> None:

		Within this context, gradients will be accumulated in module
		variables, which will later be synchronized in the first
		forward-backward pass after exiting the context. This should only be
		used on the root FSDP instance and will recursively apply to all
		children FSDP instances.

		.. note:: This likely results in higher memory usage because FSDP will
			accumulate the full model gradients (instead of gradient shards)
			until the eventual sync.

		.. note:: When used with CPU offloading, the gradients will not be
			offloaded to CPU when inside the context manager. Instead, they
			will only be offloaded right after the eventual sync.

		The norm is computed over all parameters' gradients as viewed as a single vector, and the
		gradients are modified in-place.

		Args:
			max_norm (float or int): max norm of the gradients
			norm_type (float or int): type of the used p-norm. Can be ``'inf'``
				for infinity norm.

		Returns:
			Total norm of the parameters (viewed as a single vector).

		.. note:: If every FSDP instance uses ``NO_SHARD``, meaning that no
			gradients are sharded across ranks, then you may directly use
			:func:`torch.nn.utils.clip_grad_norm_`.

		.. note:: If at least some FSDP instance uses a sharded strategy (i.e.
			one other than ``NO_SHARD``), then you should use this method
			instead of :func:`torch.nn.utils.clip_grad_norm_` since this method
			handles the fact that gradients are sharded across ranks.

		.. note:: The total norm returned will have the "largest" dtype across
			all parameters/gradients as defined by PyTorch's type promotion
			semantics. For example, if *all* parameters/gradients use a low
			precision dtype, then the returned norm's dtype will be that low
			precision dtype, but if there exists at least one parameter/
			gradient using FP32, then the returned norm's dtype will be FP32.

		.. warning:: This needs to be called on all ranks since it uses
			collective communications.

		This is the internal API that is used by all the optim_state_dict implementations.
		Given model, optim, the original optim_state_dict, this API removes the
		FSDP internal information and internal sharding from the optim_state_dict.
		Convert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.

		This is the internal API that is used by all the load optim_state_dict implementations.
		Given model, optim, and the saved optim_state_dict, this API adds the FSDP
		internal information and internal sharding to the optim_state_dict.

		Consolidates the full optimizer state on rank 0 and returns it
		as a :class:`dict` following the convention of
		:meth:`torch.optim.Optimizer.state_dict`, i.e. with keys ``"state"``
		and ``"param_groups"``. The flattened parameters in ``FSDP`` modules
		contained in ``model`` are mapped back to their unflattened parameters.

		.. warning:: This needs to be called on all ranks since it uses
			collective communications. However, if ``rank0_only=True``, then
			the state dict is only populated on rank 0, and all other ranks
			return an empty :class:`dict`.

		.. warning:: Unlike ``torch.optim.Optimizer.state_dict()``, this method
			uses full parameter names as keys instead of parameter IDs.

		.. note:: Like in :meth:`torch.optim.Optimizer.state_dict`, the tensors
			contained in the optimizer state dict are not cloned, so there may
			be aliasing surprises. For best practices, consider saving the
			returned optimizer state dict immediately, e.g. using
			``torch.save()``.

		Args:
			model (torch.nn.Module): Root module (which may or may not be a
				:class:`FullyShardedDataParallel` instance) whose parameters
				were passed into the optimizer ``optim``.
			optim (torch.optim.Optimizer): Optimizer for ``model`` 's
				parameters.
			optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):
				Input passed into the optimizer ``optim`` representing either a
				:class:`list` of parameter groups or an iterable of parameters;
				if ``None``, then this method assumes the input was
				``model.parameters()``. This argument is deprecated, and there
				is no need to pass it in anymore. (Default: ``None``)
			rank0_only (bool): If ``True``, saves the populated :class:`dict`
				only on rank 0; if ``False``, saves it on all ranks. (Default:
				``True``)
			group (dist.ProcessGroup): Model's process group or ``None`` if using
				the default process group. (Default: ``None``)

		Returns:
			Dict[str, Any]: A :class:`dict` containing the optimizer state for
			``model`` 's original unflattened parameters and including keys
			"state" and "param_groups" following the convention of
			:meth:`torch.optim.Optimizer.state_dict`. If ``rank0_only=True``,
			then nonzero ranks return an empty :class:`dict`.

		The API is similar to :meth:`full_optim_state_dict` but this API chunks
		all non-zero-dimension states to :class:`ShardedTensor` to save memory.
		This API should only be used when the model ``state_dict`` is derived
		with the context manager ``with state_dict_type(SHARDED_STATE_DICT):``.

		For the detailed usage, refer to :meth:`full_optim_state_dict`.

		.. warning:: The returned state dict contains ``ShardedTensor`` and
			cannot be directly used by the regular ``optim.load_state_dict``.

		Remaps the state in ``full_optim_state_dict`` to flattened parameters instead of unflattened
		parameters and restricts to only this rank's part of the optimizer state.
		The first argument should be the return value of :meth:`full_optim_state_dict`.

		Example::

			>>> # xdoctest: +SKIP("undefined variables")
			>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
			>>> model, optim = ...
			>>> full_osd = FSDP.full_optim_state_dict(model, optim)
			>>> torch.save(full_osd, PATH)
			>>> # Define new model with possibly different world size
			>>> new_model, new_optim = ...
			>>> full_osd = torch.load(PATH)
			>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)
			>>> new_optim.load_state_dict(sharded_osd)

		.. note:: Both :meth:`shard_full_optim_state_dict` and
			:meth:`scatter_full_optim_state_dict` may be used to get the
			sharded optimizer state dict to load. Assuming that the full
			optimizer state dict resides in CPU memory, the former requires
			each rank to have the full dict in CPU memory, where each rank
			individually shards the dict without any communication, while the
			latter requires only rank 0 to have the full dict in CPU memory,
			where rank 0 moves each shard to GPU memory (for NCCL) and
			communicates it to ranks appropriately. Hence, the former has
			higher aggregate CPU memory cost, while the latter has higher
			communication cost.

		Args:
			full_optim_state_dict (Dict[str, Any]): Optimizer state dict
				corresponding to the unflattened parameters and holding the
				full non-sharded optimizer state.
			model (torch.nn.Module): Root module (which may or may not be a
				:class:`FullyShardedDataParallel` instance) whose parameters
				correspond to the optimizer state in ``full_optim_state_dict``.
			optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):
				Input passed into the optimizer representing either a
				:class:`list` of parameter groups or an iterable of parameters;
				if ``None``, then this method assumes the input was
				``model.parameters()``. This argument is deprecated, and there
				is no need to pass it in anymore. (Default: ``None``)
			optim (Optional[torch.optim.Optimizer]): Optimizer that will load
				the state dict returned by this method. This is the preferred
				argument to use over ``optim_input``. (Default: ``None``)

		Returns:
			Dict[str, Any]: The full optimizer state dict now remapped to
			flattened parameters instead of unflattened parameters and
			restricted to only include this rank's part of the optimizer state.

		The API is similar to :meth:`shard_full_optim_state_dict`. The only
		difference is that the input ``sharded_optim_state_dict`` should be
		returned from :meth:`sharded_optim_state_dict`. Therefore, there will
		be all-gather calls on each rank to gather ``ShardedTensor`` s.

		Args:
			sharded_optim_state_dict (Dict[str, Any]): Optimizer state dict
				corresponding to the unflattened parameters and holding the
				sharded optimizer state.
			model (torch.nn.Module):
				Refer to :meth:`shard_full_optim_state_dict`.
			optim (torch.optim.Optimizer): Optimizer for ``model`` 's
				parameters.

		Returns:
			Refer to :meth:`shard_full_optim_state_dict`.

		Returns the sharded optimizer state dict on each rank.
		The return value is the same as :meth:`shard_full_optim_state_dict`, and on rank
		0, the first argument should be the return value of
		:meth:`full_optim_state_dict`.

		Example::

			>>> # xdoctest: +SKIP("undefined variables")
			>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
			>>> model, optim = ...
			>>> full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0
			>>> # Define new model with possibly different world size
			>>> new_model, new_optim, new_group = ...
			>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)
			>>> new_optim.load_state_dict(sharded_osd)

		.. note:: Both :meth:`shard_full_optim_state_dict` and
			:meth:`scatter_full_optim_state_dict` may be used to get the
			sharded optimizer state dict to load. Assuming that the full
			optimizer state dict resides in CPU memory, the former requires
			each rank to have the full dict in CPU memory, where each rank
			individually shards the dict without any communication, while the
			latter requires only rank 0 to have the full dict in CPU memory,
			where rank 0 moves each shard to GPU memory (for NCCL) and
			communicates it to ranks appropriately. Hence, the former has
			higher aggregate CPU memory cost, while the latter has higher
			communication cost.

		Args:
			full_optim_state_dict (Optional[Dict[str, Any]]): Optimizer state
				dict corresponding to the unflattened parameters and holding
				the full non-sharded optimizer state if on rank 0; the argument
				is ignored on nonzero ranks.
			model (torch.nn.Module): Root module (which may or may not be a
				:class:`FullyShardedDataParallel` instance) whose parameters
				correspond to the optimizer state in ``full_optim_state_dict``.
			optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):
				Input passed into the optimizer representing either a
				:class:`list` of parameter groups or an iterable of parameters;
				if ``None``, then this method assumes the input was
				``model.parameters()``. This argument is deprecated, and there
				is no need to pass it in anymore. (Default: ``None``)
			optim (Optional[torch.optim.Optimizer]): Optimizer that will load
				the state dict returned by this method. This is the preferred
				argument to use over ``optim_input``. (Default: ``None``)
			group (dist.ProcessGroup): Model's process group or ``None`` if
				using the default process group. (Default: ``None``)

		Returns:
			Dict[str, Any]: The full optimizer state dict now remapped to
			flattened parameters instead of unflattened parameters and
			restricted to only include this rank's part of the optimizer state.

		This can be used to achieve compatibility between optimizer state dicts from models with FSDP
		instances and ones without.

		To re-key an FSDP full optimizer state dict (i.e. from
		:meth:`full_optim_state_dict`) to use parameter IDs and be loadable to
		a non-wrapped model::

			>>> # xdoctest: +SKIP("undefined variables")
			>>> wrapped_model, wrapped_optim = ...
			>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)
			>>> nonwrapped_model, nonwrapped_optim = ...
			>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)
			>>> nonwrapped_optim.load_state_dict(rekeyed_osd)

		To re-key a normal optimizer state dict from a non-wrapped model to be
		loadable to a wrapped model::

			>>> # xdoctest: +SKIP("undefined variables")
			>>> nonwrapped_model, nonwrapped_optim = ...
			>>> osd = nonwrapped_optim.state_dict()
			>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)
			>>> wrapped_model, wrapped_optim = ...
			>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)
			>>> wrapped_optim.load_state_dict(sharded_osd)

		Returns:
			Dict[str, Any]: The optimizer state dict re-keyed using the
			parameter keys specified by ``optim_state_key_type``.
		Transform the state-dict of an optimizer corresponding to a sharded model.

		The given state-dict can be transformed to one of three types:
		1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.

		For full optimizer state_dict, all states are unflattened and not sharded.
		Rank0 only and CPU only can be specified via :meth:`state_dict_type` to
		avoid OOM.

		For sharded optimizer state_dict, all states are unflattened but sharded.
		CPU only can be specified via :meth:`state_dict_type` to further save
		memory.

		For local state_dict, no transformation will be performed. But a state
		will be converted from nn.Tensor to ShardedTensor to represent its sharding
		nature (this is not supported yet).

		Example::

			>>> # xdoctest: +SKIP("undefined variables")
			>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
			>>> from torch.distributed.fsdp import StateDictType
			>>> from torch.distributed.fsdp import FullStateDictConfig
			>>> from torch.distributed.fsdp import FullOptimStateDictConfig
			>>> # Save a checkpoint
			>>> model, optim = ...
			>>> FSDP.set_state_dict_type(
			>>>	 model,
			>>>	 StateDictType.FULL_STATE_DICT,
			>>>	 FullStateDictConfig(rank0_only=False),
			>>>	 FullOptimStateDictConfig(rank0_only=False),
			>>> )
			>>> state_dict = model.state_dict()
			>>> optim_state_dict = FSDP.optim_state_dict(model, optim)
			>>> save_a_checkpoint(state_dict, optim_state_dict)
			>>> # Load a checkpoint
			>>> model, optim = ...
			>>> state_dict, optim_state_dict = load_a_checkpoint()
			>>> FSDP.set_state_dict_type(
			>>>	 model,
			>>>	 StateDictType.FULL_STATE_DICT,
			>>>	 FullStateDictConfig(rank0_only=False),
			>>>	 FullOptimStateDictConfig(rank0_only=False),
			>>> )
			>>> model.load_state_dict(state_dict)
			>>> optim_state_dict = FSDP.optim_state_dict_to_load(
			>>>	 optim_state_dict, model, optim
			>>> )
			>>> optim.load_state_dict(optim_state_dict)

		Args:
			model (torch.nn.Module): Root module (which may or may not be a
				:class:`FullyShardedDataParallel` instance) whose parameters
				were passed into the optimizer ``optim``.
			optim (torch.optim.Optimizer): Optimizer for ``model`` 's
				parameters.
			optim_state_dict (Dict[str, Any]): the target optimizer state_dict to
				transform. If the value is None, optim.state_dict() will be used. (
				Default: ``None``)
			group (dist.ProcessGroup): Model's process group across which parameters
				are sharded or ``None`` if using the default process group. (
				Default: ``None``)

		Returns:
			Dict[str, Any]: A :class:`dict` containing the optimizer state for
			``model``. The sharding of the optimizer state is based on
			``state_dict_type``.
		Convert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.

		Given a ``optim_state_dict`` that is transformed through
		:meth:`optim_state_dict`, it gets converted to the flattened optimizer
		state_dict that can be loaded to ``optim`` which is the optimizer for
		``model``. ``model`` must be sharded by FullyShardedDataParallel.

			>>> # xdoctest: +SKIP("undefined variables")
			>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
			>>> from torch.distributed.fsdp import StateDictType
			>>> from torch.distributed.fsdp import FullStateDictConfig
			>>> from torch.distributed.fsdp import FullOptimStateDictConfig
			>>> # Save a checkpoint
			>>> model, optim = ...
			>>> FSDP.set_state_dict_type(
			>>>	 model,
			>>>	 StateDictType.FULL_STATE_DICT,
			>>>	 FullStateDictConfig(rank0_only=False),
			>>>	 FullOptimStateDictConfig(rank0_only=False),
			>>> )
			>>> state_dict = model.state_dict()
			>>> original_osd = optim.state_dict()
			>>> optim_state_dict = FSDP.optim_state_dict(
			>>>	 model,
			>>>	 optim,
			>>>	 optim_state_dict=original_osd
			>>> )
			>>> save_a_checkpoint(state_dict, optim_state_dict)
			>>> # Load a checkpoint
			>>> model, optim = ...
			>>> state_dict, optim_state_dict = load_a_checkpoint()
			>>> FSDP.set_state_dict_type(
			>>>	 model,
			>>>	 StateDictType.FULL_STATE_DICT,
			>>>	 FullStateDictConfig(rank0_only=False),
			>>>	 FullOptimStateDictConfig(rank0_only=False),
			>>> )
			>>> model.load_state_dict(state_dict)
			>>> optim_state_dict = FSDP.optim_state_dict_to_load(
			>>>	 model, optim, optim_state_dict
			>>> )
			>>> optim.load_state_dict(optim_state_dict)

		Args:
			model (torch.nn.Module): Root module (which may or may not be a
				:class:`FullyShardedDataParallel` instance) whose parameters
				were passed into the optimizer ``optim``.
			optim (torch.optim.Optimizer): Optimizer for ``model`` 's
				parameters.
			optim_state_dict (Dict[str, Any]): The optimizer states to be loaded.
			is_named_optimizer (bool): Is this optimizer a NamedOptimizer or
				KeyedOptimizer. Only set to True if ``optim`` is TorchRec's
				KeyedOptimizer or torch.distributed's NamedOptimizer.
			load_directly (bool): If this is set to True, this API will also
				call optim.load_state_dict(result) before returning the result.
				Otherwise, users are responsible to call ``optim.load_state_dict()``
				(Default: ``False``)
			group (dist.ProcessGroup): Model's process group across which parameters
				are sharded or ``None`` if using the default process group. (
				Default: ``None``)

		This is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates
		gradients across multiple workers.
		This hook can be used to implement several algorithms like
		`GossipGrad <https://arxiv.org/abs/1803.05880>`_ and gradient compression
		which involve different communication strategies for
		parameter syncs while training with :class:`FullyShardedDataParallel`.

		.. warning ::
			FSDP communication hook should be registered before running an initial forward pass
			and only once.

		Args:
			state (object): Passed to the hook to maintain any state information during the training process.
							Examples include error feedback in gradient compression,
							peers to communicate with next in `GossipGrad <https://arxiv.org/abs/1803.05880>`_, etc.
							It is locally stored by each worker
							and shared by all the gradient tensors on the worker.
			hook (Callable): Callable, which has one of the following signatures:
							1) ``hook: Callable[torch.Tensor] -> None``:
							This function takes in a Python tensor, which represents
							the full, flattened, unsharded gradient with respect to all variables
							corresponding to the model this FSDP unit is wrapping
							(that are not wrapped by other FSDP sub-units).
							It then performs all necessary processing and returns ``None``;
							2) ``hook: Callable[torch.Tensor, torch.Tensor] -> None``:
							This function takes in two Python tensors, the first one represents
							the full, flattened, unsharded gradient with respect to all variables
							corresponding to the model this FSDP unit is wrapping
							(that are not wrapped by other FSDP sub-units). The latter
							represents a pre-sized tensor to store a chunk of a sharded gradient after
							reduction.
							In both cases, callable performs all necessary processing and returns ``None``.
							Callables with signature 1 are expected to handle gradient communication for a `NO_SHARD` case.
							Callables with signature 2 are expected to handle gradient communication for sharded cases.

	Return the gradient norm of parameters ``param`` s, where the gradients are viewed as a single vector.

	The returned norm is in FP32 even if parameters/gradients are in a low precision. This is because the downstream
	use of this return value is a reduction across ranks.
	Construct a mapping from parameters to their parameter names.

	The ``model`` should not contain any :class:`FullyShardedDataParallel` instances, which
	means that none of the parameters should be ``FlatParameter`` s. As a
	result, compared to :meth:`_get_param_to_fqns`, the mapped
	values may be flattened from singleton :class:`list` s to the contained
	names themselves.

	Args:
		model (torch.nn.Module): Root module, which should not contain any
			:class:`FullyShardedDataParallel` instances.
	param_to_param_name = _get_param_to_fqn(model)
	return dict(zip(param_to_param_name.values(), param_to_param_name.keys()))

<END>

<START>
from typing import Dict, List, Set, Iterable, Sequence, Optional, Deque

from torch.fx.passes.utils.fuser_utils import fuse_by_partitions

from torch.fx.graph_module import GraphModule
from torch.fx.node import Node, _get_qualified_name
from torch.fx.passes.operator_support import OperatorSupportBase

import logging
import itertools
from copy import copy
from collections import deque

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)

class Partition:
	def __init__(self, id: Optional[int] = None, nodes: Optional[Iterable[Node]] = None):
		self.id = id
		self.nodes: Set[Node] = set(nodes) if nodes is not None else set()

	def __repr__(self) -> str:
		return str(self.nodes)

	def add_node(self, node: Node):
		self.nodes.add(node)

	def remove_node(self, node: Node):
		self.nodes.remove(node)

	def size(self):
		return len(self.nodes)

class CapabilityBasedPartitioner:

	def __init__(self,
				 graph_module: GraphModule,
				 operator_support: OperatorSupportBase,
				 allows_single_node_partition: bool = False,
				 non_compute_ops: Optional[Sequence[str]] = None,
				 allowed_single_node_partition_ops: Optional[Sequence[str]] = None,
				 ) -> None:
		self.graph_module = graph_module
		self.operator_support = operator_support
		self.allows_single_node_partition = allows_single_node_partition
		self.non_compute_ops = non_compute_ops if non_compute_ops is not None else []
		self.allowed_single_node_partition_ops = (
			allowed_single_node_partition_ops
			if allowed_single_node_partition_ops is not None
			else []
		)

	def __is_node_supported(self, node: Node) -> bool:
		return (
			self.operator_support.is_node_supported(dict(self.graph_module.named_modules()), node)
		)

	def propose_partitions(self) -> List[Partition]:
		assignment: Dict[Node, int] = {}   # mapping from node to partition_id
		partitions_by_id: Dict[int, Partition] = {}  # mapping from partition_id to partition
		new_partition_id = itertools.count()

		def maybe_merge_partition(self_id: int, other_id: int):
			merged_nodes = copy(partitions_by_id[self_id].nodes)
			merged_nodes.update(partitions_by_id[other_id].nodes)

			visited: Set[Node] = set()

			def dfs_iter_find_cycle(root_node):
				stack : Deque[Node] = deque()
				stack.append(root_node)

				while stack:
					node = stack.pop()

					if node in visited:
						continue
					if node in merged_nodes:
						return True  # found cycle, return

					if node in assignment:
						for p_node in partitions_by_id[assignment[node]].nodes:
							for user_node in p_node.users:
								if user_node not in partitions_by_id[assignment[node]].nodes:
									stack.append(user_node)
					else:
						for user_node in node.users:
							stack.append(user_node)

					visited.add(node)

				return False

			for node in merged_nodes:
				for user_node in node.users:
					if user_node not in merged_nodes and dfs_iter_find_cycle(user_node):
						return False

			partitions_by_id[self_id].nodes = merged_nodes
			for node in partitions_by_id[other_id].nodes:
				assignment[node] = self_id
			del partitions_by_id[other_id]

			return True

		def merge_single_node(node: Node, id: Optional[int]):
			if node in assignment:
				partitions_by_id[assignment[node]].remove_node(node)

			if id is None:
				assignment.pop(node)
			elif id not in partitions_by_id:
				assignment[node] = id
				partitions_by_id[id] = Partition(id=id, nodes=[node])
			else:
				assignment[node] = id
				partitions_by_id[id].add_node(node)

		logger.debug("Proposing partitions...")

		for node in reversed(self.graph_module.graph.nodes):
			merge_candidates: Dict[int, None] = {}

			if self.__is_node_supported(node) and node not in assignment:
				partition_id = next(new_partition_id)
				merge_single_node(node, partition_id)
				merge_candidates[partition_id] = None

			for node in assignment:
				merge_candidates[assignment[node]] = None

			merge_candidates_list = list(merge_candidates.keys())
			if len(merge_candidates_list) > 1:
				self_id = merge_candidates_list[0]
				for other_id in merge_candidates_list[1:]:
					maybe_merge_partition(self_id, other_id)

		logger.debug("Reassigning getitem nodes to its producer node's partition...")
		nodes_reassignment: Dict[Node, int] = {}
		for node in self.graph_module.graph.nodes:
			is_tuple_output = True
			for user in node.users:
				if user.op != "call_function" or \
				   _get_qualified_name(user.target) != "_operator.getitem":	 # type: ignore[arg-type]
					is_tuple_output = False
					break

			if is_tuple_output:
				id = assignment.get(node, None)	 # type: ignore[arg-type]
				for user in node.users:
					if assignment.get(user, None) != id:	# type: ignore[arg-type]
						nodes_reassignment[user] = id  # type: ignore[assignment]
		for node, id in nodes_reassignment.items():
			merge_single_node(node, id)

		if not self.allows_single_node_partition:
			logger.debug("Filtering out single node partitions...")
			default_non_compute_ops = {"torch.ops.aten.view", "_operator.getitem"}
			non_compute_ops = default_non_compute_ops.union(set(self.non_compute_ops))
			partitions_to_remove: List[int] = []
			for id, partition in partitions_by_id.items():
				compute_node_count = 0
				for node in partition.nodes:
					if node.op == "call_function":
						assert callable(node.target)
						if _get_qualified_name(node.target) not in non_compute_ops:
							compute_node_count += 1
						if _get_qualified_name(node.target) in self.allowed_single_node_partition_ops:
							compute_node_count += 1
				if compute_node_count <= 1:
					partitions_to_remove.append(id)
			for id in partitions_to_remove:
				del partitions_by_id[id]

		logger.debug("Partitions proposed:")
		for id, partition in partitions_by_id.items():
			logger.debug("partition #%s: %s", id, [node.name for node in partition.nodes])

		return list(partitions_by_id.values())

	def fuse_partitions(self, partitions: List[Partition]) -> GraphModule:
		logger.debug("Fusing partitions...")
		return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])

	def remove_bookend_non_compute_ops(self, partitions: List[Partition]):
		non_compute_ops = set(self.non_compute_ops)

		def is_non_compute_node(node: Node):
			return node.op == "call_function" and \
				_get_qualified_name(node.target) in non_compute_ops  # type: ignore[arg-type]

		transparent_input_nodes: Dict[Node, bool] = {}
		transparent_output_nodes: Dict[Node, bool] = {}

		def is_transparent_input_node(node: Node, partition: Set[Node], removed_nodes: Set[Node]):
			if node.op == "placeholder" or (node not in partition) or (node in removed_nodes):
				return True
			if node in transparent_input_nodes:
				return transparent_input_nodes[node]
			if is_non_compute_node(node):
				for input_n in node.all_input_nodes:
					if not is_transparent_input_node(input_n, partition, removed_nodes):
						transparent_input_nodes[node] = False
						return False
				transparent_input_nodes[node] = True
				return True
			transparent_input_nodes[node] = False
			return False

		def is_transparent_output_node(node: Node, partition: Set[Node], removed_nodes: Set[Node]):
			if node.op == "placeholder" or (node not in partition) or (node in removed_nodes):
				return True
			if node in transparent_output_nodes:
				return transparent_output_nodes[node]
			if is_non_compute_node(node):
				for output_n in node.users:
					if not is_transparent_output_node(output_n, partition, removed_nodes):
						transparent_output_nodes[node] = False
						return False
				transparent_output_nodes[node] = True
				return True
			transparent_output_nodes[node] = False
			return False

		for partition in partitions:
			remove_node: Set[Node] = set()
			for node in partition.nodes:
				if is_non_compute_node(node) and \
					(is_transparent_input_node(node, partition.nodes, remove_node) or
					 is_transparent_output_node(node, partition.nodes, remove_node)):
					remove_node.add(node)

			if len(remove_node) != 0:
				partition.nodes = partition.nodes - remove_node

	def partition_and_fuse(self) -> GraphModule:
		partitions = self.propose_partitions()
		fused_gm = self.fuse_partitions(partitions)
		return fused_gm

<END>

<START>
import os
import re
import sys
from typing import List

__all__ = [
	"check_code_for_cuda_kernel_launches",
	"check_cuda_kernel_launches",
]

exclude_files: List[str] = []


kernel_launch_start = re.compile(
	r"^.*<<<[^>]+>>>\s*\(", flags=re.MULTILINE
)

has_check = re.compile(
	r"\s*;(?![^;}]*C10_CUDA_KERNEL_LAUNCH_CHECK\(\);)", flags=re.MULTILINE
)

def find_matching_paren(s: str, startpos: int) -> int:
	opening = 0
	for i, c in enumerate(s[startpos:]):
		if c == '(':
			opening += 1
		elif c == ')':
			opening -= 1
			if opening == 0:
				return startpos + i + 1

	raise IndexError("Closing parens not found!")


def should_exclude_file(filename) -> bool:
	for exclude_suffix in exclude_files:
		if filename.endswith(exclude_suffix):
			return True
	return False


def check_code_for_cuda_kernel_launches(code, filename=None):
	if filename is None:
		filename = "##Python Function Call##"

	code = enumerate(code.split("\n"))							 # Split by line breaks
	code = [f"{lineno}: {linecode}" for lineno, linecode in code]  # Number the lines
	code = '\n'.join(code)										 # Put it back together

	num_launches_without_checks = 0
	for m in kernel_launch_start.finditer(code):
		end_paren = find_matching_paren(code, m.end() - 1)
		if has_check.match(code, end_paren):
			num_launches_without_checks += 1
			context = code[m.start():end_paren + 1]
			print(f"Missing C10_CUDA_KERNEL_LAUNCH_CHECK in '{filename}'. Context:\n{context}", file=sys.stderr)

	return num_launches_without_checks


def check_file(filename):
	if not (filename.endswith((".cu", ".cuh"))):
		return 0
	if should_exclude_file(filename):
		return 0
	with open(filename) as fo:
		contents = fo.read()
		unsafeCount = check_code_for_cuda_kernel_launches(contents, filename)
	return unsafeCount


def check_cuda_kernel_launches():
	torch_dir = os.path.dirname(os.path.realpath(__file__))
	torch_dir = os.path.dirname(torch_dir)  # Go up to parent torch
	torch_dir = os.path.dirname(torch_dir)  # Go up to parent caffe2

	kernels_without_checks = 0
	files_without_checks = []
	for root, dirnames, filenames in os.walk(torch_dir):
		if root == os.path.join(torch_dir, "build") or root == os.path.join(torch_dir, "torch/include"):
			dirnames[:] = []
			continue

		for x in filenames:
			filename = os.path.join(root, x)
			file_result = check_file(filename)
			if file_result > 0:
				kernels_without_checks += file_result
				files_without_checks.append(filename)

	if kernels_without_checks > 0:
		count_str = f"Found {kernels_without_checks} instances in " \
					f"{len(files_without_checks)} files where kernel " \
					"launches didn't have checks."
		print(count_str, file=sys.stderr)
		print("Files without checks:", file=sys.stderr)
		for x in files_without_checks:
			print(f"\t{x}", file=sys.stderr)
		print(count_str, file=sys.stderr)

	return kernels_without_checks


if __name__ == "__main__":
	unsafe_launches = check_cuda_kernel_launches()
	sys.exit(0 if unsafe_launches == 0 else 1)

<END>

<START>
import torch
from typing import Iterable, Optional


def parameters_to_vector(parameters: Iterable[torch.Tensor]) -> torch.Tensor:
	param_device = None

	vec = []
	for param in parameters:
		param_device = _check_param_device(param, param_device)

		vec.append(param.view(-1))
	return torch.cat(vec)


def vector_to_parameters(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) -> None:
	if not isinstance(vec, torch.Tensor):
		raise TypeError(f'expected torch.Tensor, but got: {torch.typename(vec)}')
	param_device = None

	pointer = 0
	for param in parameters:
		param_device = _check_param_device(param, param_device)

		num_param = param.numel()
		param.data = vec[pointer:pointer + num_param].view_as(param).data

		pointer += num_param


def _check_param_device(param: torch.Tensor, old_param_device: Optional[int]) -> int:
	support_device_types = ["cuda", torch._C._get_privateuse1_backend_name()]
	if old_param_device is None:
		old_param_device = param.get_device() if param.device.type in support_device_types else -1
	else:
		warn = False
		if param.device.type in support_device_types:  # Check if in same GPU/PrivateUse1
			warn = (param.get_device() != old_param_device)
		else:  # Check if in CPU
			warn = (old_param_device != -1)
		if warn:
			raise TypeError('Found two parameters on different devices, '
							'this is currently not supported.')
	return old_param_device

<END>

<START>
import functools
from collections import namedtuple

from typing import Callable, Iterator, Sized, TypeVar, Optional, Union, Any, Dict, List

from torch.utils.data.datapipes._decorator import functional_datapipe
from torch.utils.data._utils.collate import default_collate
from torch.utils.data.datapipes.dataframe import dataframe_wrapper as df_wrapper
from torch.utils.data.datapipes.datapipe import IterDataPipe
from torch.utils.data.datapipes.utils.common import (_check_unpickable_fn,
													 validate_input_col)

__all__ = [
	"CollatorIterDataPipe",
	"MapperIterDataPipe",
]

T_co = TypeVar("T_co", covariant=True)


@functional_datapipe("map")
class MapperIterDataPipe(IterDataPipe[T_co]):

	datapipe: IterDataPipe
	fn: Callable

	def __init__(
		self,
		datapipe: IterDataPipe,
		fn: Callable,
		input_col=None,
		output_col=None,
	) -> None:
		super().__init__()
		self.datapipe = datapipe

		_check_unpickable_fn(fn)
		self.fn = fn  # type: ignore[assignment]

		self.input_col = input_col
		if input_col is None and output_col is not None:
			raise ValueError("`output_col` must be None when `input_col` is None.")
		if isinstance(output_col, (list, tuple)):
			if len(output_col) > 1:
				raise ValueError("`output_col` must be a single-element list or tuple")
			output_col = output_col[0]
		self.output_col = output_col
		validate_input_col(fn, input_col)

	def _apply_fn(self, data):
		if self.input_col is None and self.output_col is None:
			return self.fn(data)

		if self.input_col is None:
			res = self.fn(data)
		elif isinstance(self.input_col, (list, tuple)):
			args = tuple(data[col] for col in self.input_col)
			res = self.fn(*args)
		else:
			res = self.fn(data[self.input_col])

		if isinstance(data, tuple):
			t_flag = True
			data = list(data)
		else:
			t_flag = False

		if self.output_col is None:
			if isinstance(self.input_col, (list, tuple)):
				data[self.input_col[0]] = res
				for idx in sorted(self.input_col[1:], reverse=True):
					del data[idx]
			else:
				data[self.input_col] = res
		else:
			if self.output_col == -1:
				data.append(res)
			else:
				data[self.output_col] = res

		return tuple(data) if t_flag else data

	def __iter__(self) -> Iterator[T_co]:
		for data in self.datapipe:
			yield self._apply_fn(data)

	def __len__(self) -> int:
		if isinstance(self.datapipe, Sized):
			return len(self.datapipe)
		raise TypeError(
			f"{type(self).__name__} instance doesn't have valid length"
		)


def _collate_helper(conversion, item):
	if len(item.items) > 1:
		raise Exception("Only supports one DataFrame per batch")
	df = item[0]
	columns_name = df_wrapper.get_columns(df)
	tuple_names: List = []
	tuple_values: List = []

	for name in conversion.keys():
		if name not in columns_name:
			raise Exception("Conversion keys missmatch")

	for name in columns_name:
		if name in conversion:
			if not callable(conversion[name]):
				raise Exception('Collate (DF)DataPipe requires callable as dict values')
			collation_fn = conversion[name]
		else:
			try:
				import torcharrow.pytorch as tap  # type: ignore[import]
				collation_fn = tap.rec.Default()
			except Exception as e:
				raise Exception("unable to import default collation function from the TorchArrow") from e

		tuple_names.append(str(name))
		value = collation_fn(df[name])
		tuple_values.append(value)

	tpl_cls = namedtuple("CollateResult", tuple_names)  # type: ignore[misc]
	tuple = tpl_cls(*tuple_values)
	return tuple


@functional_datapipe("collate")
class CollatorIterDataPipe(MapperIterDataPipe):

	def __init__(
		self,
		datapipe: IterDataPipe,
		conversion: Optional[
			Union[
			Callable[..., Any],
			Dict[Union[str, Any], Union[Callable, Any]],
			]
		] = default_collate,
		collate_fn: Optional[Callable] = None,
	) -> None:
		if collate_fn is not None:
			super().__init__(datapipe, fn=collate_fn)
		else:
			if callable(conversion):
				super().__init__(datapipe, fn=conversion)
			else:
				collate_fn = functools.partial(_collate_helper, conversion)
				super().__init__(datapipe, fn=collate_fn)

<END>

<START>

import dataclasses
from typing import Tuple

from torch.onnx._internal.diagnostics import infra



class _NodeMissingOnnxShapeInference(infra.Rule):

		Message template: 'The shape inference of {op_name} type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.'

		Message template: 'The shape inference of {op_name} type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.'

	def format_message(self, op_name) -> str:  # type: ignore[override]
		return self.message_default_template.format(op_name=op_name)

	def format(  # type: ignore[override]
		self, level: infra.Level, op_name
	) -> Tuple[infra.Rule, infra.Level, str]:
		return self, level, self.format_message(op_name=op_name)


class _MissingStandardSymbolicFunction(infra.Rule):

		Message template: "Exporting the operator '{op_name}' to ONNX opset version {opset_version} is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: {issue_url}."

		Message template: "Exporting the operator '{op_name}' to ONNX opset version {opset_version} is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: {issue_url}."

	def format_message(  # type: ignore[override]
		self, op_name, opset_version, supported_opset_version
	) -> str:
		return self.message_default_template.format(
			op_name=op_name,
			opset_version=opset_version,
			supported_opset_version=supported_opset_version,
		)

	def format(  # type: ignore[override]
		self, level: infra.Level, op_name, opset_version, supported_opset_version
	) -> Tuple[infra.Rule, infra.Level, str]:
		return (
			self,
			level,
			self.format_message(
				op_name=op_name,
				opset_version=opset_version,
				supported_opset_version=supported_opset_version,
			),
		)


class _FxGraphToOnnx(infra.Rule):

		Message template: 'Transforming FX graph {graph_name} to ONNX graph.'

		Message template: 'Transforming FX graph {graph_name} to ONNX graph.'

	def format_message(self, node_repr) -> str:  # type: ignore[override]
		return self.message_default_template.format(node_repr=node_repr)

	def format(  # type: ignore[override]
		self, level: infra.Level, node_repr
	) -> Tuple[infra.Rule, infra.Level, str]:
		return self, level, self.format_message(node_repr=node_repr)


class _FxPass(infra.Rule):

		Message template: 'Running {pass_name} pass.'

		Message template: 'Running {pass_name} pass.'

	def format_message(self, target) -> str:  # type: ignore[override]
		return self.message_default_template.format(target=target)

	def format(  # type: ignore[override]
		self, level: infra.Level, target
	) -> Tuple[infra.Rule, infra.Level, str]:
		return self, level, self.format_message(target=target)


class _UnsupportedFxNodeAnalysis(infra.Rule):

		Message template: 'Unsupported FX nodes: {node_op_to_target_mapping}. '

		Message template: 'Unsupported FX nodes: {node_op_to_target_mapping}. '

	def format_message(self, node, symbolic_fn) -> str:  # type: ignore[override]
		return self.message_default_template.format(node=node, symbolic_fn=symbolic_fn)

	def format(  # type: ignore[override]
		self, level: infra.Level, node, symbolic_fn
	) -> Tuple[infra.Rule, infra.Level, str]:
		return self, level, self.format_message(node=node, symbolic_fn=symbolic_fn)


class _FindOpschemaMatchedSymbolicFunction(infra.Rule):

		Message template: 'The OnnxFunction: {symbolic_fn} is the nearest match of the node {node}.'

		Message template: 'The OnnxFunction: {symbolic_fn} is the nearest match of the node {node}.'

	def format_message(self, target) -> str:  # type: ignore[override]
		return self.message_default_template.format(target=target)

	def format(  # type: ignore[override]
		self, level: infra.Level, target
	) -> Tuple[infra.Rule, infra.Level, str]:
		return self, level, self.format_message(target=target)


class _FindOperatorOverloadsInOnnxRegistry(infra.Rule):

		Message template: 'Checking if the FX node: {node} is supported in onnx registry.'

		Message template: 'Checking if the FX node: {node} is supported in onnx registry.'

	missing_custom_symbolic_function: _MissingCustomSymbolicFunction = dataclasses.field(
		default=_MissingCustomSymbolicFunction.from_sarif(
			**{
				"id": "POE0002",
				"name": "missing-custom-symbolic-function",
				"short_description": {
					"text": "Missing symbolic function for custom PyTorch operator, cannot translate node to ONNX."
				},
				"full_description": {
					"text": "Missing symbolic function for custom PyTorch operator, cannot translate node to ONNX.",
					"markdown": "Missing symbolic function for custom PyTorch operator, cannot translate node to ONNX.\n",
				},
				"message_strings": {
					"default": {
						"text": "ONNX export failed on an operator with unrecognized namespace {op_name}. If you are trying to export a custom operator, make sure you registered it with the right domain and version."
					}
				},
				"help_uri": None,
				"properties": {"deprecated": False, "tags": []},
			}
		),
		init=False,
	)

	operator_supported_in_newer_opset_version: _OperatorSupportedInNewerOpsetVersion = dataclasses.field(
		default=_OperatorSupportedInNewerOpsetVersion.from_sarif(
			**{
				"id": "POE0004",
				"name": "operator-supported-in-newer-opset-version",
				"short_description": {
					"text": "Operator is supported in newer opset version."
				},
				"full_description": {
					"text": "Operator is supported in newer opset version.",
					"markdown": "Operator is supported in newer opset version.\n\nExample:\n```python\ntorch.onnx.export(model, args, ..., opset_version=9)\n```\n",
				},
				"message_strings": {
					"default": {
						"text": "Exporting the operator '{op_name}' to ONNX opset version {opset_version} is not supported. Support for this operator was added in version {supported_opset_version}, try exporting with this version."
					}
				},
				"help_uri": None,
				"properties": {"deprecated": False, "tags": []},
			}
		),
		init=False,
	)

	fx_node_to_onnx: _FxNodeToOnnx = dataclasses.field(
		default=_FxNodeToOnnx.from_sarif(
			**{
				"id": "FXE0008",
				"name": "fx-node-to-onnx",
				"short_description": {"text": "Transforms an FX node to an ONNX node."},
				"full_description": {
					"text": "Transforms an FX node to an ONNX node.",
					"markdown": "This diagnostic tracks the transformation process from an FX Node to ONNX [Operators](https://onnx.ai/onnx/operators/).\n\nThe process of converting FX Node to ONNX Node involves dealing with six distinct node types:\n  1. `placeholder`: Represents a module input, maps to an ONNX graph input.\n  2. `call_module`: Symbolizes a call to a submodule, maps to an ONNX\n  3. `call_method`: Symbolizes a method call. Not yet implemented.\n  4. `call_function`: Symbolizes a function call. [Core ATen](https://pytorch.org/docs/stable/ir.html#core-aten-ir) is expected\n	as the function call target. The mapping from ATen to ONNX is implemented by [ONNXScript torchlib](https://github.com/microsoft/onnxscript/tree/main/onnxscript/function_libs/torch_lib/ops).\n	This [guide](https://pytorch.org/docs/stable/onnx.html#onnx-script-functions) shows how to write and register a custom symbolic function for call_function FX node.\n  5. `get_attr`: Indicates an attribute access within the current module. Maps to an ONNX graph initializer.\n  6. `output`: Represents the module's output. Maps to an ONNX graph output.\n\nFor a granular understanding of how each node type is transformed, refer to the implementation details in `FxOnnxInterpreter`.\n",
				},
				"message_strings": {
					"default": {
						"text": "Transforming FX node {node_repr} to ONNX node."
					}
				},
				"help_uri": None,
				"properties": {"deprecated": False, "tags": []},
			}
		),
		init=False,
	)

	no_symbolic_function_for_call_function: _NoSymbolicFunctionForCallFunction = dataclasses.field(
		default=_NoSymbolicFunctionForCallFunction.from_sarif(
			**{
				"id": "FXE0011",
				"name": "no-symbolic-function-for-call-function",
				"short_description": {
					"text": 'Cannot find symbolic function to convert the "call_function" FX node to ONNX.'
				},
				"full_description": {
					"text": 'Cannot find symbolic function to convert the "call_function" FX node to ONNX. ',
					"markdown": 'This error occurs when the ONNX converter is unable to find a corresponding symbolic function\nto convert a "call_function" node in the input graph to its equivalence in ONNX. The "call_function"\nnode represents a normalized function call in PyTorch, such as "torch.aten.ops.add".\n\nTo resolve this error, you can try one of the following:\n\n- If exists, apply the auto-fix suggested by the diagnostic. TODO: this part is not available yet.\n- Rewrite the model using only supported PyTorch operators or functions.\n- Follow this [guide](https://pytorch.org/docs/stable/onnx.html#onnx-script-functions) to write and\n  register a custom symbolic function for the unsupported call_function FX node.\n\nTODO: Replace above link once docs for `dynamo_export` custom op registration are available.\n',
				},
				"message_strings": {
					"default": {
						"text": 'No symbolic function to convert the "call_function" node {target} to ONNX. '
					}
				},
				"help_uri": None,
				"properties": {"deprecated": False, "tags": []},
			}
		),
		init=False,
	)

	op_level_debugging: _OpLevelDebugging = dataclasses.field(
		default=_OpLevelDebugging.from_sarif(
			**{
				"id": "FXE0013",
				"name": "op-level-debugging",
				"short_description": {
					"text": "Report any op level validation failure in warnings."
				},
				"full_description": {
					"text": "Report any op level validation failure in warnings.",
					"markdown": "This warning message indicates that during op level debugging, certain symbolic functions\nhave failed to match the results of torch ops when using real tensors generated from fake\ntensors. It is important to note that the symbolic functions may not necessarily be\nincorrect, as the validation process is non-deterministic and should only be used as a\nreference.\n\nThere are two categories of warnings that can be triggered:\n\n1. Non-validated operators:\n  If the warnings are caused by the following errors, they can be disregarded by users,\n  as these errors occur due to the non-deterministic nature of the validation. However,\n  it is important to be aware that the operators have not been validated.\n\n  - IndexError: Unsupported input arguments of randomized dimensions/indices(INT64).\n  - RuntimeError: Unsupported input arguments for torch ops are generated.\n  - ValueError: Arguments/keyword arguments do not match the signature of the symbolic function.\n\n2. Potentially wrong torchlib operators:\n  If the warnings are triggered by the following error, users should be aware that the symbolic functions\n  may be incorrect in dispatching or implementation. In such cases, it is recommended to report\n  the issue to the PyTorch-ONNX team, or create/register a custom symbolic function to replace the default one.\n\n  - AssertionError: The symbolic function is potentially wrong as the results do not match the results of torch ops.\n  - TypeError: The symbolic function is potentially wrong as the opschema doesn't match inputs.\n",
				},
				"message_strings": {
					"default": {
						"text": "FX node: {node} and its onnx function: {symbolic_fn} fails on op level validation."
					}
				},
				"help_uri": None,
				"properties": {"deprecated": False, "tags": []},
			}
		),
		init=False,
	)

	fx_node_insert_type_promotion: _FxNodeInsertTypePromotion = dataclasses.field(
		default=_FxNodeInsertTypePromotion.from_sarif(
			**{
				"id": "FXE0015",
				"name": "fx-node-insert-type-promotion",
				"short_description": {
					"text": "Determine if type promotion is required for the FX node. Insert cast nodes if needed."
				},
				"full_description": {
					"text": "Determine if type promotion is required for the FX node. Insert cast nodes if needed.",
					"markdown": "This diagnostic monitors the node-level type promotion insertion process. In PyTorch, there is an automatic process called implicit type promotion,\nwhere the input types of an operator are promoted to a common type. The determination of the common type is based on the type promotion rule specific to each operator.\nTo learn more about PyTorch's type promotion rules, refer to the [elementwise_dtypes doc](https://github.com/pytorch/pytorch/blob/f044613f78df713fb57f70c608483c9f10ad332e/torch/_prims_common/__init__.py#L1252-L1335)\nand [torch._refs ops](https://github.com/pytorch/pytorch/blob/a475ea4542dfe961c9d097e33ab5041f61c8c17f/torch/_refs/__init__.py#L484).\n\nHowever, implicit type promotion is not supported in ONNX. Therefore, to replicate the PyTorch behavior, we need to explicitly insert cast nodes.\nThis diagnostic tracks the process of node-level type promotion insertion.\n\nThe type promotion rules used by this process can be found in `torch/onnx/_internal/fx/passes/type_promotion.py.`\nTo update or add new type promotion rules, please refer to the [Note: Update type promotion rule] section.\n",
				},
				"message_strings": {
					"default": {
						"text": "Performing explicit type promotion for node {target}. "
					}
				},
				"help_uri": None,
				"properties": {"deprecated": False, "tags": []},
			}
		),
		init=False,
	)


rules = _POERules()

<END>

<START>
import dataclasses
import sys
import types
from typing import Any, Callable, Dict, List, NamedTuple, Optional, Protocol, Union

from typing_extensions import TypeAlias


if sys.version_info >= (3, 11):
	from torch._C._dynamo import eval_frame

	DynamoFrameType: TypeAlias = eval_frame._PyInterpreterFrame
else:
	DynamoFrameType: TypeAlias = types.FrameType

import torch

CacheEntry = torch._C._dynamo.eval_frame._CacheEntry

FrameState = Dict[Any, Any]


class GuardFail(NamedTuple):
	reason: str
	orig_code: types.CodeType


class GuardFn(Protocol):
	closure_vars: Dict[str, object]
	args: List[str]
	code_parts: List[str]
	verbose_code_parts: List[str]
	global_scope: Dict[str, object]
	guard_fail_fn: Optional[Callable[[GuardFail], None]]

	def __call__(self, f_locals: Dict[str, object]) -> bool:
		...


@dataclasses.dataclass
class GuardedCode:
	code: types.CodeType
	check_fn: GuardFn


class DynamoCallbackFn(Protocol):
	def __call__(
		self,
		frame: DynamoFrameType,
		cache_entry: Optional[CacheEntry],
		frame_state: FrameState,
	) -> Optional[GuardedCode]:
		...


DynamoCallback = Union[DynamoCallbackFn, None, bool]


class DynamoGuardHook(Protocol):
	def __call__(
		self,
		guard_fn: GuardFn,
		code: types.CodeType,
		f_locals: Dict[str, object],
		index: int,
		last: bool,
	) -> None:
		...


class ProfilerStartHook(Protocol):
	def __call__(
		self,
		name: str,
	) -> Any:
		...


class ProfilerEndHook(Protocol):
	def __call__(self, record: Any) -> None:
		...


class BytecodeHook(Protocol):
	def __call__(
		self, code: types.CodeType, new_code: types.CodeType
	) -> Optional[types.CodeType]:
		...

<END>

<START>
import torch
from torch.fx import GraphModule, map_arg
from torch.fx.graph import Graph, Node
from torch.ao.quantization.fx.utils import get_new_attr_name_with_prefix

from .utils import (
	get_node_first_input_and_output_type,
	getattr_from_fqn,
	NodeInputOrOutputType,
	return_first_non_observer_node,
	get_number_of_non_param_args,
	get_target_type_str,
	get_arg_indices_of_inputs_to_log,
	get_node_input_qparams,
	op_type_supports_shadowing,
	get_normalized_nth_input,
)

from .ns_types import (
	NSSingleResultValuesType,
	NSSubgraph,
	NSNodeTargetType,
)
from torch.ao.ns.fx.mappings import (
	get_node_type_to_io_type_map,
)
from torch.ao.quantization.observer import _is_activation_post_process

from typing import Dict, Tuple, Callable, List, Any, Union, Optional, Set

def _maybe_get_fqn(node: Node, gm: GraphModule) -> Optional[str]:
	fqn = None
	if hasattr(gm, '_node_name_to_scope'):
		node_to_use_for_fqn = node
		if node.op == 'call_module':
			assert isinstance(node.target, str)
			module = getattr_from_fqn(gm, node.target)
			if _is_activation_post_process(module):
				node_to_use_for_fqn = get_normalized_nth_input(node, gm, 0)
		fqn = gm._node_name_to_scope[node_to_use_for_fqn.name][0]  # type: ignore[index]
	return fqn  # type: ignore[return-value]

def _insert_logger_after_node(
	node: Node,
	gm: GraphModule,
	logger_cls: Callable,
	logger_node_name_suffix: str,
	ref_node_name: str,
	model_name: str,
	ref_name: str,
	ref_node_target_type: str,
	results_type: str,
	index_within_arg: int,
	index_of_arg: int,
	fqn: Optional[str],
) -> Node:
	logger_node_name = \
		get_new_attr_name_with_prefix(node.name + logger_node_name_suffix)(gm)
	target_type = get_target_type_str(node, gm)
	logger_obj = logger_cls(
		ref_node_name, node.name, model_name, ref_name, target_type,
		ref_node_target_type,
		results_type, index_within_arg, index_of_arg, fqn)
	setattr(gm, logger_node_name, logger_obj)
	logger_node = node.graph.create_node(
		'call_module', logger_node_name, (node,), {})
	return logger_node

def add_loggers_to_model(
	gm: GraphModule,
	node_to_instrument_inputs_to_ref_node_name: Dict[Node, Tuple[str, str]],
	node_to_instrument_outputs_to_ref_node_name: Dict[Node, Tuple[str, str]],
	logger_cls: Callable,
	model_name: str,
) -> GraphModule:

	new_graph = Graph()
	env: Dict[str, Any] = {}
	modules = dict(gm.named_modules())

	def load_arg(a):
		return map_arg(a, lambda node: env[node.name])

	for node in gm.graph.nodes:
		if node.op == 'output':
			new_graph.output(map_arg(get_normalized_nth_input(node, gm, 0), load_arg))
			continue

		if (
			(node in node_to_instrument_inputs_to_ref_node_name) or
			(node in node_to_instrument_outputs_to_ref_node_name)
		):
			fqn = _maybe_get_fqn(node, gm)

			if node in node_to_instrument_inputs_to_ref_node_name:
				ref_name, ref_node_type = node_to_instrument_inputs_to_ref_node_name[node]
				arg_indices_to_log = get_arg_indices_of_inputs_to_log(node)
				for node_arg_idx in arg_indices_to_log:
					node_arg = get_normalized_nth_input(node, gm, node_arg_idx)
					if type(node_arg) == Node:
						prev_node = env[node_arg.name]
						env[node_arg.name] = _insert_logger_after_node(
							prev_node, gm, logger_cls, '_ns_logger_', node.name,
							model_name, ref_name, ref_node_type,
							NSSingleResultValuesType.NODE_INPUT.value,
							index_within_arg=0, index_of_arg=node_arg_idx,
							fqn=fqn)
					elif type(node_arg) == torch.fx.immutable_collections.immutable_list:
						for arg_idx, arg in enumerate(node_arg):  # type: ignore[var-annotated, arg-type]
							prev_node = env[arg.name]
							env[prev_node.name] = _insert_logger_after_node(
								prev_node, gm, logger_cls, '_ns_logger_', node.name,
								model_name, ref_name, ref_node_type,
								NSSingleResultValuesType.NODE_INPUT.value,
								index_within_arg=arg_idx, index_of_arg=node_arg_idx,
								fqn=fqn)
					else:
						pass

			env[node.name] = new_graph.node_copy(node, load_arg)

			if node in node_to_instrument_outputs_to_ref_node_name:
				ref_name, ref_node_type = node_to_instrument_outputs_to_ref_node_name[node]
				env[node.name] = _insert_logger_after_node(
					env[node.name], gm, logger_cls, '_ns_logger_', node.name,
					model_name, ref_name, ref_node_type,
					NSSingleResultValuesType.NODE_OUTPUT.value,
					index_within_arg=0, index_of_arg=0, fqn=fqn)

		else:
			env[node.name] = new_graph.node_copy(node, load_arg)

	new_gm = GraphModule(gm, new_graph)
	return new_gm

def _insert_quantize_per_tensor_node(
	prev_node_c: Node,
	node_a: Node,
	gm_b: GraphModule,
	graph_c: Graph,
	scale: Union[torch.Tensor, float],
	zero_point: Union[torch.Tensor, int],
	dtype_cast_name: str,
) -> Node:
	scale_node_name = \
		get_new_attr_name_with_prefix(
			node_a.name + '_input_scale_')(gm_b)
	setattr(gm_b, scale_node_name, scale)
	scale_node = graph_c.create_node(
		'get_attr', scale_node_name, (), {}, scale_node_name)
	zero_point_node_name = \
		get_new_attr_name_with_prefix(
			node_a.name + '_input_zero_point_')(gm_b)
	setattr(gm_b, zero_point_node_name, zero_point)
	zero_point_node = graph_c.create_node(
		'get_attr', zero_point_node_name, (), {}, zero_point_node_name)
	return graph_c.create_node(
		'call_function', torch.quantize_per_tensor,
		(prev_node_c, scale_node, zero_point_node, torch.quint8), {},
		dtype_cast_name)

def _insert_dtype_cast_after_node(
	node_a: Node,
	node_c: Node,
	prev_node_c: Union[Node, List[Node]],
	gm_a: GraphModule,
	gm_b: GraphModule,
	graph_c: Graph,
	node_name_prefix: str,
	logger_cls: Callable,
	node_type_to_io_type_map: Dict[str, Set[NSNodeTargetType]],
) -> Union[Node, List[Node]]:
	dtype_cast_op = None
	dtype_cast_mod_cls = None
	dtype_cast_method = None
	dtype_cast_method_dtype = None
	dtype_cast_scale = None
	dtype_cast_zero_point = None
	node_input_type_a, _node_output_type_a = \
		get_node_first_input_and_output_type(
			node_a, gm_a, logger_cls, node_type_to_io_type_map)
	node_input_type_c, _node_output_type_c = \
		get_node_first_input_and_output_type(
			node_c, gm_b, logger_cls, node_type_to_io_type_map)

	if (
		(node_input_type_a == NodeInputOrOutputType.FP32 and
		 node_input_type_c == NodeInputOrOutputType.INT8) or
		(node_input_type_a == NodeInputOrOutputType.FP32 and
		 node_input_type_c == NodeInputOrOutputType.FP16) or
		(node_input_type_a == NodeInputOrOutputType.FP32 and
		 node_input_type_c == NodeInputOrOutputType.FP32_OR_INT8)
	):
		dtype_cast_op = torch.dequantize
	elif (
		node_input_type_a == node_input_type_c and
		node_input_type_a != NodeInputOrOutputType.UNKNOWN
	):
		dtype_cast_mod_cls = torch.nn.Identity
	elif (
		node_input_type_a == NodeInputOrOutputType.INT8 and
		node_input_type_c == NodeInputOrOutputType.FP32
	):
		node_a_input_qparams = get_node_input_qparams(
			node_a, gm_a, node_type_to_io_type_map)
		if node_a_input_qparams is not None:
			dtype_cast_op = torch.quantize_per_tensor  # type: ignore[assignment]
			dtype_cast_scale, dtype_cast_zero_point = node_a_input_qparams
	elif (
		node_input_type_a == NodeInputOrOutputType.FP16 and
		node_input_type_c == NodeInputOrOutputType.FP32
	):
		dtype_cast_method = 'to'
		dtype_cast_method_dtype = torch.float16
	else:
		raise AssertionError(
			f"dtype cast from {node_input_type_c} {node_c.format_node()} to " +
			f"{node_input_type_a} {node_a.format_node()} needs to be implemented")

	if isinstance(prev_node_c, Node):
		new_dtype_cast_name = \
			get_new_attr_name_with_prefix(node_name_prefix)(gm_b)
		if dtype_cast_op:
			if dtype_cast_scale is not None and dtype_cast_zero_point is not None:
				return _insert_quantize_per_tensor_node(
					prev_node_c, node_a, gm_b, graph_c, dtype_cast_scale,
					dtype_cast_zero_point, new_dtype_cast_name)
			else:
				return graph_c.create_node(
					'call_function', dtype_cast_op, (prev_node_c,), {},
					new_dtype_cast_name)
		elif dtype_cast_method:
			return graph_c.create_node(
				'call_method', dtype_cast_method,
				(prev_node_c, dtype_cast_method_dtype), {}, new_dtype_cast_name)
		else:
			assert dtype_cast_mod_cls
			dtype_cast_mod = dtype_cast_mod_cls()
			setattr(gm_b, new_dtype_cast_name, dtype_cast_mod)
			return graph_c.create_node(
				'call_module', new_dtype_cast_name, (prev_node_c,), {},
				new_dtype_cast_name)
	elif isinstance(prev_node_c, list):
		results = []
		for prev_node_c_inner in prev_node_c:
			new_dtype_cast_name = \
				get_new_attr_name_with_prefix(node_name_prefix)(gm_b)
			if dtype_cast_op:
				new_dtype_cast_node = graph_c.create_node(
					'call_function', dtype_cast_op, (prev_node_c_inner,), {},
					new_dtype_cast_name)
				results.append(new_dtype_cast_node)
			else:
				assert dtype_cast_mod_cls
				dtype_cast_mod = dtype_cast_mod_cls()
				setattr(gm_b, new_dtype_cast_name, dtype_cast_mod)
				new_dtype_cast_node = graph_c.create_node(
					'call_module', new_dtype_cast_name, (prev_node_c_inner,), {},
					new_dtype_cast_name)
				results.append(new_dtype_cast_node)
		return results
	else:
		raise AssertionError(f"type f{type(prev_node_c)} is not handled")

def _copy_node_from_a_to_c(
	node_a: Node,
	gm_a: GraphModule,
	gm_b: GraphModule,
	graph_c: Graph,
) -> Node:
	if node_a.op == 'get_attr':
		node_a_copy_name = \
			get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
		node_a_obj = getattr_from_fqn(gm_a, node_a.target)  # type: ignore[arg-type]
		if torch.is_tensor(node_a_obj):
			node_a_obj = node_a_obj.detach()
		setattr(gm_b, node_a_copy_name, node_a_obj)
		node_a_copy = graph_c.create_node(
			node_a.op, node_a_copy_name, (), {}, node_a_copy_name)
		return node_a_copy
	elif node_a.op == 'call_method':
		assert node_a.target in ('dequantize', 'to'), \
			f"target {node_a.target} is not implemented"
		if node_a.target == 'dequantize':
			arg_copy = _copy_node_from_a_to_c(
				get_normalized_nth_input(node_a, gm_a, 0),
				gm_a, gm_b, graph_c)  # type: ignore[arg-type]
			node_a_copy_name = \
				get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
			node_a_copy = graph_c.create_node(
				node_a.op, node_a.target, (arg_copy,), {}, node_a_copy_name)
			return node_a_copy
		else:  # to
			arg_copy = _copy_node_from_a_to_c(
				get_normalized_nth_input(node_a, gm_a, 0), gm_a, gm_b, graph_c)  # type: ignore[arg-type]
			node_a_copy_name = \
				get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
			node_a_copy = graph_c.create_node(
				node_a.op, node_a.target,
				(arg_copy, get_normalized_nth_input(node_a, gm_a, 1)),
				{}, node_a_copy_name)
			return node_a_copy

	else:
		raise AssertionError(
			f"handling of node {node_a.format_node()} with op {node_a.op} is not implemented")

def _can_insert_copy_of_subgraph_a(
	subgraph_a: NSSubgraph,
	gm_a: GraphModule,
	num_non_param_args_node_a: int,
) -> bool:
	nodes = []
	cur_node = subgraph_a.end_node
	while cur_node != subgraph_a.start_node:
		nodes.append(cur_node)
		cur_node = get_normalized_nth_input(cur_node, gm_a, 0)  # type: ignore[assignment]
	nodes.append(cur_node)
	nodes.reverse()

	def _can_insert(node_a_arg, gm_a):
		if isinstance(node_a_arg, Node):
			arg_a = return_first_non_observer_node(node_a_arg, gm_a)
			if arg_a.op == 'call_method':
				return arg_a.target in ('dequantize', 'to')
			elif arg_a.op == 'get_attr':
				return True
			else:
				return False
		elif isinstance(node_a_arg, (list, tuple)):
			for el in node_a_arg:
				if not isinstance(el, Node):
					return False
		return True

	for node_a in nodes:

		local_num_non_param_args_node_a = num_non_param_args_node_a \
			if node_a is nodes[0] else 1

		norm_args_kwargs = node_a.normalized_arguments(
			gm_a, normalize_to_only_use_kwargs=True)
		if norm_args_kwargs is not None:
			norm_args, norm_kwargs = norm_args_kwargs
		else:
			norm_args, norm_kwargs = node_a.args, node_a.kwargs

		cur_idx = 0

		while cur_idx < len(norm_args):
			if cur_idx == 0:
				pass
			elif cur_idx == 1 and local_num_non_param_args_node_a == 2:
				pass
			else:
				if not _can_insert(norm_args[cur_idx], gm_a):
					return False
			cur_idx += 1

		for kwarg_val in norm_kwargs.values():
			if cur_idx == 0:
				pass
			elif cur_idx == 1 and local_num_non_param_args_node_a == 2:
				pass
			else:
				if not _can_insert(kwarg_val, gm_a):
					return False
			cur_idx += 1

	return True

def _insert_copy_of_subgraph_a_after_input_node_c(
	input_node_c: Union[Node, List[Node]],
	input_node_c_2: Optional[Union[Node, List[Node]]],
	subgraph_a: NSSubgraph,
	gm_a: GraphModule,
	gm_b: GraphModule,
	node_name_prefix: str,
) -> Node:
	if isinstance(input_node_c, Node):
		graph_c = input_node_c.graph
	else:
		assert isinstance(input_node_c, list)
		graph_c = input_node_c[0].graph

	nodes_of_a = [subgraph_a.end_node]
	cur_node = subgraph_a.end_node
	while cur_node != subgraph_a.start_node:
		cur_node = get_normalized_nth_input(cur_node, gm_a, 0)  # type: ignore[assignment]
		nodes_of_a.insert(0, cur_node)

	cur_node_a = nodes_of_a[0]
	cur_node_c = _insert_copy_of_node_a_after_input_node_c(
		input_node_c,
		input_node_c_2,
		cur_node_a,
		gm_a,
		gm_b,
		node_name_prefix)
	for cur_idx_a in range(1, len(nodes_of_a)):
		cur_node_a = nodes_of_a[cur_idx_a]
		prev_node_c = cur_node_c  # previous added node is the input to next node
		cur_node_c = _insert_copy_of_node_a_after_input_node_c(
			prev_node_c,
			None,
			cur_node_a,
			gm_a,
			gm_b,
			node_name_prefix)
	return cur_node_c


def _insert_copy_of_node_a_after_input_node_c(
	input_node_c: Union[Node, List[Node]],
	input_node_c_2: Optional[Union[Node, List[Node]]],
	node_a: Node,
	gm_a: GraphModule,
	gm_b: GraphModule,
	node_name_prefix: str,
) -> Node:
	if isinstance(input_node_c, Node):
		graph_c = input_node_c.graph
	else:
		assert isinstance(input_node_c, list)
		graph_c = input_node_c[0].graph

	norm_args_kwargs = node_a.normalized_arguments(
		gm_a, normalize_to_only_use_kwargs=True)
	if norm_args_kwargs is not None:
		norm_args, norm_kwargs = norm_args_kwargs
	else:
		norm_args, norm_kwargs = node_a.args, node_a.kwargs

	new_args = []
	new_kwargs = {}

	def _copy_arg(arg):
		if isinstance(arg, Node):
			arg = return_first_non_observer_node(arg, gm_a)
			arg = _copy_node_from_a_to_c(arg, gm_a, gm_b, graph_c)
			return arg
		elif isinstance(arg, (int, float, torch.dtype)):
			return arg
		elif isinstance(kwarg_val, (list, tuple)):
			for el in kwarg_val:
				assert not isinstance(el, Node), \
					"handling of Node inside list is not implemented"
			return arg
		else:
			raise AssertionError(
				f"handling for kwarg of type {type(kwarg_val)} is not implemented")

	cur_idx = 0

	while cur_idx < len(norm_args):
		if cur_idx == 0:
			new_arg = input_node_c
		elif cur_idx == 1 and input_node_c_2 is not None:
			new_arg = input_node_c_2
		else:
			new_arg = _copy_arg(norm_args[cur_idx])
		new_args.append(new_arg)
		cur_idx += 1

	for kwarg_name, kwarg_val in norm_kwargs.items():
		if cur_idx == 0:
			new_kwargs[kwarg_name] = input_node_c
		elif cur_idx == 1 and input_node_c_2 is not None:
			new_kwargs[kwarg_name] = input_node_c_2
		else:
			new_kwargs[kwarg_name] = _copy_arg(kwarg_val)
		cur_idx += 1

	new_args = tuple(new_args)  # type: ignore[assignment]

	node_a_shadows_c_name = \
		get_new_attr_name_with_prefix(node_name_prefix)(gm_b)

	if node_a.op == 'call_module':
		new_mod_copy_name = \
			get_new_attr_name_with_prefix(node_name_prefix)(gm_b)
		assert isinstance(node_a.target, str)
		mod_a = getattr_from_fqn(gm_a, node_a.target)
		setattr(gm_b, new_mod_copy_name, mod_a)
		node_a_shadows_c = graph_c.create_node(
			node_a.op, new_mod_copy_name, new_args,
			new_kwargs, node_a_shadows_c_name)
		return node_a_shadows_c
	else:
		assert node_a.op in ('call_function', 'call_method')
		node_a_shadows_c = graph_c.create_node(
			node_a.op, node_a.target, new_args,
			new_kwargs, node_a_shadows_c_name)
		return node_a_shadows_c

def create_a_shadows_b(
	name_a: str,
	gm_a: GraphModule,
	name_b: str,
	gm_b: GraphModule,
	matched_subgraph_pairs: Dict[str, Tuple[NSSubgraph, NSSubgraph]],
	logger_cls: Callable,
	should_log_inputs: bool,
	node_type_to_io_type_map: Optional[Dict[str, Set[NSNodeTargetType]]] = None,
) -> GraphModule:

	if node_type_to_io_type_map is None:
		node_type_to_io_type_map = get_node_type_to_io_type_map()

	graph_c = Graph()
	env_c: Dict[str, Any] = {}
	modules = dict(gm_b.named_modules())

	def load_arg(a):
		return map_arg(a, lambda node: env_c[node.name])

	start_node_b_to_matched_subgraph_a_and_name = {}
	end_node_b_to_matched_subgraph_a_and_name = {}
	for match_name, match in matched_subgraph_pairs.items():
		subgraph_a, subgraph_b = match
		ref_node_type_a = get_target_type_str(subgraph_a.base_op_node, gm_a)
		ref_node_type_b = get_target_type_str(subgraph_b.base_op_node, gm_b)
		start_node_b_to_matched_subgraph_a_and_name[subgraph_b.start_node] = \
			(subgraph_a, match_name, ref_node_type_a, ref_node_type_b)
		end_node_b_to_matched_subgraph_a_and_name[subgraph_b.end_node] = \
			(subgraph_a, match_name, ref_node_type_a, ref_node_type_b)

	for node_b in gm_b.graph.nodes:
		if node_b.op == 'output':
			graph_c.output(map_arg(node_b.args[0], load_arg))
			continue

		node_b_is_start_node = node_b in start_node_b_to_matched_subgraph_a_and_name
		node_b_is_end_node = node_b in end_node_b_to_matched_subgraph_a_and_name

		if (node_b_is_start_node or node_b_is_end_node):

			if node_b_is_start_node:
				subgraph_a, ref_name, ref_node_type_a, ref_node_type_b = \
					start_node_b_to_matched_subgraph_a_and_name[node_b]
			else:
				assert node_b_is_end_node
				subgraph_a, ref_name, ref_node_type_a, ref_node_type_b = \
					end_node_b_to_matched_subgraph_a_and_name[node_b]

			all_op_types_support_shadowing = (
				op_type_supports_shadowing(subgraph_a.start_node) and
				op_type_supports_shadowing(node_b)
			)
			if not all_op_types_support_shadowing:
				print(
					f'skipping shadow loggers for node_b: {get_target_type_str(node_b, gm_b)}' +
					f', start_node_a: {get_target_type_str(subgraph_a.start_node, gm_a)}' +
					', unsupported')
				env_c[node_b.name] = graph_c.node_copy(node_b, load_arg)
				continue

			node_input_type_a, node_output_type_a = \
				get_node_first_input_and_output_type(
					subgraph_a.start_node, gm_a, logger_cls,
					node_type_to_io_type_map)
			node_input_type_b, node_output_type_b = \
				get_node_first_input_and_output_type(
					node_b, gm_b, logger_cls,
					node_type_to_io_type_map)
			node_io_types_known_a_and_b = (
				node_input_type_a != NodeInputOrOutputType.UNKNOWN and
				node_output_type_a != NodeInputOrOutputType.UNKNOWN and
				node_input_type_b != NodeInputOrOutputType.UNKNOWN and
				node_output_type_b != NodeInputOrOutputType.UNKNOWN
			)
			if not node_io_types_known_a_and_b:
				print(
					f'skipping shadow loggers for node_b: {get_target_type_str(node_b, gm_b)}' +
					f', start_node_a: {get_target_type_str(subgraph_a.start_node, gm_a)}' +
					', unknown dtype cast')
				env_c[node_b.name] = graph_c.node_copy(node_b, load_arg)
				continue

			if (
				node_input_type_a == NodeInputOrOutputType.INT8 and
				node_input_type_b == NodeInputOrOutputType.FP32
			):
				node_a_input_qparams = get_node_input_qparams(
					subgraph_a.start_node, gm_a, node_type_to_io_type_map)
				if not node_a_input_qparams:
					print(
						f'skipping shadow loggers for node_b: {get_target_type_str(node_b, gm_b)}' +
						f', start_node_a: {get_target_type_str(subgraph_a.start_node, gm_a)}' +
						', unknown input qparams')
					env_c[node_b.name] = graph_c.node_copy(node_b, load_arg)
					continue

			num_non_param_args_node_a = \
				get_number_of_non_param_args(subgraph_a.start_node, gm_a)
			if not _can_insert_copy_of_subgraph_a(subgraph_a, gm_a, num_non_param_args_node_a):
				print(
					f'skipping shadow loggers for node_b: {get_target_type_str(node_b, gm_b)}' +
					f', start_node_a: {get_target_type_str(subgraph_a.start_node, gm_a)}' +
					', unhandled logic in subgraph copy')
				env_c[node_b.name] = graph_c.node_copy(node_b, load_arg)
				continue

			fqn_base_a = _maybe_get_fqn(subgraph_a.base_op_node, gm_a)
			fqn_base_b = _maybe_get_fqn(subgraph_b.base_op_node, gm_b)

			if node_b_is_start_node:

				if should_log_inputs:
					prev_node_b = get_normalized_nth_input(node_b, gm_b, 0)
					if isinstance(prev_node_b, Node):
						prev_node_c = env_c[prev_node_b.name]
						env_c[prev_node_c.name] = _insert_logger_after_node(
							prev_node_c, gm_b, logger_cls, '_ns_logger_b_inp_',
							node_b.name, name_b, ref_name, ref_node_type_b,
							NSSingleResultValuesType.NODE_INPUT.value,
							index_within_arg=0, index_of_arg=0,
							fqn=fqn_base_b)
					elif isinstance(prev_node_b, list):
						prev_node_c_list = [env_c[arg.name] for arg in prev_node_b]

						for arg_idx, arg in enumerate(prev_node_b):
							prev_node_c = prev_node_c_list[arg_idx]
							env_c[prev_node_c.name] = _insert_logger_after_node(
								prev_node_c, gm_b, logger_cls, '_ns_logger_b_inp_',
								node_b.name, name_b, ref_name, ref_node_type_b,
								NSSingleResultValuesType.NODE_INPUT.value,
								index_within_arg=arg_idx, index_of_arg=0,
								fqn=fqn_base_b)
					else:
						raise AssertionError(f"type {type(prev_node_b)} is not handled yet")

			if node_b_is_start_node or node_b_is_end_node:
				env_c[node_b.name] = graph_c.node_copy(node_b, load_arg)
				node_c = env_c[node_b.name]


			if node_b_is_start_node:

				prev_node_c = get_normalized_nth_input(node_c, gm_b, 0)
				if should_log_inputs:
					if isinstance(prev_node_c, Node):
						prev_node_c = get_normalized_nth_input(node_c, gm_b, 0)
					elif isinstance(prev_node_c, list):
						prev_node_c = [get_normalized_nth_input(arg, gm_b, 0) for arg in prev_node_c]
				dtype_cast_node = _insert_dtype_cast_after_node(
					subgraph_a.start_node, node_c, prev_node_c, gm_a, gm_b, graph_c,
					node_b.name + '_dtype_cast_', logger_cls,
					node_type_to_io_type_map)

				if should_log_inputs:

<END>

<START>

import collections
from typing import Deque, List, Set, Tuple

import torch.nn as nn
from torch.distributed._composable.contract import _get_registry
from torch.distributed.fsdp._common_utils import _FSDPState, _get_module_fsdp_state




def _composable(module: nn.Module) -> bool:
	registry = _get_registry(module)
	if registry is None:
		return True
	return "replicate" not in registry


def _get_fsdp_states_with_modules(
	module: nn.Module,
) -> Tuple[List[_FSDPState], List[nn.Module]]:
	fsdp_states: List[_FSDPState] = []
	fsdp_modules: List[nn.Module] = []
	visited_fsdp_states: Set[_FSDPState] = set()
	visited_modules: Set[nn.Module] = set()

	deque: Deque[nn.Module] = collections.deque([module])
	while deque:
		submodule = deque.popleft()
		visited_modules.add(submodule)
		if not _composable(submodule):
			continue
		for child_module in reversed(list(submodule.children())):
			if child_module not in visited_modules:
				deque.appendleft(child_module)
		optional_state = _get_module_fsdp_state(submodule)
		if optional_state is not None and optional_state not in visited_fsdp_states:
			visited_fsdp_states.add(optional_state)
			fsdp_states.append(optional_state)
			fsdp_modules.append(submodule)
	return fsdp_states, fsdp_modules


def _get_fsdp_states(module: nn.Module) -> List[_FSDPState]:
	Returns all ``FlatParamHandle`` s in the module tree rooted at ``module``
	following the rules in :func:`_get_fsdp_state`.

<END>

<START>
import torch
from ._common_operator_config_utils import (
	_get_binary_op_configs,
	_get_bn_configs,
	_get_cat_config,
	_get_conv_configs,
	_get_default_op_configs,
	_get_embedding_op_configs,
	_get_fixed_qparams_op_configs,
	_get_linear_configs,
	_get_rnn_op_configs,
	_get_share_qparams_op_configs,
)
from .backend_config import BackendConfig, DTypeConfig, DTypeWithConstraints

__all__ = [
	"get_qnnpack_backend_config",
]


qnnpack_weighted_op_quint8_dtype_config = DTypeConfig(
	input_dtype=torch.quint8,
	output_dtype=torch.quint8,
	weight_dtype=torch.qint8,
	bias_dtype=torch.float,
)

qnnpack_default_op_quint8_dtype_config = DTypeConfig(
	input_dtype=torch.quint8,
	output_dtype=torch.quint8,
)

qnnpack_default_op_fp16_dtype_config = DTypeConfig(
	input_dtype=torch.float16,
	output_dtype=torch.float16,
	weight_dtype=torch.float16,
	bias_dtype=torch.float16,
)

qnnpack_default_dynamic_int8_dtype_config = DTypeConfig(
	input_dtype=torch.quint8,
	output_dtype=torch.float,
	weight_dtype=torch.qint8,
	bias_dtype=torch.float,
	is_dynamic=True,
)

qnnpack_default_dynamic_float16_dtype_config = DTypeConfig(
	input_dtype=torch.float16,
	output_dtype=torch.float,
	weight_dtype=torch.float16,
	bias_dtype=torch.float,
	is_dynamic=True,
)

qnnpack_weight_only_quint8_dtype_config = DTypeConfig(
	input_dtype=torch.float,
	output_dtype=torch.float,
	weight_dtype=torch.quint8,
)

qnnpack_weight_only_quint4x2_dtype_config = DTypeConfig(
	input_dtype=torch.float,
	output_dtype=torch.float,
	weight_dtype=torch.quint4x2,
)




qnnpack_act_qint8_scale_min_2_neg_12 = DTypeWithConstraints(
	dtype=torch.qint8,
	scale_min_lower_bound=2 ** -12,
)

qnnpack_weight_qint8_neg_127_to_127_scale_min_2_neg_12 = DTypeWithConstraints(
	dtype=torch.qint8,
	quant_min_lower_bound=-127,
	quant_max_upper_bound=127,
	scale_min_lower_bound=2 ** -12,
)

qnnpack_weighted_op_qint8_symmetric_dtype_config = DTypeConfig(
	input_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
	output_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
	weight_dtype=qnnpack_weight_qint8_neg_127_to_127_scale_min_2_neg_12,
	bias_dtype=torch.float,
)

qnnpack_default_op_qint8_symmetric_dtype_config = DTypeConfig(
	input_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
	output_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
)



def get_qnnpack_backend_config() -> BackendConfig:
	conv_dtype_configs = [
		qnnpack_weighted_op_qint8_symmetric_dtype_config,
		qnnpack_weighted_op_quint8_dtype_config,
	]
	linear_dtype_configs = [
		qnnpack_weighted_op_qint8_symmetric_dtype_config,
		qnnpack_weighted_op_quint8_dtype_config,
		qnnpack_default_dynamic_int8_dtype_config,
		qnnpack_default_dynamic_float16_dtype_config,
	]
	binary_op_dtype_configs = [
		qnnpack_default_op_qint8_symmetric_dtype_config,
		qnnpack_default_op_quint8_dtype_config,
	]
	default_op_dtype_configs = [
		qnnpack_default_op_qint8_symmetric_dtype_config,
		qnnpack_default_op_quint8_dtype_config,
	]
	fixed_qparams_op_dtype_configs = [
		qnnpack_default_op_qint8_symmetric_dtype_config,
		qnnpack_default_op_quint8_dtype_config,
	]
	share_qparams_op_dtype_configs = [
		qnnpack_default_op_qint8_symmetric_dtype_config,
		qnnpack_default_op_quint8_dtype_config,
	]
	rnn_op_dtype_configs = [
		qnnpack_default_dynamic_int8_dtype_config,
		qnnpack_default_dynamic_float16_dtype_config,
	]
	embedding_op_dtype_configs = [
		qnnpack_weight_only_quint8_dtype_config,
		qnnpack_weight_only_quint4x2_dtype_config,
	]
	return BackendConfig("qnnpack") \
		.set_backend_pattern_configs(_get_conv_configs(conv_dtype_configs)) \
		.set_backend_pattern_configs(_get_linear_configs(linear_dtype_configs)) \
		.set_backend_pattern_configs(_get_binary_op_configs(binary_op_dtype_configs)) \
		.set_backend_pattern_config(_get_cat_config(default_op_dtype_configs)) \
		.set_backend_pattern_configs(_get_default_op_configs(default_op_dtype_configs)) \
		.set_backend_pattern_configs(_get_fixed_qparams_op_configs(fixed_qparams_op_dtype_configs)) \
		.set_backend_pattern_configs(_get_share_qparams_op_configs(share_qparams_op_dtype_configs)) \
		.set_backend_pattern_configs(_get_bn_configs(default_op_dtype_configs)) \
		.set_backend_pattern_configs(_get_rnn_op_configs(rnn_op_dtype_configs)) \
		.set_backend_pattern_configs(_get_embedding_op_configs(embedding_op_dtype_configs))

<END>

<START>

import torch
import torch._inductor

aten = torch.ops.aten
prims = torch.ops.prims

from torch._inductor.pattern_matcher import (
   Arg,
   CallFunction,
   CallFunctionVarArgs,
   CallMethod,
   CallMethodVarArgs,
   CallModule,
   CallModuleVarArgs,
   ExclusiveKeywordArg,
   Ignored,
   KeywordArg,
   ListOf,
   MultiOutputPattern,
   PatternExpr,
   RepeatedExpr,
   _TargetArgsExpr,
   _TargetExpr,
   _TargetExprVarArgs,
)
rand_default = CallFunction(aten.rand.default, Ignored(), dtype=Ignored(), device=Ignored(), pin_memory=False)
gt_Scalar = CallFunction(aten.gt.Scalar, rand_default, KeywordArg('dropout_p'), _users=2)
expand_default = CallFunction(aten.expand.default, KeywordArg('query'), Ignored())
view_default = CallFunction(aten.view.default, expand_default, Ignored(), _users=2)
permute_default = CallFunction(aten.permute.default, KeywordArg('key'), Ignored())
expand_default_1 = CallFunction(aten.expand.default, permute_default, Ignored())
view_default_1 = CallFunction(aten.view.default, expand_default_1, Ignored(), _users=2)
bmm_default = CallFunction(aten.bmm.default, view_default, view_default_1)
view_default_2 = CallFunction(aten.view.default, bmm_default, Ignored())
div_Tensor = CallFunction(aten.div.Tensor, view_default_2, Ignored())
add_Tensor = CallFunction(aten.add.Tensor, div_Tensor, KeywordArg('attn_mask'), _users=2)
amax_default = CallFunction(aten.amax.default, add_Tensor, Ignored(), True)
sub_Tensor = CallFunction(aten.sub.Tensor, add_Tensor, amax_default)
exp_default = CallFunction(aten.exp.default, sub_Tensor, _users=2)
sum_dim_IntList = CallFunction(aten.sum.dim_IntList, exp_default, Ignored(), True)
div_Tensor_1 = CallFunction(aten.div.Tensor, exp_default, sum_dim_IntList, _users=2)
mul_Tensor = CallFunction(aten.mul.Tensor, gt_Scalar, div_Tensor_1)
mul_Tensor_1 = CallFunction(aten.mul.Tensor, mul_Tensor, Ignored())
expand_default_2 = CallFunction(aten.expand.default, mul_Tensor_1, Ignored())
view_default_3 = CallFunction(aten.view.default, expand_default_2, Ignored(), _users=2)
expand_default_3 = CallFunction(aten.expand.default, KeywordArg('value'), Ignored())
view_default_4 = CallFunction(aten.view.default, expand_default_3, Ignored(), _users=2)
bmm_default_1 = CallFunction(aten.bmm.default, view_default_3, view_default_4)
view_default_5 = CallFunction(aten.view.default, bmm_default_1, Ignored())
view_default_6 = CallFunction(aten.view.default, KeywordArg('tangents_1'), Ignored(), _users=2)
permute_default_1 = CallFunction(aten.permute.default, view_default_4, Ignored())
bmm_default_2 = CallFunction(aten.bmm.default, view_default_6, permute_default_1)
view_default_7 = CallFunction(aten.view.default, bmm_default_2, Ignored())
convert_element_type_default = CallFunction(prims.convert_element_type.default, gt_Scalar, Ignored())
mul_Tensor_2 = CallFunction(aten.mul.Tensor, convert_element_type_default, Ignored())
mul_Tensor_3 = CallFunction(aten.mul.Tensor, view_default_7, mul_Tensor_2)
clone_default = CallFunction(aten.clone.default, mul_Tensor_3, memory_format=torch.contiguous_format)
alias_default = CallFunction(aten.alias.default, div_Tensor_1)
alias_default_1 = CallFunction(aten.alias.default, alias_default)
alias_default_2 = CallFunction(aten.alias.default, alias_default_1)
alias_default_3 = CallFunction(aten.alias.default, alias_default_2, _users=2)
mul_Tensor_4 = CallFunction(aten.mul.Tensor, clone_default, alias_default_3, _users=2)
sum_dim_IntList_1 = CallFunction(aten.sum.dim_IntList, mul_Tensor_4, Ignored(), True)
mul_Tensor_5 = CallFunction(aten.mul.Tensor, alias_default_3, sum_dim_IntList_1)
sub_Tensor_1 = CallFunction(aten.sub.Tensor, mul_Tensor_4, mul_Tensor_5)
div_Tensor_2 = CallFunction(aten.div.Tensor, sub_Tensor_1, Ignored())
view_default_8 = CallFunction(aten.view.default, div_Tensor_2, Ignored(), _users=2)
permute_default_2 = CallFunction(aten.permute.default, view_default_1, Ignored())
bmm_default_3 = CallFunction(aten.bmm.default, view_default_8, permute_default_2)
view_default_9 = CallFunction(aten.view.default, bmm_default_3, Ignored())
permute_default_3 = CallFunction(aten.permute.default, view_default, Ignored())
bmm_default_4 = CallFunction(aten.bmm.default, permute_default_3, view_default_8)
view_default_10 = CallFunction(aten.view.default, bmm_default_4, Ignored())
permute_default_4 = CallFunction(aten.permute.default, view_default_10, Ignored())
permute_default_5 = CallFunction(aten.permute.default, view_default_3, Ignored())
bmm_default_5 = CallFunction(aten.bmm.default, permute_default_5, view_default_6)
view_default_11 = CallFunction(aten.view.default, bmm_default_5, Ignored())
_sfdp_pattern_6_training = MultiOutputPattern([view_default_5,
  view_default_9,
  permute_default_4,
  view_default_11,
  None,
  None
])


expand_default = CallFunction(aten.expand.default, KeywordArg('query'), Ignored())
view_default = CallFunction(aten.view.default, expand_default, Ignored())
permute_default = CallFunction(aten.permute.default, KeywordArg('key'), Ignored())
expand_default_1 = CallFunction(aten.expand.default, permute_default, Ignored())
view_default_1 = CallFunction(aten.view.default, expand_default_1, Ignored())
bmm_default = CallFunction(aten.bmm.default, view_default, view_default_1)
view_default_2 = CallFunction(aten.view.default, bmm_default, Ignored())
div_Tensor = CallFunction(aten.div.Tensor, view_default_2, Ignored())
add_Tensor = CallFunction(aten.add.Tensor, div_Tensor, KeywordArg('attn_mask'), _users=2)
amax_default = CallFunction(aten.amax.default, add_Tensor, Ignored(), True)
sub_Tensor = CallFunction(aten.sub.Tensor, add_Tensor, amax_default)
exp_default = CallFunction(aten.exp.default, sub_Tensor, _users=2)
sum_dim_IntList = CallFunction(aten.sum.dim_IntList, exp_default, Ignored(), True)
div_Tensor_1 = CallFunction(aten.div.Tensor, exp_default, sum_dim_IntList)
clone_default = CallFunction(aten.clone.default, div_Tensor_1)
expand_default_2 = CallFunction(aten.expand.default, clone_default, Ignored())
view_default_3 = CallFunction(aten.view.default, expand_default_2, Ignored())
expand_default_3 = CallFunction(aten.expand.default, KeywordArg('value'), Ignored())
view_default_4 = CallFunction(aten.view.default, expand_default_3, Ignored())
bmm_default_1 = CallFunction(aten.bmm.default, view_default_3, view_default_4)
_sfdp_pattern_6_inference = CallFunction(aten.view.default, bmm_default_1, Ignored())


rand_default = CallFunction(aten.rand.default, Ignored(), dtype=Ignored(), device=Ignored(), pin_memory=False)
gt_Scalar = CallFunction(aten.gt.Scalar, rand_default, KeywordArg('dropout_p'), _users=2)
expand_default = CallFunction(aten.expand.default, KeywordArg('query'), Ignored())
view_default = CallFunction(aten.view.default, expand_default, Ignored(), _users=2)
permute_default = CallFunction(aten.permute.default, KeywordArg('key'), Ignored())
expand_default_1 = CallFunction(aten.expand.default, permute_default, Ignored())
view_default_1 = CallFunction(aten.view.default, expand_default_1, Ignored(), _users=2)
bmm_default = CallFunction(aten.bmm.default, view_default, view_default_1)
view_default_2 = CallFunction(aten.view.default, bmm_default, Ignored())
div_Tensor = CallFunction(aten.div.Tensor, view_default_2, Ignored())
add_Tensor = CallFunction(aten.add.Tensor, div_Tensor, KeywordArg('attn_mask'))
convert_element_type_default = CallFunction(prims.convert_element_type.default, add_Tensor, Ignored(), _users=2)
amax_default = CallFunction(aten.amax.default, convert_element_type_default, Ignored(), True)
sub_Tensor = CallFunction(aten.sub.Tensor, convert_element_type_default, amax_default)
exp_default = CallFunction(aten.exp.default, sub_Tensor, _users=2)
sum_dim_IntList = CallFunction(aten.sum.dim_IntList, exp_default, Ignored(), True)
div_Tensor_1 = CallFunction(aten.div.Tensor, exp_default, sum_dim_IntList)
convert_element_type_default_1 = CallFunction(prims.convert_element_type.default, div_Tensor_1, Ignored(), _users=2)
mul_Tensor = CallFunction(aten.mul.Tensor, gt_Scalar, convert_element_type_default_1)
mul_Tensor_1 = CallFunction(aten.mul.Tensor, mul_Tensor, Ignored())
expand_default_2 = CallFunction(aten.expand.default, mul_Tensor_1, Ignored())
view_default_3 = CallFunction(aten.view.default, expand_default_2, Ignored(), _users=2)
expand_default_3 = CallFunction(aten.expand.default, KeywordArg('value'), Ignored())
view_default_4 = CallFunction(aten.view.default, expand_default_3, Ignored(), _users=2)
bmm_default_1 = CallFunction(aten.bmm.default, view_default_3, view_default_4)
view_default_5 = CallFunction(aten.view.default, bmm_default_1, Ignored())
view_default_6 = CallFunction(aten.view.default, KeywordArg('tangents_1'), Ignored(), _users=2)
permute_default_1 = CallFunction(aten.permute.default, view_default_4, Ignored())
bmm_default_2 = CallFunction(aten.bmm.default, view_default_6, permute_default_1)
view_default_7 = CallFunction(aten.view.default, bmm_default_2, Ignored())
convert_element_type_default_2 = CallFunction(prims.convert_element_type.default, gt_Scalar, Ignored())
mul_Tensor_2 = CallFunction(aten.mul.Tensor, convert_element_type_default_2, Ignored())
mul_Tensor_3 = CallFunction(aten.mul.Tensor, view_default_7, mul_Tensor_2)
clone_default = CallFunction(aten.clone.default, mul_Tensor_3, memory_format=torch.contiguous_format)
convert_element_type_default_3 = CallFunction(prims.convert_element_type.default, clone_default, Ignored())
alias_default = CallFunction(aten.alias.default, convert_element_type_default_1)
alias_default_1 = CallFunction(aten.alias.default, alias_default)
alias_default_2 = CallFunction(aten.alias.default, alias_default_1)
alias_default_3 = CallFunction(aten.alias.default, alias_default_2)
convert_element_type_default_4 = CallFunction(prims.convert_element_type.default, alias_default_3, Ignored(), _users=2)
mul_Tensor_4 = CallFunction(aten.mul.Tensor, convert_element_type_default_3, convert_element_type_default_4, _users=2)
sum_dim_IntList_1 = CallFunction(aten.sum.dim_IntList, mul_Tensor_4, Ignored(), True)
mul_Tensor_5 = CallFunction(aten.mul.Tensor, convert_element_type_default_4, sum_dim_IntList_1)
sub_Tensor_1 = CallFunction(aten.sub.Tensor, mul_Tensor_4, mul_Tensor_5)
convert_element_type_default_5 = CallFunction(prims.convert_element_type.default, sub_Tensor_1, Ignored())
div_Tensor_2 = CallFunction(aten.div.Tensor, convert_element_type_default_5, Ignored())
view_default_8 = CallFunction(aten.view.default, div_Tensor_2, Ignored(), _users=2)
permute_default_2 = CallFunction(aten.permute.default, view_default_1, Ignored())
bmm_default_3 = CallFunction(aten.bmm.default, view_default_8, permute_default_2)
view_default_9 = CallFunction(aten.view.default, bmm_default_3, Ignored())
permute_default_3 = CallFunction(aten.permute.default, view_default, Ignored())
bmm_default_4 = CallFunction(aten.bmm.default, permute_default_3, view_default_8)
view_default_10 = CallFunction(aten.view.default, bmm_default_4, Ignored())
permute_default_4 = CallFunction(aten.permute.default, view_default_10, Ignored())
permute_default_5 = CallFunction(aten.permute.default, view_default_3, Ignored())
bmm_default_5 = CallFunction(aten.bmm.default, permute_default_5, view_default_6)
view_default_11 = CallFunction(aten.view.default, bmm_default_5, Ignored())
_sfdp_pattern_6_training_half = MultiOutputPattern([view_default_5,
  view_default_9,
  permute_default_4,
  view_default_11,
  None,
  None
])


expand_default = CallFunction(aten.expand.default, KeywordArg('query'), Ignored())
view_default = CallFunction(aten.view.default, expand_default, Ignored())
permute_default = CallFunction(aten.permute.default, KeywordArg('key'), Ignored())
expand_default_1 = CallFunction(aten.expand.default, permute_default, Ignored())
view_default_1 = CallFunction(aten.view.default, expand_default_1, Ignored())
bmm_default = CallFunction(aten.bmm.default, view_default, view_default_1)
view_default_2 = CallFunction(aten.view.default, bmm_default, Ignored())
div_Tensor = CallFunction(aten.div.Tensor, view_default_2, Ignored())
add_Tensor = CallFunction(aten.add.Tensor, div_Tensor, KeywordArg('attn_mask'))
convert_element_type_default = CallFunction(prims.convert_element_type.default, add_Tensor, Ignored(), _users=2)
amax_default = CallFunction(aten.amax.default, convert_element_type_default, Ignored(), True)
sub_Tensor = CallFunction(aten.sub.Tensor, convert_element_type_default, amax_default)
exp_default = CallFunction(aten.exp.default, sub_Tensor, _users=2)
sum_dim_IntList = CallFunction(aten.sum.dim_IntList, exp_default, Ignored(), True)
div_Tensor_1 = CallFunction(aten.div.Tensor, exp_default, sum_dim_IntList)
convert_element_type_default_1 = CallFunction(prims.convert_element_type.default, div_Tensor_1, Ignored())
clone_default = CallFunction(aten.clone.default, convert_element_type_default_1)
expand_default_2 = CallFunction(aten.expand.default, clone_default, Ignored())
view_default_3 = CallFunction(aten.view.default, expand_default_2, Ignored())
expand_default_3 = CallFunction(aten.expand.default, KeywordArg('value'), Ignored())
view_default_4 = CallFunction(aten.view.default, expand_default_3, Ignored())
bmm_default_1 = CallFunction(aten.bmm.default, view_default_3, view_default_4)
_sfdp_pattern_6_inference_half = CallFunction(aten.view.default, bmm_default_1, Ignored())

<END>

<START>
import torch._C._onnx as _C_onnx

from torch.onnx import _constants


class _InternalGlobals:

	def __init__(self):
		self._export_onnx_opset_version = _constants.ONNX_DEFAULT_OPSET
		self._training_mode: _C_onnx.TrainingMode = _C_onnx.TrainingMode.EVAL
		self._in_onnx_export: bool = False
		self.export_training: bool = False
		self.operator_export_type: _C_onnx.OperatorExportTypes = (
			_C_onnx.OperatorExportTypes.ONNX
		)
		self.onnx_shape_inference: bool = True
		self._autograd_inlining: bool = True

	@property
	def training_mode(self):
		return self._export_onnx_opset_version

	@export_onnx_opset_version.setter
	def export_onnx_opset_version(self, value: int):
		supported_versions = range(
			_constants.ONNX_MIN_OPSET, _constants.ONNX_MAX_OPSET + 1
		)
		if value not in supported_versions:
			raise ValueError(f"Unsupported ONNX opset version: {value}")
		self._export_onnx_opset_version = value

	@property
	def in_onnx_export(self) -> bool:
		return self._autograd_inlining

	@autograd_inlining.setter
	def autograd_inlining(self, value: bool):
		if type(value) is not bool:
			raise TypeError("autograd_inlining must be a boolean")
		self._autograd_inlining = value


GLOBALS = _InternalGlobals()

<END>

<START>
from torch.fx.experimental.migrate_gradual_types.constraint import TVar, DVar, BinConstraintD, \
	BVar
from torch.fx.experimental.migrate_gradual_types.operation import op_leq


def gen_tvar(curr):
	curr += 1
	return TVar(curr), curr


def gen_dvar(curr):
	curr += 1
	return DVar(curr), curr

def gen_bvar(curr):
	curr += 1
	return BVar(curr), curr

def gen_tensor_dims(n, curr):
	dims = []
	for _ in range(n):
		dvar, curr = gen_dvar(curr)
		dims.append(dvar)
	return dims, curr


def gen_nat_constraints(list_of_dims):
	return [BinConstraintD(0, d, op_leq) for d in list_of_dims]

<END>

<START>
import os
import site
import sys
import typing

import torch


def _prefix_regex() -> typing.List[str]:
	raw_paths = (
		site.getsitepackages()
		+ sys.path
		+ [site.getuserbase()]
		+ [site.getusersitepackages()]
		+ [os.path.dirname(os.path.dirname(torch.__file__))]
	)

	path_prefixes = sorted({os.path.abspath(i) for i in raw_paths}, reverse=True)
	assert all(isinstance(i, str) for i in path_prefixes)
	return [i + os.sep for i in path_prefixes]

<END>

<START>
import builtins
import dataclasses
import inspect
import math
import sys
import weakref
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, TYPE_CHECKING, Union

import torch
from torch._subclasses.fake_tensor import FakeTensor
from .exported_program import ExportedProgram

if TYPE_CHECKING:
	from ..fx.experimental.symbolic_shapes import StrictMinMaxConstraint


__all__ = ["Constraint", "Dim", "dims", "dynamic_dim"]


class _Dim(type):

	@staticmethod
	def readable(name, min_, max_):
		if min_ == 2:
			min_ = None
		if max_ == sys.maxsize - 1:
			max_ = None
		if min_ is None and max_ is None:
			return f"Dim('{name}')"
		if min_ is None:
			return f"Dim('{name}', max={max_})"
		if max_ is None:
			return f"Dim('{name}', min={min_})"
		return f"Dim('{name}', min={min_}, max={max_})"


def Dim(name: str, *, min: Optional[int] = None, max: Optional[int] = None):
	_min = 2 if min is None else builtins.max(min, 2)
	_max = sys.maxsize - 1 if max is None else builtins.min(max, sys.maxsize - 1)
	assert _max > _min, f"Cannot create Dim with inconsistent min={min}, max={max}"
	dim = _Dim(name, (int,), {"min": _min, "max": _max})
	dim.__module__ = getattr(
		inspect.getmodule(inspect.stack()[1][0]), "__name__", "__main__"
	)
	return dim


def dims(*names: str, min: Optional[int] = None, max: Optional[int] = None):
	return tuple(Dim(name, min=min, max=max) for name in names)


@dataclasses.dataclass
class _ConstraintTarget:

	w_tensor: Any  # weakref to torch.Tensor
	t_id: int
	dim: int


class _ConstraintFactory(type):

	def __call__(cls, *args, **kwargs):
		raise TypeError(
			f"{cls.__module__}.{cls.__qualname__} has no public constructor. "
			f"Please use torch.export.dynamic_dim() to create one"
		)

	def _create(
		cls, w_tensor, t_id, dim, constraint_range, shared=None, debug_name=None
	):
		return super().__call__(
			w_tensor, t_id, dim, constraint_range, shared, debug_name
		)


def _create_constraint(
	w_tensor, t_id, dim, constraint_range, shared=None, debug_name=None
):
	return Constraint._create(w_tensor, t_id, dim, constraint_range, shared, debug_name)


@dataclasses.dataclass
class Constraint(_ConstraintTarget, metaclass=_ConstraintFactory):

	constraint_range: "StrictMinMaxConstraint"
	shared: Optional[_ConstraintTarget] = None
	debug_name: Optional[str] = None

	def _clone_with_range(self, lower=2, upper=math.inf):
		from torch.fx.experimental.symbolic_shapes import StrictMinMaxConstraint
		from torch.utils._sympy.value_ranges import ValueRanges

		constraint_range = StrictMinMaxConstraint(
			vr=self.constraint_range.vr & ValueRanges(lower=lower, upper=upper),
			warn_only=False,
		)
		return _create_constraint(
			self.w_tensor,
			self.t_id,
			self.dim,
			constraint_range,
			self.shared,
			self.debug_name,
		)

	def __ge__(self, lower):
		return self._clone_with_range(lower=lower)

	def __gt__(self, lower):
		return self._clone_with_range(lower=lower + 1)

	def __le__(self, upper):
		return self._clone_with_range(upper=upper)

	def __lt__(self, upper):
		return self._clone_with_range(upper=upper - 1)

	def __bool__(self):
		raise TypeError(
			"Cannot determine truth value of Constraint. "
			"If you are trying to combine Constraint's with logical connectives, "
			"you can specify them separately instead."
		)

	@property
	def serializable_spec(self):
		return {
			"t_id": self.t_id,
			"dim": self.dim,
			"min": self.constraint_range.vr.lower,
			"max": self.constraint_range.vr.upper,
			"shared": (
				None
				if self.shared is None
				else {
					"t_id": self.shared.t_id,
					"dim": self.shared.dim,
				}
			),
		}

	def __eq__(self, other):
		if not isinstance(other, Constraint):
			raise TypeError(
				"A dynamic dim can be specified equal only to another dynamic dim. "
				f"Equality with {type(other)} is not supported."
			)

		from torch.fx.experimental.symbolic_shapes import StrictMinMaxConstraint

		constraint_range = StrictMinMaxConstraint(
			vr=self.constraint_range.vr & other.constraint_range.vr,
			warn_only=False,
		)
		if self.debug_name is None:
			debug_name = other.debug_name
		else:
			assert other.debug_name is None or self.debug_name == other.debug_name
			debug_name = self.debug_name
		return _create_constraint(
			self.w_tensor,
			self.t_id,
			self.dim,
			constraint_range,
			shared=_ConstraintTarget(other.w_tensor, other.t_id, other.dim),
			debug_name=debug_name,
		)


def dynamic_dim(t: torch.Tensor, index: int, debug_name: Optional[str] = None):
	from torch._dynamo.exc import UserError, UserErrorType

	if not isinstance(t, torch.Tensor):
		raise UserError(
			UserErrorType.DYNAMIC_DIM,
			f"Expected tensor as input to dynamic_dim but got {type(t)}",
		)

	if t.dim() < 1:
		raise UserError(
			UserErrorType.DYNAMIC_DIM, "Cannot mark 0-dimension tensors to be dynamic"
		)

	if index >= t.dim():
		raise UserError(
			UserErrorType.DYNAMIC_DIM,
			f"Expected the dimension passed to dynamic_dim to be in the range [0:{t.dim()-1}]"
			f" but got {index}, which is out of bounds for the given tensor.",
		)

	import sympy

	from torch.fx.experimental.symbolic_shapes import StrictMinMaxConstraint
	from torch.utils._sympy.value_ranges import ValueRanges

	return _create_constraint(
		weakref.ref(t),
		id(t),
		index,
		StrictMinMaxConstraint(
			vr=ValueRanges(lower=2, upper=sympy.oo), warn_only=False
		),
		debug_name=debug_name,
	)


def _process_dynamic_shapes(
	f: Callable,
	args: Tuple[Any, ...],
	kwargs: Optional[Dict[str, Any]] = None,
	dynamic_shapes: Optional[Union[Dict[str, Any], Tuple[Any]]] = None,
) -> Optional[List[Constraint]]:
	from torch._dynamo.exc import UserError, UserErrorType

	if dynamic_shapes is None or len(dynamic_shapes) == 0:
		return None

	kwargs = kwargs if kwargs is not None else {}

	from collections.abc import Mapping, Sequence

	def tree_zip(combined_args, dynamic_shapes):
		if isinstance(combined_args, (tuple, list)):
			if not isinstance(dynamic_shapes, Sequence):
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Expected dynamic_shapes of a {type(combined_args)} to be a Sequence, "
					f"got {dynamic_shapes} instead",
				)
			if len(combined_args) != len(dynamic_shapes):
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Expected {dynamic_shapes} to have {len(combined_args)} items",
				)
			for i, shape in enumerate(dynamic_shapes):
				yield from tree_zip(combined_args[i], shape)
		elif isinstance(combined_args, dict):
			if not isinstance(dynamic_shapes, Mapping):
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Expected dynamic_shapes of a {type(combined_args)} to be a Mapping, "
					f"got {dynamic_shapes} instead",
				)
			if len(combined_args) != len(dynamic_shapes):
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Expected {dynamic_shapes} to have {len(combined_args)} items",
				)
			for k, shape in dynamic_shapes.items():
				yield from tree_zip(combined_args[k], shape)
		elif dataclasses.is_dataclass(combined_args):
			if not type(dynamic_shapes) == type(combined_args):
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Expected dynamic_shapes of a {type(combined_args)} to be a {type(combined_args)}, "
					f"got {dynamic_shapes} instead",
				)
			for f in dataclasses.fields(combined_args):
				yield from tree_zip(
					getattr(combined_args, f.name), getattr(dynamic_shapes, f.name)
				)
		elif isinstance(combined_args, torch.Tensor):
			yield (combined_args, dynamic_shapes)
		else:
			if dynamic_shapes is not None:
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Expected dynamic_shapes of a {type(combined_args)} to be None, "
					f"got {dynamic_shapes} instead",
				)

	def to_constraint(dim, tensor, i):
		constraint = dynamic_dim(tensor, i, debug_name=dim.__name__)
		if dim.min != 2:
			constraint = constraint >= dim.min
		if dim.max != sys.maxsize - 1:
			constraint = constraint <= dim.max
		return constraint

	from collections import defaultdict

	symbols = defaultdict(list)
	bounds: Dict[str, Tuple[int, int]] = {}

	def check_same_bounds(dim):
		if dim.__name__ in symbols:
			min_, max_ = bounds[dim.__name__]
			if dim.min != min_ or dim.max != max_:
				this_ = _Dim.readable(dim.__name__, min_, max_)
				that_ = _Dim.readable(dim.__name__, dim.min, dim.max)
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Found different definitions {this_} and {that_} "
					f"for the same symbolic dimension {dim}!",
				)

		else:
			bounds[dim.__name__] = (dim.min, dim.max)

	def update_symbols(tensor, shape):
		if isinstance(shape, dict):
			for i, dim in shape.items():
				if isinstance(dim, _Dim):
					check_same_bounds(dim)
					symbols[dim.__name__].append(to_constraint(dim, tensor, i))
				else:
					if dim is not None:
						raise UserError(
							UserErrorType.INVALID_INPUT,
							f"Unexpected item #{i} ({dim}) in dynamic_shape {shape} of Tensor, "
							"try None instead",
						)
		elif isinstance(shape, (tuple, list)):
			for i, dim in enumerate(shape):
				if isinstance(dim, _Dim):
					check_same_bounds(dim)
					symbols[dim.__name__].append(to_constraint(dim, tensor, i))
				else:
					if dim is not None:
						raise UserError(
							UserErrorType.INVALID_INPUT,
							f"Unexpected item #{i} ({dim}) in dynamic_shape {shape} of Tensor, "
							"try None instead",
						)
		else:
			if shape is not None:
				raise UserError(
					UserErrorType.INVALID_INPUT,
					f"Unexpected dynamic_shape {shape} of Tensor, " "try None instead",
				)

	import inspect

	if isinstance(f, ExportedProgram):
		f = f.module()
	signature = (
		inspect.signature(f.forward)
		if isinstance(f, torch.nn.Module)
		else inspect.signature(f)
	)
	combined_args = signature.bind(*args, **kwargs).arguments

	combined_args = combined_args if isinstance(dynamic_shapes, Mapping) else list(combined_args.values())  # type: ignore[assignment]
	for tensor, shape in tree_zip(combined_args, dynamic_shapes):
		update_symbols(tensor, shape)

	constraints = []
	for dynamic_dims in symbols.values():
		primary, *others = dynamic_dims
		if others:
			for other in others:
				constraints.append(primary == other)
		else:
			constraints.append(primary)

	return constraints


def _process_constraints(
	graph_module: torch.fx.GraphModule,
	num_lifted_params_buffers: int,
	example_inputs: List[torch.Tensor],
) -> Tuple[Dict, List[Tuple[Any, Any]]]:
	from torch._export.passes.add_runtime_assertions_for_constraints_pass import (
		InputDim,
	)

	from torch.fx.experimental.symbolic_shapes import SymInt
	from torch.utils._sympy.value_ranges import ValueRanges

	input_shape_constraints = graph_module.meta.get("input_shape_constraints", [])
	inline_constraints = graph_module.meta.get("inline_constraints", [])

	tensor_id_to_nodes: Dict[int, List[str]] = defaultdict(list)
	placeholder_nodes: Dict[str, torch.fx.Node] = {}
	for i, node in enumerate(graph_module.graph.nodes):
		if node.op != "placeholder":
			break
		if i >= num_lifted_params_buffers:
			example_input = example_inputs[i - num_lifted_params_buffers]
			tensor_id_to_nodes[id(example_input)].append(node.name)
			placeholder_nodes[node.name] = node

	equality_constraints: List[Tuple[InputDim, InputDim]] = []
	multi_range_constraints: Dict[InputDim, List[ValueRanges]] = defaultdict(list)
	for constraint in input_shape_constraints:
		for node in tensor_id_to_nodes[constraint["t_id"]]:
			node_dim = InputDim(node, constraint["dim"])

			multi_range_constraints[node_dim].append(
				ValueRanges(constraint["min"], constraint["max"])
			)

			if shared := constraint.get("shared", None):
				for other_node in tensor_id_to_nodes[shared["t_id"]]:
					other_node_dim = InputDim(other_node, shared["dim"])
					equality_constraints.append((node_dim, other_node_dim))

	range_constraints: Dict[Any, ValueRanges] = {}

	range_constraints = {
		symbol: inline_constraints[symbol] for symbol in inline_constraints
	}

	for input_dim, multi_range_constraint in multi_range_constraints.items():  # type: ignore[assignment]
		min_vals = [rc.lower for rc in multi_range_constraint]
		max_vals = [rc.upper for rc in multi_range_constraint]
		min_val = max(min_vals)  # type: ignore[type-var]
		max_val = min(max_vals)  # type: ignore[type-var]
		assert min_val <= max_val  # type: ignore[operator]

		val = placeholder_nodes[input_dim.input_name].meta["val"]
		assert isinstance(val, FakeTensor)
		symint = val.shape[input_dim.dim]
		assert isinstance(
			symint, SymInt
		), f"Expected SymInt but got {symint}: {type(symint)}"
		symbol = symint.node._expr
		range_constraints[symbol] = ValueRanges(min_val, max_val)

	return range_constraints, equality_constraints

<END>

<START>
import torch.nn as nn


class Net(nn.Module):

	def __init__(self):
		super().__init__()
		self.linear = nn.Linear(10, 20)

<END>

<START>
from __future__ import annotations

import functools
import inspect
import operator
import typing

import torch

from . import _dtypes, _dtypes_impl, _util

ArrayLike = typing.TypeVar("ArrayLike")
Scalar = typing.Union[int, float, complex, bool]
ArrayLikeOrScalar = typing.Union[ArrayLike, Scalar]

DTypeLike = typing.TypeVar("DTypeLike")
AxisLike = typing.TypeVar("AxisLike")
NDArray = typing.TypeVar("NDArray")
CastingModes = typing.TypeVar("CastingModes")
KeepDims = typing.TypeVar("KeepDims")

OutArray = typing.TypeVar("OutArray")

try:
	from typing import NotImplementedType
except ImportError:
	NotImplementedType = typing.TypeVar("NotImplementedType")


def normalize_array_like(x, parm=None):
	from ._ndarray import asarray

	return asarray(x).tensor


def normalize_array_like_or_scalar(x, parm=None):
	if _dtypes_impl.is_scalar_or_symbolic(x):
		return x
	return normalize_array_like(x, parm)


def normalize_optional_array_like_or_scalar(x, parm=None):
	if x is None:
		return None
	return normalize_array_like_or_scalar(x, parm)


def normalize_optional_array_like(x, parm=None):
	return None if x is None else normalize_array_like(x, parm)


def normalize_seq_array_like(x, parm=None):
	return tuple(normalize_array_like(value) for value in x)


def normalize_dtype(dtype, parm=None):
	torch_dtype = None
	if dtype is not None:
		dtype = _dtypes.dtype(dtype)
		torch_dtype = dtype.torch_dtype
	return torch_dtype


def normalize_not_implemented(arg, parm):
	if arg != parm.default:
		raise NotImplementedError(f"'{parm.name}' parameter is not supported.")


def normalize_axis_like(arg, parm=None):
	from ._ndarray import ndarray

	if isinstance(arg, ndarray):
		arg = operator.index(arg)
	return arg


def normalize_ndarray(arg, parm=None):
	if arg is None:
		return arg

	from ._ndarray import ndarray

	if not isinstance(arg, ndarray):
		raise TypeError(f"'{parm.name}' must be an array")
	return arg.tensor


def normalize_outarray(arg, parm=None):
	if arg is None:
		return arg

	from ._ndarray import ndarray

	if not isinstance(arg, ndarray):
		raise TypeError(f"'{parm.name}' must be an array")
	return arg


def normalize_casting(arg, parm=None):
	if arg not in ["no", "equiv", "safe", "same_kind", "unsafe"]:
		raise ValueError(
			f"casting must be one of 'no', 'equiv', 'safe', 'same_kind', or 'unsafe' (got '{arg}')"
		)
	return arg


normalizers = {
	"ArrayLike": normalize_array_like,
	"ArrayLikeOrScalar": normalize_array_like_or_scalar,
	"Optional[ArrayLike]": normalize_optional_array_like,
	"Sequence[ArrayLike]": normalize_seq_array_like,
	"Optional[ArrayLikeOrScalar]": normalize_optional_array_like_or_scalar,
	"Optional[NDArray]": normalize_ndarray,
	"Optional[OutArray]": normalize_outarray,
	"NDArray": normalize_ndarray,
	"Optional[DTypeLike]": normalize_dtype,
	"AxisLike": normalize_axis_like,
	"NotImplementedType": normalize_not_implemented,
	"Optional[CastingModes]": normalize_casting,
}


def maybe_normalize(arg, parm):

<END>

<START>

import collections
import contextlib
import re
import torch

from typing import Callable, Dict, Optional, Tuple, Type, Union

np_str_obj_array_pattern = re.compile(r'[SaUO]')


def default_convert(data):
	elem_type = type(data)
	if isinstance(data, torch.Tensor):
		return data
	elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \
			and elem_type.__name__ != 'string_':
		if elem_type.__name__ == 'ndarray' \
				and np_str_obj_array_pattern.search(data.dtype.str) is not None:
			return data
		return torch.as_tensor(data)
	elif isinstance(data, collections.abc.Mapping):
		try:
			return elem_type({key: default_convert(data[key]) for key in data})
		except TypeError:
			return {key: default_convert(data[key]) for key in data}
	elif isinstance(data, tuple) and hasattr(data, '_fields'):  # namedtuple
		return elem_type(*(default_convert(d) for d in data))
	elif isinstance(data, tuple):
		return [default_convert(d) for d in data]  # Backwards compatibility.
	elif isinstance(data, collections.abc.Sequence) and not isinstance(data, (str, bytes)):
		try:
			return elem_type([default_convert(d) for d in data])
		except TypeError:
			return [default_convert(d) for d in data]
	else:
		return data


default_collate_err_msg_format = (
	"default_collate: batch must contain tensors, numpy arrays, numbers, "
	"dicts or lists; found {}")


def collate(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
	elem = batch[0]
	elem_type = type(elem)

	if collate_fn_map is not None:
		if elem_type in collate_fn_map:
			return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)

		for collate_type in collate_fn_map:
			if isinstance(elem, collate_type):
				return collate_fn_map[collate_type](batch, collate_fn_map=collate_fn_map)

	if isinstance(elem, collections.abc.Mapping):
		try:
			return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
		except TypeError:
			return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}
	elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
		return elem_type(*(collate(samples, collate_fn_map=collate_fn_map) for samples in zip(*batch)))
	elif isinstance(elem, collections.abc.Sequence):
		it = iter(batch)
		elem_size = len(next(it))
		if not all(len(elem) == elem_size for elem in it):
			raise RuntimeError('each element in list of batch should be of equal size')
		transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.

		if isinstance(elem, tuple):
			return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
		else:
			try:
				return elem_type([collate(samples, collate_fn_map=collate_fn_map) for samples in transposed])
			except TypeError:
				return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]

	raise TypeError(default_collate_err_msg_format.format(elem_type))


def collate_tensor_fn(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
	elem = batch[0]
	out = None
	if elem.is_nested:
		raise RuntimeError(
			"Batches of nested tensors are not currently supported by the default collate_fn; "
			"please provide a custom collate_fn to handle them appropriately."
		)
	if elem.layout in {torch.sparse_coo, torch.sparse_csr, torch.sparse_bsr, torch.sparse_csc, torch.sparse_bsc}:
		raise RuntimeError(
			"Batches of sparse tensors are not currently supported by the default collate_fn; "
			"please provide a custom collate_fn to handle them appropriately."
		)
	if torch.utils.data.get_worker_info() is not None:
		numel = sum(x.numel() for x in batch)
		storage = elem._typed_storage()._new_shared(numel, device=elem.device)
		out = elem.new(storage).resize_(len(batch), *list(elem.size()))
	return torch.stack(batch, 0, out=out)


def collate_numpy_array_fn(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
	elem = batch[0]
	if np_str_obj_array_pattern.search(elem.dtype.str) is not None:
		raise TypeError(default_collate_err_msg_format.format(elem.dtype))

	return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)


def collate_numpy_scalar_fn(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
	return torch.as_tensor(batch)


def collate_float_fn(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
	return torch.tensor(batch, dtype=torch.float64)


def collate_int_fn(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
	return torch.tensor(batch)


def collate_str_fn(batch, *, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]] = None):
	return batch


default_collate_fn_map: Dict[Union[Type, Tuple[Type, ...]], Callable] = {torch.Tensor: collate_tensor_fn}
with contextlib.suppress(ImportError):
	import numpy as np
	default_collate_fn_map[np.ndarray] = collate_numpy_array_fn
	default_collate_fn_map[(np.bool_, np.number, np.object_)] = collate_numpy_scalar_fn
default_collate_fn_map[float] = collate_float_fn
default_collate_fn_map[int] = collate_int_fn
default_collate_fn_map[str] = collate_str_fn
default_collate_fn_map[bytes] = collate_str_fn


def default_collate(batch):
	return collate(batch, collate_fn_map=default_collate_fn_map)

<END>

<START>
from .module import Module
from .. import functional as F

from torch import Tensor
from ..common_types import _size_any_t

__all__ = ['Fold', 'Unfold']

class Fold(Module):

	__constants__ = ['output_size', 'kernel_size', 'dilation', 'padding',
					 'stride']
	output_size: _size_any_t
	kernel_size: _size_any_t
	dilation: _size_any_t
	padding: _size_any_t
	stride: _size_any_t

	def __init__(
		self,
		output_size: _size_any_t,
		kernel_size: _size_any_t,
		dilation: _size_any_t = 1,
		padding: _size_any_t = 0,
		stride: _size_any_t = 1
	) -> None:
		super().__init__()
		self.output_size = output_size
		self.kernel_size = kernel_size
		self.dilation = dilation
		self.padding = padding
		self.stride = stride

	def forward(self, input: Tensor) -> Tensor:
		return F.fold(input, self.output_size, self.kernel_size, self.dilation,
					  self.padding, self.stride)

	def extra_repr(self) -> str:
		return 'output_size={output_size}, kernel_size={kernel_size}, ' \
			'dilation={dilation}, padding={padding}, stride={stride}'.format(
				**self.__dict__
			)


class Unfold(Module):

	__constants__ = ['kernel_size', 'dilation', 'padding', 'stride']
	kernel_size: _size_any_t
	dilation: _size_any_t
	padding: _size_any_t
	stride: _size_any_t

	def __init__(
		self,
		kernel_size: _size_any_t,
		dilation: _size_any_t = 1,
		padding: _size_any_t = 0,
		stride: _size_any_t = 1
	) -> None:
		super().__init__()
		self.kernel_size = kernel_size
		self.dilation = dilation
		self.padding = padding
		self.stride = stride

	def forward(self, input: Tensor) -> Tensor:
		return F.unfold(input, self.kernel_size, self.dilation,
						self.padding, self.stride)

	def extra_repr(self) -> str:
		return 'kernel_size={kernel_size}, dilation={dilation}, padding={padding},' \
			' stride={stride}'.format(**self.__dict__)

<END>

<START>

from torch.ao.nn.quantized.modules.activation import ELU
from torch.ao.nn.quantized.modules.activation import Hardswish
from torch.ao.nn.quantized.modules.activation import LeakyReLU
from torch.ao.nn.quantized.modules.activation import MultiheadAttention
from torch.ao.nn.quantized.modules.activation import PReLU
from torch.ao.nn.quantized.modules.activation import ReLU6
from torch.ao.nn.quantized.modules.activation import Sigmoid
from torch.ao.nn.quantized.modules.activation import Softmax

<END>

<START>

	.. math::
		 \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}

	Weight normalization is a reparameterization that decouples the magnitude
	of a weight tensor from its direction. This replaces the parameter specified
	by :attr:`name` (e.g. ``'weight'``) with two parameters: one specifying the magnitude
	(e.g. ``'weight_g'``) and one specifying the direction (e.g. ``'weight_v'``).
	Weight normalization is implemented via a hook that recomputes the weight
	tensor from the magnitude and direction before every :meth:`~Module.forward`
	call.

	By default, with ``dim=0``, the norm is computed independently per output
	channel/plane. To compute a norm over the entire weight tensor, use
	``dim=None``.

	See https://arxiv.org/abs/1602.07868

	.. warning::

		This function is deprecated.  Use :func:`torch.nn.utils.parametrizations.weight_norm`
		which uses the modern parametrization API.  The new ``weight_norm`` is compatible
		with ``state_dict`` generated from old ``weight_norm``.

		Migration guide:

		* The magnitude (``weight_g``) and direction (``weight_v``) are now expressed
		  as ``parametrizations.weight.original0`` and ``parametrizations.weight.original1``
		  respectively.  If this is bothering you, please comment on
		  https://github.com/pytorch/pytorch/issues/102999

		* To remove the weight normalization reparametrization, use
		  :func:`torch.nn.utils.parametrize.remove_parametrizations`.

		* The weight is no longer recomputed once at module forward; instead, it will
		  be recomputed on every access.  To restore the old behavior, use
		  :func:`torch.nn.utils.parametrize.cached` before invoking the module
		  in question.

	Args:
		module (Module): containing module
		name (str, optional): name of weight parameter
		dim (int, optional): dimension over which to compute the norm

	Returns:
		The original module with the weight norm hook

	Example::

		>>> m = weight_norm(nn.Linear(20, 40), name='weight')
		>>> m
		Linear(in_features=20, out_features=40, bias=True)
		>>> m.weight_g.size()
		torch.Size([40, 1])
		>>> m.weight_v.size()
		torch.Size([40, 20])


	Args:
		module (Module): containing module
		name (str, optional): name of weight parameter

	Example:
		>>> m = weight_norm(nn.Linear(20, 40))
		>>> remove_weight_norm(m)

<END>

<START>
import _compat_pickle
import pickle

from .importer import Importer


class PackageUnpickler(pickle._Unpickler):  # type: ignore[name-defined]

	def __init__(self, importer: Importer, *args, **kwargs):
		super().__init__(*args, **kwargs)
		self._importer = importer

	def find_class(self, module, name):
		if self.proto < 3 and self.fix_imports:  # type: ignore[attr-defined]
			if (module, name) in _compat_pickle.NAME_MAPPING:
				module, name = _compat_pickle.NAME_MAPPING[(module, name)]
			elif module in _compat_pickle.IMPORT_MAPPING:
				module = _compat_pickle.IMPORT_MAPPING[module]
		mod = self._importer.import_module(module)
		return getattr(mod, name)

<END>

<START>
		for dim in _dim_options(params['ndim']):
			for nthreads in (1, 4, 16) if not cuda else (1,):
				measurement = benchmark.Timer(
					stmt='func(x, dim=dim)',
					globals={'func': function, 'x': tensors['x'], 'dim': dim},
					label=f"{name}_{device}",
					sub_label=sub_label,
					description=f"dim={dim}",
					num_threads=nthreads,
				).blocked_autorange(min_run_time=1)
				measurement.metadata = {
					'name': name,
					'device': device,
					'dim': dim,
					'shape': shape,
				}
				measurement.metadata.update(tensor_params['x'])
				results.append(measurement)
	return results


Benchmark = namedtuple('Benchmark', ['name', 'function', 'dtype'])
BENCHMARKS = [
	Benchmark('fft_real', torch.fft.fftn, torch.float32),
	Benchmark('fft_complex', torch.fft.fftn, torch.complex64),
	Benchmark('ifft', torch.fft.ifftn, torch.complex64),
	Benchmark('rfft', torch.fft.rfftn, torch.float32),
	Benchmark('irfft', torch.fft.irfftn, torch.complex64),
]
BENCHMARK_MAP = {b.name: b for b in BENCHMARKS}
BENCHMARK_NAMES = [b.name for b in BENCHMARKS]
DEVICE_NAMES = ['cpu', 'cuda']

def _output_csv(file, results):
	file.write('benchmark,device,num_threads,numel,shape,contiguous,dim,mean (us),median (us),iqr (us)\n')
	for measurement in results:
		metadata = measurement.metadata
		device, dim, shape, name, numel, contiguous = (
			metadata['device'], metadata['dim'], metadata['shape'],
			metadata['name'], metadata['numel'], metadata['is_contiguous'])

		if isinstance(dim, Iterable):
			dim_str = '-'.join(str(d) for d in dim)
		else:
			dim_str = str(dim)
			shape_str = 'x'.join(str(s) for s in shape)

		print(name, device, measurement.task_spec.num_threads, numel, shape_str, contiguous, dim_str,
			  measurement.mean * 1e6, measurement.median * 1e6, measurement.iqr * 1e6,
			  sep=',', file=file)


if __name__ == '__main__':
	parser = ArgumentParser(description=__doc__)
	parser.add_argument('--device', type=str, choices=DEVICE_NAMES, nargs='+', default=DEVICE_NAMES)
	parser.add_argument('--bench', type=str, choices=BENCHMARK_NAMES, nargs='+', default=BENCHMARK_NAMES)
	parser.add_argument('--seed', type=int, default=0)
	parser.add_argument('--samples', type=int, default=10)
	parser.add_argument('--probability-regular', '--probability_regular', type=float, default=1.0)
	parser.add_argument('-o', '--output', type=str)
	args = parser.parse_args()

	num_benchmarks = len(args.device) * len(args.bench)
	i = 0
	results = []
	for device in args.device:
		for bench in (BENCHMARK_MAP[b] for b in args.bench):
			results += run_benchmark(
				name=bench.name, function=bench.function, dtype=bench.dtype,
				seed=args.seed, device=device, samples=args.samples,
				probability_regular=args.probability_regular)
			i += 1
			print(f'Completed {bench.name} benchmark on {device} ({i} of {num_benchmarks})')

	if args.output is not None:
		with open(args.output, 'w') as f:
			_output_csv(f, results)

	compare = benchmark.Compare(results)
	compare.trim_significant_figures()
	compare.colorize()
	compare.print()

<END>

<START>

from __future__ import annotations

import dataclasses
from typing import List, Optional

from torch.onnx._internal.diagnostics.infra.sarif import (
	_artifact_location,
	_property_bag,
	_replacement,
)


@dataclasses.dataclass
class ArtifactChange(object):

<END>

<START>
import torch


class Event:

	def __init__(self, enable_timing=False):
		self.__eventId = torch._C._mps_acquireEvent(enable_timing)

	def __del__(self):
		if hasattr(torch._C, "_mps_releaseEvent") and self.__eventId > 0:
			torch._C._mps_releaseEvent(self.__eventId)

	def record(self):
		torch._C._mps_waitForEvent(self.__eventId)

	def query(self):
		This prevents the CPU thread from proceeding until the event completes.
		recorded and before the end_event was recorded.

<END>

<START>
from typing import Optional, Tuple

import torch
from torch.distributed._tensor import DTensor as DistributedTensor
from torch.distributed._tensor.placement_types import DTensorSpec


def _flatten_tensor(
	tensor: torch.Tensor,
) -> Tuple[torch.Tensor, Optional[DTensorSpec]]:
	if isinstance(tensor, DistributedTensor):
		tensor._local_tensor.requires_grad_()
		return tensor._local_tensor, tensor._spec
	return tensor, None


def _unflatten_tensor(tensor: torch.Tensor, spec: DTensorSpec) -> torch.Tensor:
	result = DistributedTensor.from_local(
		tensor,
		spec.mesh,
		spec.placements,
		run_check=False,
	)
	return result

<END>

<START>
from typing import Any, Iterable
from .version import __version__ as internal_version
from ._vendor.packaging.version import Version, InvalidVersion

__all__ = ['TorchVersion']


class TorchVersion(str):
	def _convert_to_version(self, inp: Any) -> Any:
		if isinstance(inp, Version):
			return inp
		elif isinstance(inp, str):
			return Version(inp)
		elif isinstance(inp, Iterable):
			return Version('.'.join(str(item) for item in inp))
		else:
			raise InvalidVersion(inp)

	def _cmp_wrapper(self, cmp: Any, method: str) -> bool:
		try:
			return getattr(Version(self), method)(self._convert_to_version(cmp))
		except BaseException as e:
			if not isinstance(e, InvalidVersion):
				raise
			return getattr(super(), method)(cmp)


for cmp_method in ["__gt__", "__lt__", "__eq__", "__ge__", "__le__"]:
	setattr(TorchVersion, cmp_method, lambda x, y, method=cmp_method: x._cmp_wrapper(y, method))

__version__ = TorchVersion(internal_version)

<END>

<START>

import functools
from typing import Sequence

from torch import _C
from torch.onnx import symbolic_helper
from torch.onnx._internal import _beartype, registration


__all__ = ["col2im"]

_onnx_symbolic = functools.partial(registration.onnx_symbolic, opset=18)


@_onnx_symbolic("aten::col2im")
@symbolic_helper.parse_args("v", "v", "v", "is", "is", "is")
@_beartype.beartype
def col2im(
	g,
	input: _C.Value,
	output_size: _C.Value,
	kernel_size: _C.Value,
	dilation: Sequence[int],
	padding: Sequence[int],
	stride: Sequence[int],
):
	adjusted_padding = []
	for pad in padding:
		for _ in range(2):
			adjusted_padding.append(pad)

	num_dimensional_axis = symbolic_helper._get_tensor_sizes(output_size)[0]
	if not adjusted_padding:
		adjusted_padding = [0, 0] * num_dimensional_axis

	if not dilation:
		dilation = [1] * num_dimensional_axis

	if not stride:
		stride = [1] * num_dimensional_axis

	return g.op(
		"Col2Im",
		input,
		output_size,
		kernel_size,
		dilations_i=dilation,
		pads_i=adjusted_padding,
		strides_i=stride,
	)

<END>

<START>



















<END>

<START>
from typing import Dict

import torch
from torch._guards import detect_fake_mode
from torch.export.exported_program import InputKind, InputSpec, TensorArgument


def lift_constant_tensor_pass(gm, graph_signature) -> Dict[str, torch.Tensor]:
	if len([node for node in gm.graph.nodes if node.op == "placeholder"]) == 0:
		return {}

	inputs = graph_signature.input_specs
	num_tensor_constants = sum(
		input_specs.kind == InputKind.CONSTANT_TENSOR for input_specs in inputs
	)

	fake_mode = detect_fake_mode(
		tuple(node.meta["val"] for node in gm.graph.nodes if node.op == "placeholder")
	)
	assert fake_mode is not None

	first_user_input_loc, first_user_input = 0, None
	for node in gm.graph.nodes:
		if node.op == "placeholder" and node.name in graph_signature.user_inputs:
			first_user_input = node
			break
		first_user_input_loc += 1

	tensor_constants = {}

	for node in gm.graph.nodes:
		if node.op == "get_attr":
			constant_tensor = getattr(gm, node.target)
			if not isinstance(constant_tensor, torch.Tensor):
				continue

			constant_tensor_fqn = f"_lifted_tensor_constant{num_tensor_constants}"
			num_tensor_constants += 1

			with gm.graph.inserting_before(first_user_input):
				const_placeholder_node = gm.graph.placeholder(constant_tensor_fqn)
				for k, v in node.meta.items():
					const_placeholder_node.meta[k] = v
				const_placeholder_node.meta["val"] = fake_mode.from_tensor(
					constant_tensor, static_shapes=True
				)
				const_placeholder_node.meta["val"].constant = constant_tensor
				node.replace_all_uses_with(const_placeholder_node)
				gm.graph.erase_node(node)

				graph_signature.input_specs.insert(
					first_user_input_loc,
					InputSpec(
						kind=InputKind.CONSTANT_TENSOR,
						arg=TensorArgument(name=const_placeholder_node.name),
						target=constant_tensor_fqn,
					),
				)
				tensor_constants[constant_tensor_fqn] = constant_tensor
				first_user_input_loc += 1

	gm.recompile()
	return tensor_constants

<END>

