{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "eb4737e4-b920-41cd-887b-eea2cc1e9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877d5de-19bd-4f8d-b9ce-9d453abbc2be",
   "metadata": {},
   "source": [
    "# extract text and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "44fb714b-e291-4639-b0ed-7dd074b8ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_comment(line):\n",
    "    # Define a function to check if a line is a comment\n",
    "    line = line.strip()\n",
    "    if line.startswith('#') or line.startswith(\"'''\") or line.startswith('\"\"\"'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_non_comments(source_directory, target_directory):\n",
    "    # Process all .py files in the specified directory and subdirectories\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                target_file_path = os.path.join(target_directory, file.replace('.py', '.txt'))\n",
    "                with open(file_path, 'r') as source_file, open(target_file_path, 'w') as target_file:\n",
    "                    non_comments = []\n",
    "                    comment_block = False\n",
    "                    \n",
    "                    for line in source_file:\n",
    "                        # Check for the start or end of a comment block\n",
    "                        # if \"r'''\" in line and \"'''\" in line or 'r\"\"\"' in line and '\"\"\"' in line:\n",
    "                        #     continue\n",
    "                        # if \"'''\" in line or '\"\"\"' in line:\n",
    "                        #     comment_block = not comment_block\n",
    "                        #     continue\n",
    "                        # if \"r'''\" in line or 'r\"\"\"' in line:\n",
    "                        #     comment_block = not comment_block\n",
    "                        #     continue\n",
    "                        if line.count(\"'''\") == 1 or line.count('\"\"\"') == 1:\n",
    "                            comment_block = not comment_block\n",
    "                            continue\n",
    "                        # If it's not a comment or part of a comment block, save it\n",
    "                        if not is_comment(line) and not comment_block:\n",
    "                            non_comments.append(line)\n",
    "                        # Write non-comment lines to a target .txt file\n",
    "                    target_file.writelines(non_comments)\n",
    "\n",
    "# # Define the path to the local repository (change this to the actual path of your local repo)\n",
    "# # source_directory = '/path/to/your/local/pytorch/repo'\n",
    "source_directory = '../examples/'\n",
    "# # target_directory = '/path/to/your/output/directory'\n",
    "target_directory = './dataset/raw/'\n",
    "\n",
    "\n",
    "# # Create the target directory if it doesn't exist\n",
    "# os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "# # Call the function to start extracting non-comment lines\n",
    "# extract_non_comments(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "954bd61d-0495-4afd-b754-36969e202bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file created as 'sample_scripts.txt' with contents from 57 files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def combine_files(directory, output_file, sample=False, num_files_to_sample=100, seed=111, start_token=\"<START>\", end_token=\"<END>\"):\n",
    "    \"\"\"\n",
    "    Combine content from a specified number of text files in a directory into one file, \n",
    "    with start and end tokens between contents from each file.\n",
    "\n",
    "    :param directory: Path to the directory containing text files.\n",
    "    :param output_file: Name of the output file to create.\n",
    "    :param num_files_to_sample: Number of files to sample and combine.\n",
    "    :param start_token: The start token to be added before each file's content.\n",
    "    :param end_token: The end token to be added after each file's content.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # List all text files in the directory\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    files = all_files\n",
    "\n",
    "    if sample:\n",
    "        # Sample the specified number of files\n",
    "        random.seed(seed)\n",
    "        files = random.sample(all_files, min(num_files_to_sample, len(all_files)))\n",
    "\n",
    "    # Start combining the sampled files\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as infile:\n",
    "                # outfile.write(start_token + '\\n')\n",
    "                content = infile.read()\n",
    "                content_with_tabs = content.replace('    ', '\\t')\n",
    "                outfile.write(content_with_tabs + '\\n')\n",
    "                # outfile.write(end_token + '\\n\\n')\n",
    "\n",
    "    print(f\"Combined file created as '{output_file}' with contents from {len(files)} files.\")\n",
    "  \n",
    "# Example usage\n",
    "combine_files('dataset/raw/', 'sample_scripts.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3f31df58-4700-4380-88a8-bbb297c931df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "# data_file = 'sample_scripts.txt'\n",
    "# data_file = 'dataset/adamw.txt'\n",
    "with open('sample_scripts.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6c19f8cc-ea26-4b5b-ab62-2d22cdbc15b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  118465\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "6e0b01c0-81dc-4641-83a3-792396633be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "from datautils import MyTrainDataset\n",
      "\n",
      "import torch.multiprocessing as mp\n",
      "from torch.utils.data.distributed import DistributedSampler\n",
      "from torch.nn.parallel import DistributedDataParallel as DDP\n",
      "from torch.distributed import init_process_group, destroy_process_group\n",
      "import os\n",
      "\n",
      "\n",
      "def ddp_setup():\n",
      "\tinit_process_group(backend=\"nccl\")\n",
      "\ttorch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
      "\n",
      "class Trainer:\n",
      "\tdef __init__(\n",
      "\t\tself,\n",
      "\t\tmodel: torch.nn.Module,\n",
      "\t\ttrain_data: DataLoader,\n",
      "\t\toptimizer: torch.optim.Optimizer,\n",
      "\t\tsave_every: int,\n",
      "\t\tsnapshot_path: str,\n",
      "\t) -> None:\n",
      "\t\tself.gpu_id = int(os.environ[\"LOCAL_RANK\"])\n",
      "\t\tself.model = model.to(self.gpu_id)\n",
      "\t\tself.train_data = train_data\n",
      "\t\tself.optimizer = optimizer\n",
      "\t\tself.save_every = save_every\n",
      "\t\tself.epochs_run = 0\n",
      "\t\tself.snapshot_path = snapshot_path\n",
      "\t\tif os.path.exists(snapshot_path):\n",
      "\t\t\tprint(\"Loading snapshot\")\n",
      "\t\t\tself._load_snapshot(snapshot_path)\n",
      "\n",
      "\t\tse\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b72fd8ba-74d7-4040-a498-eacaa9b0e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%\\'()*+,-./0123456789:<=>?@ABCDEFGHIKLMNOPRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz{|}'"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "77b7f505-3c24-41cf-8c16-070ba7d26eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbdd55-c84b-462a-83fa-5b9272b1e97f",
   "metadata": {},
   "source": [
    "# encoding and decoding for chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "444a70cd-2509-42c9-bdac-214c6076d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 74, 77, 76, 79, 81, 2, 81, 76, 79, 64, 69]\n",
      "import torch\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "ch_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_ch = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [ch_to_idx[ch] for ch in s] # encoder: take a string, output a list of mapping idx\n",
    "decode = lambda l: ''.join([idx_to_ch[idx] for idx in l]) # decoder: take a list of index, output a string\n",
    "\n",
    "print(encode(\"import torch\"))\n",
    "print(decode(encode(\"import torch\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "9f4c44dc-93a7-4184-a915-42f1d8400198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([118465]) torch.int64\n",
      "tensor([70, 74, 77, 76, 79, 81,  2, 81, 76, 79, 64, 69,  1, 70, 74, 77, 76, 79,\n",
      "        81,  2, 81, 76, 79, 64, 69, 15, 75, 75, 15, 67, 82, 75, 64, 81, 70, 76,\n",
      "        75, 62, 73,  2, 62, 80,  2, 38,  1, 67, 79, 76, 74,  2, 81, 76, 79, 64,\n",
      "        69, 15, 82, 81, 70, 73, 80, 15, 65, 62, 81, 62,  2, 70, 74, 77, 76, 79,\n",
      "        81,  2, 36, 62, 81, 62, 80, 66, 81, 13,  2, 36, 62, 81, 62, 43, 76, 62,\n",
      "        65, 66, 79,  1, 67, 79, 76, 74,  2, 65])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9427660-f082-46be-97d9-cd471b8aa26c",
   "metadata": {},
   "source": [
    "# train dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "725693c4-15e1-4d3b-a036-cb52b2b42089",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40248d-dabc-4c5e-a2cf-8970ebfe5cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "6cacc69e-43e0-4561-8475-7f7305c1203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([70]) the target: 74\n",
      "when input is tensor([70, 74]) the target: 77\n",
      "when input is tensor([70, 74, 77]) the target: 76\n",
      "when input is tensor([70, 74, 77, 76]) the target: 79\n",
      "when input is tensor([70, 74, 77, 76, 79]) the target: 81\n",
      "when input is tensor([70, 74, 77, 76, 79, 81]) the target: 2\n",
      "when input is tensor([70, 74, 77, 76, 79, 81,  2]) the target: 81\n",
      "when input is tensor([70, 74, 77, 76, 79, 81,  2, 81]) the target: 76\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length+1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f83221-8143-46b7-812d-02eea5067558",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "08cf2024-2f73-4275-83b4-dff95ec0050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "context_length = 256\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "\n",
    "num_heads = 6\n",
    "emb_dim = 64 * num_heads\n",
    "num_layers = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca0020-50db-4d97-88fa-abe53f5fe019",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "9ec801a0-ce4b-4011-877f-0067e665d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([64, 256])\n",
      "tensor([[84, 76, 79,  ..., 75, 76, 74],\n",
      "        [80, 66, 80,  ..., 80, 66, 73],\n",
      "        [69, 15, 75,  ..., 79, 75,  2],\n",
      "        ...,\n",
      "        [10,  1,  1,  ..., 86, 74, 63],\n",
      "        [13,  2, 45,  ..., 80, 86, 74],\n",
      "        [81,  2, 75,  ..., 68,  2, 62]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([64, 256])\n",
      "tensor([[76, 79, 65,  ..., 76, 74, 70],\n",
      "        [66, 80, 60,  ..., 66, 73, 67],\n",
      "        [15, 75, 75,  ..., 75,  2, 80],\n",
      "        ...,\n",
      "        [ 1,  1, 70,  ..., 74, 63, 76],\n",
      "        [ 2, 45, 76,  ..., 86, 74, 63],\n",
      "        [ 2, 75, 15,  ...,  2, 62, 80]], device='cuda:0')\n",
      "----\n",
      "when input is [84] the target: 76\n",
      "when input is [84, 76] the target: 79\n",
      "when input is [84, 76, 79] the target: 65\n",
      "when input is [84, 76, 79, 65] the target: 60\n",
      "when input is [84, 76, 79, 65, 60] the target: 81\n",
      "when input is [84, 76, 79, 65, 60, 81] the target: 66\n",
      "when input is [84, 76, 79, 65, 60, 81, 66] the target: 75\n",
      "when input is [84, 76, 79, 65, 60, 81, 66, 75] the target: 80\n",
      "when input is [80] the target: 66\n",
      "when input is [80, 66] the target: 80\n",
      "when input is [80, 66, 80] the target: 60\n",
      "when input is [80, 66, 80, 60] the target: 84\n",
      "when input is [80, 66, 80, 60, 84] the target: 70\n",
      "when input is [80, 66, 80, 60, 84, 70] the target: 81\n",
      "when input is [80, 66, 80, 60, 84, 70, 81] the target: 69\n",
      "when input is [80, 66, 80, 60, 84, 70, 81, 69] the target: 9\n",
      "when input is [69] the target: 15\n",
      "when input is [69, 15] the target: 75\n",
      "when input is [69, 15, 75] the target: 75\n",
      "when input is [69, 15, 75, 75] the target: 15\n",
      "when input is [69, 15, 75, 75, 15] the target: 77\n",
      "when input is [69, 15, 75, 75, 15, 77] the target: 62\n",
      "when input is [69, 15, 75, 75, 15, 77, 62] the target: 79\n",
      "when input is [69, 15, 75, 75, 15, 77, 62, 79] the target: 62\n",
      "when input is [73] the target: 2\n",
      "when input is [73, 2] the target: 64\n",
      "when input is [73, 2, 64] the target: 69\n",
      "when input is [73, 2, 64, 69] the target: 66\n",
      "when input is [73, 2, 64, 69, 66] the target: 64\n",
      "when input is [73, 2, 64, 69, 66, 64] the target: 72\n",
      "when input is [73, 2, 64, 69, 66, 64, 72] the target: 77\n",
      "when input is [73, 2, 64, 69, 66, 64, 72, 77] the target: 76\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    start_idxs = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    context_idxs = torch.stack([data[start_idx : start_idx+context_length] for start_idx in start_idxs])\n",
    "    target_idxs = torch.stack([data[start_idx+1 : start_idx+context_length+1] for start_idx in start_idxs])\n",
    "    \n",
    "    context_idxs, target_idxs = context_idxs.to(device), target_idxs.to(device)\n",
    "    \n",
    "    return context_idxs, target_idxs\n",
    "\n",
    "context_idxs, target_idxs = get_batch('train')\n",
    "print('inputs:')\n",
    "print(context_idxs.shape)\n",
    "print(context_idxs)\n",
    "print('targets:')\n",
    "print(target_idxs.shape)\n",
    "print(target_idxs)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(4): # batch dimension\n",
    "    for step in range(8): # context length dimension\n",
    "        context = context_idxs[b, :step+1]\n",
    "        target = target_idxs[b,step]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b83ccd-440b-4cbb-a3de-819660488096",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "486d2ed1-d798-4a78-b0b1-7a04d2d5a00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "b3880a56-be00-480d-a300-dfaaae9ece79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 91])\n",
      "tensor(4.9763, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # works as a look up table for the probability of the next char for each current char\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.position_embedding_table = nn.Embedding(context_length, emb_dim)\n",
    "        self.blocks = nn.Sequential(*[Block(emb_dim, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.ln_final = nn.LayerNorm(emb_dim) # the final layer norm before output\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_idxs, target_idxs=None):\n",
    "        B, T = context_idxs.shape # num of batches; num of total steps in context_length\n",
    "\n",
    "        # context_idxs, target_idxs are both (B,T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(context_idxs) # (B, T, emb_dim)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, emb_dim)\n",
    "        x = token_emb + position_emb # (B, T, emb_dim)\n",
    "        x = self.blocks(x) # (B, T, head_size)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size), now the feature_dim is vocab_size again\n",
    "        \n",
    "        if target_idxs is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, D = logits.shape # num of batches; num of total steps in context_length; num of feature dimension\n",
    "            logits = logits.view(B * T, D) # now D == vocab_size == number of classes\n",
    "            target_idxs = target_idxs.view(B * T)\n",
    "            loss = F.cross_entropy(logits, target_idxs)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, context_idxs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # trim input\n",
    "            input_idxs = context_idxs[:, -context_length:]\n",
    "            # forward\n",
    "            logits, loss = self(input_idxs)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, D) tensor for the last step\n",
    "            probs = F.softmax(logits, dim=-1) # predicted_label (B, D)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            # torch.multinomial: Returns a tensor where each row contains num_samples indices \n",
    "            # sampled from the multinomial probability distribution located in the corresponding row of tensor input.\n",
    "            pred_idxs = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            context_idxs = torch.cat((context_idxs, pred_idxs), dim=1) # (B, T+1)\n",
    "        return context_idxs\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''\n",
    "    self-attention with only one head\n",
    "    '''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # attention-score\n",
    "        weight = q @ k.transpose(-2,-1) * D**-0.5 # (B, T, D) @ (B, D, T) ---> (B, T, T)\n",
    "        # D**-0.5: to relief the influence of large value makes the vector after softmax looks like one-hot vector.\n",
    "\n",
    "        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T), the upper-right triangle will be -inf\n",
    "        weight = F.softmax(weight, dim=-1) # (B, T, T)\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        # weighted-aggregation of values based on the attention-score\n",
    "        v = self.value(x) # (B, T, D)\n",
    "        out = weight @ v # (B, T, T) @ (B, T, D) --------> (B, T, D)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    multiple heads fo self-attention in parallel\n",
    "    '''\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    '''\n",
    "    a simple linear layer with activation in decoder, + projection\n",
    "    '''\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim), # the inner dimension is 4 * D, based on the original paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''\n",
    "    a decoder block without cross-attentioin part\n",
    "    '''\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = emb_dim // num_heads\n",
    "        self.attention = MultiHeadAttention(num_heads, self.head_size)\n",
    "        self.ffwd = FeedForward(emb_dim)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "model = GPTLanguageModel().to(device)\n",
    "logits, loss = model(context_idxs, target_idxs)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# decode 5 batches of data, the initial start char is 'i'\n",
    "# [decode(model.generate(context_idxs=torch.full((5, 1), 75, dtype=torch.long).to(device), max_new_tokens=100)[i].tolist()) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "06bd74f5-5346-40af-b033-aa2800a905f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111,\n",
      "         0.0000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000]])\n",
      "tensor([[0.1157, 0.2485, 0.9113],\n",
      "        [0.0553, 0.2639, 0.1699],\n",
      "        [0.3641, 0.9834, 0.5603],\n",
      "        [0.2958, 0.3170, 0.2767],\n",
      "        [0.5954, 0.6977, 0.5031],\n",
      "        [0.6098, 0.1969, 0.1853],\n",
      "        [0.2700, 0.4689, 0.6992],\n",
      "        [0.2840, 0.2913, 0.5957],\n",
      "        [0.5312, 0.7020, 0.2222],\n",
      "        [0.6867, 0.0429, 0.2974]])\n",
      "tensor([[0.1157, 0.2485, 0.9113],\n",
      "        [0.0855, 0.2562, 0.5406],\n",
      "        [0.1784, 0.4986, 0.5472],\n",
      "        [0.2077, 0.4532, 0.4795],\n",
      "        [0.2853, 0.5021, 0.4843],\n",
      "        [0.3393, 0.4512, 0.4344],\n",
      "        [0.3294, 0.4537, 0.4723],\n",
      "        [0.3238, 0.4334, 0.4877],\n",
      "        [0.3468, 0.4633, 0.4582],\n",
      "        [0.3808, 0.4212, 0.4421]])\n"
     ]
    }
   ],
   "source": [
    "weight_test = torch.tril(torch.ones(10,10))\n",
    "print(weight_test)\n",
    "weight_test = weight_test.masked_fill(weight_test == 0, float('-inf'))\n",
    "print(weight_test)\n",
    "weight_test = F.softmax(weight_test, dim=-1)\n",
    "print(weight_test)\n",
    "v_test = torch.rand((10,3))\n",
    "print(v_test)\n",
    "out = weight_test @ v_test\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07188173-cb6b-4565-bca2-ea1bee2d5c3d",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "7fa8e344-7697-4d9e-ad4f-1a0de5a4564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df9566-6a45-47e1-a12f-68a738176440",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "86798e56-c560-4216-9e69-22cb753fe490",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    res = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, y = get_batch(split)\n",
    "            logits, loss = model(X, y)\n",
    "            losses[k] = loss.item()\n",
    "        res[split] = losses.mean()\n",
    "    model.train()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "696868ff-0c80-4654-b54f-143a63f74dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494c1a9619114212bb618cbd86c15e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.9563, val loss 4.9562\n",
      "step 100: train loss 2.6412, val loss 2.7066\n",
      "step 200: train loss 2.4702, val loss 2.5987\n",
      "step 300: train loss 2.1111, val loss 2.3333\n",
      "step 400: train loss 1.6781, val loss 1.9988\n",
      "step 500: train loss 1.3750, val loss 1.8004\n",
      "step 600: train loss 1.1585, val loss 1.6911\n",
      "step 700: train loss 0.9687, val loss 1.5995\n",
      "step 800: train loss 0.8173, val loss 1.5766\n",
      "step 900: train loss 0.6938, val loss 1.5487\n",
      "step 1000: train loss 0.5920, val loss 1.5371\n",
      "step 1100: train loss 0.4870, val loss 1.5436\n",
      "step 1200: train loss 0.4087, val loss 1.5792\n",
      "step 1300: train loss 0.3409, val loss 1.6299\n",
      "step 1400: train loss 0.2868, val loss 1.6485\n",
      "step 1500: train loss 0.2413, val loss 1.7017\n",
      "step 1600: train loss 0.2089, val loss 1.7568\n",
      "step 1700: train loss 0.1775, val loss 1.8216\n",
      "step 1800: train loss 0.1564, val loss 1.8644\n",
      "step 1900: train loss 0.1376, val loss 1.9169\n",
      "step 2000: train loss 0.1256, val loss 1.9623\n",
      "step 2100: train loss 0.1172, val loss 2.0124\n",
      "step 2200: train loss 0.1071, val loss 2.0349\n",
      "step 2300: train loss 0.1003, val loss 2.0884\n",
      "step 2400: train loss 0.0939, val loss 2.1360\n",
      "step 2500: train loss 0.0896, val loss 2.1144\n",
      "step 2600: train loss 0.0860, val loss 2.1925\n",
      "step 2700: train loss 0.0826, val loss 2.2264\n",
      "step 2800: train loss 0.0791, val loss 2.2478\n",
      "step 2900: train loss 0.0770, val loss 2.2550\n",
      "step 3000: train loss 0.0760, val loss 2.2942\n",
      "step 3100: train loss 0.0723, val loss 2.3132\n",
      "step 3200: train loss 0.0724, val loss 2.3114\n",
      "step 3300: train loss 0.0706, val loss 2.3429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[409], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(context_idxs, target_idxs)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in tqdm(range(max_iters)): # increase number of steps for good results... \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    context_idxs, target_idxs = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(context_idxs, target_idxs)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "45e42c0d-e619-4f26-a15f-04ecb6c36e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "from PIL import init_transform=optim.LOptimizer,\n",
      "\t\trank,\n",
      "\tget_local_loss = int(os.environ[0], dowtaset=dtor_wesource)\n",
      "\tmodel = None\n",
      "\t\tsave_model_and_snapshot()\n",
      "\tinterpreter.set_input_names(out_node.denatasetsor)\n",
      "\n",
      "\tgraph.lint(f\"Snapshor TParallel epoce\"\n",
      "\n",
      "\tone = Sequeen(\n",
      "\t\trank = 0.to(\n",
      "\t\targs.save_dir, args.num_asee_args)\n",
      "\telse:\n",
      "\t\tprint(\"So EPOR: RUNX RU is not args...\")\n",
      "\t\treturn n\n",
      "\n",
      "\tfnode.rander()\n",
      "\ttrain_args = model.seed()\n",
      "\tparam_rrefs.append(RResized(args)\n",
      "\n",
      "\t()\n",
      "\tself, g = haph.owner()\n",
      "\t\tself.rewards = []\n",
      "\tfor for ward(path, Dataset) in self.num_thexemore_batch():\n",
      "\t\ttarget = target.to(self.local_rank0(object)\n",
      "\t\tnet_model_path.set_run_transer()\n",
      "\n",
      "\n",
      "import torch\n",
      "from torch.distributed.tensor.detal()\n",
      "\tprint(f\"{data.load().sharded_drmo_(\"emodel\", \"replarelelu\")\n",
      "\n",
      "\n",
      "rank_log(_rank, logger, memoter_method, default=None,\n",
      "\t\t\tcheckpoint_wrapper=int_wr,\n",
      "\t\t)\n",
      "\tfull_osd = None\n",
      "\n",
      "\tif rank == 0:\n",
      "\t\tfull_osd = torch.load(optim_state)\n",
      "\t\t\tif bfor k.dist_cp.open()\n",
      "\n",
      "\t\t\tPut = list(drint.tim_heckpoint_dir)\n",
      "\t\tostart = torch.fnn.Linear(nn.Linear(mlp_dim, rounded_hidden, bias=False)\n",
      "\n",
      "\t\tself.in_proj = nn.Dropout(dropout)\n",
      "\n",
      "\t\tself.encoder = nn.Embedding(ntoken, ninp)\n",
      "\t\tsel = torch.cuda.Lintar(20, -1)\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\treturn self.emodel.recos_([()] # | Trans * self.in + self.inex.sitem()\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\treturn x * y\n",
      "\n",
      "\n",
      "mem _log_utils\n",
      "\n",
      "def remorinit_me__(self, image_dir, input_transform=True,\n",
      "\t\t\tshuffle=FalseConfig,\n",
      "\t\t\tnum_workers=sestructions=\"parser.set5\"\n",
      "\tdef stroy_epoch(epoch, epoch, source, targets=(interput_train_te),\n",
      "\t\ttrainer, pt_tem_athode=1):\n",
      "\t\tself.local_rank = 0 # int in t(e size) + yource, _size)\n",
      "\t\tself.value_cont(enum_counter(Net, st_loader)\n",
      "\t\tself.drop(output, labels).sum()\n",
      "\t\treturn if_self.model(source_grads, train_data.loader_kwargs,\n",
      "\t\t\tsampler.se_rrent_tepoch(epoch=int_device)\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\treturn self.data[x]\n",
      "\n",
      "\tdef convert_meapport(self.encoder: Dict):\n",
      "\t\t\tself.optimizer.zero_grad()\n",
      "\t\tout = torch.randint(os.environ[\"LOCAL_RANK\"])\n",
      "\t\tself.train_dataset = torch.optim_state_dict()\n",
      "\t\tif rank == 0:\n",
      "\t\t\tself.ps_rpc._sepoc(epochUpoch)\n",
      "\t\t\tif self.gpu_id == and epoch\n",
      "\n",
      "\t\t\tself._run_epoch(epoch)\n",
      "\n",
      "\tdef get_model_path = Dist_file():\n",
      "\t\tself._run_eead = torch.load(f, gpu_id)\n",
      "\t\tself.local_rank = 0 and epoch % self.saved == 0:\n",
      "\t\t\t\tprint(f\"Putting repoch {epoch} Resed: {} ...'.format(zer_set_target_model_path)\n",
      "\n",
      "\tmodel.eval()\n",
      "\n",
      "\tosetut_loss = 0\n",
      "\twikih gze(num_classe_y *= v_world_size) \\\n",
      "\t\toptimizer.step()\n",
      "\n",
      "\tmed_log(\"StateDictTyDen updated model\")\n",
      "\n",
      "\n",
      "def run_train(rank, args, model, \"kwargs, \"typer numer n wit_methodel\"\n",
      "\n",
      "hrememe_nume_inputhod(\"SomW rundated model\"\n",
      "\t\tsharded = model.snapshot(os.environ['MASTER_PORT')\n",
      "\tprint(f\"Snapshot not in direce! Runn..\")\n",
      "\t\tparser.set_append(target_to_names, opt_path)\n",
      "\t\treturn\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\tremote_params(fnet) = []\n",
      "\t\treturn worker int(parameters.to('MNIST_hogward, train', train=True,\n",
      "\t\t\t\t\t\t   thelp=\"path torchemodel withe model to be zain zipped,oprocess, saved 32)\n",
      "\ttrain_loader = 3e-3\n",
      "\tprint('Ext--usingze ybouiled.gzit(1, 30, 'type=int, default=00./mnt.300d',\n",
      "\t\t\t\thelp='initial learning state.')\n",
      "\tparser.add_argument('--seed', type=int, default=543,\n",
      "\t\t\t\t\thelp='random seed')\n",
      "parser.add_argument('--word_en',', type=int,\n",
      "\t\t\t\t\t\thelp='ddisabule. None,\n",
      "\t\t\t\t\t\thelp='hintdon to for anding the in labed word.')\n",
      "\tparser.add_argument('--pre_nodes', type=int, default=1, metavar='N',\n",
      "\t\t\t\thelp='how model be (default: 50)')\n",
      "\tparser.add_argument('--log_every', type=int, default=50,\n",
      "\t\t\t\t\t\thelp='renderation/'model word to selve.')\n",
      "\tparser.add_argument('--lr', action='store_true',\n",
      "\t\t\t\t\t\thelp='render')\n",
      "\toptions = torch.join(zippedden, target.txt')\n",
      "\tprint(f'output_node = net')\n",
      "\n",
      "\tparser.add_argument('--log_engterv_thr = returns.temperate_nod(\n",
      "\t\t'get_atexport_onn', target_de=False)\n",
      "\n",
      "\tdef forward(self, kind, target : DataLoader, 'wikihonllding istribute.')\n",
      "\tparser.add_argument('--device, type=int, default='1000',\n",
      "\t\t\t\t\thelp='rang the parser.')\n",
      "\tparser.add_argument('--d_ember', type=int, default=100,\n",
      "\t\t\t\t\t\thelp='the number of report oecal def tor runn.tensor(device):\n",
      "\t\tassert = select_model(self.dropout)\n",
      "\t\tself.rnn = nn.Linear(10, 30)\n",
      "\t\tself.out_proj = nn.Linear(32, hidd_hidden, 1)\n",
      "\n",
      "\tdef forward(self, X):\n",
      "\t\treturn self.drop(output))\n",
      "\n",
      "\n",
      "class DatasetFomFolder(dest, self, args.batch_size):\n",
      "\tdataset, load=1\n",
      "\tbatch_size = int(ntokenizer, (int(os.path, -requiret))\n",
      "\texce = Trainer(model, self.init_h_dden, path)\n",
      "\t\tself.valid = nn.Emodel()\n",
      "\t\tself.set2 = 0\n",
      "\tdef forward(self, input, hidden(self):\n",
      "\t\t\tout = self.input_device(out)\n",
      "\t\treturn self.decoded(out)\n",
      "\n",
      "\n",
      "def grad_and_text(model, ch=1, h, kpint_repoint_find, modelp=_rect5_model,\n",
      "\t\targs=(int,\n",
      "\t\t\tself.decoder_rref, optim='cale//yannewd scription.')\n",
      "\t\t\tset.rpc.readd_cons[0]\n",
      "\t\t\ttrain_trainer_set = torch.utils.init_grad_temport('Eraind{r}'..format(\n",
      "\t\t\tf'consing iterationalGe ard reporte, Ward ping\n",
      "\n",
      "\t\tenv.terpret('call_method')\n",
      "\n",
      "\tprint(f\"Solved! Running reward is now {} and \"\n",
      "\t\t\t\topesoutputs = returns.Dr() - returns.mean())\n",
      "\n",
      "\toptimizer.step()\n",
      "\t\traise\n",
      "\n",
      "\n",
      "def runzip((zipp):\n",
      "\n",
      "\n",
      "def project, Path inion(os.get_t):\n",
      "\treturn arget.epoch(\n",
      "\tdevice_id=\"ct\" init_it_mem__(set_dir, get_dir, mesh_oinde=int_meax_optionsh, rank=None, shardint(os.config.name)\n",
      "\t\tsuper(1, x).to(device)\n",
      "\n",
      "\t\tstyle_transform = transforms.path.GD(affenerame).ext_dirs()\n",
      "\t\tif not args.Local_loss in None:\n",
      "\t\tself.snapshot_path = snapshot_path\n",
      "\t\tloc = f\t\tsdp_loss[0] += correct\n",
      "\n",
      "\tdef _len__(self.data)\n",
      "\t\tself.optimizer.zer.zero_grad()\n",
      "\n",
      "\t\toptimizer = optim.SGD(self.model.parameters(), self.device_type=\" \"_model\"\n",
      "\t\tsnaps_rget = self.train_dataing\":\n",
      "\t\t\tsource = source.to(self.local_rank)\n",
      "\t\t\t\ttargets = targets.to(self.gpu_id)\n",
      "\t\tself._run_batch(source, targets)\n",
      "\n",
      "\tdef _save_snapshot(self, epoch):\n",
      "\t\tmodel = self.model\n",
      "\t\tsnapshot = {\n",
      "\t\t\t\"MODEL_STATE\"://\": self.model.state_dict(), load_snapshot[\"MODEL_STATE\"])\n",
      "\t\tself.epochs_run = snapshot[\"EPOCHS_RUN\"]\n",
      "\t\tprint(f\"Docheckpoint distroy = Training = {}\")\n",
      "\t\tsnapshot = for and(\n",
      "\t\tself.load_epochs_run, self.model.parameters(), lr=0.local_rank)\n",
      "\t\tself.confine = torch.cuda.is_available()\n",
      "\t\tself.destross = Salerction(options.MNISTEL_STM)\n",
      "\n",
      "\t\tself.config.data_loader_worker = FileSystemFer(train_dataset, remoterics_trate_dataset,**ptim)\n",
      "\t\treturn model,\n",
      "\t\t\targs.append(args.lownker, name))\n",
      "\n",
      "\tenv_reward = FSDP.functions(\n",
      "\t\t\tself.optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
      "\n",
      "\tm = torch.rand(20, 5).transp_arameter_server\n",
      "\n",
      "\tworld_size = None(128, 1).to(device)\n",
      "\n",
      "\n",
      "import torch\n",
      "import os\n",
      "import torch.distributed as dist\n",
      "f get_date_model_path = \"shard.exist.pt\"):\n",
      "\ttp_model = ToTenionfig.unifonder(m: kend int, tokenizer.pint_zero_finint__()\n",
      "\t\tif not quiet\n",
      "\n",
      "import torch\n",
      "from Pathredistributed.optim import DistributedOptimizer\n",
      "from torch.distributed.fsdp import (\n",
      "\tFullyShardedDataParallel as FSDP,\n",
      "\tCPUOffload,\n",
      "\tMixedPrecision,\n",
      "\tBackwardPrefetch,\n",
      "\tShardingStrategy,\n",
      "\tFullStateDictConfionalGlaten param_server,\n",
      "\tDefaultSaveDataCon_MoptimixedPle,\n",
      "\tparam_server_f)\n",
      "\tref forward(self, X):\n",
      "\t\tself.word(X)\n",
      "\t\tself.init_proj = self.rection_pad(x_parameter_', type=torch.grad(), for pat)\n",
      "\t\treturn headd_en = model.base(kind_dim, 'path', sample_d', save_every')\n",
      "trans = torch.cuda.men('save_eversnation', args.type=int, type=Total['INFORdelBlock'])\n",
      "\t\tprint(f\"SoT uploading down the model.')\n",
      "\t\t\toutput_node = nn\n",
      "\t\telse:\n",
      "\t\traise RuntimeError('Untions['UA-177Block'].56_)\n",
      "\tname = torch.optionsor.div_transform.Optimizer_checkpoint_model_panding)\n",
      "\t\tself.train_data.sampler(train_datataset, rank=rank, num_replicas=world_size)\n",
      "\n",
      "\ttest_path = torch.set_device(\n",
      "\t\tprint(f\"optimizer {optim_state} dist_autograd..\")\n",
      "\n",
      "\treturn args\n",
      "\n",
      "\twithord = FileSystemRequentir(\n",
      "\t\tsaved_actions_gradient, res_train_sform=True)\n",
      "\n",
      "\tpt_setup()\n",
      "\n",
      "\n",
      "import torch\n",
      "import os\n",
      "import torch.multiprocessing as mp\n",
      "import torch.nn.functional as F\n",
      "from torch.distributed.fsdp import ShardingStrategym\n",
      "from torch.distributed.fsdp import ShardingedDistributed.fsdp\n",
      "from torch.distributions.destributed.algorad import DistributedSampler\n",
      "from torch.nn.parallel import DistributedDataParallel as DDP\n",
      "from torch.utils.data.distributed import DistributedSampler\n",
      "from torch.nn.parallel import init_device_mesh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class ToyModel(nn.Module):\n",
      "\n",
      "\tdef __init__(self):\n",
      "\t\tsuper(TransoyModel, self).__init__()\n",
      "\t\tself.relu = self.in1(affine1(x))\n",
      "\t\tself.affine1 = nn.Linear(128, 10)\n",
      "\t\tself.action_head = nn.Linear(128, 2)\n",
      "\n",
      "\t\tself.res1 = []\n",
      "\t\tself.reward = []\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\tx = F.relu(x)\n",
      "\t\taction_prob = F.softmax(self.action_head(x), dim=-1)\n",
      "\n",
      "\t\tself.value_helu(self.net18_x)\n",
      "\n",
      "\t\th_relu2_2 = features_features_y(self.relu2(self.relu(self.net1())))\n",
      "\n",
      "\t\tout = self.in3(4)\n",
      "\t\tx = F.relu(self.fc1(x)\n",
      "\t\tout = self.deconv2(y)\n",
      "\t\treturn x\n",
      "\n",
      "\n",
      "class ConvLayer(torch.nn.Module):\n",
      "\n",
      "\tdef forward(self, x):\n",
      "\t\treturn self.net1(x)\n",
      "\n",
      "def model(optim dive, device, self):\n",
      "\t\treturn suped.code(self.fc1(x))\n",
      "\n",
      "\n",
      "def forward(x, word):\n",
      "\t\tx = x.symbolic_trace(output)\n",
      "\t\treturn x.device.rewards[:-1], append_findx = m.graph.placemen()\n",
      "\t\tif node.op == 'output':\n",
      "\t\t\tinput_node = new_graph.call_function(x_wrappe)\n",
      "\t\toutput_proxy = n.target(*prace_atorgs, **proxy_onnx_eent\n",
      "\t\telify_args.export_onnx(expret__)\n",
      "\n",
      "\t\t\toutput = self.convert_to_features(self.ref_model(ps)\n",
      "\t\treturn x + ['__()\n",
      "\t\treturn out_numpy.finc_idx}]\n",
      "\n",
      "\n",
      "tracer = torch.fx.proxy.GraphAppendingTracer(graph)\n",
      "\treturn aced.grap\n",
      "\n",
      "\toutput_node = False\n",
      "\n",
      "class Snapsh(output_node.m') as optim.node('get_to_num': False,\n",
      "\t'projection': True}\n",
      "\tif not us_cuda:\n",
      "\t\tusource_num_ter(hars_kwargs)\n",
      "\n",
      "\n",
      "def load_dataset(model, 'model_very': Transforme,\n",
      "\t\t\t\t\thelp='ename = ['_empy', transfor_value_true}', defat=False,\n",
      "\t\t\t\t\t  help='pin_memory': True,\n",
      "\t\t\t\t\thelp='senaving the of train eval_dirsion': False,\n",
      "\t\t\t\t\t\thelp='num_workers_nork = Nunt pretrions_tatics_grategy.updateFalse\n",
      "\n",
      "\tclass Trainer(model, saanne):\n",
      "\tif not None:\n",
      "\t\tif not None:\n",
      "\t\t\tsupec.Opend(args.gs, output_name)\n",
      "\t\tif os.max_epoch(epoch)\n",
      "\t\t\tif epoch.set_run == 0\n",
      "\t\t\tself.optimizer.zero_grad(0set_to_none=False)\n",
      "\t\t\tself.data =  self.optimizer.zero_grad()\n",
      "\t\t\tx = self.optimizer.step()\n",
      "\t\tx = utils.nopy(x)\n",
      "\t\ty = self.requ(x)\n",
      "\t\treturn = self.fc2(x\n"
     ]
    }
   ],
   "source": [
    "test_string = \"\"\"import torch\"\"\"\n",
    "print(decode(model.generate(context_idxs = torch.tensor(encode(test_string)).view(1, len(test_string)).to(device), max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3d71c-672a-4aab-8e83-2de2ab1d9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(encode('import')).view(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14367d3-b4d3-4910-8185-a3ebae8fd4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426e38c-cf24-40de-b6ed-c69f24ac60a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
