{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c537fb-fae5-4279-a852-54077a741a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fb714b-e291-4639-b0ed-7dd074b8ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def is_comment(line):\n",
    "    # Define a function to check if a line is a comment\n",
    "    line = line.strip()\n",
    "    if line.startswith('#') or line.startswith(\"'''\") or line.startswith('\"\"\"'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_non_comments(source_directory, target_directory):\n",
    "    # Process all .py files in the specified directory and subdirectories\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                target_file_path = os.path.join(target_directory, file.replace('.py', '.txt'))\n",
    "                with open(file_path, 'r') as source_file, open(target_file_path, 'w') as target_file:\n",
    "                    non_comments = []\n",
    "                    comment_block = False\n",
    "                    \n",
    "                    for line in source_file:\n",
    "                        # Check for the start or end of a comment block\n",
    "                        if \"''\" in line or '\"\"\"' in line:\n",
    "                            comment_block = not comment_block\n",
    "                            continue\n",
    "                        # If it's not a comment or part of a comment block, save it\n",
    "                        if not is_comment(line) and not comment_block:\n",
    "                            non_comments.append(line)\n",
    "                        # Write non-comment lines to a target .txt file\n",
    "                    target_file.writelines(non_comments)\n",
    "\n",
    "# # Define the path to the local repository (change this to the actual path of your local repo)\n",
    "# # source_directory = '/path/to/your/local/pytorch/repo'\n",
    "# source_directory = '.'\n",
    "# # target_directory = '/path/to/your/output/directory'\n",
    "# target_directory = './dataset/'\n",
    "\n",
    "\n",
    "# # Create the target directory if it doesn't exist\n",
    "# os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "# # Call the function to start extracting non-comment lines\n",
    "# extract_non_comments(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954bd61d-0495-4afd-b754-36969e202bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file created as 'sample_scripts.txt' with contents from 100 files.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def combine_files(directory, output_file, num_files_to_sample, start_token=\"<START>\", end_token=\"<END>\"):\n",
    "    \"\"\"\n",
    "    Combine content from a specified number of text files in a directory into one file, \n",
    "    with start and end tokens between contents from each file.\n",
    "\n",
    "    :param directory: Path to the directory containing text files.\n",
    "    :param output_file: Name of the output file to create.\n",
    "    :param num_files_to_sample: Number of files to sample and combine.\n",
    "    :param start_token: The start token to be added before each file's content.\n",
    "    :param end_token: The end token to be added after each file's content.\n",
    "    \"\"\"\n",
    "    # List all text files in the directory\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    \n",
    "    # Sample the specified number of files\n",
    "    sampled_files = random.sample(all_files, min(num_files_to_sample, len(all_files)))\n",
    "\n",
    "    # Start combining the sampled files\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for filename in sampled_files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as infile:\n",
    "                outfile.write(start_token + '\\n')\n",
    "                outfile.write(infile.read() + '\\n')\n",
    "                outfile.write(end_token + '\\n\\n')\n",
    "\n",
    "    print(f\"Combined file created as '{output_file}' with contents from {len(sampled_files)} files.\")\n",
    "\n",
    "# Example usage\n",
    "combine_files('dataset/', 'sample_scripts.txt', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f31df58-4700-4380-88a8-bbb297c931df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('sample_scripts.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c19f8cc-ea26-4b5b-ab62-2d22cdbc15b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1486366\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0b01c0-81dc-4641-83a3-792396633be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>\\n\\nimport builtins\\nimport itertools\\nimport logging\\nimport math\\nimport operator\\nimport sys\\nfrom functools import lru_cache, update_wrapper\\nfrom typing import Optional, Type, TYPE_CHECKING, Union\\n\\nfrom torch import (  # noqa: F401\\n    sym_float,\\n    sym_ite,\\n    sym_max,\\n    sym_min,\\n    sym_not,\\n    sym_sqrt,\\n    SymBool,\\n    SymFloat,\\n    SymInt,\\n)\\n\\nfrom torch.fx.experimental._sym_dispatch_mode import (\\n    handle_sym_dispatch,\\n    sym_function_mode,\\n)\\n\\nif TYPE_CHECKING:\\n    from torch.fx.experimental.symbolic_shapes import ShapeEnv\\n\\nlog = logging.getLogger(__name__)\\n\\n\\n__all__ = [\"SymNode\", \"method_to_operator\", \"magic_methods\", \"sym_sqrt\"]\\n\\n\\nSymTypes = (SymInt, SymFloat, SymBool)\\n\\n\\ndef _to_symtype(t):\\n    if t is bool:\\n        return SymBool\\n    if t is int:\\n        return SymInt\\n    if t is float:\\n        return SymFloat\\n    return t\\n\\n\\nclass SymNode:\\n\\n    def __init__(\\n        self,\\n        expr,\\n        shape_env,\\n        pytype,\\n        hint: Optional[Union[int, float, bool]]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47b6ec-8e33-4100-9312-292c0d00b265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
